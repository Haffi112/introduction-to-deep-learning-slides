<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Convolutional Neural Networks - Introduction to Deep Learning</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    <link rel="stylesheet" href="css/cnn-custom.css">
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Convolutional Neural Networks</h1>
                <p>Chapter 7.1: From Fully Connected Layers to Convolutions</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>
            
            <!-- Section 1: Introduction to CNNs (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.1", "url": "https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html"}, {"text": "LeCun et al. (1995) - Convolutional Networks for Images", "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-95b.pdf"}]'>
                    <h2 class="truncate-title">The Problem with Images as Vectors</h2>
                    <div class="two-column" style="font-size: 0.75em;">
                        <div class="column">
                            <h4>Traditional Approach</h4>
                            <ul class="fragment">
                                <li>Flatten 2D images to 1D vectors</li>
                                <li>Feed through fully connected MLP</li>
                                <li>Ignore spatial structure</li>
                                <li>Treat pixels independently</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>The Issues</h4>
                            <ul class="fragment">
                                <li>Loss of spatial relationships</li>
                                <li>Order of features doesn't matter</li>
                                <li>Nearby pixels are related</li>
                                <li>Massive parameter explosion</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>Key Insight:</strong> Images have rich spatial structure that we're throwing away!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Parameter Explosion Problem</h2>
                    <div class="parameter-demo">
                        <h4>One Megapixel Image Example</h4>
                        <div class="calculation-steps" style="font-size: 0.5em;">
                            <div class="fragment">
                                <p><strong>Input:</strong> 1000 × 1000 pixels = 1,000,000 dimensions</p>
                            </div>
                            <div class="fragment">
                                <p><strong>Hidden Layer:</strong> 1000 units (aggressive reduction!)</p>
                            </div>
                            <div class="fragment">
                                <p><strong>Parameters:</strong> 10<sup>6</sup> × 10<sup>3</sup> = <span class="highlight">10<sup>9</sup> parameters</span></p>
                            </div>
                            <div class="fragment emphasis-box mt-lg">
                                <p>That's <strong>1 billion parameters</strong> for just ONE layer!</p>
                                <p>And 1000 hidden units grossly underestimates what we need for good representations</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Krizhevsky et al. (2012) - ImageNet Classification with Deep CNNs", "url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"}]'>
                    <h2 class="truncate-title">The CNN Revolution</h2>
                    <div class="timeline-container">
                        <div class="timeline">
                            <div class="timeline-item fragment">
                                <div class="year">1995</div>
                                <div class="event">LeNet</div>
                                <div class="description">First successful CNN for digits</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2012</div>
                                <div class="event">AlexNet</div>
                                <div class="description">ImageNet breakthrough</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">Today</div>
                                <div class="event">Ubiquitous</div>
                                <div class="description">Vision, audio, text, graphs</div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <p>CNNs leverage <span class="tooltip">spatial structure<span class="tooltiptext">The fact that nearby pixels are more related than distant ones</span></span> to achieve:</p>
                        <ul>
                            <li>Sample efficiency</li>
                            <li>Computational efficiency</li>
                            <li>Better generalization</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is treating images as flattened vectors problematic for neural networks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the images too small",
                                "correct": false,
                                "explanation": "Flattening doesnt change the total number of pixels, just their arrangement."
                            },
                            {
                                "text": "It destroys spatial relationships between pixels",
                                "correct": true,
                                "explanation": "Correct! Flattening loses the 2D structure where nearby pixels are related, treating all pixels as independent features."
                            },
                            {
                                "text": "It makes training faster",
                                "correct": false,
                                "explanation": "Actually, flattening leads to more parameters and slower training."
                            },
                            {
                                "text": "It only works for black and white images",
                                "correct": false,
                                "explanation": "Flattening works for any image type but is inefficient regardless of color channels."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 2: The Invariance Problem (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">The Invariance Challenge</h2>
                    <div class="waldo-demo">
                        <h4>Where's Waldo?</h4>
                        <img src="images/waldo-football.jpg" alt="Where's Waldo scene" style="max-width: 60%; margin: 20px auto; display: block;">
                        <div class="fragment" style="font-size: 0.6em;">
                            <p>The challenge: <strong>What Waldo looks like</strong> doesn't depend on <strong>where Waldo is located</strong></p>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                            <p>We need a system that can detect patterns regardless of their position in the image</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Translation Invariance</h2>
                    <div id="translation-demo" class="visualization-container">
                        <div class="demo-controls">
                            <button id="move-object" class="ui-button">Move Object</button>
                            <button id="reset-position" class="ui-button secondary">Reset</button>
                        </div>
                        <svg id="translation-svg" width="600" height="300"></svg>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.6em;">
                        <h4>Key Properties:</h4>
                        <ul>
                            <li>Same feature detector works everywhere</li>
                            <li>Shared weights across positions</li>
                            <li>Dramatically fewer parameters</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Design Principles for Vision</h2>
                    <div class="principles-grid" style="font-size: 0.6em;">
                        <div class="principle fragment">
                            <div class="icon">🔄</div>
                            <h4>Translation Invariance</h4>
                            <p>Network responds similarly to the same patch regardless of location</p>
                        </div>
                        <div class="principle fragment">
                            <div class="icon">📍</div>
                            <h4>Locality</h4>
                            <p>Early layers focus on local regions without regard for distant content</p>
                        </div>
                        <div class="principle fragment">
                            <div class="icon">🔭</div>
                            <h4>Hierarchical Features</h4>
                            <p>Deeper layers capture progressively longer-range features</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does translation invariance mean in the context of CNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The network can translate text from one language to another",
                                "correct": false,
                                "explanation": "Translation here refers to spatial movement, not language translation."
                            },
                            {
                                "text": "The network produces the same output for an object regardless of its position",
                                "correct": true,
                                "explanation": "Correct! Translation invariance means detecting the same pattern wherever it appears in the image."
                            },
                            {
                                "text": "The network only works with images that have been moved",
                                "correct": false,
                                "explanation": "Translation invariance is about handling objects at any position, not requiring movement."
                            },
                            {
                                "text": "The network changes its weights based on object position",
                                "correct": false,
                                "explanation": "Actually, CNNs use the same weights regardless of position - thats the key to translation invariance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 3: From Fully Connected to Convolutions (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Constraining the MLP</h2>
                    <p>Starting with a fully connected layer for 2D images:</p>
                    <div class="math-progression" style="font-size: 0.6em;">
                        <div class="fragment">
                            <h4>Full Connectivity</h4>
                            <p>$$[\mathbf{H}]_{i,j} = [\mathbf{U}]_{i,j} + \sum_k \sum_l [\mathsf{W}]_{i,j,k,l} [\mathbf{X}]_{k,l}$$</p>
                            <p class="annotation">Each output depends on EVERY input pixel</p>
                        </div>
                        <div class="fragment">
                            <h4>Re-indexing</h4>
                            <p>$$[\mathbf{H}]_{i,j} = [\mathbf{U}]_{i,j} + \sum_a \sum_b [\mathsf{V}]_{i,j,a,b} [\mathbf{X}]_{i+a,j+b}$$</p>
                            <p class="annotation">Express as offsets from output position</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                        <p>Parameters needed: 1000² × 1000² = <strong>10<sup>12</sup></strong> (one trillion!)</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Applying Translation Invariance</h2>
                    <div class="constraint-demo">
                        <h4>The Key Constraint</h4>
                        <div class="fragment" style="font-size: 0.6em;">
                            <p>If the detector shouldn't depend on position:</p>
                            <p>$$[\mathsf{V}]_{i,j,a,b} = [\mathbf{V}]_{a,b}$$</p>
                            <p>Weights only depend on the <strong>offset</strong>, not the <strong>position</strong>!</p>
                        </div>
                        <h4>Simplified Form</h4>
                        <div class="fragment mt-md" style="font-size: 0.6em;">
                            <p>$$[\mathbf{H}]_{i,j} = u + \sum_a \sum_b [\mathbf{V}]_{a,b} [\mathbf{X}]_{i+a,j+b}$$</p>
                            <p class="annotation">This is a <span class="highlight">convolution</span>!</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.4em;">
                            <p>Parameters reduced to: ~2000 × 2000 = <strong>4 × 10<sup>6</sup></strong></p>
                            <p class="explanation">Why ~2000×2000? The offset indices (a,b) can range from -1000 to +1000 in each direction (actually 2001×2001 including zero), giving us a single shared filter that we reuse at every pixel position.</p>
                            <p>That's 250,000× fewer parameters!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Convolution Demo</h2>
                    <div id="convolution-demo" class="conv-visualization">
                        <div class="demo-controls">
                            <label>Kernel Size: 
                                <select id="kernel-size">
                                    <option value="3">3×3</option>
                                    <option value="5">5×5</option>
                                </select>
                            </label>
                            <button id="step-conv" class="ui-button">Step</button>
                            <button id="animate-conv" class="ui-button">Animate</button>
                            <button id="reset-conv" class="ui-button secondary">Reset</button>
                        </div>
                        <div id="conv-container"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "How does applying translation invariance reduce the number of parameters?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It removes all parameters",
                                "correct": false,
                                "explanation": "We still need parameters for the convolution kernel, just far fewer than before."
                            },
                            {
                                "text": "It makes weights independent of absolute position",
                                "correct": true,
                                "explanation": "Correct! By sharing the same weights across all positions, we only need to learn one set of weights for the entire image."
                            },
                            {
                                "text": "It makes the network smaller",
                                "correct": false,
                                "explanation": "The network output size remains the same; we just use fewer parameters to compute it."
                            },
                            {
                                "text": "It only processes part of the image",
                                "correct": false,
                                "explanation": "Translation invariance still processes the entire image, just with shared weights."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 4: Locality Principle (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">The Locality Principle</h2>
                    <div class="locality-illustration" style="font-size: 0.6em;">
                        <h4>Why Look Far Away?</h4>
                        <div class="fragment">
                            <p>To understand what's happening at position (i, j), do we need to look at pixels 500 positions away?</p>
                        </div>
                        <div class="fragment mt-md">
                            <div class="emphasis-box">
                                <p><strong>Locality assumption:</strong> Set $[\mathbf{V}]_{a,b} = 0$ for $|a| > \Delta$ or $|b| > \Delta$</p>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <p>Now our convolution becomes:</p>
                            <p>$$[\mathbf{H}]_{i,j} = u + \sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} [\mathbf{V}]_{a,b} [\mathbf{X}]_{i+a,j+b}$$</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Receptive Field Visualization</h2>
                    <div id="receptive-field-locality-demo" class="visualization-container">
                        <div class="demo-controls">
                            <label>Kernel Size (2Δ+1): 
                                <input type="range" id="delta-slider" min="1" max="7" value="3" step="2">
                                <span id="delta-value">3×3</span>
                            </label>
                        </div>
                        <svg id="locality-field-svg" width="600" height="400"></svg>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.6em;">
                        <p>Parameters with locality: $(2\Delta + 1)^2$</p>
                        <p>For Δ = 5: only <strong>121 parameters</strong> per kernel!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Reduction Summary</h2>
                    <div class="parameter-comparison" style="font-size: 0.6em;">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Approach</th>
                                    <th>Parameters</th>
                                    <th>Reduction</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td>Fully Connected</td>
                                    <td>10<sup>12</sup></td>
                                    <td>—</td>
                                </tr>
                                <tr class="fragment">
                                    <td>+ Translation Invariance</td>
                                    <td>4 × 10<sup>6</sup></td>
                                    <td>250,000×</td>
                                </tr>
                                <tr class="fragment">
                                    <td>+ Locality (Δ=5)</td>
                                    <td>121</td>
                                    <td>8.3 billion×</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>From <strong>trillions</strong> to <strong>hundreds</strong> of parameters!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main benefit of the locality principle in CNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the network faster to train by using only nearby pixels",
                                "correct": true,
                                "explanation": "Correct! By only connecting to nearby pixels, we drastically reduce parameters and computation."
                            },
                            {
                                "text": "It allows the network to process larger images",
                                "correct": false,
                                "explanation": "Locality doesnt change the size of images we can process, just how efficiently we process them."
                            },
                            {
                                "text": "It makes the network ignore important features",
                                "correct": false,
                                "explanation": "Locality captures local features, which can be combined in deeper layers for global understanding."
                            },
                            {
                                "text": "It only works for small images",
                                "correct": false,
                                "explanation": "Locality works for any image size - its about local connections, not image dimensions."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 5: Convolution Mathematics (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">The Mathematics of Convolution</h2>
                    <div class="math-definition" style="font-size: 0.6em;">
                        <h4>Continuous Convolution</h4>
                        <div class="fragment">
                            <p>For functions $f, g: \mathbb{R}^d \to \mathbb{R}$:</p>
                            <p>$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x} - \mathbf{z}) d\mathbf{z}$$</p>
                            <p class="annotation">Measure overlap between f and "flipped" g shifted by x</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Discrete 2D Convolution</h4>
                            <p>$$(f * g)(i, j) = \sum_a \sum_b f(a, b) g(i-a, j-b)$$</p>
                            <p class="annotation">Sum over all valid positions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Convolution vs Cross-Correlation</h2>
                    <div class="comparison-viz">
                        <div class="two-column">
                            <div class="column">
                                <h4>Convolution</h4>
                                <p>$$\sum_a \sum_b f(a, b) g(i-a, j-b)$$</p>
                                <p class="fragment">Kernel is "flipped"</p>
                            </div>
                            <div class="column">
                                <h4>Cross-Correlation</h4>
                                <p>$$\sum_a \sum_b f(a, b) g(i+a, j+b)$$</p>
                                <p class="fragment">Kernel is not flipped</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                            <p>In deep learning, we often use cross-correlation but call it "convolution"</p>
                            <p>Since we learn the kernels, the distinction doesn't matter!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Convolution Calculator</h2>
                    <div id="conv-calculator" class="calculator-demo">
                        <div class="input-grid">
                            <h4>Input</h4>
                            <div id="input-matrix"></div>
                        </div>
                        <div class="kernel-grid">
                            <h4>Kernel</h4>
                            <div id="kernel-matrix"></div>
                        </div>
                        <div class="output-grid">
                            <h4>Output</h4>
                            <div id="output-matrix"></div>
                        </div>
                        <div class="demo-controls mt-md">
                            <button id="randomize-input" class="ui-button">Random Input</button>
                            <select id="preset-kernel">
                                <option value="identity">Identity</option>
                                <option value="edge">Edge Detection</option>
                                <option value="blur">Blur</option>
                                <option value="sharpen">Sharpen</option>
                            </select>
                            <button id="compute-conv" class="ui-button">Compute</button>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why doesnt it matter that deep learning uses cross-correlation instead of true convolution?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The operations are mathematically identical",
                                "correct": false,
                                "explanation": "Convolution and cross-correlation are different operations - one flips the kernel, the other doesnt."
                            },
                            {
                                "text": "Because we learn the kernel weights from data",
                                "correct": true,
                                "explanation": "Correct! Since the network learns the optimal kernel values, it can learn a flipped version if needed."
                            },
                            {
                                "text": "Cross-correlation is faster to compute",
                                "correct": false,
                                "explanation": "Both operations have similar computational complexity."
                            },
                            {
                                "text": "Images are symmetric anyway",
                                "correct": false,
                                "explanation": "Images are generally not symmetric, and this isnt why the distinction doesnt matter."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 6: Channels and Feature Maps (Vertical) -->
            <section>
                <section data-sources='[{"text": "Understanding CNN Channels", "url": "https://d2l.ai/chapter_convolutional-neural-networks/channels.html"}]'>
                    <h2 class="truncate-title">Beyond Grayscale: Channels</h2>
                    <div class="channels-intro">
                        <h4>Real Images Have Multiple Channels</h4>
                        <div class="fragment">
                            <div class="channel-grid">
                                <div class="channel red">
                                    <div class="channel-viz">R</div>
                                    <p>Red Channel</p>
                                </div>
                                <div class="channel green">
                                    <div class="channel-viz">G</div>
                                    <p>Green Channel</p>
                                </div>
                                <div class="channel blue">
                                    <div class="channel-viz">B</div>
                                    <p>Blue Channel</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <p>Images are 3rd-order tensors: Height × Width × Channels</p>
                            <p>Example: 1024 × 1024 × 3 for RGB image</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Multi-Channel Convolution</h2>
                    <div class="multichannel-math" style="font-size: 0.6em;">
                        <h4>Extending to Multiple Channels</h4>
                        <div class="fragment">
                            <p>Input: $[\mathsf{X}]_{i,j,c}$ (position i,j, channel c)</p>
                            <p>Kernel: $[\mathsf{V}]_{a,b,c,d}$ (offset a,b, input channel c, output channel d)</p>
                        </div>
                        <div class="fragment mt-md">
                            <p>Multi-channel convolution:</p>
                            <p>$$[\mathsf{H}]_{i,j,d} = \sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a,b,c,d} [\mathsf{X}]_{i+a,j+b,c}$$</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Each output channel combines information from ALL input channels</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Feature Maps Visualization</h2>
                    <div id="feature-maps-demo" class="feature-viz">
                        <div class="demo-controls">
                            <label>Number of Filters: 
                                <input type="range" id="num-filters" min="1" max="6" value="3">
                                <span id="filter-count">3</span>
                            </label>
                            <button id="apply-filters" class="ui-button">Apply Filters</button>
                        </div>
                        <div id="feature-maps-container"></div>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.6em;">
                        <p>Each filter learns to detect different features:</p>
                        <ul>
                            <li>Edges at different orientations</li>
                            <li>Textures and patterns</li>
                            <li>Color combinations</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In a CNN with 3 input channels and 64 output channels, how many separate 2D kernels are there?",
                        "type": "single",
                        "options": [
                            {
                                "text": "3 kernels",
                                "correct": false,
                                "explanation": "This would only allow one kernel per input channel, not creating multiple output channels."
                            },
                            {
                                "text": "64 kernels",
                                "correct": false,
                                "explanation": "Each output channel needs to process ALL input channels, not just one kernel total."
                            },
                            {
                                "text": "192 kernels (3 × 64)",
                                "correct": true,
                                "explanation": "Correct! Each of the 64 output channels needs 3 kernels (one per input channel), giving 192 total 2D kernels."
                            },
                            {
                                "text": "67 kernels (3 + 64)",
                                "correct": false,
                                "explanation": "Channels are not additive - we need kernels for each input-output channel pair."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 7: Hierarchical Features (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Building Hierarchical Representations</h2>
                    <div class="hierarchy-viz">
                        <h4>From Pixels to Concepts</h4>
                        <div class="layer-progression">
                            <div class="layer fragment">
                                <div class="layer-box layer1">
                                    <h5>Layer 1</h5>
                                    <p>Edges & Gradients</p>
                                </div>
                                <div class="arrow">→</div>
                            </div>
                            <div class="layer fragment">
                                <div class="layer-box layer2">
                                    <h5>Layer 2</h5>
                                    <p>Textures & Patterns</p>
                                </div>
                                <div class="arrow">→</div>
                            </div>
                            <div class="layer fragment">
                                <div class="layer-box layer3">
                                    <h5>Layer 3</h5>
                                    <p>Parts & Objects</p>
                                </div>
                                <div class="arrow">→</div>
                            </div>
                            <div class="layer fragment">
                                <div class="layer-box layer4">
                                    <h5>Layer 4+</h5>
                                    <p>Complex Concepts</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Each layer builds on the previous, creating increasingly abstract representations</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Receptive Field Growth</h2>
                    <div id="receptive-growth-demo" class="growth-viz">
                        <div class="demo-controls">
                            <label>Network Depth: 
                                <input type="range" id="depth-slider" min="1" max="5" value="1">
                                <span id="depth-value">1 layer</span>
                            </label>
                        </div>
                        <svg id="receptive-growth-svg" width="750" height="400"></svg>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.6em;">
                        <p>With 3×3 kernels: Layer 1 sees 3×3 pixels, Layer 2 sees 5×5 pixels, Layer 3 sees 7×7 pixels, Layer n sees (2n+1)×(2n+1) pixels</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Power of Composition</h2>
                    <div class="composition-demo" style="font-size: 0.6em;">
                        <h4>Combining Simple Operations</h4>
                        <div class="fragment">
                            <p>Linear operations + Nonlinearities = Complex Functions</p>
                        </div>
                        <div class="fragment mt-md">
                            <div class="formula-progression">
                                <p>Layer 1: $\mathbf{H}^{(1)} = \sigma(\mathbf{X} * \mathbf{W}^{(1)})$</p>
                                <p>Layer 2: $\mathbf{H}^{(2)} = \sigma(\mathbf{H}^{(1)} * \mathbf{W}^{(2)})$</p>
                                <p>Layer 3: $\mathbf{H}^{(3)} = \sigma(\mathbf{H}^{(2)} * \mathbf{W}^{(3)})$</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Deep networks learn both features AND how to combine them</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do deeper CNN layers typically capture more complex features?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They have more parameters",
                                "correct": false,
                                "explanation": "Parameter count alone doesnt determine feature complexity - its about composition."
                            },
                            {
                                "text": "They have larger receptive fields and build on earlier features",
                                "correct": true,
                                "explanation": "Correct! Deeper layers see larger image regions and combine simpler features from earlier layers into complex patterns."
                            },
                            {
                                "text": "They use different activation functions",
                                "correct": false,
                                "explanation": "CNNs typically use the same activation function throughout the network."
                            },
                            {
                                "text": "They process images at higher resolution",
                                "correct": false,
                                "explanation": "Actually, deeper layers often have lower spatial resolution due to pooling or striding."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 8: Summary and Key Concepts (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">CNN Architecture Summary</h2>
                    <div class="architecture-summary" style="font-size: 0.6em;">
                        <h4>Key Components of CNNs</h4>
                        <div class="component-grid">
                            <div class="component fragment">
                                <h5>🔲 Convolution Layers</h5>
                                <p>Local connections with shared weights</p>
                            </div>
                            <div class="component fragment">
                                <h5>🎯 Feature Maps</h5>
                                <p>Multiple channels detecting different patterns</p>
                            </div>
                            <div class="component fragment">
                                <h5>📏 Kernel/Filter</h5>
                                <p>Small weight matrices (3×3, 5×5, etc.)</p>
                            </div>
                            <div class="component fragment">
                                <h5>🔄 Weight Sharing</h5>
                                <p>Same weights used across all positions</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why CNNs Work: The Complete Picture</h2>
                    <div class="why-cnns-work" style="font-size: 0.6em;">
                        <ol>
                            <li class="fragment">
                                <strong>Exploit Structure:</strong> Images have spatial structure we can leverage
                            </li>
                            <li class="fragment">
                                <strong>Parameter Efficiency:</strong> From trillions to hundreds of parameters
                            </li>
                            <li class="fragment">
                                <strong>Translation Invariance:</strong> Detect patterns anywhere in the image
                            </li>
                            <li class="fragment">
                                <strong>Hierarchical Learning:</strong> Build complex features from simple ones
                            </li>
                            <li class="fragment">
                                <strong>Computational Efficiency:</strong> Convolutions are highly parallelizable
                            </li>
                        </ol>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>CNNs are the <strong>right</strong> architecture for spatial data!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Beyond Images: CNN Applications</h2>
                    <div class="applications-grid" style="font-size: 0.6em;">
                        <div class="application fragment">
                            <div class="icon">🎵</div>
                            <h5>Audio Processing</h5>
                            <p>1D convolutions on waveforms</p>
                        </div>
                        <div class="application fragment">
                            <div class="icon">📝</div>
                            <h5>Text Analysis</h5>
                            <p>1D convolutions on sequences</p>
                        </div>
                        <div class="application fragment">
                            <div class="icon">📊</div>
                            <h5>Time Series</h5>
                            <p>Temporal pattern detection</p>
                        </div>
                        <div class="application fragment">
                            <div class="icon">🔗</div>
                            <h5>Graph Networks</h5>
                            <p>Convolutions on graph structures</p>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <p>The principles of locality and weight sharing apply broadly!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which combination of principles allows CNNs to be so effective for image processing?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Translation invariance through weight sharing",
                                "correct": true,
                                "explanation": "Weight sharing ensures the same pattern detector works everywhere in the image."
                            },
                            {
                                "text": "Locality through limited receptive fields",
                                "correct": true,
                                "explanation": "Local connections dramatically reduce parameters while capturing spatial relationships."
                            },
                            {
                                "text": "Hierarchical feature learning through depth",
                                "correct": true,
                                "explanation": "Multiple layers build increasingly complex representations from simple features."
                            },
                            {
                                "text": "Using only grayscale images",
                                "correct": false,
                                "explanation": "CNNs work with any number of input channels, not just grayscale."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9: The Cross-Correlation Operation (Deep Dive) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.2", "url": "https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html"}]'>
                    <h2 class="truncate-title">The Cross-Correlation Operation: Mathematical Foundation</h2>
                    <div class="math-definition" style="font-size: 0.6em;">
                        <h4>Formal Definition</h4>
                        <div class="fragment">
                            <p>For input tensor $\mathbf{X}$ and kernel tensor $\mathbf{K}$:</p>
                            <p>$$[\mathbf{Y}]_{i,j} = \sum_{a=0}^{k_h-1} \sum_{b=0}^{k_w-1} [\mathbf{K}]_{a,b} \cdot [\mathbf{X}]_{i+a,j+b}$$</p>
                            <p class="annotation">Element-wise multiplication and summation over kernel window</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Step-by-Step Example</h4>
                            <p>Input: $\begin{bmatrix} 0 & 1 & 2 \\ 3 & 4 & 5 \\ 6 & 7 & 8 \end{bmatrix}$ 
                               Kernel: $\begin{bmatrix} 0 & 1 \\ 2 & 3 \end{bmatrix}$</p>
                            <p class="fragment">Position (0,0): $0 \times 0 + 1 \times 1 + 3 \times 2 + 4 \times 3 = 19$</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                        <p><span class="tooltip">Cross-correlation<span class="tooltiptext">An operation that slides a kernel across an input, computing weighted sums at each position</span></span> is the fundamental operation in CNNs</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Cross-Correlation Calculator</h2>
                    <div id="cross-correlation-demo" class="interactive-demo">
                        <div class="demo-controls" style="margin-bottom: 20px;">
                            <button id="step-correlation" class="ui-button">Step Forward</button>
                            <button id="animate-correlation" class="ui-button">Animate</button>
                            <button id="reset-correlation" class="ui-button secondary">Reset</button>
                        </div>
                        <div class="correlation-grids" style="display: flex; justify-content: space-around; align-items: center;">
                            <div class="input-display">
                                <h4>Input (3×3)</h4>
                                <div id="correlation-input"></div>
                            </div>
                            <div class="kernel-display">
                                <h4>Kernel (2×2)</h4>
                                <div id="correlation-kernel"></div>
                            </div>
                            <div class="output-display">
                                <h4>Output (2×2)</h4>
                                <div id="correlation-output"></div>
                            </div>
                        </div>
                        <div id="calculation-display" class="mt-md" style="text-align: center; font-family: monospace;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Output Size Calculation</h2>
                    <div class="output-size-demo" style="font-size: 0.6em;">
                        <h4>Computing Output Dimensions</h4>
                        <div class="fragment">
                            <div class="formula-box">
                                <p>$$\text{Output Height} = n_h - k_h + 1$$</p>
                                <p>$$\text{Output Width} = n_w - k_w + 1$$</p>
                            </div>
                            <p class="annotation">Where $n_h, n_w$ are input dimensions and $k_h, k_w$ are kernel dimensions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Given a 5×5 input and a 3×3 kernel, what is the output size without padding?",
                        "type": "single",
                        "options": [
                            {
                                "text": "5×5",
                                "correct": false,
                                "explanation": "The output is smaller than the input when using convolution without padding."
                            },
                            {
                                "text": "3×3",
                                "correct": true,
                                "explanation": "Correct! Using the formula (5-3+1)×(5-3+1) = 3×3."
                            },
                            {
                                "text": "2×2",
                                "correct": false,
                                "explanation": "This would be the result with a 4×4 kernel, not 3×3."
                            },
                            {
                                "text": "7×7",
                                "correct": false,
                                "explanation": "The output cannot be larger than the input without padding."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 10: Implementation of Convolution Layers -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Convolution Implementation", "url": "https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#convolutional-layers"}]'>
                    <h2 class="truncate-title">Implementing Cross-Correlation</h2>
                    <div class="code-implementation" style="font-size: 0.6em;">
                        <h4>Core corr2d Function</h4>
                        <pre><code class="language-python">def corr2d(X, K):
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y</code></pre>
                        <div class="fragment mt-md">
                            <p><strong>Key Steps:</strong></p>
                            <ul>
                                <li>Calculate output dimensions based on input and kernel sizes</li>
                                <li>Slide kernel across input with nested loops</li>
                                <li>Compute element-wise multiplication and sum at each position</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Conv2D Layer Class</h2>
                    <div class="conv2d-implementation">
                        <h4>Building a Convolutional Layer</h4>
                        <pre><code class="language-python">class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias</code></pre>
                        <div class="fragment mt-md">
                            <h4>Components:</h4>
                            <ul>
                                <li><span class="tooltip">weight<span class="tooltiptext">The learnable kernel parameters, randomly initialized</span></span>: Kernel parameters (randomly initialized)</li>
                                <li><span class="tooltip">bias<span class="tooltiptext">A single scalar value added to all outputs</span></span>: Scalar bias term</li>
                                <li>forward: Applies cross-correlation and adds bias</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Implementations</h2>
                    <div class="framework-comparison" style="font-size: 0.6em;">
                        <h4>Cross-Framework Consistency</h4>
                        <div class="two-column">
                            <div class="column">
                                <h5>PyTorch</h5>
                                <pre><code class="language-python">conv2d = nn.Conv2d(
    in_channels=1,
    out_channels=1,
    kernel_size=3
)</code></pre>
                            </div>
                            <div class="column">
                                <h5>TensorFlow</h5>
                                <pre><code class="language-python">conv2d = tf.keras.layers.Conv2D(
    filters=1,
    kernel_size=3
)</code></pre>
                            </div>
                        </div>
                        <div class="fragment mt-md emphasis-box">
                            <p>All major frameworks provide optimized convolution implementations with similar APIs</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are the two learnable parameters in a basic Conv2D layer?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Kernel weights",
                                "correct": true,
                                "explanation": "The kernel (or filter) weights are the primary learnable parameters."
                            },
                            {
                                "text": "Bias term",
                                "correct": true,
                                "explanation": "A scalar bias is added to each output position."
                            },
                            {
                                "text": "Stride value",
                                "correct": false,
                                "explanation": "Stride is a hyperparameter, not a learnable parameter."
                            },
                            {
                                "text": "Input dimensions",
                                "correct": false,
                                "explanation": "Input dimensions are determined by the data, not learned."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 11: Edge Detection Example -->
            <section>
                <section data-sources='[{"text": "Edge Detection with Convolutions", "url": "https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#object-edge-detection-in-images"}]'>
                    <h2 class="truncate-title">Edge Detection with Convolutions</h2>
                    <div class="edge-detection-intro">
                        <h4>Creating a Test Image</h4>
                        <div class="fragment">
                            <pre><code class="language-python">X = torch.ones((6, 8))
X[:, 2:6] = 0  # Middle columns are black
# Result:
# [[1, 1, 0, 0, 0, 0, 1, 1],
#  [1, 1, 0, 0, 0, 0, 1, 1],
#  [1, 1, 0, 0, 0, 0, 1, 1],
#  [1, 1, 0, 0, 0, 0, 1, 1],
#  [1, 1, 0, 0, 0, 0, 1, 1],
#  [1, 1, 0, 0, 0, 0, 1, 1]]</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>A simple pattern with vertical edges between white (1) and black (0) regions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Edge Detection Kernel</h2>
                    <div class="edge-kernel-explanation" style="font-size: 0.6em;">
                        <h4>Finite Difference Operator</h4>
                        <div class="fragment">
                            <pre><code class="language-python">K = torch.tensor([[1.0, -1.0]])</code></pre>
                            <p>This kernel computes: $x_{i,j} - x_{i,j+1}$</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Mathematical Interpretation</h4>
                            <p>Discrete approximation of the horizontal derivative:</p>
                            <p>$$-\partial_j f(i,j) = \lim_{\epsilon \to 0} \frac{f(i,j) - f(i,j+\epsilon)}{\epsilon}$$</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Output Interpretation</h4>
                            <ul>
                                <li><strong>+1</strong>: Edge from white to black</li>
                                <li><strong>-1</strong>: Edge from black to white</li>
                                <li><strong>0</strong>: No edge (uniform region)</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Edge Detection Demo</h2>
                    <div id="edge-detection-interactive" class="interactive-demo">
                        <div class="demo-controls" style="margin-bottom: 20px;">
                            <button id="apply-horizontal" class="ui-button">Horizontal Edges</button>
                            <button id="apply-vertical" class="ui-button">Vertical Edges</button>
                            <button id="apply-custom" class="ui-button">Custom Pattern</button>
                            <button id="reset-edge" class="ui-button secondary">Reset</button>
                        </div>
                        <div class="edge-visualization" style="display: flex; justify-content: space-around;">
                            <div>
                                <h4>Input Image</h4>
                                <canvas id="edge-input-canvas" width="200" height="150"></canvas>
                            </div>
                            <div>
                                <h4>Kernel</h4>
                                <div id="edge-kernel-display"></div>
                            </div>
                            <div>
                                <h4>Output</h4>
                                <canvas id="edge-output-canvas" width="200" height="150"></canvas>
                            </div>
                        </div>
                        <div id="edge-explanation" class="mt-md" style="text-align: center;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does the kernel [[1, -1]] detect in an image?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Horizontal edges",
                                "correct": false,
                                "explanation": "This kernel detects changes along the horizontal direction, which are vertical edges."
                            },
                            {
                                "text": "Vertical edges",
                                "correct": true,
                                "explanation": "Correct! The kernel computes horizontal differences, detecting vertical transitions."
                            },
                            {
                                "text": "Diagonal edges",
                                "correct": false,
                                "explanation": "Diagonal edge detection requires kernels with diagonal structure."
                            },
                            {
                                "text": "Corners",
                                "correct": false,
                                "explanation": "Corner detection requires more complex operations than a simple difference kernel."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 12: Learning Kernels from Data -->
            <section>
                <section data-sources='[{"text": "Learning Kernels", "url": "https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#learning-a-kernel"}]'>
                    <h2 class="truncate-title">Learning Kernels from Data</h2>
                    <div class="learning-setup" style="font-size: 0.6em;">
                        <h4>The Learning Problem</h4>
                        <div class="fragment">
                            <p>Given input-output pairs, can we learn the kernel that produces the mapping?</p>
                            <pre><code class="language-python"># Setup: We have input X and desired output Y
# Goal: Learn kernel K such that X * K ≈ Y

conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)
X = X.reshape((1, 1, 6, 8))  # Batch, Channel, Height, Width
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # Learning rate</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>We'll use gradient descent to minimize the squared error loss</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Loop Implementation</h2>
                    <div class="training-code" style="font-size: 0.6em;">
                        <h4>Gradient Descent for Kernel Learning</h4>
                        <pre><code class="language-python">for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # Update the kernel
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {l.sum():.3f}')

# Output:
# epoch 2, loss 16.481
# epoch 4, loss 5.069
# epoch 6, loss 1.794
# epoch 8, loss 0.688
# epoch 10, loss 0.274</code></pre>
                        <div class="fragment mt-md">
                            <p>Final learned kernel: <code>[[1.0398, -0.9328]]</code></p>
                            <p>Target kernel was: <code>[[1.0, -1.0]]</code></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Kernel Learning Demo</h2>
                    <div id="kernel-learning-demo" class="interactive-demo">
                        <div class="demo-controls" style="margin-bottom: 20px;">
                            <label>Learning Rate: 
                                <input type="range" id="learning-rate" min="0.01" max="0.1" step="0.01" value="0.03">
                                <span id="lr-value">0.03</span>
                            </label>
                            <button id="start-learning" class="ui-button">Start Training</button>
                            <button id="reset-learning" class="ui-button secondary">Reset</button>
                        </div>
                        <div class="learning-visualization" style="display: flex; justify-content: space-around;">
                            <div>
                                <h4>Target Kernel</h4>
                                <div id="target-kernel"></div>
                            </div>
                            <div>
                                <h4>Current Kernel</h4>
                                <div id="current-kernel"></div>
                            </div>
                            <div>
                                <h4>Loss Curve</h4>
                                <svg id="loss-curve" width="250" height="150"></svg>
                            </div>
                        </div>
                        <div id="epoch-display" class="mt-md" style="text-align: center;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why can CNNs learn effective kernels from data?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Kernels are predefined for specific tasks",
                                "correct": false,
                                "explanation": "CNNs learn kernels from data, not use predefined ones."
                            },
                            {
                                "text": "Gradient descent can optimize kernel weights to minimize loss",
                                "correct": true,
                                "explanation": "Correct! Backpropagation computes gradients for kernel weights, allowing optimization."
                            },
                            {
                                "text": "Random kernels always work well",
                                "correct": false,
                                "explanation": "Random initialization is just the starting point; learning is essential."
                            },
                            {
                                "text": "Kernels dont need to be learned",
                                "correct": false,
                                "explanation": "Learning appropriate kernels is crucial for CNN performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            

            <!-- Section 14: Feature Maps and Receptive Fields -->
            <section>
                <section data-sources='[{"text": "Feature Maps and Receptive Fields", "url": "https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#feature-map-and-receptive-field"}, {"text": "Field (1987) - Visual Cortex", "url": "https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-4-12-2379"}]'>
                    <h2 class="truncate-title">Feature Maps: Learned Spatial Representations</h2>
                    <div class="feature-maps-explanation" style="font-size: 0.6em;">
                        <h4>What is a Feature Map?</h4>
                        <div class="fragment">
                            <p>The output of a convolutional layer, representing learned features in spatial dimensions</p>
                            <div class="feature-map-grid" style="display: flex; justify-content: space-around; margin-top: 20px;">
                                <div class="feature-box">
                                    <div class="icon">🔍</div>
                                    <h5>Detection</h5>
                                    <p>Each position indicates presence of learned pattern</p>
                                </div>
                                <div class="feature-box">
                                    <div class="icon">📍</div>
                                    <h5>Localization</h5>
                                    <p>Spatial structure is preserved</p>
                                </div>
                                <div class="feature-box">
                                    <div class="icon">📊</div>
                                    <h5>Activation</h5>
                                    <p>Intensity shows feature strength</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><span class="tooltip">Feature maps<span class="tooltiptext">2D arrays where each element represents the activation of a learned feature detector at a specific spatial location</span></span> encode what patterns are present and where</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Receptive Field Growth</h2>
                    <div class="receptive-field-explanation" style="font-size: 0.6em;">
                        <h4>How Deep Networks See More</h4>
                        <div class="fragment">
                            <p>The <span class="tooltip">receptive field<span class="tooltiptext">The region of the input that influences a particular output neuron</span></span> of a neuron is all input locations that affect its output</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Receptive Field Expansion</h4>
                            <p>With 3×3 kernels stacked:</p>
                            <ul>
                                <li>Layer 1: 3×3 receptive field</li>
                                <li>Layer 2: 5×5 receptive field</li>
                                <li>Layer 3: 7×7 receptive field</li>
                                <li>Layer n: (2n+1)×(2n+1) receptive field</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>Deeper layers integrate information from progressively larger regions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Biological Connection: Visual Cortex</h2>
                    <div class="biological-connection">
                        <h4>CNNs Mirror Biology</h4>
                        <div class="fragment">
                            <img src="images/field-visual-cortex.jpg" 
                                 alt="Field (1987) Visual Cortex Filters" 
                                 style="max-width: 70%; margin: 20px auto; display: block; object-fit: cover; object-position: top; height: 300px;">
                            <p class="caption">Field (1987): Natural image statistics and cortical cell responses</p>
                        </div>
                        <div class="fragment mt-md" style="font-size: 0.6em;">
                            <p>Biological neurons in visual cortex act like convolutional filters:</p>
                            <ul>
                                <li>Edge detectors at various orientations</li>
                                <li>Local receptive fields</li>
                                <li>Hierarchical processing</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Receptive Field Calculator</h2>
                    <div id="receptive-field-demo" class="interactive-demo">
                        <div class="demo-controls" style="margin-bottom: 20px;">
                            <label>Number of Layers: 
                                <input type="range" id="num-layers" min="1" max="6" value="3">
                                <span id="layers-value">3</span>
                            </label>
                            <label>Kernel Size: 
                                <select id="kernel-size-rf">
                                    <option value="3">3×3</option>
                                    <option value="5">5×5</option>
                                    <option value="7">7×7</option>
                                </select>
                            </label>
                        </div>
                        <div class="rf-visualization">
                            <svg id="calculator-field-svg" width="600" height="400"></svg>
                        </div>
                        <div id="rf-calculation" class="mt-md" style="text-align: center; font-family: monospace;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What determines the receptive field size in a deep CNN?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Number of layers",
                                "correct": true,
                                "explanation": "More layers mean each neuron can see a larger input region."
                            },
                            {
                                "text": "Kernel size at each layer",
                                "correct": true,
                                "explanation": "Larger kernels increase receptive field growth rate."
                            },
                            {
                                "text": "Stride and pooling operations",
                                "correct": true,
                                "explanation": "Striding and pooling also expand the receptive field."
                            },
                            {
                                "text": "Number of parameters",
                                "correct": false,
                                "explanation": "Parameter count doesnt directly determine receptive field size."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 16: Padding Fundamentals (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.3", "url": "https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html"}]'>
                    <h2 class="truncate-title">The Vanishing Pixels Problem</h2>
                    <div class="padding-intro" style="font-size: 0.6em;">
                        <h4>What Happens to Our Image Edges?</h4>
                        <div class="fragment">
                            <p>Starting with a 240×240 pixel image:</p>
                            <ul>
                                <li>After 10 layers of 5×5 convolutions → 200×200 pixels</li>
                                <li>We lose <strong>30%</strong> of our image!</li>
                                <li>Boundary information is completely lost</li>
                            </ul>
                        </div>
                        <div class="fragment mt-md">
                            <img src="images/conv-reuse.svg" alt="Pixel utilization" style="max-width: 70%; margin: 20px auto; display: block;">
                            <p class="caption">Pixel utilization for different kernel sizes - corners are barely used!</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Problem:</strong> Important information at image boundaries gets lost through successive convolutions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Zero Padding: The Solution</h2>
                    <div class="padding-explanation">
                        <h4>Adding Extra Pixels Around the Border</h4>
                        <div class="fragment" style="font-size: 0.6em;">
                            <img src="images/conv-pad.svg" alt="Padding visualization" style="max-width: 60%; margin: 20px auto; display: block;">
                            <p class="caption">2D cross-correlation with padding - zeros added around the input</p>
                        </div>
                        <div class="fragment mt-md" style="font-size: 0.6em;">
                            <p><span class="tooltip">Zero padding<span class="tooltiptext">Adding rows and columns of zeros around the input tensor to preserve spatial dimensions</span></span> increases effective input size:</p>
                            <ul>
                                <li>Original: 3×3 input</li>
                                <li>With padding=1: 5×5 effective input</li>
                                <li>Output: 4×4 (larger than without padding!)</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                            <p>Padding preserves spatial information and controls output dimensions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Padding Mathematics</h2>
                    <div class="padding-math">
                        <div class="fragment" style="font-size: 0.6em;">
                            <div class="formula-box">
                                <p>$$\text{Output Height} = n_h - k_h + p_h + 1$$</p>
                                <p>$$\text{Output Width} = n_w - k_w + p_w + 1$$</p>
                            </div>
                        </div>
                        <div class="fragment mt-md" style="font-size: 0.6em;">
                            <div id="padding-calculator" class="interactive-demo">
                                <div class="demo-controls" style="display: flex; gap: 20px; justify-content: center; align-items: center; margin-bottom: 20px;">
                                    <label>Input Size: 
                                        <input type="range" id="input-size-pad" min="3" max="10" value="5">
                                        <span id="input-size-pad-value">5×5</span>
                                    </label>
                                    <label>Kernel Size: 
                                        <input type="range" id="kernel-size-pad" min="1" max="5" value="3" step="2">
                                        <span id="kernel-size-pad-value">3×3</span>
                                    </label>
                                    <label>Padding: 
                                        <input type="range" id="padding-amount" min="0" max="3" value="1">
                                        <span id="padding-value">1</span>
                                    </label>
                                </div>
                                <div id="padding-viz-container"></div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Same vs Valid Padding</h2>
                    <div class="padding-types" style="font-size: 0.6em;">
                        <h4>Two Common Padding Strategies</h4>
                        <div class="two-column">
                            <div class="column fragment">
                                <h5><span class="tooltip">Valid Padding<span class="tooltiptext">No padding added; output is smaller than input</span></span></h5>
                                <ul>
                                    <li>No padding (p = 0)</li>
                                    <li>Output smaller than input</li>
                                    <li>No "artificial" pixels</li>
                                    <li>Used when size reduction is desired</li>
                                </ul>
                            </div>
                            <div class="column fragment">
                                <h5><span class="tooltip">Same Padding<span class="tooltiptext">Padding chosen to keep output size equal to input size</span></span></h5>
                                <ul>
                                    <li>p = k - 1 (for odd kernels)</li>
                                    <li>Output same size as input</li>
                                    <li>Preserves spatial dimensions</li>
                                    <li>Most common in practice</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Why Odd Kernel Sizes?</h4>
                            <p>With odd kernels (3×3, 5×5, 7×7):</p>
                            <ul>
                                <li>Symmetric padding on all sides</li>
                                <li>Natural center pixel</li>
                                <li>Output Y[i,j] computed from window centered at X[i,j]</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main purpose of padding in CNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make computations faster",
                                "correct": false,
                                "explanation": "Padding actually adds more computations, not reduces them."
                            },
                            {
                                "text": "To preserve spatial dimensions and edge information",
                                "correct": true,
                                "explanation": "Correct! Padding prevents the output from shrinking and preserves information at image boundaries."
                            },
                            {
                                "text": "To add noise to the input",
                                "correct": false,
                                "explanation": "Padding adds zeros (or other values), not noise. Its purpose is dimensional control."
                            },
                            {
                                "text": "To reduce the number of parameters",
                                "correct": false,
                                "explanation": "Padding doesnt affect the number of parameters in the kernel."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 17: Stride Mechanics (Vertical) -->
            <section>
                <section data-sources='[{"text": "Stride in Convolutions", "url": "https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#stride"}]'>
                    <h2 class="truncate-title">Understanding Stride</h2>
                    <div class="stride-intro" style="font-size: 0.6em;">
                        <h4>Moving the Kernel Window</h4>
                        <div class="fragment">
                            <p><span class="tooltip">Stride<span class="tooltiptext">The number of pixels the convolution kernel moves in each step</span></span> controls how far we slide the kernel at each step</p>
                            <ul>
                                <li><strong>Stride = 1:</strong> Move one pixel at a time (default)</li>
                                <li><strong>Stride = 2:</strong> Skip every other position</li>
                                <li><strong>Stride > 2:</strong> Skip multiple positions</li>
                            </ul>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Why Use Stride > 1?</h4>
                            <ul>
                                <li>🚀 Computational efficiency</li>
                                <li>📉 Downsampling feature maps</li>
                                <li>🔍 Capturing larger-scale features</li>
                                <li>💾 Reducing memory requirements</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Stride is a powerful tool for controlling computational cost and feature map resolution</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Stride in Action</h2>
                    <div class="stride-visualization" style="font-size: 0.6em;">
                        <div class="fragment mt-md">
                            <div id="stride-demo" class="interactive-demo">
                                <div class="demo-controls" style="margin-bottom: 20px;">
                                    <label>Horizontal Stride: 
                                        <input type="range" id="h-stride" min="1" max="3" value="1">
                                        <span id="h-stride-value">1</span>
                                    </label>
                                    <label>Vertical Stride: 
                                        <input type="range" id="v-stride" min="1" max="3" value="1">
                                        <span id="v-stride-value">1</span>
                                    </label>
                                    <button id="animate-stride" class="ui-button">Animate</button>
                                    <button id="reset-stride" class="ui-button secondary">Reset</button>
                                </div>
                                <div id="stride-viz-container"></div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Output Size with Stride</h2>
                    <div class="stride-math" style="font-size: 0.6em;">
                        <h4>The Stride Formula</h4>
                        <div class="fragment">
                            <div class="formula-box">
                                <p>$$\text{Output Height} = \left\lfloor \frac{n_h - k_h + p_h + s_h}{s_h} \right\rfloor$$</p>
                                <p>$$\text{Output Width} = \left\lfloor \frac{n_w - k_w + p_w + s_w}{s_w} \right\rfloor$$</p>
                            </div>
                            <p class="annotation">Floor function ⌊·⌋ rounds down to nearest integer</p>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Special Case: Halving Dimensions</h4>
                            <p>With padding = k-1 and stride = 2:</p>
                            <ul>
                                <li>Input: n × n</li>
                                <li>Output: ≈ n/2 × n/2</li>
                                <li>Common for downsampling in CNNs</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Benefits of Stride</h2>
                    <div class="stride-benefits" style="font-size: 0.6em;">
                        <h4>Efficiency Gains from Striding</h4>
                        <div class="fragment">
                            <div class="comparison-table" style="margin: 20px auto;">
                                <table>
                                    <thead>
                                        <tr>
                                            <th>Stride</th>
                                            <th>Output Size</th>
                                            <th>Computations</th>
                                            <th>Reduction</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>1×1</td>
                                            <td>n×n</td>
                                            <td>n²</td>
                                            <td>1×</td>
                                        </tr>
                                        <tr>
                                            <td>2×2</td>
                                            <td>(n/2)×(n/2)</td>
                                            <td>n²/4</td>
                                            <td>4×</td>
                                        </tr>
                                        <tr>
                                            <td>3×3</td>
                                            <td>(n/3)×(n/3)</td>
                                            <td>n²/9</td>
                                            <td>9×</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Statistical Benefits</h4>
                            <ul>
                                <li>Reduced <span class="tooltip">overfitting<span class="tooltiptext">When a model memorizes training data instead of learning general patterns</span></span> through downsampling</li>
                                <li>Forces network to learn robust features</li>
                                <li>Natural hierarchical representation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "If you have a 10×10 input, 3×3 kernel, no padding, and stride=2, what is the output size?",
                        "type": "single",
                        "options": [
                            {
                                "text": "8×8",
                                "correct": false,
                                "explanation": "This would be the result with stride=1, not stride=2."
                            },
                            {
                                "text": "5×5",
                                "correct": false,
                                "explanation": "Close, but check the calculation: ⌊(10-3+0+2)/2⌋ = ⌊9/2⌋ = 4."
                            },
                            {
                                "text": "4×4",
                                "correct": true,
                                "explanation": "Correct! Using the formula: ⌊(10-3+2)/2⌋ = ⌊9/2⌋ = 4 for each dimension."
                            },
                            {
                                "text": "3×3",
                                "correct": false,
                                "explanation": "This would require a larger stride or larger kernel."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 18: Padding and Stride Combined (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Combining Padding and Stride</h2>
                    <div class="combined-intro" style="font-size: 0.6em;">
                        <h4>Working Together for Flexible Control</h4>
                        <div class="fragment">
                            <p>Padding and stride give us complete control over output dimensions:</p>
                            <ul>
                                <li><strong>Padding:</strong> Controls size preservation</li>
                                <li><strong>Stride:</strong> Controls downsampling rate</li>
                                <li><strong>Together:</strong> Any desired output size</li>
                            </ul>
                        </div>
                        <div class="fragment mt-md">
                            <h4>The Complete Formula</h4>
                            <div class="formula-box">
                                <p>$$\text{Output} = \left\lfloor \frac{\text{Input} - \text{Kernel} + \text{Padding} + \text{Stride}}{\text{Stride}} \right\rfloor$$</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Most networks use specific combinations for predictable behavior</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Common Configurations</h2>
                    <div class="common-configs" style="font-size: 0.6em;">
                        <h4>Frequently Used Padding-Stride Combinations</h4>
                        <div class="config-grid" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; max-width: 800px; margin: 20px auto;">
                            <div class="config-box fragment" style="padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #10099F;">
                                <h5>Preserve Size</h5>
                                <code>kernel=3, padding=1, stride=1</code>
                                <p>Output = Input size</p>
                                <p class="small">Used in ResNet, VGG</p>
                            </div>
                            <div class="config-box fragment" style="padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                <h5>Halve Size</h5>
                                <code>kernel=3, padding=1, stride=2</code>
                                <p>Output ≈ Input/2</p>
                                <p class="small">Common downsampling</p>
                            </div>
                            <div class="config-box fragment" style="padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #FFA05F;">
                                <h5>Aggressive Reduction</h5>
                                <code>kernel=7, padding=3, stride=2</code>
                                <p>Large receptive field + downsampling</p>
                                <p class="small">Used in stem layers</p>
                            </div>
                            <div class="config-box fragment" style="padding: 15px; background: #f9f9f9; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <h5>No Padding</h5>
                                <code>kernel=5, padding=0, stride=1</code>
                                <p>Output = Input - 4</p>
                                <p class="small">Valid convolution</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Asymmetric Padding and Stride</h2>
                    <div class="asymmetric-config" style="font-size: 0.75em;">
                        <h4>Different Values for Height and Width</h4>
                        <div class="fragment">
                            <p>Sometimes we need different padding/stride for each dimension:</p>
                            <pre><code class="language-python"># PyTorch example
conv2d = nn.Conv2d(
    in_channels=1, 
    out_channels=1,
    kernel_size=(5, 3),  # 5×3 kernel
    padding=(2, 1),       # 2 vertical, 1 horizontal
    stride=(3, 4)         # 3 vertical, 4 horizontal
)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Use Cases</h4>
                            <ul>
                                <li><strong>Audio:</strong> Different time vs frequency resolution</li>
                                <li><strong>Video:</strong> Different spatial vs temporal stride</li>
                                <li><strong>Text:</strong> Different word vs character dimensions</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which configuration would maintain the spatial dimensions of a 224×224 input?",
                        "type": "single",
                        "options": [
                            {
                                "text": "kernel=5, padding=0, stride=1",
                                "correct": false,
                                "explanation": "This would reduce the output to 220×220 (224-5+1)."
                            },
                            {
                                "text": "kernel=3, padding=1, stride=1",
                                "correct": true,
                                "explanation": "Correct! With same padding (p=k-1) and stride=1, output equals input size."
                            },
                            {
                                "text": "kernel=3, padding=1, stride=2",
                                "correct": false,
                                "explanation": "This would halve the dimensions to approximately 112×112."
                            },
                            {
                                "text": "kernel=7, padding=0, stride=1",
                                "correct": false,
                                "explanation": "This would reduce the output to 218×218 (224-7+1)."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 19: Implementation Details (Vertical) -->
            <section>
                <section data-sources='[{"text": "PyTorch Conv2d Documentation", "url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"}]'>
                    <h2 class="truncate-title">PyTorch Implementation</h2>
                    <div class="pytorch-implementation" style="font-size: 0.6em;">
                        <h4>Using Conv2d with Padding and Stride</h4>
                        <div class="fragment">
                            <pre><code class="language-python">import torch
import torch.nn as nn

# Basic convolution with padding and stride
conv = nn.Conv2d(
    in_channels=3,      # RGB input
    out_channels=64,    # 64 feature maps
    kernel_size=3,      # 3×3 kernel
    padding=1,          # Same padding
    stride=1            # No downsampling
)

# Using LazyConv2d (infers in_channels)
lazy_conv = nn.LazyConv2d(
    out_channels=32,
    kernel_size=5,
    padding=2,          # (5-1)/2 = 2 for same padding
    stride=2            # Downsample by 2
)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Padding Modes</h4>
                            <pre><code class="language-python"># Different padding modes
conv_zeros = nn.Conv2d(3, 64, 3, padding=1, padding_mode='zeros')     # Default
conv_reflect = nn.Conv2d(3, 64, 3, padding=1, padding_mode='reflect') # Mirror
conv_replicate = nn.Conv2d(3, 64, 3, padding=1, padding_mode='replicate') # Edge</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Handling Even Kernels</h2>
                    <div class="even-kernels" style="font-size: 0.6em;">
                        <h4>The Asymmetry Problem</h4>
                        <div class="fragment">
                            <p>With even kernel sizes (2×2, 4×4), padding becomes asymmetric:</p>
                            <div class="formula-box">
                                <p>For kernel size = 4, same padding needs p = 3</p>
                                <p>But 3 can't be split evenly!</p>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Solutions</h4>
                            <pre><code class="language-python"># PyTorch handles this automatically
conv_even = nn.Conv2d(3, 64, kernel_size=4, padding=2)
# Top: 2 pixels, Bottom: 1 pixel (or vice versa)

# Explicit asymmetric padding
conv_explicit = nn.Conv2d(3, 64, 4, padding=(2, 1))  # (top/bottom, left/right)</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>This is why odd kernels (3×3, 5×5, 7×7) are strongly preferred!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computing Output Dimensions in Code</h2>
                    <div class="output-computation">
                        <h4>Helper Functions for Size Calculation</h4>
                        <pre><code class="language-python">def conv_output_size(input_size, kernel_size, padding=0, stride=1):
    """Calculate output size for a convolution layer."""
    return (input_size - kernel_size + 2 * padding) // stride + 1

def same_padding(kernel_size):
    """Calculate padding needed for 'same' convolution."""
    return (kernel_size - 1) // 2

# Example usage
input_h, input_w = 224, 224
kernel_size = 5
padding = same_padding(kernel_size)  # padding = 2
stride = 2

output_h = conv_output_size(input_h, kernel_size, padding, stride)
output_w = conv_output_size(input_w, kernel_size, padding, stride)
print(f"Output size: {output_h}×{output_w}")  # Output: 112×112</code></pre>
                        <div class="fragment mt-md">
                            <p>Use these helpers to plan network architectures!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why are odd kernel sizes preferred in CNNs?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "They allow symmetric padding on all sides",
                                "correct": true,
                                "explanation": "Odd kernels can be padded equally on both sides."
                            },
                            {
                                "text": "They have a natural center pixel",
                                "correct": true,
                                "explanation": "Odd kernels have a single center point for alignment."
                            },
                            {
                                "text": "They are computationally more efficient",
                                "correct": false,
                                "explanation": "Computational efficiency is similar for odd and even kernels."
                            },
                            {
                                "text": "Output location directly corresponds to input center",
                                "correct": true,
                                "explanation": "With odd kernels, Y[i,j] is computed from window centered at X[i,j]."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 20: Advanced Topics (Vertical) -->
            <section>
                <section data-sources='[{"text": "Alsallakh et al. (2020) - Mind the PAD", "url": "https://arxiv.org/abs/2010.02178"}]'>
                    <h2 class="truncate-title">Alternative Padding Types</h2>
                    <div class="padding-types">
                        <h4>Beyond Zero Padding</h4>
                        <div id="padding-types-demo" class="interactive-demo">
                            <div class="demo-controls" style="margin-bottom: 20px;">
                                <select id="padding-type">
                                    <option value="zero">Zero Padding</option>
                                    <option value="reflect">Reflect Padding</option>
                                    <option value="replicate">Replicate Padding</option>
                                    <option value="circular">Circular Padding</option>
                                </select>
                                <button id="apply-padding-type" class="ui-button">Apply</button>
                            </div>
                            <div id="padding-type-viz" style="display: flex; justify-content: space-around; margin-top: 20px;">
                                <div>
                                    <h5>Original</h5>
                                    <canvas id="original-canvas" width="150" height="150"></canvas>
                                </div>
                                <div>
                                    <h5>With Padding</h5>
                                    <canvas id="padded-canvas" width="200" height="200"></canvas>
                                </div>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <ul>
                                <li><strong>Zero:</strong> Default, adds black borders</li>
                                <li><strong>Reflect:</strong> Mirrors edge pixels (reduces artifacts)</li>
                                <li><strong>Replicate:</strong> Extends edge values</li>
                                <li><strong>Circular:</strong> Wraps around (for periodic signals)</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Fractional Strides: Transposed Convolutions</h2>
                    <div class="transposed-conv" style="font-size: 0.6em;">
                        <h4>Going in Reverse: Upsampling</h4>
                        <div class="fragment">
                            <p>What if we want stride < 1? Enter <span class="tooltip">transposed convolutions<span class="tooltiptext">Also called deconvolution, these operations increase spatial dimensions</span></span>:</p>
                            <pre><code class="language-python"># Transposed convolution (upsampling)
deconv = nn.ConvTranspose2d(
    in_channels=64,
    out_channels=32,
    kernel_size=3,
    stride=2,           # Upsamples by factor of 2
    padding=1,
    output_padding=1    # Adjusts output size
)

# Input: 16×16 → Output: 32×32</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Applications</h4>
                            <ul>
                                <li>Decoder networks (autoencoders)</li>
                                <li>Generative models (GANs)</li>
                                <li>Semantic segmentation</li>
                                <li>Super-resolution</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Dilated Convolutions</h2>
                    <div class="dilated-conv" style="font-size: 0.4em;">
                        <h4>Expanding Receptive Field Without Pooling</h4>
                        <div class="fragment">
                            <p><span class="tooltip">Dilated convolutions<span class="tooltiptext">Convolutions with gaps between kernel elements, also called atrous convolutions</span></span> insert spaces between kernel elements:</p>
                            <div id="dilated-demo" style="text-align: center; margin: 20px 0;">
                                <svg id="dilated-svg" width="550" height="300"></svg>
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <pre><code class="language-python"># Dilated convolution
dilated_conv = nn.Conv2d(
    in_channels=3,
    out_channels=64,
    kernel_size=3,
    dilation=2    # Spacing between kernel elements
)
# Effective receptive field: 5×5 with only 9 parameters!</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Dilated convolutions capture multi-scale context without losing resolution</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Design Principles Summary</h2>
                    <div class="design-principles">
                        <h4>Best Practices for Padding and Stride</h4>
                        <div class="principles-list">
                            <ol>
                                <li class="fragment">
                                    <strong>Use odd kernels:</strong> 3×3, 5×5, 7×7 for symmetric padding
                                </li>
                                <li class="fragment">
                                    <strong>Standard downsampling:</strong> stride=2 with appropriate padding
                                </li>
                                <li class="fragment">
                                    <strong>Preserve dimensions:</strong> padding = (kernel_size - 1) / 2
                                </li>
                                <li class="fragment">
                                    <strong>First layer:</strong> Often larger kernel (7×7) with stride=2
                                </li>
                                <li class="fragment">
                                    <strong>Avoid excessive padding:</strong> Can lead to border artifacts
                                </li>
                                <li class="fragment">
                                    <strong>Consider alternatives:</strong> Dilated/transposed for specific needs
                                </li>
                            </ol>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When would you use dilated convolutions instead of regular convolutions with stride?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When you want to downsample the feature map",
                                "correct": false,
                                "explanation": "Dilated convolutions maintain spatial resolution, they dont downsample."
                            },
                            {
                                "text": "When you need a larger receptive field without losing resolution",
                                "correct": true,
                                "explanation": "Correct! Dilated convolutions expand receptive field while maintaining output size."
                            },
                            {
                                "text": "When you want to upsample the feature map",
                                "correct": false,
                                "explanation": "Use transposed convolutions for upsampling, not dilated convolutions."
                            },
                            {
                                "text": "When you need fewer parameters",
                                "correct": false,
                                "explanation": "Dilated convolutions use the same number of parameters as regular convolutions."
                            }
                        ]
                    }'></div>
                </section>
            </section>


            <!-- Section: Multiple Input Channels (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.4", "url": "https://d2l.ai/chapter_convolutional-neural-networks/channels.html"}]'>
                    <h2 class="truncate-title">Beyond Single Channels: The Real World of Images</h2>
                    <div class="two-column" style="font-size: 0.6em;">
                        <div class="column">
                            <h4>Grayscale Images</h4>
                            <ul>
                                <li>Single channel (1D per pixel)</li>
                                <li>Values represent intensity</li>
                                <li>Shape: height × width</li>
                                <li>Common in MNIST, X-rays</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Color Images (RGB)</h4>
                            <ul>
                                <li>Three channels (R, G, B)</li>
                                <li>Each channel: 0-255 intensity</li>
                                <li>Shape: 3 × height × width</li>
                                <li>Standard in modern vision</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                        <p><strong>Key Challenge:</strong> How do we handle convolutions when inputs have multiple channels?</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mathematical Formulation: Multi-Channel Convolution</h2>
                    <div class="math-content" style="font-size: 0.6em;">
                        <p>For input with <span class="tooltip">c<sub>i</sub> channels<span class="tooltiptext">The channel dimension, e.g., 3 for RGB images</span></span>:</p>
                        <div class="fragment">
                            <h4>Kernel Shape</h4>
                            <p>$$\text{Kernel: } \color{#10099F}{c_i} \times \color{#2DD2C0}{k_h} \times \color{#FFA05F}{k_w}$$</p>
                            <ul>
                                <li><span style="color: #10099F">c<sub>i</sub></span>: number of input channels</li>
                                <li><span style="color: #2DD2C0">k<sub>h</sub></span>: kernel height</li>
                                <li><span style="color: #FFA05F">k<sub>w</sub></span>: kernel width</li>
                            </ul>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Cross-Correlation with Multiple Channels</h4>
                            <p>$$Y = \sum_{c=1}^{c_i} X^{(c)} \star K^{(c)}$$</p>
                            <p class="small">where ⋆ denotes 2D cross-correlation</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Each channel is convolved separately, then results are summed</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visual Example: Two-Channel Convolution</h2>
                    <div class="visualization-container">
                        <img src="images/conv-multi-in.svg" alt="Multi-channel convolution diagram" style="max-width: 80%; margin: 20px auto; display: block;">
                        <div class="fragment mt-md">
                            <h4>Computation Example</h4>
                            <p>Output[0,0] = <span style="color: #10099F">(1×1 + 2×2 + 4×3 + 5×4)</span> + <span style="color: #2DD2C0">(0×0 + 1×1 + 3×2 + 4×3)</span> = <strong>56</strong></p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Notice: Two 2D kernels (one per input channel) produce a single 2D output</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation: Multi-Channel Cross-Correlation</h2>
                    <pre class="fragment"><code class="language-python">def corr2d_multi_in(X, K):
    """
    Multi-channel cross-correlation
    X shape: (c_i, h, w) 
    K shape: (c_i, k_h, k_w)
    Returns: (h_out, w_out)
    """
    # Sum over all input channels
    return sum(corr2d(x, k) for x, k in zip(X, K))</code></pre>
                    
                    <div class="fragment mt-md">
                        <h4>Numerical Example</h4>
                        <pre><code class="language-python"># Two input channels, each 3x3
X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],    # Channel 1
                  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])   # Channel 2

# Two 2x2 kernels (one per channel)
K = torch.tensor([[[0, 1], [2, 3]],    # Kernel for channel 1
                  [[1, 2], [3, 4]]])    # Kernel for channel 2

output = corr2d_multi_in(X, K)
# Result: [[56, 72], [104, 120]]</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In multi-channel convolution, how are the results from each channel combined?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They are concatenated to form multiple output channels",
                                "correct": false,
                                "explanation": "Concatenation would create multiple outputs. Multi-channel input convolution sums the results."
                            },
                            {
                                "text": "They are summed element-wise to produce a single output channel",
                                "correct": true,
                                "explanation": "Correct! Each input channel is convolved with its corresponding kernel, then all results are summed."
                            },
                            {
                                "text": "They are averaged to reduce noise",
                                "correct": false,
                                "explanation": "Averaging would lose information. The standard operation is summation, not averaging."
                            },
                            {
                                "text": "Only the maximum value across channels is kept",
                                "correct": false,
                                "explanation": "Max pooling is a different operation. Channel results are summed in convolution."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Multiple Output Channels (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.4.2", "url": "https://d2l.ai/chapter_convolutional-neural-networks/channels.html#multiple-output-channels"}]'>
                    <h2 class="truncate-title">Multiple Output Channels: Detecting Diverse Features</h2>
                    <div class="concept-grid">
                        <div class="concept-item fragment">
                            <h4>Why Multiple Outputs?</h4>
                            <ul>
                                <li>Different kernels detect different features</li>
                                <li>Edge detectors, color detectors, texture detectors</li>
                                <li>Each output channel = one feature map</li>
                            </ul>
                        </div>
                        <div class="concept-item fragment">
                            <h4>Trade-offs</h4>
                            <ul>
                                <li>Spatial resolution ↓ as we go deeper</li>
                                <li><span class="tooltip">Channel depth<span class="tooltiptext">The number of feature maps/channels at a layer</span></span> ↑ for richer representations</li>
                                <li>Computational cost increases</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>Key Insight:</strong> Channels work together to create joint representations, not independently</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mathematical Framework for Multiple Outputs</h2>
                    <div class="math-content">
                        <h4>Complete Kernel Tensor Shape</h4>
                        <p>$$\text{Kernel: } \color{#FC8484}{c_o} \times \color{#10099F}{c_i} \times \color{#2DD2C0}{k_h} \times \color{#FFA05F}{k_w}$$</p>
                        <ul class="fragment">
                            <li><span style="color: #FC8484">c<sub>o</sub></span>: number of output channels</li>
                            <li><span style="color: #10099F">c<sub>i</sub></span>: number of input channels</li>
                            <li><span style="color: #2DD2C0">k<sub>h</sub></span> × <span style="color: #FFA05F">k<sub>w</sub></span>: spatial kernel dimensions</li>
                        </ul>
                        
                        <div class="fragment mt-md">
                            <h4>Computational Complexity</h4>
                            <p>$$\mathcal{O}(h \cdot w \cdot k^2 \cdot c_i \cdot c_o)$$</p>
                            <p class="small">For a 256×256 image, 5×5 kernel, 128 input and output channels:</p>
                            <p class="small"><strong>≈ 53 billion operations!</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visualizing Multiple Output Generation</h2>
                    <div class="multi-output-diagram">
                        <h4>Each Output Channel Has Its Own Set of Kernels</h4>
                        <div style="display: flex; justify-content: space-around; align-items: center; margin: 30px 0;">
                            <div style="text-align: center;">
                                <div style="background: #E3F2FD; padding: 20px; border-radius: 8px;">
                                    <strong>Input</strong><br>
                                    c<sub>i</sub> × H × W
                                </div>
                            </div>
                            <div style="text-align: center;">
                                <div style="background: #FFF3E0; padding: 15px; border-radius: 8px;">
                                    <strong>Kernels</strong><br>
                                    c<sub>o</sub> × c<sub>i</sub> × k × k
                                </div>
                            </div>
                            <div style="text-align: center;">
                                <div style="background: #FCE4EC; padding: 20px; border-radius: 8px;">
                                    <strong>Output</strong><br>
                                    c<sub>o</sub> × H' × W'
                                </div>
                            </div>
                        </div>
                        <p class="fragment">Each of the c<sub>o</sub> output channels is computed independently using its own set of c<sub>i</sub> kernels</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation: Multi-Input Multi-Output</h2>
                    <pre><code class="language-python">def corr2d_multi_in_out(X, K):
    """
    Multi-input multi-output cross-correlation
    X shape: (c_i, h, w)
    K shape: (c_o, c_i, k_h, k_w)
    Returns: (c_o, h_out, w_out)
    """
    # Stack results for each output channel
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)</code></pre>
                    
                    <div class="fragment mt-md">
                        <h4>Example: Creating 3 Output Channels</h4>
                        <pre><code class="language-python"># Input: 2 channels, 3x3 each
X = torch.randn(2, 3, 3)

# Create 3 different kernels for 3 outputs
K = torch.stack((K, K + 1, K + 2), 0)  # Shape: (3, 2, 2, 2)

output = corr2d_multi_in_out(X, K)
# Result shape: (3, 2, 2) - three feature maps!</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Visualization: Feature Map Generation</h2>
                    <div id="output-channels-demo" class="visualization-container">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Output Channels: 
                                <input type="range" id="output-channels-slider" min="1" max="4" value="2" style="width: 100px;">
                                <span id="output-channels-value" style="font-family: monospace;">2</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Kernel Type:
                                <select id="kernel-type" style="padding: 4px;">
                                    <option value="edge">Edge Detection</option>
                                    <option value="blur">Blur</option>
                                    <option value="sharpen">Sharpen</option>
                                    <option value="random">Random</option>
                                </select>
                            </label>
                            <button id="generate-outputs" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Generate</button>
                        </div>
                        <svg id="output-channels-svg" width="800" height="350"></svg>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the shape of the kernel tensor for a convolution with 3 input channels, 64 output channels, and 5×5 spatial dimensions?",
                        "type": "single",
                        "options": [
                            {
                                "text": "3 × 64 × 5 × 5",
                                "correct": false,
                                "explanation": "The output channels come first in the kernel tensor, not the input channels."
                            },
                            {
                                "text": "64 × 3 × 5 × 5",
                                "correct": true,
                                "explanation": "Correct! The kernel shape is (c_o, c_i, k_h, k_w) = (64, 3, 5, 5)."
                            },
                            {
                                "text": "5 × 5 × 3 × 64",
                                "correct": false,
                                "explanation": "The spatial dimensions are last, and channel dimensions come first."
                            },
                            {
                                "text": "3 × 5 × 5 × 64",
                                "correct": false,
                                "explanation": "This incorrectly mixes spatial and channel dimensions."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: 1×1 Convolutions (Vertical) -->
            <section>
                <section data-sources='[{"text": "Lin et al. (2013) - Network in Network", "url": "https://arxiv.org/abs/1312.4400"}, {"text": "Dive into Deep Learning - Chapter 7.4.3", "url": "https://d2l.ai/chapter_convolutional-neural-networks/channels.html#times-1-convolutional-layer"}]'>
                    <h2 class="truncate-title">The Surprising 1×1 Convolution</h2>
                    <div class="paradox-explanation" style="font-size: 0.6em;">
                        <h4>The Apparent Contradiction</h4>
                        <div class="two-column">
                            <div class="column fragment">
                                <h5>What We Expect from Convolutions</h5>
                                <ul>
                                    <li>Spatial correlation</li>
                                    <li>Local receptive fields</li>
                                    <li>Pattern detection across space</li>
                                </ul>
                            </div>
                            <div class="column fragment">
                                <h5>What 1×1 Convolutions Do</h5>
                                <ul>
                                    <li>No spatial correlation (k=1)</li>
                                    <li>Single pixel receptive field</li>
                                    <li>Only operates on channels!</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Insight:</strong> 1×1 convolutions are <span class="tooltip">pointwise convolutions<span class="tooltiptext">Operations that transform channels at each spatial location independently</span></span> - they mix channels, not spatial information</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visual Explanation: Channel Transformation</h2>
                    <div class="visualization-container">
                        <img src="images/conv-1x1.svg" alt="1×1 convolution diagram" style="max-width: 70%; margin: 20px auto; display: block;">
                        <div class="fragment mt-md" style="font-size: 0.75em;">
                            <h4>What's Happening?</h4>
                            <ul>
                                <li>Each pixel location gets a new value</li>
                                <li>New value = linear combination of all input channels at that location</li>
                                <li>Same transformation applied at every spatial position</li>
                                <li>Equivalent to a fully connected layer applied per-pixel</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mathematical Equivalence to Fully Connected</h2>
                    <div class="math-comparison">
                        <h4>1×1 Convolution as Matrix Multiplication</h4>
                        <pre><code class="language-python">def corr2d_multi_in_out_1x1(X, K):
    """
    1×1 convolution via matrix multiplication
    X shape: (c_i, h, w)
    K shape: (c_o, c_i, 1, 1) - reshaped to (c_o, c_i)
    """
    c_i, h, w = X.shape
    c_o = K.shape[0]
    
    # Reshape for matrix multiplication
    X = X.reshape((c_i, h * w))        # Flatten spatial dims
    K = K.reshape((c_o, c_i))          # Remove 1×1 spatial dims
    
    # Matrix multiplication (fully connected)
    Y = torch.matmul(K, X)              # (c_o, c_i) × (c_i, h*w) = (c_o, h*w)
    
    return Y.reshape((c_o, h, w))       # Restore spatial dims</code></pre>
                        
                        <div class="fragment emphasis-box mt-md" style="font-size: 0.75em;">
                            <p>Each spatial position undergoes the same c<sub>i</sub> → c<sub>o</sub> transformation</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Applications and Benefits of 1×1 Convolutions</h2>
                    <div class="applications-grid" style="font-size: 0.6em;">
                        <div class="application-card fragment">
                            <h4>1. Dimensionality Reduction</h4>
                            <p>Reduce channels before expensive operations</p>
                            <p class="small">Example: 256→64 channels before 5×5 conv</p>
                            <p class="small">Saves: 75% of computations!</p>
                        </div>
                        <div class="application-card fragment">
                            <h4>2. Increase Non-linearity</h4>
                            <p>Add depth without changing resolution</p>
                            <p class="small">1×1 conv → ReLU → 1×1 conv</p>
                            <p class="small">More expressive power</p>
                        </div>
                        <div class="application-card fragment">
                            <h4>3. Cross-Channel Interactions</h4>
                            <p>Let channels "talk" to each other</p>
                            <p class="small">Combine features from different detectors</p>
                        </div>
                        <div class="application-card fragment">
                            <h4>4. <span class="tooltip">Bottleneck Architectures<span class="tooltiptext">Design pattern that reduces dimensions, processes, then expands back</span></span></h4>
                            <p>Used in ResNet, Inception</p>
                            <p class="small">1×1 down → 3×3 conv → 1×1 up</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Practical Example: Network in Network</h2>
                    <div class="nin-example">
                        <h4>MLP-Conv Layer Structure</h4>
                        <pre><code class="language-python"># Traditional convolution block
conv_block = nn.Sequential(
    nn.Conv2d(128, 256, kernel_size=3, padding=1),
    nn.ReLU()
)

# Network-in-Network block (with 1×1 convs)
nin_block = nn.Sequential(
    nn.Conv2d(128, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(256, 256, kernel_size=1),  # 1×1 conv
    nn.ReLU(),
    nn.Conv2d(256, 256, kernel_size=1),  # 1×1 conv
    nn.ReLU()
)</code></pre>
                        <div class="fragment" style="font-size: 0.6em;">
                            <p><strong>Benefits of Network-in-Network:</strong></p>
                            <ul>
                                <li><strong>Enhanced Non-linearity:</strong> Multiple ReLU activations create more complex decision boundaries</li>
                                <li><strong>Better Feature Mixing:</strong> 1×1 convolutions allow channels to interact and combine features</li>
                                <li><strong>Efficient Computation:</strong> Similar cost to traditional blocks but with richer representations</li>
                                <li><strong>Improved Expressiveness:</strong> Acts like a mini neural network at each spatial location</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statements about 1×1 convolutions are correct?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "They can change the number of channels",
                                "correct": true,
                                "explanation": "Yes! 1×1 convolutions can transform from c_i to c_o channels."
                            },
                            {
                                "text": "They preserve spatial dimensions when stride=1",
                                "correct": true,
                                "explanation": "Correct! With stride=1, spatial dimensions remain unchanged."
                            },
                            {
                                "text": "They are equivalent to a fully connected layer at each pixel",
                                "correct": true,
                                "explanation": "Yes! Each pixel gets the same linear transformation of its channels."
                            },
                            {
                                "text": "They capture spatial patterns in the image",
                                "correct": false,
                                "explanation": "No, 1×1 convolutions only mix channels at each location, not across locations."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 22: Introduction to Pooling (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 7.5", "url": "https://d2l.ai/chapter_convolutional-neural-networks/pooling.html"}]'>
                    <h2 class="truncate-title">The Need for Pooling Layers</h2>
                    <div class="two-column" style="font-size: 0.6em;">
                        <div class="column">
                            <h4>The Challenge</h4>
                            <ul class="fragment">
                                <li>CNNs need global understanding</li>
                                <li>Deep layers need larger <span class="tooltip">receptive fields<span class="tooltiptext">The input region that affects a particular neuron's output</span></span></li>
                                <li>Pure convolutions are too sensitive to pixel shifts</li>
                                <li>Computational cost grows with depth</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Pooling Solutions</h4>
                            <ul class="fragment">
                                <li>Aggregate local information</li>
                                <li>Provide <span class="tooltip">translation invariance<span class="tooltiptext">Output remains similar when input is shifted slightly</span></span></li>
                                <li>Reduce spatial dimensions</li>
                                <li>Decrease computational load</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>Dual Purpose:</strong> Mitigate sensitivity to location AND spatially downsample representations</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Translation Problem</h2>
                    <div class="translation-example" style="font-size: 0.6em;">
                        <h4>Edge Detection Sensitivity</h4>
                        <div class="fragment">
                            <p>Consider an image <code>X</code> with a sharp edge between black and white</p>
                            <p>Shift the entire image by one pixel: <code>Z[i, j] = X[i, j + 1]</code></p>
                        </div>
                        <div class="fragment mt-md">
                            <div class="two-column">
                                <div class="column">
                                    <h5>Without Pooling</h5>
                                    <ul>
                                        <li>Output changes drastically</li>
                                        <li>Edge shifts by exactly one pixel</li>
                                        <li>Feature maps are different</li>
                                    </ul>
                                </div>
                                <div class="column">
                                    <h5>With 2×2 Max Pooling</h5>
                                    <ul>
                                        <li>Output remains similar</li>
                                        <li>Tolerates shifts up to 1 pixel</li>
                                        <li>Feature presence preserved</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Real-world importance: Camera vibration, object movement, and positioning variations</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Pooling vs Convolution: Key Differences</h2>
                    <div class="comparison-table" style="font-size: 0.6em;">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Convolution</th>
                                    <th>Pooling</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td><strong>Parameters</strong></td>
                                    <td>Learnable weights</td>
                                    <td>No parameters</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Operation</strong></td>
                                    <td>Linear combination</td>
                                    <td>Aggregation (max/avg)</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Purpose</strong></td>
                                    <td>Feature extraction</td>
                                    <td>Downsampling & invariance</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Channels</strong></td>
                                    <td>Can change count</td>
                                    <td>Preserves count</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Nature</strong></td>
                                    <td>Learned from data</td>
                                    <td>Fixed, deterministic</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.8em;">
                        <p>Pooling is a <strong>fixed</strong> operation - it always does the same thing!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are the two main purposes of pooling layers?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Provide translation invariance",
                                "correct": true,
                                "explanation": "Yes! Pooling makes the network less sensitive to small shifts in the input."
                            },
                            {
                                "text": "Reduce spatial dimensions",
                                "correct": true,
                                "explanation": "Correct! Pooling downsamples the spatial resolution, reducing computation."
                            },
                            {
                                "text": "Learn feature representations",
                                "correct": false,
                                "explanation": "Pooling has no learnable parameters - it uses fixed aggregation functions."
                            },
                            {
                                "text": "Increase the number of channels",
                                "correct": false,
                                "explanation": "Pooling preserves the number of channels, operating on each independently."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 23: Maximum and Average Pooling (Vertical) -->
            <section>
                <section data-sources='[{"text": "Riesenhuber & Poggio (1999) - Hierarchical models of object recognition", "url": "https://www.nature.com/articles/nn1199_1019"}, {"text": "Dive into Deep Learning - Section 7.5.1", "url": "https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#maximum-pooling-and-average-pooling"}]'>
                    <h2 class="truncate-title">Maximum and Average Pooling</h2>
                    <div class="pooling-types" style="font-size: 0.6em;">
                        <h4>Two Main Aggregation Functions</h4>
                        <div class="two-column mt-md">
                            <div class="column fragment">
                                <h5>Maximum Pooling</h5>
                                <p>$$Y[i,j] = \max_{(m,n) \in R_{i,j}} X[m,n]$$</p>
                                <ul>
                                    <li>Takes maximum value in window</li>
                                    <li>Detects feature presence</li>
                                    <li>Most commonly used</li>
                                    <li>Better for feature detection</li>
                                </ul>
                            </div>
                            <div class="column fragment">
                                <h5>Average Pooling</h5>
                                <p>$$Y[i,j] = \frac{1}{|R_{i,j}|} \sum_{(m,n) \in R_{i,j}} X[m,n]$$</p>
                                <ul>
                                    <li>Computes mean value</li>
                                    <li>Smooths the output</li>
                                    <li>Historically older</li>
                                    <li>Better for downsampling</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>In practice:</strong> Max-pooling is preferred in almost all cases</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visual Example: 2×2 Max Pooling</h2>
                    <div class="pooling-example" style="font-size: 0.6em;">
                        <img src="images/pooling-diagram.jpg" alt="Max pooling with 2×2 window" style="max-width: 60%; margin: 20px auto; display: block;">
                        <div class="fragment mt-md">
                            <h4>Computation Steps</h4>
                            <div class="computation-grid">
                                <div class="step">$$\max(0, 1, 3, 4) = 4$$</div>
                                <div class="step">$$\max(1, 2, 4, 5) = 5$$</div>
                                <div class="step">$$\max(3, 4, 6, 7) = 7$$</div>
                                <div class="step">$$\max(4, 5, 7, 8) = 8$$</div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Each 2×2 region becomes a single value - 75% spatial reduction!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation: Pooling Functions</h2>
                    <pre><code class="language-python">def pool2d(X, pool_size, mode='max'):
    """
    2D pooling operation
    X: input tensor
    pool_size: (height, width) of pooling window
    mode: 'max' or 'avg'
    """
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y

# Example usage
X = torch.tensor([[0.0, 1.0, 2.0], 
                  [3.0, 4.0, 5.0], 
                  [6.0, 7.0, 8.0]])
                  
max_pooled = pool2d(X, (2, 2), 'max')  # [[4., 5.], [7., 8.]]
avg_pooled = pool2d(X, (2, 2), 'avg')  # [[2., 3.], [5., 6.]]</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Demo: Pooling Operations</h2>
                    <div id="pooling-demo" class="visualization-container">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Pool Size: 
                                <input type="range" id="pool-size-slider" min="2" max="4" value="2" style="width: 100px;">
                                <span id="pool-size-value" style="font-family: monospace;">2×2</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Type:
                                <select id="pool-type" style="padding: 4px;">
                                    <option value="max">Max Pooling</option>
                                    <option value="avg">Average Pooling</option>
                                </select>
                            </label>
                            <button id="animate-pooling" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Animate</button>
                            <button id="reset-pooling" style="background: #2DD2C0; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Reset</button>
                        </div>
                        <svg id="pooling-svg" width="800" height="400"></svg>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Max Pooling Dominates</h2>
                    <div class="max-pooling-advantages" style="font-size: 0.6em;">
                        <h4>Historical Context</h4>
                        <ul class="fragment">
                            <li><strong>1990:</strong> Early use in speech recognition (Yamaguchi et al.)</li>
                            <li><strong>1999:</strong> Cognitive neuroscience model (Riesenhuber & Poggio)</li>
                            <li><strong>2012+:</strong> Standard in all modern CNNs</li>
                        </ul>
                        
                        <div class="fragment mt-lg">
                            <h4>Advantages of Max Pooling</h4>
                            <div class="advantage-grid">
                                <div class="advantage-item">
                                    <h5>Feature Detection</h5>
                                    <p>If a feature exists anywhere in the region, max pooling captures it</p>
                                </div>
                                <div class="advantage-item">
                                    <h5>Biological Inspiration</h5>
                                    <p>Mimics hierarchical processing in visual cortex</p>
                                </div>
                                <div class="advantage-item">
                                    <h5>Noise Suppression</h5>
                                    <p>Ignores non-maximal responses that might be noise</p>
                                </div>
                                <div class="advantage-item">
                                    <h5>Sharper Features</h5>
                                    <p>Preserves strong activations better than averaging</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "For edge detection with a 2×2 max pooling layer, what happens when an edge moves by one pixel?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The output changes completely",
                                "correct": false,
                                "explanation": "Max pooling provides tolerance to small shifts within the pooling window."
                            },
                            {
                                "text": "The output remains the same if the edge is still within the pooling window",
                                "correct": true,
                                "explanation": "Correct! If the maximum value is still within the 2×2 window, the output is unchanged."
                            },
                            {
                                "text": "The output is averaged between old and new positions",
                                "correct": false,
                                "explanation": "Max pooling takes the maximum, not the average of values."
                            },
                            {
                                "text": "The pooling layer learns to adapt to the shift",
                                "correct": false,
                                "explanation": "Pooling has no learnable parameters - it always performs the same fixed operation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 24: Padding and Stride in Pooling (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 7.5.2", "url": "https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#padding-and-stride"}]'>
                    <h2 class="truncate-title">Padding and Stride in Pooling</h2>
                    <div class="pooling-parameters" style="font-size: 0.5em;">
                        <h4>Output Dimension Formula</h4>
                        <div class="formula-box fragment">
                            <p>$$\text{Output Size} = \left\lfloor \frac{\text{Input Size} + 2 \times \text{Padding} - \text{Pool Size}}{\text{Stride}} \right\rfloor + 1$$</p>
                        </div>
                        
                        <div class="fragment mt-lg">
                            <h4>Default Behavior</h4>
                            <div class="two-column">
                                <div class="column">
                                    <h5>Framework Defaults</h5>
                                    <ul>
                                        <li>Stride = Pool Size (no overlap)</li>
                                        <li>Padding = 0 (valid padding)</li>
                                        <li>Example: 3×3 pool → stride 3</li>
                                    </ul>
                                </div>
                                <div class="column">
                                    <h5>Common Override</h5>
                                    <ul>
                                        <li>2×2 pool, stride 2 (standard)</li>
                                        <li>Reduces size by half</li>
                                        <li>No overlapping windows</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Unlike convolutions, pooling rarely uses padding except for size matching</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Code Examples: Various Configurations</h2>
                    <pre><code class="language-python">import torch.nn as nn

# Create input: batch=1, channels=1, height=4, width=4
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))

# Default: pool_size = stride
pool2d = nn.MaxPool2d(3)  # 3×3 pooling, stride=3
output1 = pool2d(X)  # Shape: (1, 1, 1, 1), Value: [[[[10]]]]

# Custom stride and padding
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
output2 = pool2d(X)  # Shape: (1, 1, 2, 2), Values: [[[[5, 7], [13, 15]]]]

# Non-square pooling
pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
output3 = pool2d(X)  # Shape: (1, 1, 2, 2), Values: [[[[5, 7], [13, 15]]]]

# Common configuration: 2×2 pool, stride 2
pool2d = nn.MaxPool2d(2, stride=2)  # Halves spatial dimensions
output4 = pool2d(X)  # Shape: (1, 1, 2, 2)</code></pre>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.6em;">
                        <p>Most common: 2×2 pooling with stride 2 - simple and effective!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visual Comparison: Overlapping vs Non-overlapping</h2>
                    <div class="stride-comparison" style="font-size: 0.6em;">
                        <div class="two-column">
                            <div class="column">
                                <h4>Non-overlapping (Stride = Pool Size)</h4>
                                <ul>
                                    <li>Each input used once</li>
                                    <li>Clean downsampling</li>
                                    <li>Maximum reduction</li>
                                    <li>Standard approach</li>
                                </ul>
                                <div class="visual-example" style="background: #f0f0f0; padding: 10px; border-radius: 5px; margin-top: 10px;">
                                    <p style="font-family: monospace; font-size: 14px;">
                                        [1 2 | 3 4]<br>
                                        [5 6 | 7 8]<br>
                                        -----+-----<br>
                                        [9 0 | 1 2]<br>
                                        [3 4 | 5 6]
                                    </p>
                                </div>
                            </div>
                            <div class="column">
                                <h4>Overlapping (Stride < Pool Size)</h4>
                                <ul>
                                    <li>Windows overlap</li>
                                    <li>Smoother transitions</li>
                                    <li>Less aggressive reduction</li>
                                    <li>Rarely used</li>
                                </ul>
                                <div class="visual-example" style="background: #f0f0f0; padding: 10px; border-radius: 5px; margin-top: 10px;">
                                    <p style="font-family: monospace; font-size: 14px;">
                                        [1 2 3] → [2 3 4]<br>
                                        [5 6 7] → [6 7 8]<br>
                                        [9 0 1] → [0 1 2]<br>
                                        ↓         ↓
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the output size of a 32×32 image after 2×2 max pooling with stride 2 and no padding?",
                        "type": "single",
                        "options": [
                            {
                                "text": "32×32",
                                "correct": false,
                                "explanation": "Pooling with stride 2 reduces dimensions. The output would be 32×32 only with stride 1 and padding 1."
                            },
                            {
                                "text": "16×16",
                                "correct": true,
                                "explanation": "Correct! With 2×2 pooling and stride 2: (32 - 2)/2 + 1 = 16"
                            },
                            {
                                "text": "15×15",
                                "correct": false,
                                "explanation": "This would require different parameters. Check the formula: ⌊(32-2)/2⌋ + 1 = 16"
                            },
                            {
                                "text": "8×8",
                                "correct": false,
                                "explanation": "This would be the result of 4×4 pooling with stride 4, not 2×2 with stride 2."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 25: Pooling with Multiple Channels (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 7.5.3", "url": "https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#multiple-channels"}]'>
                    <h2 class="truncate-title">Pooling with Multiple Channels</h2>
                    <div class="multi-channel-pooling" style="font-size: 0.6em;">
                        <h4>Key Principle: Channel Independence</h4>
                        <div class="fragment">
                            <p>Unlike convolution, pooling operates on each channel <strong>separately</strong></p>
                            <div class="formula-box mt-md">
                                <p>Input: <span style="color: #10099F">C × H × W</span> → Pooling → Output: <span style="color: #10099F">C × H' × W'</span></p>
                                <p class="small mt-sm">Number of channels <strong>C</strong> is preserved!</p>
                            </div>
                        </div>
                        
                        <div class="fragment mt-lg">
                            <h4>Mathematical Formulation</h4>
                            <p>For each channel c:</p>
                            <p>$$Y^{(c)}[i,j] = \text{pool}(X^{(c)}[i \cdot s : i \cdot s + p, j \cdot s : j \cdot s + q])$$</p>
                            <p class="small">where pool is max or avg, s is stride, p×q is pool size</p>
                        </div>
                        
                        <div class="fragment emphasis-box mt-lg">
                            <p>Each feature map is pooled independently - no cross-channel interaction!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Example: RGB Image Pooling</h2>
                    <div class="rgb-pooling-example">
                        <h4>Processing Color Images</h4>
                        <div class="visual-flow" style="text-align: center;">
                            <div class="fragment">
                                <div style="display: inline-block; margin: 10px; padding: 15px; background: #ffebee; border-radius: 8px;">
                                    <strong>Red Channel</strong><br>
                                    256×256 → Pool → 128×128
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="display: inline-block; margin: 10px; padding: 15px; background: #e8f5e9; border-radius: 8px;">
                                    <strong>Green Channel</strong><br>
                                    256×256 → Pool → 128×128
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="display: inline-block; margin: 10px; padding: 15px; background: #e3f2fd; border-radius: 8px;">
                                    <strong>Blue Channel</strong><br>
                                    256×256 → Pool → 128×128
                                </div>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <p><strong>Result:</strong> 3×256×256 image becomes 3×128×128</p>
                            <p class="small">Spatial resolution halved, but all 3 color channels preserved</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation: Multi-Channel Pooling</h2>
                    <pre><code class="language-python"># Create input with 2 channels
X = torch.cat((torch.arange(16).reshape(1, 1, 4, 4),
               torch.arange(16).reshape(1, 1, 4, 4) + 1), 1)
print(f"Input shape: {X.shape}")  # (1, 2, 4, 4)

# Apply 2×2 max pooling with stride 2
pool2d = nn.MaxPool2d(2, stride=2)
Y = pool2d(X)
print(f"Output shape: {Y.shape}")  # (1, 2, 2, 2)

# Verify channels are processed independently
print("Channel 0 output:")
print(Y[0, 0])  # [[5, 7], [13, 15]]

print("Channel 1 output:")  
print(Y[0, 1])  # [[6, 8], [14, 16]] - exactly channel 0 + 1

# Key insight: Each channel pooled separately
# No mixing between channels during pooling!</code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p>Pooling is embarassingly parallel across channels - perfect for GPU acceleration!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Practical Implications</h2>
                    <div class="implications-grid" style="font-size: 0.6em;">
                        <div class="implication-card fragment">
                            <h4>Memory Efficiency</h4>
                            <p>2×2 pooling with stride 2 reduces memory by 75% per layer</p>
                            <p class="small">Example: 256×256×64 → 128×128×64</p>
                            <p class="small">From 4MB to 1MB per batch element</p>
                        </div>
                        <div class="implication-card fragment">
                            <h4>Computation Speed</h4>
                            <p>Subsequent convolutions are 4× faster after 2×2 pooling</p>
                            <p class="small">Quadratic reduction in spatial operations</p>
                        </div>
                        <div class="implication-card fragment">
                            <h4>Feature Hierarchy</h4>
                            <p>Deeper layers see larger receptive fields</p>
                            <p class="small">Essential for understanding global context</p>
                        </div>
                        <div class="implication-card fragment">
                            <h4>Information Preservation</h4>
                            <p>Channel count maintained ensures feature diversity</p>
                            <p class="small">Spatial reduction balanced by channel depth</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "After applying 2×2 max pooling to a tensor of shape (64, 224, 224), what is the output shape?",
                        "type": "single",
                        "options": [
                            {
                                "text": "(64, 112, 112)",
                                "correct": true,
                                "explanation": "Correct! Pooling preserves channels (64) and halves spatial dimensions with 2×2 stride 2."
                            },
                            {
                                "text": "(32, 112, 112)",
                                "correct": false,
                                "explanation": "Pooling does not change the number of channels, only spatial dimensions."
                            },
                            {
                                "text": "(64, 111, 111)",
                                "correct": false,
                                "explanation": "With 2×2 pooling and stride 2: ⌊(224-2)/2⌋ + 1 = 112, not 111."
                            },
                            {
                                "text": "(128, 112, 112)",
                                "correct": false,
                                "explanation": "Pooling cannot increase the number of channels - it operates on each independently."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 26: Advanced Pooling Concepts (Vertical) -->
            <section>
                <section data-sources='[{"text": "Zeiler & Fergus (2013) - Stochastic Pooling", "url": "https://arxiv.org/abs/1301.3557"}, {"text": "Graham (2014) - Fractional Max-Pooling", "url": "https://arxiv.org/abs/1412.6071"}]'>
                    <h2 class="truncate-title">Beyond Basic Pooling: Advanced Techniques</h2>
                    <div class="advanced-pooling-types" style="font-size: 0.6em;">
                        <h4>Evolution of Pooling Methods</h4>
                        <div class="timeline-vertical">
                            <div class="timeline-item fragment">
                                <div class="year">Classic</div>
                                <div class="method">Max & Average Pooling</div>
                                <div class="description">Deterministic, simple, effective</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2013</div>
                                <div class="method"><span class="tooltip">Stochastic Pooling<span class="tooltiptext">Randomly selects elements based on probability proportional to their values</span></span></div>
                                <div class="description">Adds regularization through randomness</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2014</div>
                                <div class="method"><span class="tooltip">Fractional Max-Pooling<span class="tooltiptext">Uses random pooling regions of varying sizes</span></span></div>
                                <div class="description">Non-integer downsampling ratios</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">Modern</div>
                                <div class="method"><span class="tooltip">Global Average Pooling<span class="tooltiptext">Averages entire feature map to single value</span></span></div>
                                <div class="description">Replaces fully connected layers</div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Stochastic Pooling</h2>
                    <div class="stochastic-pooling" style="font-size: 0.6em;">
                        <h4>Probabilistic Selection</h4>
                        <div class="two-column mt-md">
                            <div class="column">
                                <h5>How It Works</h5>
                                <ol>
                                    <li>Normalize region to probabilities</li>
                                    <li>Sample based on these probabilities</li>
                                    <li>Select sampled element</li>
                                </ol>
                                <pre class="fragment"><code class="language-python"># Stochastic pooling concept
region = [[1, 2], [3, 4]]
probs = softmax(region.flatten())
# [0.03, 0.09, 0.24, 0.64]
# Sample: likely picks 4, sometimes 3</code></pre>
                            </div>
                            <div class="column">
                                <h5>Benefits</h5>
                                <ul>
                                    <li>Built-in regularization</li>
                                    <li>Reduces overfitting</li>
                                    <li>Training-time randomness</li>
                                    <li>Test-time: weighted average</li>
                                </ul>
                                <div class="fragment emphasis-box mt-md">
                                    <p>Slight accuracy improvement in some cases</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Global Average Pooling (GAP)</h2>
                    <div class="gap-explanation" style="font-size: 0.4em;">
                        <h4>From Feature Maps to Predictions</h4>
                        <div class="fragment">
                            <p><strong>Core Concept:</strong> Average entire spatial dimension per channel to get one value per feature map</p>
                            <p>$$\text{GAP}(X) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X[:, i, j]$$</p>
                        </div>
                        
                        <div class="fragment mt-md">
                            <h4>Step-by-Step Process</h4>
                            <div class="two-column">
                                <div class="column">
                                    <ol>
                                        <li><strong>Input:</strong> Feature maps (C, H, W)</li>
                                        <li><strong>Operation:</strong> Average each spatial dimension</li>
                                        <li><strong>Output:</strong> Vector of size C</li>
                                        <li><strong>Classification:</strong> Direct mapping to classes</li>
                                    </ol>
                                </div>
                                <div class="column">
                                    <ol>
                                        <strong>Example:</strong>
                                        <li>Input: (512, 7, 7) feature maps</li>
                                        <li>GAP: Average each 7×7 → (512,)</li>
                                        <li>If 512 classes: Direct predictions!</li>
                                    </ol>
                                </div>
                            </div>
                        </div>
                        
                        <div class="fragment mt-lg">
                            <div style="display: flex; justify-content: space-between; gap: 2rem;">
                                <div class="impact-item" style="flex: 1; text-align: center;">
                                    <h5>Replaces FC Layers</h5>
                                    <p>No more flatten → massive FC</p>
                                    <p class="small">Dramatic parameter reduction</p>
                                </div>
                                <div class="impact-item" style="flex: 1; text-align: center;">
                                    <h5>Variable Input Size</h5>
                                    <p>Works with any spatial dimension</p>
                                    <p class="small">More flexible architectures</p>
                                </div>
                                <div class="impact-item" style="flex: 1; text-align: center;">
                                    <h5>Interpretability</h5>
                                    <p>Direct channel → class mapping</p>
                                    <p class="small">Class activation maps (CAM)</p>
                                </div>
                            </div>
                        </div>
                        
                        <div class="fragment mt-md">
                            <h4>Traditional vs GAP Architecture</h4>
                            <div class="two-column">
                                <div class="column">
                                    <h5>Traditional CNN</h5>
                                    <pre><code class="language-python"># Traditional approach
conv_features = conv_layers(x)  # (B, 512, 7, 7)
flattened = conv_features.flatten(1)  # (B, 25088)
fc1 = Linear(25088, 4096)(flattened)
fc2 = Linear(4096, 1000)(fc1)  # 4M+ parameters!</code></pre>
                                </div>
                                <div class="column">
                                    <h5>GAP CNN</h5>
                                    <pre><code class="language-python"># GAP approach
conv_features = conv_layers(x)  # (B, 1000, 7, 7)
gap_output = F.adaptive_avg_pool2d(conv_features, 1)
predictions = gap_output.squeeze()  # (B, 1000)
# Zero additional parameters!</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Complexity Analysis</h2>
                    <div class="complexity-analysis" style="font-size: 0.5em;">
                        <h4>Pooling Computational Cost</h4>
                        <div class="formula-box">
                            <p>Given input shape: <span style="color: #10099F">C × H × W</span></p>
                            <p>Pool window: <span style="color: #2DD2C0">p<sub>h</sub> × p<sub>w</sub></span></p>
                            <p>Stride: <span style="color: #FFA05F">(s<sub>h</sub>, s<sub>w</sub>)</span></p>
                        </div>
                        
                        <div class="fragment mt-md">
                            <h5>Operations Count</h5>
                            <p>$$\mathcal{O}\left(C \times \frac{H}{s_h} \times \frac{W}{s_w} \times p_h \times p_w\right)$$</p>
                        </div>
                        
                        <div class="fragment mt-lg">
                            <h4>Comparison: 2×2 Max Pooling vs 3×3 Convolution</h4>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Operation</th>
                                        <th>FLOPs per Output</th>
                                        <th>Parameters</th>
                                        <th>Memory Access</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>2×2 Max Pool</td>
                                        <td>3 comparisons</td>
                                        <td>0</td>
                                        <td>4 reads, 1 write</td>
                                    </tr>
                                    <tr>
                                        <td>3×3 Conv</td>
                                        <td>9C multiplications + 8C additions</td>
                                        <td>9C + 1</td>
                                        <td>9C reads, 1 write</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        
                        <div class="fragment emphasis-box mt-lg">
                            <p>Pooling is ~100× faster than convolution for typical channel counts!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Pooling in Modern Architectures</h2>
                    <div class="modern-usage" style="font-size: 0.6em;">
                        <h4>Typical CNN Structure</h4>
                        <div class="architecture-flow">
                            <div class="layer-block fragment">Conv → ReLU → Conv → ReLU → <strong>Pool</strong></div>
                            <div class="arrow">↓</div>
                            <div class="layer-block fragment">Conv → ReLU → Conv → ReLU → <strong>Pool</strong></div>
                            <div class="arrow">↓</div>
                            <div class="layer-block fragment">Conv → ReLU → Conv → ReLU → <strong>Pool</strong></div>
                            <div class="arrow">↓</div>
                            <div class="layer-block fragment"><strong>Global Average Pool</strong> → Predictions</div>
                        </div>
                        
                        <div class="fragment mt-lg">
                            <h4>Design Principles</h4>
                            <ul>
                                <li>Pool after 2-3 conv layers (not after each)</li>
                                <li>Standard: 2×2 max pooling, stride 2</li>
                                <li>Spatial reduction of 75% per pooling layer</li>
                                <li>End with global pooling instead of FC layers</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of Global Average Pooling over traditional fully connected layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It increases the number of parameters",
                                "correct": false,
                                "explanation": "GAP dramatically reduces parameters by eliminating fully connected layers."
                            },
                            {
                                "text": "It allows variable input sizes and reduces parameters",
                                "correct": true,
                                "explanation": "Correct! GAP works with any spatial size and has zero parameters, unlike FC layers."
                            },
                            {
                                "text": "It provides better feature extraction",
                                "correct": false,
                                "explanation": "GAP is for aggregation, not feature extraction. Convolutions extract features."
                            },
                            {
                                "text": "It speeds up training by adding randomness",
                                "correct": false,
                                "explanation": "GAP is deterministic. Stochastic pooling adds randomness, not GAP."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- LeNet: The First Convolutional Neural Network -->
            <section>
                <!-- Title Slide -->
                <section data-sources='[{"text": "Gradient-based learning applied to document recognition - LeCun et al. 1998", "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"}, {"text": "Dive into Deep Learning - Chapter 7.6", "url": "https://d2l.ai/chapter_convolutional-neural-networks/lenet.html"}]'>
                    <h1 class="truncate-title">LeNet: The First Convolutional Neural Network</h1>
                    <div class="emphasis-box" style="font-size: 0.6em;">
                        <p>Pioneering architecture that revolutionized computer vision</p>
                        <p>Developed by Yann LeCun at AT&T Bell Labs (1989-1998)</p>
                    </div>
                    <p class="mt-lg">From handwritten digits to modern deep learning</p>
                </section>

                <!-- Historical Context Section (Vertical) -->
                <section>
                    <h2 class="truncate-title">Historical Significance</h2>
                    <div class="two-column-layout" style="font-size: 0.6em;">
                        <div>
                            <h3>The Journey</h3>
                            <ul>
                                <li><strong>1989:</strong> First successful CNN training via <span class="tooltip">backpropagation<span class="tooltiptext">Algorithm for computing gradients efficiently through the network layers</span></span></li>
                                <li><strong>1998:</strong> LeNet-5 published</li>
                                <li>Achieved <strong>&lt;1% error rate</strong> on digit recognition</li>
                                <li>Deployed in ATM check processing systems</li>
                            </ul>
                        </div>
                        <div>
                            <h3>Key Innovation</h3>
                            <p>Combined multiple ideas:</p>
                            <ul>
                                <li>Local connectivity</li>
                                <li>Weight sharing</li>
                                <li>Spatial subsampling</li>
                                <li>End-to-end training</li>
                            </ul>
                        </div>
                    </div>
                    <div class="emphasis-box mt-lg">
                        <p>💡 Some ATMs still run the original 1990s LeNet code!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Problem Context: Handwritten Digit Recognition</h2>
                    <div class="demo-container">
                        <p>MNIST Dataset: 28×28 grayscale images of handwritten digits (0-9)</p>
                        <div id="mnist-samples-container" style="display: flex; justify-content: center; gap: 20px; margin: 30px 0;">
                            <canvas id="mnist-canvas" width="280" height="280" style="border: 2px solid #10099F; border-radius: 5px;"></canvas>
                            <div style="text-align: left;">
                                <h4>Challenges:</h4>
                                <ul>
                                    <li>Varying handwriting styles</li>
                                    <li>Different stroke widths</li>
                                    <li>Rotation and scaling</li>
                                    <li>Noise and artifacts</li>
                                </ul>
                                <p class="mt-md"><strong>Traditional approaches:</strong></p>
                                <ul>
                                    <li>Hand-crafted features</li>
                                    <li>Template matching</li>
                                    <li>Support Vector Machines</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Historical Context</h2>
                    <div data-mcq='{
                        "question": "What was the most significant achievement of LeNet in the 1990s?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It was the first neural network ever created",
                                "correct": false,
                                "explanation": "Neural networks existed before LeNet. LeNet was the first successful CNN."
                            },
                            {
                                "text": "It achieved less than 1% error rate on handwritten digit recognition",
                                "correct": true,
                                "explanation": "Correct! This performance matched state-of-the-art SVMs and proved CNNs viability."
                            },
                            {
                                "text": "It introduced the backpropagation algorithm",
                                "correct": false,
                                "explanation": "Backpropagation existed before; LeNet showed it could train CNNs effectively."
                            },
                            {
                                "text": "It was the first network to use GPUs",
                                "correct": false,
                                "explanation": "LeNet was developed before GPUs were used for deep learning."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Architecture Section (Vertical) -->
                <section data-sources='[{"text": "LeNet-5 Architecture Diagram", "url": "https://d2l.ai/chapter_convolutional-neural-networks/lenet.html#lenet"}]'>
                    <h2 class="truncate-title">LeNet-5 Architecture Overview</h2>
                    <img src="images/lenet.svg" alt="LeNet-5 Architecture" style="width: 90%; max-width: 900px;">
                    <div class="two-column-layout mt-md" style="font-size: 0.6em;">
                        <div>
                            <h4>Convolutional Encoder</h4>
                            <ul>
                                <li>2 convolutional layers</li>
                                <li>2 pooling layers</li>
                                <li>Feature extraction</li>
                            </ul>
                        </div>
                        <div>
                            <h4>Dense Classifier</h4>
                            <ul>
                                <li>3 fully connected layers</li>
                                <li>120 → 84 → 10 neurons</li>
                                <li>Classification head</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Detailed Layer Specifications</h2>
                    <table style="font-size: 0.6em;">
                        <thead>
                            <tr>
                                <th>Layer</th>
                                <th>Type</th>
                                <th>Parameters</th>
                                <th>Output Shape</th>
                                <th>Activation</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Input</td>
                                <td>-</td>
                                <td>-</td>
                                <td>1 × 28 × 28</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Conv1</td>
                                <td>Conv2D</td>
                                <td>6 filters, 5×5, padding=2</td>
                                <td>6 × 28 × 28</td>
                                <td><span class="tooltip">Sigmoid<span class="tooltiptext">σ(x) = 1/(1+e^(-x)), outputs between 0 and 1</span></span></td>
                            </tr>
                            <tr>
                                <td>Pool1</td>
                                <td><span class="tooltip">AvgPool2D<span class="tooltiptext">Averages values in each 2×2 window</span></span></td>
                                <td>2×2, stride=2</td>
                                <td>6 × 14 × 14</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Conv2</td>
                                <td>Conv2D</td>
                                <td>16 filters, 5×5</td>
                                <td>16 × 10 × 10</td>
                                <td>Sigmoid</td>
                            </tr>
                            <tr>
                                <td>Pool2</td>
                                <td>AvgPool2D</td>
                                <td>2×2, stride=2</td>
                                <td>16 × 5 × 5</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Flatten</td>
                                <td>Reshape</td>
                                <td>-</td>
                                <td>400</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>FC1</td>
                                <td>Dense</td>
                                <td>400 → 120</td>
                                <td>120</td>
                                <td>Sigmoid</td>
                            </tr>
                            <tr>
                                <td>FC2</td>
                                <td>Dense</td>
                                <td>120 → 84</td>
                                <td>84</td>
                                <td>Sigmoid</td>
                            </tr>
                            <tr>
                                <td>Output</td>
                                <td>Dense</td>
                                <td>84 → 10</td>
                                <td>10</td>
                                <td>Softmax</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <section>
                    <h2 class="truncate-title">Feature Maps Visualization</h2>
                    <img src="images/lenet-vert.svg" alt="LeNet Vertical Architecture" style="height: 70%; max-width: 600px;">
                    <p>Hierarchical feature learning: edges → patterns → objects</p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Architecture</h2>
                    <div data-mcq='{
                        "question": "Why does LeNet use average pooling instead of max pooling?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Average pooling is always better than max pooling",
                                "correct": false,
                                "explanation": "Not true. Max pooling often works better but was not yet discovered."
                            },
                            {
                                "text": "Max pooling had not been widely adopted when LeNet was developed",
                                "correct": true,
                                "explanation": "Correct! Max pooling and ReLU came later. LeNet used the best practices of its time."
                            },
                            {
                                "text": "Average pooling requires less computation",
                                "correct": false,
                                "explanation": "Both operations have similar computational complexity."
                            },
                            {
                                "text": "Average pooling preserves more information",
                                "correct": false,
                                "explanation": "Max pooling actually preserves the strongest features better."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Implementation Section (Vertical) -->
                <section>
                    <h2 class="truncate-title">PyTorch Implementation</h2>
                    <pre><code class="language-python">import torch.nn as nn

class LeNet(nn.Module):
"""The LeNet-5 model."""
def __init__(self, num_classes=10):
    super().__init__()
    self.features = nn.Sequential(
        # First convolutional block
        nn.Conv2d(1, 6, kernel_size=5, padding=2),
        nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        
        # Second convolutional block
        nn.Conv2d(6, 16, kernel_size=5),
        nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )
    
    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(16 * 5 * 5, 120),
        nn.Sigmoid(),
        nn.Linear(120, 84),
        nn.Sigmoid(),
        nn.Linear(84, num_classes)
    )
    
    # Initialize weights
    self.apply(self._init_weights)

def _init_weights(self, module):
    if isinstance(module, (nn.Linear, nn.Conv2d)):
        nn.init.xavier_uniform_(module.weight)</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Forward Pass Implementation</h2>
                    <pre><code class="language-python">    def forward(self, x):
    """
    Forward pass through LeNet.
    Args:
        x: Input tensor of shape (batch_size, 1, 28, 28)
    Returns:
        Output tensor of shape (batch_size, 10)
    """
    # Extract features through conv layers
    x = self.features(x)
    
    # Classify through fully connected layers
    x = self.classifier(x)
    
    return x

# Example usage
model = LeNet(num_classes=10)
batch_size = 32
dummy_input = torch.randn(batch_size, 1, 28, 28)
output = model(dummy_input)
print(f"Output shape: {output.shape}")  # (32, 10)</code></pre>
                    <div class="emphasis-box mt-md">
                        <p><strong>Key Point:</strong> <span class="tooltip">Xavier initialization<span class="tooltiptext">Weights initialized with variance 2/(fan_in + fan_out) to maintain signal magnitude</span></span> prevents vanishing/exploding gradients</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Layer Output Shapes</h2>
                    <div class="demo-container">
                        <pre><code class="language-python">def print_layer_shapes(model, input_shape=(1, 1, 28, 28)):
"""Visualize how data flows through LeNet"""
x = torch.randn(*input_shape)

# Track shapes through feature extractor
for i, layer in enumerate(model.features):
    x = layer(x)
    print(f"{layer.__class__.__name__:15} output shape: {str(x.shape):20}")

# Track shapes through classifier
for layer in model.classifier:
    x = layer(x)
    print(f"{layer.__class__.__name__:15} output shape: {str(x.shape):20}")</code></pre>
                        <div class="output-box">
                            <pre>Conv2d          output shape: torch.Size([1, 6, 28, 28])
Sigmoid         output shape: torch.Size([1, 6, 28, 28])
AvgPool2d       output shape: torch.Size([1, 6, 14, 14])
Conv2d          output shape: torch.Size([1, 16, 10, 10])
Sigmoid         output shape: torch.Size([1, 16, 10, 10])
AvgPool2d       output shape: torch.Size([1, 16, 5, 5])
Flatten         output shape: torch.Size([1, 400])
Linear          output shape: torch.Size([1, 120])
Sigmoid         output shape: torch.Size([1, 120])
Linear          output shape: torch.Size([1, 84])
Sigmoid         output shape: torch.Size([1, 84])
Linear          output shape: torch.Size([1, 10])</pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Implementation</h2>
                    <div data-mcq='{
                        "question": "What is the total number of parameters in the first convolutional layer (Conv1)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "150 (6 × 5 × 5)",
                                "correct": false,
                                "explanation": "You forgot the bias terms! Each of the 6 filters has a bias."
                            },
                            {
                                "text": "156 (6 × 5 × 5 × 1 + 6)",
                                "correct": true,
                                "explanation": "Correct! 6 filters × (5×5 weights + 1 bias) = 6 × 26 = 156 parameters."
                            },
                            {
                                "text": "750 (6 × 5 × 5 × 5)",
                                "correct": false,
                                "explanation": "The input has 1 channel, not 5. Formula: out_channels × (kernel_size² × in_channels + 1)."
                            },
                            {
                                "text": "25 (5 × 5)",
                                "correct": false,
                                "explanation": "This is just one kernel. Conv1 has 6 filters, each with 5×5 weights plus bias."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Training Section (Vertical) -->
                <section>
                    <h2 class="truncate-title">Training LeNet on Fashion-MNIST</h2>
                    <pre><code class="language-python">import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Data preparation
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.5,), (0.5,))
])

trainset = datasets.FashionMNIST(
root='./data', train=True, 
download=True, transform=transform
)
trainloader = DataLoader(
trainset, batch_size=128, 
shuffle=True, num_workers=2
)

# Training setup
model = LeNet(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training parameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
num_epochs = 10</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Training Loop</h2>
                    <pre><code class="language-python">def train_epoch(model, dataloader, criterion, optimizer, device):
model.train()
running_loss = 0.0
correct = 0
total = 0

for inputs, labels in dataloader:
    inputs, labels = inputs.to(device), labels.to(device)
    
    # Zero gradients
    optimizer.zero_grad()
    
    # Forward pass
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    # Backward pass and optimize
    loss.backward()
    optimizer.step()
    
    # Statistics
    running_loss += loss.item()
    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += predicted.eq(labels).sum().item()

epoch_loss = running_loss / len(dataloader)
accuracy = 100. * correct / total
return epoch_loss, accuracy</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Comparison with Fully Connected Networks</h2>
                    <table style="font-size: 0.6em;">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Parameters</th>
                                <th>Fashion-MNIST Accuracy</th>
                                <th>Key Advantage</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Linear Model</td>
                                <td>7,850</td>
                                <td>~83%</td>
                                <td>Simple, fast</td>
                            </tr>
                            <tr>
                                <td>MLP (256-128)</td>
                                <td>235,146</td>
                                <td>~87%</td>
                                <td>Non-linear</td>
                            </tr>
                            <tr>
                                <td><strong>LeNet-5</strong></td>
                                <td><strong>44,426</strong></td>
                                <td><strong>~89%</strong></td>
                                <td><strong>Spatial awareness</strong></td>
                            </tr>
                            <tr>
                                <td>Modern CNN (ResNet)</td>
                                <td>11M+</td>
                                <td>~94%</td>
                                <td>State-of-the-art</td>
                            </tr>
                        </tbody>
                    </table>
                    <div class="emphasis-box mt-md">
                        <p>LeNet achieves better accuracy with <strong>5× fewer parameters</strong> than MLP!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Training</h2>
                    <div data-mcq='{
                        "question": "Why does LeNet perform better than MLPs despite having fewer parameters?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Convolutional layers exploit spatial locality in images",
                                "correct": true,
                                "explanation": "Yes! CNNs use local receptive fields that match the spatial structure of images."
                            },
                            {
                                "text": "Weight sharing reduces overfitting",
                                "correct": true,
                                "explanation": "Correct! Sharing weights across spatial locations acts as regularization."
                            },
                            {
                                "text": "Pooling layers provide translation invariance",
                                "correct": true,
                                "explanation": "Yes! Pooling makes features more robust to small shifts in position."
                            },
                            {
                                "text": "LeNet uses better optimization algorithms",
                                "correct": false,
                                "explanation": "Both models can use the same optimizers (SGD, Adam, etc.)."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Modern Improvements Section (Vertical) -->
                <section>
                    <h2 class="truncate-title">Modernizing LeNet</h2>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; font-size: 0.6em;">
                        <div>
                            <h3>Original LeNet (1998)</h3>
                            <ul>
                                <li>Sigmoid activation</li>
                                <li>Average pooling</li>
                                <li>Tanh output layer</li>
                                <li>Custom loss function</li>
                            </ul>
                        </div>
                        <div>
                            <h3>Modern Version</h3>
                            <ul>
                                <li><span class="tooltip">ReLU<span class="tooltiptext">f(x) = max(0, x), faster and avoids vanishing gradients</span></span> activation</li>
                                <li>Max pooling</li>
                                <li>Softmax + CrossEntropy</li>
                                <li><span class="tooltip">Batch Normalization<span class="tooltiptext">Normalizes inputs to each layer, accelerating training</span></span></li>
                            </ul>
                        </div>
                    </div>
                    <pre><code class="language-python">class ModernLeNet(nn.Module):
def __init__(self, num_classes=10):
    super().__init__()
    self.features = nn.Sequential(
        nn.Conv2d(1, 6, kernel_size=5, padding=2),
        nn.BatchNorm2d(6),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        nn.Conv2d(6, 16, kernel_size=5),
        nn.BatchNorm2d(16),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Architectural Experiments</h2>
                    <div class="demo-container" style="font-size: 0.6em;">
                        <h3>Try These Modifications:</h3>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                            <div>
                                <h4>1. Kernel Size</h4>
                                <pre><code class="language-python"># Try 3×3 kernels (modern standard)
nn.Conv2d(1, 32, kernel_size=3, padding=1)</code></pre>
                                <p>Effect: More layers needed, but more expressive</p>
                            </div>
                            <div>
                                <h4>2. More Channels</h4>
                                <pre><code class="language-python"># Increase channel depth
nn.Conv2d(1, 32, kernel_size=5)
nn.Conv2d(32, 64, kernel_size=5)</code></pre>
                                <p>Effect: Better feature extraction, more parameters</p>
                            </div>
                            <div>
                                <h4>3. Dropout</h4>
                                <pre><code class="language-python"># Add dropout for regularization
nn.Dropout(0.5)</code></pre>
                                <p>Effect: Reduces overfitting</p>
                            </div>
                            <div>
                                <h4>4. Global Average Pooling</h4>
                                <pre><code class="language-python"># Replace FC layers
nn.AdaptiveAvgPool2d((1, 1))</code></pre>
                                <p>Effect: Fewer parameters, more robust</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">LeNet's Legacy</h2>
                    <div class="timeline-container" style="font-size: 0.6em;">
                        <ul style="list-style: none; padding: 0;">
                            <li><strong>1998:</strong> LeNet-5 published</li>
                            <li><strong>2012:</strong> AlexNet wins ImageNet (deeper LeNet-style architecture)</li>
                            <li><strong>2014:</strong> VGGNet (very deep, uniform architecture)</li>
                            <li><strong>2015:</strong> ResNet (skip connections enable 100+ layers)</li>
                            <li><strong>Today:</strong> Vision Transformers challenge CNN dominance</li>
                        </ul>
                    </div>
                    <div class="emphasis-box mt-lg">
                        <p><strong>Core Ideas Still Used Today:</strong></p>
                        <ul>
                            <li>Alternating convolution and pooling</li>
                            <li>Increasing channel depth</li>
                            <li>Decreasing spatial dimensions</li>
                            <li>End-to-end learning</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Modern Improvements</h2>
                    <div data-mcq='{
                        "question": "Which modern improvement has the MOST impact on LeNet performance?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Replacing sigmoid with ReLU activation",
                                "correct": true,
                                "explanation": "Correct! ReLU solves vanishing gradients and speeds up training dramatically."
                            },
                            {
                                "text": "Using color images instead of grayscale",
                                "correct": false,
                                "explanation": "Color can help but activation functions have a more fundamental impact on training."
                            },
                            {
                                "text": "Increasing the learning rate",
                                "correct": false,
                                "explanation": "Higher learning rates can cause instability; activation functions are more crucial."
                            },
                            {
                                "text": "Using CPU instead of GPU",
                                "correct": false,
                                "explanation": "GPUs speed up training but dont improve model performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Exercises Section (Single slide, no vertical) -->
            <section data-sources='[{"text": "LeNet Exercises from D2L", "url": "https://d2l.ai/chapter_convolutional-neural-networks/lenet.html#exercises"}]'>
                <h2 class="truncate-title">Exercises: Implement and Experiment</h2>
                <div style="text-align: left; max-width: 800px; margin: 0 auto;">
                    <h3>Implementation Challenges:</h3>
                    <ol>
                        <li><strong>Modernize LeNet:</strong>
                            <ul>
                                <li>Replace average pooling with max pooling</li>
                                <li>Use ReLU instead of sigmoid</li>
                                <li>Add batch normalization</li>
                                <li>Compare performance on Fashion-MNIST</li>
                            </ul>
                        </li>
                        <li><strong>Architecture Search:</strong>
                            <ul>
                                <li>Try different kernel sizes (3×3, 7×7)</li>
                                <li>Vary number of channels (8, 32, 64)</li>
                                <li>Add/remove convolutional layers</li>
                                <li>Plot accuracy vs. parameter count</li>
                            </ul>
                        </li>
                        <li><strong>Visualization Tasks:</strong>
                            <ul>
                                <li>Display first layer filters</li>
                                <li>Show activation maps for different inputs</li>
                                <li>Compare features: MNIST vs Fashion-MNIST</li>
                            </ul>
                        </li>
                        <li><strong>Advanced:</strong>
                            <ul>
                                <li>Implement <span class="tooltip">data augmentation<span class="tooltiptext">Random transformations (rotation, scaling) to increase training data diversity</span></span></li>
                                <li>Try different optimizers (Adam, RMSprop)</li>
                                <li>Add learning rate scheduling</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </section>

            </section>

        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7/dist/d3.min.js"></script>
    
    <!-- Shared resources -->
    <script src="../shared/js/multiple-choice.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/title-handler.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/d3-utils.js"></script>
    <script src="../shared/js/animation-lib.js"></script>
    
    <!-- CNN specific scripts -->
    <script src="js/convolution-demo.js"></script>
    <script src="js/invariance-viz.js"></script>
    <script src="js/locality-viz.js"></script>
    <script src="js/channels-viz.js"></script>
    <script src="js/parameter-comparison.js"></script>
    <script src="js/cross-correlation-calculator.js"></script>
    <script src="js/kernel-learning-demo.js"></script>
    <script src="js/edge-detection-demo.js"></script>
    <script src="js/receptive-field-calculator.js"></script>
    
    <!-- Padding and Stride scripts -->
    <script src="js/padding-stride-demo.js"></script>
    <script src="js/stride-animator.js"></script>
    <script src="js/padding-types-demo.js"></script>
    <script src="js/dilated-conv-demo.js"></script>
    
    <!-- Channel-related scripts -->
    <script src="js/multi-channel-conv-demo.js"></script>
    <script src="js/output-channels-viz.js"></script>
    <script src="js/channel-mixer-demo.js"></script>
    
    <!-- Pooling-related scripts -->
    <script src="js/pooling-demo.js"></script>
    <script src="js/pooling-calculator.js"></script>
    <script src="js/multi-channel-pooling.js"></script>
    <script src="js/pooling-types-comparison.js"></script>
    
    <!-- LeNet-related scripts -->
    <script src="js/lenet-performance-comparison.js"></script>
    <script src="js/mnist-samples.js"></script>
    
    <script>
        // Initialize Reveal.js
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes ]
        });
    </script>
</body>
</html>