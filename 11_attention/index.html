<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Mechanisms and Transformers - Introduction to Deep Learning</title>

    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    <link rel="stylesheet" href="css/attention-custom.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Attention Mechanisms and Transformers</h1>
                <p>Chapter 11: The Architecture Revolution</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>

            <!-- Single Vertical Section: Chapter Overview -->
            <section>
                <!-- Slide 1: The Architecture Landscape (Early Deep Learning) -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">The Early Deep Learning Boom: Architecture Landscape of the 2010s</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The Dominant Architectures</h4>
                            <p>The earliest years of deep learning were driven by three core architectures:</p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 20px 0; font-size: 0.85em;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">Multilayer Perceptrons</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Fully connected networks</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Convolutional Networks</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Dominated computer vision</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FFA05F; margin-top: 0;">Recurrent Networks</h5>
                                    <p style="font-size: 0.9em; margin: 0;">State-of-the-art in NLP</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0; font-size: 0.95em;">‚è∞ <strong>Remarkably little had changed:</strong> These architectures were recognizable as scaled-up versions of ideas from <strong>nearly 30 years earlier</strong></p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 2: Methodological Innovations -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">Innovations Within the Classic Framework</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <p>While core architectures remained stable, many new <strong>methodological innovations</strong> entered practitioners' toolkits:</p>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #10099F;">üîå <span class="tooltip">ReLU Activations<span class="tooltiptext">Rectified Linear Unit: f(x) = max(0, x). Solves vanishing gradient problem and speeds up training</span></span></h5>
                                    <p style="margin: 0; font-size: 0.9em;">Faster training, less vanishing gradients</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0;">üîó <span class="tooltip">Residual Layers<span class="tooltiptext">Skip connections that allow gradients to flow directly through networks, enabling much deeper models</span></span></h5>
                                    <p style="margin: 0; font-size: 0.9em;">Enabled very deep networks</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B;">üìä <span class="tooltip">Batch Normalization<span class="tooltiptext">Technique that normalizes layer inputs across mini-batches, stabilizing training and allowing higher learning rates</span></span></h5>
                                    <p style="margin: 0; font-size: 0.9em;">Stabilized training</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #FC8484;">üé≤ <span class="tooltip">Dropout<span class="tooltiptext">Regularization technique that randomly drops units during training to prevent overfitting</span></span></h5>
                                    <p style="margin: 0; font-size: 0.9em;">Reduced overfitting</p>
                                </div>
                            </div>
                            <div style="background: #F5F5F5; padding: 15px; border-radius: 8px; margin-top: 15px;">
                                <h5 style="color: #10099F;">üìà <span class="tooltip">Adaptive Learning Rate Schedules<span class="tooltiptext">Algorithms like Adam, RMSprop that automatically adjust learning rates for each parameter during training</span></span></h5>
                                <p style="margin: 0; font-size: 0.9em;">Improved optimization convergence</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 25px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0;"><strong>Key Point:</strong> Despite thousands of papers proposing alternatives, classical CNN and <span class="tooltip">LSTM<span class="tooltiptext">Long Short-Term Memory: A type of RNN with gating mechanisms that can learn long-term dependencies</span></span> architectures retained state-of-the-art status</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: The Drivers of Deep Learning Success -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">What Drove Deep Learning's Rapid Emergence?</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <p>The rapid emergence of deep learning appeared primarily attributable to two shifts:</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 25px;">
                                <div style="text-align: center;">
                                    <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 30px; border-radius: 12px; min-height: 200px; display: flex; flex-direction: column; justify-content: center;">
                                        <div style="font-size: 3em; margin-bottom: 15px;">üñ•Ô∏è</div>
                                        <h4 style="color: white; margin: 0 0 10px 0;">Computational Resources</h4>
                                        <p style="font-size: 0.9em; margin: 0;">Innovations in <strong>parallel computing with GPUs</strong> enabled training of much larger models</p>
                                    </div>
                                </div>
                                <div style="text-align: center;">
                                    <div style="background: linear-gradient(135deg, #2DD2C0 0%, #00FFBA 100%); color: #262626; padding: 30px; border-radius: 12px; min-height: 200px; display: flex; flex-direction: column; justify-content: center;">
                                        <div style="font-size: 3em; margin-bottom: 15px;">üíæ</div>
                                        <h4 style="margin: 0 0 10px 0;">Massive Data</h4>
                                        <p style="font-size: 0.9em; margin: 0;">Cheap storage and Internet services provided <strong>enormous datasets</strong> for training</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 30px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;">üí° While these factors remain primary drivers of increasing power, we are <strong>now witnessing a sea change</strong> in the landscape of dominant architectures</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #1 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What were the primary drivers of the rapid emergence of deep learning in the 2010s according to the chapter?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The invention of completely new neural network architectures",
                                "correct": false,
                                "explanation": "Actually, the core architectures (MLPs, CNNs, RNNs) had changed remarkably little from their antecedents of nearly 30 years earlier."
                            },
                            {
                                "text": "GPU-enabled parallel computing and availability of massive data",
                                "correct": true,
                                "explanation": "Correct! The text states that innovations in parallel computing with GPUs and availability of massive data resources (thanks to cheap storage and Internet services) were the primary drivers."
                            },
                            {
                                "text": "The development of ReLU activations and batch normalization",
                                "correct": false,
                                "explanation": "While these were important methodological innovations, they were enhancements to existing architectures, not the primary drivers of the emergence of deep learning."
                            },
                            {
                                "text": "The introduction of attention mechanisms in all models",
                                "correct": false,
                                "explanation": "Attention mechanisms came later. The early deep learning boom was driven by computational resources and data availability."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 4: The Current Landscape - Transformers Everywhere -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}, {"text": "Vaswani et al. (2017) - Attention is All You Need", "url": "https://arxiv.org/abs/1706.03762"}]'>
                    <h2 class="truncate-title">The Present Moment: Transformers Dominate Nearly Everything</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 20px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0;">üéØ The New Default in NLP</h4>
                                <p style="font-size: 0.95em; margin-bottom: 10px;">For <strong>nearly all natural language processing tasks</strong>, the dominant models are based on the <span class="tooltip" style="border-bottom-color: white;">Transformer architecture<span class="tooltiptext">An architecture based purely on attention mechanisms, without recurrent or convolutional layers</span></span></p>
                                <p style="font-size: 0.9em; margin: 0;"><strong>Default approach:</strong> Grab a large Transformer-based pretrained model, adapt output layers, and fine-tune on available data</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <h4 style="color: #10099F;">Popular Pretrained Transformers</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 3px solid #10099F;">
                                    <strong style="color: #10099F;">BERT</strong> <span style="font-size: 0.8em;">(Devlin et al., 2018)</span>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">Bidirectional Encoder Representations from Transformers</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 3px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">ELECTRA</strong> <span style="font-size: 0.8em;">(Clark et al., 2020)</span>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">Efficiently Learning an Encoder that Classifies Token Replacements Accurately</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px; border-left: 3px solid #FAC55B;">
                                    <strong style="color: #FFA05F;">RoBERTa</strong> <span style="font-size: 0.8em;">(Liu et al., 2019)</span>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">A Robustly Optimized BERT Pretraining Approach</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 12px; border-radius: 8px; border-left: 3px solid #FC8484;">
                                    <strong style="color: #FC8484;">Longformer</strong> <span style="font-size: 0.8em;">(Beltagy et al., 2020)</span>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">The Long-Document Transformer</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: GPT Models and Beyond NLP -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}, {"text": "Brown et al. (2020) - Language Models are Few-Shot Learners", "url": "https://arxiv.org/abs/2005.14165"}, {"text": "Radford et al. (2019) - GPT-2", "url": "https://openai.com/research/better-language-models"}]'>
                    <h2 class="truncate-title">Large Language Models and Transformers Beyond NLP</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #2DD2C0 0%, #00FFBA 100%); padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                                <h4 style="margin-top: 0;">ü§ñ <span class="tooltip">Large Language Models<span class="tooltiptext">Transformer models with billions of parameters trained on massive text corpora, capable of few-shot learning</span></span> (LLMs)</h4>
                                <p style="font-size: 0.95em; margin: 0;">OpenAI's GPT-2 and GPT-3 models (Generative Pretrained Transformers) have generated breathless news coverage due to their remarkable capabilities</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Transformers in Diverse Domains</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.85em;">
                                <div style="text-align: center; background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                    <div style="font-size: 2em; margin-bottom: 8px;">üëÅÔ∏è</div>
                                    <strong style="color: #10099F;">Computer Vision</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;"><span class="tooltip">Vision Transformers<span class="tooltiptext">Transformers adapted for image processing by treating image patches as tokens</span></span> for image recognition, object detection, segmentation, superresolution</p>
                                </div>
                                <div style="text-align: center; background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                    <div style="font-size: 2em; margin-bottom: 8px;">üé§</div>
                                    <strong style="color: #2DD2C0;">Speech Recognition</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">Competitive methods using Transformers (Gulati et al., 2020)</p>
                                </div>
                                <div style="text-align: center; background: #FFFBF0; padding: 15px; border-radius: 8px;">
                                    <div style="font-size: 2em; margin-bottom: 8px;">üéÆ</div>
                                    <strong style="color: #FFA05F;">Reinforcement Learning</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">Decision Transformers (Chen et al., 2021)</p>
                                </div>
                                <div style="text-align: center; background: #FFF5F5; padding: 15px; border-radius: 8px;">
                                    <div style="font-size: 2em; margin-bottom: 8px;">üï∏Ô∏è</div>
                                    <strong style="color: #FC8484;">Graph Neural Networks</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">Generalization to graphs (Dwivedi & Bresson, 2020)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #2 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In which domains have Transformers shown success beyond natural language processing?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Computer vision (image recognition, object detection, segmentation)",
                                "correct": true,
                                "explanation": "Correct! Vision Transformers have emerged as a default model for diverse vision tasks."
                            },
                            {
                                "text": "Speech recognition",
                                "correct": true,
                                "explanation": "Correct! Transformers have shown up as competitive methods for speech recognition."
                            },
                            {
                                "text": "Reinforcement learning",
                                "correct": true,
                                "explanation": "Correct! Decision Transformers have been applied to reinforcement learning tasks."
                            },
                            {
                                "text": "Graph neural networks",
                                "correct": true,
                                "explanation": "Correct! Transformers have been generalized to work with graph-structured data."
                            },
                            {
                                "text": "Only natural language processing tasks",
                                "correct": false,
                                "explanation": "Incorrect. While Transformers dominate NLP, they have successfully expanded to many other domains as listed above."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 6: The Core Idea - Attention -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}, {"text": "Bahdanau et al. (2014) - Neural Machine Translation", "url": "https://arxiv.org/abs/1409.0473"}]'>
                    <h2 class="truncate-title">The Core Innovation: The Attention Mechanism</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 25px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0;">üí° What is Attention?</h4>
                                <p style="font-size: 0.95em; margin: 0;">The <strong>attention mechanism</strong> is the core idea behind the Transformer model. It was originally envisioned as an <strong>enhancement for encoder-decoder RNNs</strong> applied to <span class="tooltip" style="border-bottom-color: white;">sequence-to-sequence<span class="tooltiptext">Models that transform an input sequence (e.g., English sentence) into an output sequence (e.g., French translation)</span></span> applications like machine translation</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <h4 style="color: #10099F;">The Original Problem (Sutskever et al., 2014)</h4>
                            <div style="background: #FFF5F5; padding: 20px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <p style="font-size: 0.95em; margin-bottom: 15px;">In the first sequence-to-sequence models for machine translation:</p>
                                <div style="text-align: center; padding: 15px; background: white; border-radius: 8px; margin: 10px 0;">
                                    <strong style="color: #FC8484;">Encoder</strong> ‚Üí
                                    <span style="background: #FC8484; color: white; padding: 5px 15px; border-radius: 5px; margin: 0 10px;">Single Fixed-Length Vector</span> ‚Üí
                                    <strong style="color: #FC8484;">Decoder</strong>
                                </div>
                                <p style="font-size: 0.9em; margin: 10px 0 0 0;"><strong>Problem:</strong> The entire input sequence was compressed by the encoder into a <strong>single fixed-length vector</strong> ‚Äî an information bottleneck!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: The Attention Intuition -->
                <section data-sources='[{"text": "Bahdanau et al. (2014) - Neural Machine Translation", "url": "https://arxiv.org/abs/1409.0473"}]'>
                    <h2 class="truncate-title">The Intuition Behind Attention</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <h4 style="color: #10099F; margin: 0 0 8px 0;">Key Insights</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin: 0 0 8px 0;">
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">1Ô∏è‚É£ Don't Compress</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Rather than compressing the input, the decoder should <strong>revisit the input sequence at every step</strong></p>
                                </div>
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">2Ô∏è‚É£ Selective Focus</h5>
                                    <p style="font-size: 0.9em; margin: 0;">The decoder should <strong>selectively focus</strong> on particular parts of the input sequence at particular decoding steps</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 8px;">
                            <h4 style="color: #10099F; margin: 0 0 8px 0;">How It Works</h4>
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="font-size: 0.9em; margin-bottom: 6px;"><strong>Encoder produces:</strong> A representation of length equal to the original input sequence (not compressed!)</p>
                                <p style="font-size: 0.9em; margin-bottom: 6px;"><strong>At decoding time:</strong> The decoder receives a <span class="tooltip">context vector<span class="tooltiptext">A weighted sum of encoder representations, where weights determine how much to focus on each input position</span></span> consisting of a <strong>weighted sum</strong> of the representations at each time step</p>
                                <p style="font-size: 0.9em; margin: 0;"><strong>The weights:</strong> Determine how much each step's context "focuses" on each input token ‚Äî and they are <strong>differentiable</strong> so they can be learned!</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 6px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 12px;">
                            <p style="margin: 0; font-size: 0.9em;">üí° The decoder <strong>dynamically attends</strong> to different parts of the input, rather than being stuck with a single fixed representation</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: Early Success of Attention -->
                <section data-sources='[{"text": "Bahdanau et al. (2014) - Neural Machine Translation", "url": "https://arxiv.org/abs/1409.0473"}]'>
                    <h2 class="truncate-title">Early Success: Attention as an Enhancement to RNNs</h2>
                    <div style="font-size: 0.67em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Immediate Benefits</h4>
                            <div style="background: #F0FFF9; padding: 15px; border-radius: 10px; border-left: 4px solid #2DD2C0; margin-bottom: 15px;">
                                <p style="font-size: 0.95em; margin: 0;"><strong>‚úì Better Performance:</strong> Attention-enhanced RNNs performed better than the original encoder-decoder sequence-to-sequence architectures</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Qualitative Insights from Attention Weights</h4>
                            <p style="font-size: 0.95em;">Researchers discovered that nice insights sometimes emerged from inspecting the pattern of attention weights:</p>
                            <div style="background: #FFFBF0; padding: 15px; border-radius: 10px; margin-top: 12px;">
                                <h5 style="color: #FFA05F; margin-top: 0;">Example: English ‚Üí French Translation</h5>
                                <div style="text-align: center; padding: 15px; background: white; border-radius: 8px; margin: 12px 0;">
                                    <div style="font-size: 1.2em; margin-bottom: 15px;">
                                        <span style="color: #10099F;">"my <strong style="background: #2DD2C0; color: white; padding: 3px 8px; border-radius: 4px;">feet</strong> hurt"</span>
                                    </div>
                                    <div style="font-size: 1.5em; margin: 10px 0;">‚Üì</div>
                                    <div style="font-size: 1.2em;">
                                        <span style="color: #FC8484;">"j'ai mal au <strong style="background: #2DD2C0; color: white; padding: 3px 8px; border-radius: 4px;">pieds</strong>"</span>
                                    </div>
                                </div>
                                <p style="font-size: 0.9em; margin: 10px 0 0 0;">When generating the French word "pieds" (feet), the neural network assigned <strong>high attention weights</strong> to the representation of "feet" in the English sentence</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 9: Interpretability Claims -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">Attention and Interpretability: A Cautionary Note</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <p style="font-size: 0.95em; margin-bottom: 12px;"><strong>The Observation:</strong> Attention models often assigned high attention weights to cross-lingual synonyms when generating corresponding words in the target language</p>
                                <p style="font-size: 0.95em; margin: 0;">This led to claims that attention mechanisms confer <strong>"interpretability"</strong></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #FFFBF0; padding: 25px; border-radius: 10px; border-left: 4px solid #FAC55B;">
                                <h4 style="color: #FFA05F; margin-top: 0;">‚ö†Ô∏è Important Caveat</h4>
                                <p style="font-size: 0.95em; margin-bottom: 12px;">What precisely the attention weights <strong>mean</strong> ‚Äî i.e., how, if at all, they should be <em>interpreted</em> ‚Äî remains a <strong>hazy research topic</strong></p>
                                <div class="emphasis-box" style="background: white; border: 2px solid #FAC55B; padding: 15px; margin-top: 15px;">
                                    <p style="margin: 0; font-size: 0.9em; color: #262626;">üí≠ <strong>Open Question:</strong> Can we truly interpret attention weights as explanations for model behavior, or are they simply learned features that happen to correlate with our intuitions?</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #3 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What was the key problem with the original encoder-decoder sequence-to-sequence models that attention was designed to solve?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The models were too slow to train",
                                "correct": false,
                                "explanation": "Speed was not the primary issue. The problem was the information bottleneck created by compressing the entire input into a single fixed-length vector."
                            },
                            {
                                "text": "The entire input was compressed into a single fixed-length vector, creating an information bottleneck",
                                "correct": true,
                                "explanation": "Correct! Attention allows the decoder to revisit the entire input sequence and selectively focus on different parts at each decoding step, rather than relying on a single compressed representation."
                            },
                            {
                                "text": "The models could not handle variable-length input sequences",
                                "correct": false,
                                "explanation": "RNNs can handle variable-length sequences. The problem was that they compressed all the information into a fixed-length vector regardless of input length."
                            },
                            {
                                "text": "The models lacked interpretability",
                                "correct": false,
                                "explanation": "While attention did provide some qualitative insights, interpretability was not the primary problem being solved. The main issue was the information bottleneck."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 10: Attention Becomes More Significant -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}, {"text": "Vaswani et al. (2017) - Attention is All You Need", "url": "https://arxiv.org/abs/1706.03762"}]'>
                    <h2 class="truncate-title">Attention Mechanisms: Beyond Enhancement to Architecture</h2>
                    <div style="font-size: 0.66em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 18px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0;">üöÄ A Paradigm Shift</h4>
                                <p style="font-size: 0.95em; margin: 0;">Attention mechanisms soon emerged as <strong>more significant concerns</strong> beyond their usefulness as an enhancement for encoder-decoder RNNs</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4 style="color: #10099F;">The Transformer (Vaswani et al., 2017)</h4>
                            <div style="background: #F5F5FF; padding: 18px; border-radius: 10px; border-left: 4px solid #10099F;">
                                <h5 style="color: #10099F; margin-top: 0;">Paper Title: <em>"Attention is All You Need"</em></h5>
                                <p style="font-size: 0.95em; margin-bottom: 12px;"><strong>Original Application:</strong> Machine translation</p>
                                <div style="background: white; padding: 15px; border-radius: 8px; margin: 12px 0;">
                                    <p style="font-size: 0.95em; margin-bottom: 12px;"><strong>Revolutionary Idea:</strong></p>
                                    <ul style="font-size: 0.9em; margin: 0; text-align: left;">
                                        <li style="margin-bottom: 8px;"><strong>Dispensed with recurrent connections altogether</strong></li>
                                        <li style="margin-bottom: 8px;">Instead relied on <strong>cleverly arranged attention mechanisms</strong> to capture all relationships among input and output tokens</li>
                                        <li>No RNNs, no convolutions ‚Äî just attention!</li>
                                    </ul>
                                </div>
                                <div class="emphasis-box" style="background: #2DD2C0; color: white; padding: 15px; border-radius: 8px;">
                                    <p style="margin: 0; font-size: 0.95em;">‚ú® <strong>Result:</strong> The architecture performed remarkably well</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: The Transformer Takes Over -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">The Rapid Ascendance of Transformers (2018 Onwards)</h2>
                    <div style="font-size: 0.72em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #2DD2C0 0%, #00FFBA 100%); padding: 25px; border-radius: 10px; margin-bottom: 25px;">
                                <h4 style="margin-top: 0;">üìÖ Timeline</h4>
                                <p style="font-size: 0.95em; margin: 0;"><strong>By 2018:</strong> The Transformer began showing up in the <strong>majority of state-of-the-art</strong> natural language processing systems</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">What Changed?</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ Architecture</h5>
                                    <p style="font-size: 0.95em; margin: 0;">Transformers replaced traditional RNN and CNN architectures</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ Training Paradigm</h5>
                                    <p style="font-size: 0.95em; margin: 0;">Dominant practice became large-scale pretraining + fine-tuning</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 25px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0; font-size: 0.95em;">üí° <strong>Key Insight:</strong> The gap between Transformers and traditional architectures grew <strong>especially wide</strong> when applied in the pretraining paradigm</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: The Pretraining Paradigm -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">The Pretraining Paradigm in Natural Language Processing</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The New Dominant Practice</h4>
                            <div style="background: #F5F5FF; padding: 25px; border-radius: 10px; border-left: 4px solid #10099F; margin-bottom: 25px;">
                                <div style="display: flex; align-items: center; gap: 20px;">
                                    <div style="flex: 1;">
                                        <h5 style="color: #10099F; margin-top: 0;">Step 1: Pretrain</h5>
                                        <p style="font-size: 0.95em; margin: 0;">Train <strong>large-scale models</strong> on enormous generic background corpora to optimize some <span class="tooltip">self-supervised pretraining objective<span class="tooltiptext">Learning tasks that don't require labeled data, like predicting masked words or next sentences</span></span></p>
                                    </div>
                                    <div style="font-size: 3em; color: #10099F;">‚Üí</div>
                                    <div style="flex: 1;">
                                        <h5 style="color: #2DD2C0; margin-top: 0;">Step 2: Fine-tune</h5>
                                        <p style="font-size: 0.95em; margin: 0;">Adapt these models using the available <strong>downstream data</strong> for specific tasks</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Why This Approach Works So Well</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 3px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">Massive Scale</strong>
                                    <p style="margin: 5px 0 0 0;">Models learn rich representations from huge amounts of text</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 3px solid #FAC55B;">
                                    <strong style="color: #FFA05F;">Transfer Learning</strong>
                                    <p style="margin: 5px 0 0 0;">Pretrained knowledge transfers to downstream tasks</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFF5F5; border-left: 3px solid #FC8484;">
                                    <strong style="color: #FC8484;">Data Efficiency</strong>
                                    <p style="margin: 5px 0 0 0;">Less labeled data needed for specific tasks</p>
                                </div>
                                <div class="emphasis-box" style="background: #F5F5F5; border-left: 3px solid #262626;">
                                    <strong style="color: #262626;">General Knowledge</strong>
                                    <p style="margin: 5px 0 0 0;">Models capture broad linguistic patterns</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Foundation Models -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}, {"text": "Bommasani et al. (2021) - On the Opportunities and Risks of Foundation Models", "url": "https://arxiv.org/abs/2108.07258"}]'>
                    <h2 class="truncate-title">The Emergence of Foundation Models</h2>
                    <div style="font-size: 0.66em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 18px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0;">üèõÔ∏è What are <span class="tooltip" style="border-bottom-color: white;">Foundation Models<span class="tooltiptext">Large-scale pretrained models that serve as a foundation for many downstream applications</span></span>?</h4>
                                <p style="font-size: 0.95em; margin: 0;">Large-scale pretrained models that serve as a <strong>foundation</strong> for many downstream applications are now sometimes called <strong>foundation models</strong> (Bommasani et al., 2021)</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4 style="color: #10099F;">The Convergence of Two Trends</h4>
                            <div style="background: #FFFBF0; padding: 18px; border-radius: 10px; border-left: 4px solid #FAC55B;">
                                <p style="font-size: 0.95em; margin-bottom: 12px;">The <strong>ascendance of Transformers</strong> coincided with the ascendance of <strong>large-scale pretrained models</strong></p>
                                <div style="background: white; padding: 15px; border-radius: 8px; margin-top: 12px;">
                                    <p style="font-size: 0.95em; margin-bottom: 10px;"><strong>Result:</strong> The gap between Transformers and traditional architectures grew <strong>especially wide</strong> in the pretraining paradigm</p>
                                    <ul style="font-size: 0.9em; margin: 10px 0 0 20px; text-align: left;">
                                        <li style="margin-bottom: 8px;">Transformers are particularly well-suited for large-scale pretraining</li>
                                        <li style="margin-bottom: 8px;">They can process sequences in parallel (unlike RNNs)</li>
                                        <li>They capture long-range dependencies effectively</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #4 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What was the key architectural innovation of the Transformer (Vaswani et al., 2017)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Adding attention mechanisms to existing RNN architectures",
                                "correct": false,
                                "explanation": "This describes the earlier Bahdanau attention mechanism. The Transformer went further by eliminating RNNs entirely."
                            },
                            {
                                "text": "Dispensing with recurrent connections and relying solely on attention mechanisms",
                                "correct": true,
                                "explanation": "Correct! The Transformer dispensed with recurrent connections altogether and relied purely on cleverly arranged attention mechanisms to capture all relationships. Hence the paper title: \"Attention is All You Need\"."
                            },
                            {
                                "text": "Using convolutional layers instead of recurrent layers",
                                "correct": false,
                                "explanation": "The Transformer uses neither convolutional nor recurrent layers - it relies purely on attention mechanisms."
                            },
                            {
                                "text": "Introducing the pretraining and fine-tuning paradigm",
                                "correct": false,
                                "explanation": "While Transformers work well with pretraining, the pretraining paradigm was not the architectural innovation of the original Transformer paper."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 14: Chapter Roadmap -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"}]'>
                    <h2 class="truncate-title">What We'll Cover in This Chapter</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 1em; margin-bottom: 25px;">This chapter will introduce attention models, starting from basics and building up to state-of-the-art architectures:</p>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr; gap: 12px;">
                                <div style="background: linear-gradient(to right, #F5F5FF, white); padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <div style="display: flex; align-items: center; gap: 12px;">
                                        <div style="font-size: 1.8em; color: #10099F;">1Ô∏è‚É£</div>
                                        <div>
                                            <strong style="color: #10099F; font-size: 1.05em;">Basic Intuitions</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.9em;">The most basic intuitions and simplest instantiations of the attention idea</p>
                                        </div>
                                    </div>
                                </div>
                                <div style="background: linear-gradient(to right, #F0FFF9, white); padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <div style="display: flex; align-items: center; gap: 12px;">
                                        <div style="font-size: 1.8em; color: #2DD2C0;">2Ô∏è‚É£</div>
                                        <div>
                                            <strong style="color: #2DD2C0; font-size: 1.05em;">The Transformer Architecture</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.9em;">Building up to the full Transformer model for sequence-to-sequence tasks</p>
                                        </div>
                                    </div>
                                </div>
                                <div style="background: linear-gradient(to right, #FFFBF0, white); padding: 15px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <div style="display: flex; align-items: center; gap: 12px;">
                                        <div style="font-size: 1.8em; color: #FFA05F;">3Ô∏è‚É£</div>
                                        <div>
                                            <strong style="color: #FFA05F; font-size: 1.05em;">Vision Transformer</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.9em;">How Transformers were adapted for computer vision tasks</p>
                                        </div>
                                    </div>
                                </div>
                                <div style="background: linear-gradient(to right, #FFF5F5, white); padding: 15px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                    <div style="display: flex; align-items: center; gap: 12px;">
                                        <div style="font-size: 1.8em; color: #FC8484;">4Ô∏è‚É£</div>
                                        <div>
                                            <strong style="color: #FC8484; font-size: 1.05em;">Modern Pretrained Models</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.9em;">The landscape of modern Transformer-based pretrained models and foundation models</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 30px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 20px;">
                            <p style="margin: 0; font-size: 0.95em;">üöÄ <strong>Let's begin the journey</strong> from basic attention mechanisms to the architectures powering modern AI!</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #5 - Final Comprehensive Question -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statement best describes the relationship between the ascendance of Transformers and the pretraining paradigm?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Transformers were invented specifically for the pretraining paradigm",
                                "correct": false,
                                "explanation": "The Transformer was originally designed for machine translation. The pretraining paradigm emerged as a separate trend that happened to work especially well with Transformers."
                            },
                            {
                                "text": "The gap between Transformers and traditional architectures grew especially wide when applied in the pretraining paradigm",
                                "correct": true,
                                "explanation": "Correct! The text states that the ascendance of Transformers coincided with the ascendance of large-scale pretrained models, and the gap between Transformers and traditional architectures grew especially wide in this pretraining context."
                            },
                            {
                                "text": "Traditional RNN architectures perform better than Transformers in the pretraining paradigm",
                                "correct": false,
                                "explanation": "Incorrect. Transformers significantly outperform traditional architectures in the pretraining paradigm, which is why they have become dominant."
                            },
                            {
                                "text": "Pretraining is only useful for NLP tasks, not for computer vision",
                                "correct": false,
                                "explanation": "Incorrect. The chapter mentions that Vision Transformers have also emerged as default models for diverse vision tasks, showing that pretraining works across domains."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Vertical Section 2: Queries, Keys, and Values -->
            <section>
                <!-- Slide 1: Title Slide -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1: Queries, Keys, and Values", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h1 class="truncate-title">11.1 Queries, Keys, and Values</h1>
                    <p>The Foundation of Attention Mechanisms</p>
                    <p class="mt-lg">
                        <small>Building blocks for understanding Transformers</small>
                    </p>
                </section>

                <!-- Slide 2: The Fixed-Size Input Problem -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">The Challenge of Fixed-Size Inputs</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484;">
                                <h4 style="color: #FC8484; margin-top: 0;">‚ö†Ô∏è The Problem</h4>
                                <p style="font-size: 0.95em; margin: 0;">All networks we've reviewed so far crucially relied on input being of a <strong>well-defined size</strong></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <h4 style="color: #10099F;">Examples of Fixed-Size Constraints</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 3px solid #10099F;">
                                    <strong style="color: #10099F;">CNNs</strong>
                                    <p style="margin: 8px 0 0 0;">ImageNet images: <code style="background: #EEEEEE; padding: 2px 6px; border-radius: 3px;">224 √ó 224</code> pixels<br>
                                    Networks specifically tuned to this size</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 3px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">RNNs</strong>
                                    <p style="margin: 8px 0 0 0;">Process one token at a time sequentially<br>
                                    Fixed size handled per step</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 3px solid #FAC55B;">
                                    <strong style="color: #FFA05F;">Seq2Seq</strong>
                                    <p style="margin: 8px 0 0 0;">Entire input compressed into <strong>single fixed-length vector</strong><br>
                                    Information bottleneck!</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 25px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0; font-size: 0.95em;">üí° For truly <strong>variable-size inputs with varying information content</strong> (like long text sequences), this becomes a significant problem</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Database Analogy - Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Database Analogy: A Different Approach</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 25px; border-radius: 10px; margin-bottom: 25px;">
                                <h4 style="color: white; margin-top: 0;">üíæ Think About Databases</h4>
                                <p style="font-size: 0.95em; margin: 0;">In their simplest form, databases are collections of <strong>keys</strong> (\(k\)) and <strong>values</strong> (\(v\))</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Example Database \(\mathcal{D}\)</h4>
                            <div style="background: white; padding: 20px; border-radius: 10px; border: 2px solid #EEEEEE; margin: 15px 0;">
                                <table style="width: 100%; border-collapse: collapse; font-size: 0.95em;">
                                    <thead style="background: #10099F; color: white;">
                                        <tr>
                                            <th style="padding: 12px; text-align: left; border: 1px solid #EEEEEE;">Key (Last Name)</th>
                                            <th style="padding: 12px; text-align: left; border: 1px solid #EEEEEE;">Value (First Name)</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr style="background: #F5F5FF;">
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Zhang"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Aston"</code></td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Lipton"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Zachary"</code></td>
                                        </tr>
                                        <tr style="background: #F5F5FF;">
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Li"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Mu"</code></td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Smola"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Alex"</code></td>
                                        </tr>
                                        <tr style="background: #F5F5FF;">
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Hu"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Rachel"</code></td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Werness"</code></td>
                                            <td style="padding: 10px; border: 1px solid #EEEEEE;"><code>"Brent"</code></td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Database Operations -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Querying the Database</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 8px;">Exact Query Example</h4>
                            <div style="background: #F0FFF9; padding: 12px; border-radius: 10px; border-left: 4px solid #2DD2C0; margin-bottom: 12px;">
                                <div style="text-align: center; font-size: 1.1em;">
                                    <p style="margin: 0 0 10px 0;"><strong>Query \(q\):</strong> <code style="background: #2DD2C0; color: white; padding: 5px 15px; border-radius: 5px;">"Li"</code></p>
                                    <div style="font-size: 1.5em; margin: 8px 0;">‚Üì</div>
                                    <p style="margin: 0;"><strong>Result:</strong> <code style="background: #00FFBA; padding: 5px 15px; border-radius: 5px;">"Mu"</code></p>
                                </div>
                                <p style="margin: 10px 0 0 0; font-size: 0.9em;">‚úì The query returns the value associated with the matching key</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 8px;">Approximate Match</h4>
                            <div style="background: #FFFBF0; padding: 12px; border-radius: 10px; border-left: 4px solid #FAC55B;">
                                <p style="font-size: 0.95em; margin-bottom: 10px;">If <code>("Li", "Mu")</code> was <strong>not</strong> in \(\mathcal{D}\), there would be no exact match</p>
                                <div style="background: white; padding: 12px; border-radius: 8px;">
                                    <p style="margin: 0;"><strong>With approximate matching:</strong> We could retrieve <code>("Lipton", "Zachary")</code> as a similar key</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #F5F5FF; border-left: 4px solid #10099F; padding: 12px;">
                            <p style="margin: 0;">üí° This simple example teaches us fundamental principles that carry over to attention mechanisms</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: Key Insights from Database Analogy -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Key Insights from the Database Analogy</h2>
                    <div style="font-size: 0.62em;">
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 12px;">
                                    <h5 style="color: #10099F; margin-top: 0; margin-bottom: 8px;">1Ô∏è‚É£ Scale Independence</h5>
                                    <p style="font-size: 0.95em; margin: 0;">We can design queries \(q\) that operate on \((k, v)\) pairs in a manner that is <strong>valid regardless of database size</strong></p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 12px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0; margin-bottom: 8px;">2Ô∏è‚É£ Content-Dependent</h5>
                                    <p style="font-size: 0.95em; margin: 0;">The <strong>same query</strong> can receive <strong>different answers</strong> according to the contents of the database</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 12px;">
                                    <h5 style="color: #FFA05F; margin-top: 0; margin-bottom: 8px;">3Ô∏è‚É£ Simple Operations</h5>
                                    <p style="font-size: 0.95em; margin: 0;">The "code" for operating on a <strong>large state space</strong> can be quite simple (exact match, approximate match, top-\(k\))</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 12px;">
                                    <h5 style="color: #FC8484; margin-top: 0; margin-bottom: 8px;">4Ô∏è‚É£ No Compression</h5>
                                    <p style="font-size: 0.95em; margin: 0;">There is <strong>no need to compress or simplify</strong> the database to make operations effective</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0; margin-bottom: 8px;">üéØ The Connection to Attention</h4>
                                <p style="font-size: 0.95em; margin: 0;">These database principles lead directly to the <strong>attention mechanism</strong> ‚Äî one of the most exciting concepts in deep learning of the past decade!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #1 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is a key advantage of the database approach compared to compressing inputs into a fixed-length vector?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Databases are faster to query than neural networks",
                                "correct": false,
                                "explanation": "Speed is not the primary advantage being discussed. The focus is on handling variable-size data without information loss."
                            },
                            {
                                "text": "Operations can work regardless of database size without compression or simplification",
                                "correct": true,
                                "explanation": "Correct! The database analogy shows that we can design queries that operate on (key, value) pairs effectively without needing to compress the entire database into a single fixed-length representation."
                            },
                            {
                                "text": "Databases always provide exact matches for any query",
                                "correct": false,
                                "explanation": "Incorrect. Databases may not have exact matches, which is why approximate matching is mentioned. The advantage is the ability to query without compression."
                            },
                            {
                                "text": "All queries return the same result for a given database",
                                "correct": false,
                                "explanation": "Incorrect. Different queries can return different results based on the database contents - this is actually one of the advantages mentioned."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 6: Attention Mechanism - Formal Definition -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}, {"text": "Bahdanau et al. (2014) - Neural Machine Translation", "url": "https://arxiv.org/abs/1409.0473"}]'>
                    <h2 class="truncate-title">Attention Mechanism: Formal Definition</h2>
                    <div style="font-size: 0.72em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                                <h4 style="color: white; margin-top: 0;">üîë From Database to Attention</h4>
                                <p style="font-size: 0.95em; margin: 0;">The attention mechanism extends the database analogy to deep learning with <strong>differentiable operations</strong></p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Components</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 20px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 3px solid #10099F; text-align: center;">
                                    <strong style="color: #10099F; font-size: 1.1em;">Database \(\mathcal{D}\)</strong>
                                    <p style="margin: 10px 0 0 0;">Collection of \(m\) tuples:<br>
                                    \(\{(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)\}\)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 3px solid #2DD2C0; text-align: center;">
                                    <strong style="color: #2DD2C0; font-size: 1.1em;"><span class="tooltip">Keys<span class="tooltiptext">Vectors that are matched against the query to determine relevance</span></span> \(\mathbf{k}_i\)</strong>
                                    <p style="margin: 10px 0 0 0;">Vectors to be matched<br>against the query</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 3px solid #FAC55B; text-align: center;">
                                    <strong style="color: #FFA05F; font-size: 1.1em;"><span class="tooltip">Values<span class="tooltiptext">Vectors that are aggregated based on attention weights to form the output</span></span> \(\mathbf{v}_i\)</strong>
                                    <p style="margin: 10px 0 0 0;">Vectors to be<br>aggregated/retrieved</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="background: #FFF5F5; padding: 15px; border-radius: 8px; border-left: 3px solid #FC8484; text-align: center;">
                                <strong style="color: #FC8484; font-size: 1.1em;"><span class="tooltip">Query<span class="tooltiptext">A vector representing what information we are looking for</span></span> \(\mathbf{q}\)</strong>
                                <p style="margin: 10px 0 0 0;">Specifies what we're looking for in the database</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: The Attention Pooling Formula -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Attention Pooling: The Core Formula</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 15px;">
                                <h4 style="color: #10099F; margin-top: 0; margin-bottom: 10px;">üìê The <span class="tooltip">Attention Pooling<span class="tooltiptext">A weighted sum operation where weights determine how much to focus on each value based on query-key compatibility</span></span> Formula</h4>
                                <div style="text-align: center; font-size: 1.15em; margin: 12px 0;">
                                    $$\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i$$
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <h4 style="color: #10099F; margin-bottom: 8px;">Breaking It Down</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.9em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #2DD2C0;">\(\alpha(\mathbf{q}, \mathbf{k}_i) \in \mathbb{R}\)</strong>
                                    <p style="margin: 8px 0 0 0;">Scalar <strong>attention weights</strong> for \(i = 1, \ldots, m\)<br>
                                    Determines how much to focus on each key-value pair</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #FFA05F;">\(\sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i\)</strong>
                                    <p style="margin: 8px 0 0 0;"><strong>Weighted sum</strong> of values<br>
                                    Creates a linear combination of all values based on weights</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 12px;">
                            <p style="margin: 0;">üí° The attention mechanism <strong>pays particular attention</strong> to terms where \(\alpha\) is large ‚Äî hence the name "attention"!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: Four Types of Attention Weights -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Special Cases: Types of Attention Weights</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 1.05em; margin-bottom: 20px;">We have several important special cases depending on the properties of the weights \(\alpha(\mathbf{q}, \mathbf{k}_i)\):</p>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 15px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ Nonnegative Weights</h5>
                                    <div style="font-size: 0.95em;">
                                        <p style="margin: 0 0 8px 0;"><strong>Condition:</strong> \(\alpha(\mathbf{q}, \mathbf{k}_i) \geq 0\) for all \(i\)</p>
                                        <p style="margin: 0; background: white; padding: 10px; border-radius: 5px;"><strong>Result:</strong> Output is in the <span class="tooltip">convex cone<span class="tooltiptext">The set of all nonnegative linear combinations of the values</span></span> spanned by values \(\mathbf{v}_i\)</p>
                                    </div>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ <span class="tooltip">Convex Combination<span class="tooltiptext">Weights that are nonnegative and sum to 1, creating a weighted average</span></span> ‚≠ê</h5>
                                    <div style="font-size: 0.95em;">
                                        <p style="margin: 0 0 8px 0;"><strong>Conditions:</strong> \(\sum_i \alpha(\mathbf{q}, \mathbf{k}_i) = 1\) and \(\alpha(\mathbf{q}, \mathbf{k}_i) \geq 0\)</p>
                                        <p style="margin: 0; background: white; padding: 10px; border-radius: 5px;"><strong>Most common in deep learning!</strong></p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FFA05F; margin-top: 0;">3Ô∏è‚É£ Exact Match (Database Query)</h5>
                                    <div style="font-size: 0.95em;">
                                        <p style="margin: 0 0 8px 0;"><strong>Condition:</strong> Exactly one \(\alpha(\mathbf{q}, \mathbf{k}_i) = 1\), all others \(= 0\)</p>
                                        <p style="margin: 0; background: white; padding: 10px; border-radius: 5px;"><strong>Result:</strong> Returns a single value (like traditional database)</p>
                                    </div>
                                </div>
                                <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484;">
                                    <h5 style="color: #FC8484; margin-top: 0;">4Ô∏è‚É£ Average Pooling</h5>
                                    <div style="font-size: 0.95em;">
                                        <p style="margin: 0 0 8px 0;"><strong>Condition:</strong> \(\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{1}{m}\) for all \(i\)</p>
                                        <p style="margin: 0; background: white; padding: 10px; border-radius: 5px;"><strong>Result:</strong> Simple average across entire database</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #2 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the attention mechanism formula, what role do the attention weights Œ±(q, k_i) play?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They are the values to be retrieved from the database",
                                "correct": false,
                                "explanation": "Incorrect. The values to be retrieved are v_i. The attention weights Œ± determine how much to weight each value."
                            },
                            {
                                "text": "They are scalar weights that determine how much to focus on each key-value pair",
                                "correct": true,
                                "explanation": "Correct! The attention weights Œ±(q, k_i) are scalars that weight each value v_i in the sum, determining how much the mechanism pays attention to each pair."
                            },
                            {
                                "text": "They are the queries that we want to match",
                                "correct": false,
                                "explanation": "Incorrect. The query q is a separate input to the attention mechanism. The weights Œ± are computed based on the compatibility between q and k_i."
                            },
                            {
                                "text": "They must always sum to exactly 1",
                                "correct": false,
                                "explanation": "Incorrect. While this is the most common case (convex combination), there are other valid configurations like nonnegative weights that do not necessarily sum to 1."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 9: Weight Normalization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Ensuring Weights Sum to One: Normalization</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 12px;">
                                <h4 style="color: #10099F; margin-top: 0; margin-bottom: 8px;">üéØ Goal: Convex Combination</h4>
                                <p style="font-size: 0.95em; margin: 0;">A common strategy for ensuring weights sum to 1 is to <strong>normalize</strong> them</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <h4 style="color: #10099F; margin-bottom: 8px;">Normalization Formula</h4>
                            <div style="background: white; padding: 15px; border-radius: 10px; border: 2px solid #EEEEEE;">
                                <div style="text-align: center; font-size: 1.15em; margin: 10px 0;">
                                    $$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\alpha(\mathbf{q}, \mathbf{k}_i)}{\sum_j \alpha(\mathbf{q}, \mathbf{k}_j)}$$
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <h4 style="color: #10099F; margin-bottom: 8px;">How It Works</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.9em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 3px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">Numerator</strong>
                                    <p style="margin: 8px 0 0 0;">Original (unnormalized) weight for key \(i\)</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px; border-left: 3px solid #FAC55B;">
                                    <strong style="color: #FFA05F;">Denominator</strong>
                                    <p style="margin: 8px 0 0 0;">Sum of all weights ensures normalization: \(\sum_i \alpha(\mathbf{q}, \mathbf{k}_i) = 1\)</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 12px;">
                            <p style="margin: 0;">‚ö†Ô∏è This normalization assumes weights are <strong>already nonnegative</strong>. For arbitrary weights, we need something more...</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Softmax Attention -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Softmax Attention: The Standard Approach</h2>
                    <div style="font-size: 0.62em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 12px; border-radius: 10px; margin-bottom: 12px;">
                                <h4 style="color: white; margin-top: 0; margin-bottom: 8px;">üí° The <span class="tooltip" style="border-bottom-color: white;">Softmax<span class="tooltiptext">A function that converts arbitrary real numbers into a probability distribution (nonnegative values that sum to 1)</span></span> Solution</h4>
                                <p style="font-size: 0.95em; margin: 0;">To ensure weights are <strong>both nonnegative AND normalized</strong>, we use exponentiation + normalization</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 8px;">Two-Step Process</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px; font-size: 0.9em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 3px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0; margin-bottom: 6px;">Step 1: Define Scoring Function</h5>
                                    <p style="margin: 0;">Pick <strong>any</strong> function \(a(\mathbf{q}, \mathbf{k})\)<br>
                                    (Can output arbitrary real numbers)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 3px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0; margin-bottom: 6px;">Step 2: Apply Softmax</h5>
                                    <p style="margin: 0;">Apply softmax operation to get valid attention weights</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: white; border: 2px solid #10099F; padding: 15px;">
                                <h4 style="color: #10099F; margin-top: 0; margin-bottom: 10px;">üìê Softmax Attention Formula</h4>
                                <div style="text-align: center; font-size: 1.1em; margin: 10px 0;">
                                    $$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_j \exp(a(\mathbf{q}, \mathbf{k}_j))}$$
                                </div>
                                <div style="margin-top: 10px; font-size: 0.9em; text-align: center;">
                                    <p style="margin: 0;">where \(a(\mathbf{q}, \mathbf{k}_i)\) is the <strong>attention scoring function</strong></p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Why Softmax Works -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Why Softmax is the Dominant Choice</h2>
                    <div style="font-size: 0.58em;">
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 6px;">Key Advantages of Softmax Attention</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 8px; margin-bottom: 8px;">
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 10px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0; margin-bottom: 4px; font-size: 0.9em;">‚úÖ Differentiable</h5>
                                    <p style="font-size: 0.9em; margin: 0;">The <span class="tooltip">exponential function<span class="tooltiptext">exp(x) = e^x, where e ‚âà 2.718. It's smooth, always positive, and has nice derivative properties</span></span> and division are both differentiable</p>
                                </div>
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 10px;">
                                    <h5 style="color: #10099F; margin-top: 0; margin-bottom: 4px; font-size: 0.9em;">‚úÖ No Vanishing Gradients</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Gradients never vanish completely ‚Äî important for training deep networks</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 10px;">
                                    <h5 style="color: #FFA05F; margin-top: 0; margin-bottom: 4px; font-size: 0.9em;">‚úÖ Framework Support</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Available in all deep learning frameworks (PyTorch, TensorFlow, JAX, MXNet)</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 10px;">
                                    <h5 style="color: #FC8484; margin-top: 0; margin-bottom: 4px; font-size: 0.9em;">‚úÖ Valid Probabilities</h5>
                                    <p style="font-size: 0.9em; margin: 0;">Always produces nonnegative weights that sum to 1</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="background: #FFFBF0; padding: 10px; border-radius: 10px; border-left: 4px solid #FAC55B;">
                                <h5 style="color: #FFA05F; margin-top: 0; margin-bottom: 4px; font-size: 0.9em;">‚öôÔ∏è Alternative: RL Attention</h5>
                                <p style="font-size: 0.9em; margin: 0;">Non-differentiable models can use <strong>reinforcement learning</strong> (Mnih et al., 2014), but training is complex. Most research uses differentiable mechanisms.</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 8px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 10px;">
                            <p style="margin: 0;">üéØ <strong>Bottom Line:</strong> Softmax attention combines simplicity, effectiveness, and trainability</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #3 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are the key properties that make softmax attention the standard choice in deep learning?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "It is differentiable, allowing gradient-based optimization",
                                "correct": true,
                                "explanation": "Correct! Differentiability is crucial for training with backpropagation."
                            },
                            {
                                "text": "It guarantees nonnegative weights that sum to 1",
                                "correct": true,
                                "explanation": "Correct! The exponential function ensures nonnegativity, and normalization ensures the sum equals 1."
                            },
                            {
                                "text": "Gradients never vanish completely",
                                "correct": true,
                                "explanation": "Correct! This is an important property for training deep networks effectively."
                            },
                            {
                                "text": "It is the only possible way to compute attention weights",
                                "correct": false,
                                "explanation": "Incorrect. While softmax is the most common, alternatives exist (e.g., reinforcement learning-based attention), though they are more complex to train."
                            },
                            {
                                "text": "It requires no learnable parameters",
                                "correct": false,
                                "explanation": "Incorrect. While softmax itself is parameter-free, the attention scoring function a(q, k) typically contains learnable parameters."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 12: QKV Diagram -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Visualizing the Attention Mechanism</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 12px; border-radius: 10px; margin-bottom: 12px;">
                                <h4 style="color: white; margin-top: 0; margin-bottom: 6px;">üé® The Complete Picture</h4>
                                <p style="font-size: 0.95em; margin: 0;">The attention mechanism computes a linear combination over values via attention pooling</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="text-align: center;">
                                <img src="images/qkv.svg" alt="Queries, Keys, and Values Diagram" style="max-width: 70%; height: auto; margin: 12px 0;">
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 8px;">The Flow</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 8px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 8px; border-left: 3px solid #10099F; text-align: center;">
                                    <strong style="color: #10099F;">1. Query meets Keys</strong>
                                    <p style="margin: 8px 0 0 0;">Compute compatibility \(\alpha(\mathbf{q}, \mathbf{k}_i)\)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 8px; border-left: 3px solid #2DD2C0; text-align: center;">
                                    <strong style="color: #2DD2C0;">2. Weight Values</strong>
                                    <p style="margin: 8px 0 0 0;">Use weights to scale each value \(\mathbf{v}_i\)</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 10px; border-radius: 8px; border-left: 3px solid #FAC55B; text-align: center;">
                                    <strong style="color: #FFA05F;">3. Aggregate</strong>
                                    <p style="margin: 8px 0 0 0;">Sum weighted values to get output</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #F5F5FF; border-left: 4px solid #10099F; padding: 12px;">
                            <p style="margin: 0;">üí° The actual "code" (the query) can be quite concise, even though the space to operate on (the database) is significant</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Interactive Heatmap Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Visualizing Attention Weights</h2>
                    <div style="font-size: 0.52em;">
                        <div class="fragment">
                            <p style="font-size: 1.0em; margin-bottom: 3px;">One benefit of attention is that it can be quite intuitive. When weights are nonnegative and sum to 1, we can <em>interpret</em> large weights as selecting components of relevance.</p>
                        </div>
                        <div class="fragment">
                            <div class="interactive-controls" style="display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 3px; padding: 4px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                                <label style="display: flex; align-items: center; gap: 8px;">
                                    Pattern:
                                    <select id="attention-pattern" style="padding: 4px 8px; border: 1px solid #EEEEEE; border-radius: 5px; background: white; font-size: 1em;">
                                        <option value="identity">Identity (Query = Key)</option>
                                        <option value="uniform">Uniform (Average Pooling)</option>
                                        <option value="random">Random Weights</option>
                                    </select>
                                </label>
                            </div>
                        </div>
                        <div id="attention-heatmap" style="display: flex; justify-content: center; margin-top: 2px;"></div>
                        <div class="fragment emphasis-box" style="margin-top: 3px; background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 6px;">
                            <p style="margin: 0; font-size: 0.9em;">‚ö†Ô∏è <strong>Important caveat:</strong> What the attention weights <em>mean</em> remains a hazy research topic. Use caution when making interpretability claims!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 14: Code Implementation - Part 1 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Implementing Attention Visualization</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 1.05em; margin-bottom: 15px;">The <code>show_heatmaps</code> function visualizes attention weights as heatmaps</p>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Function Signature and Parameters</h4>
                            <pre><code class="python" style="font-size: 0.85em;">def show_heatmaps(matrices, xlabel, ylabel, titles=None,
                  figsize=(2.5, 2.5), cmap='Reds'):
    """Show heatmaps of matrices.

    Parameters:
    -----------
    matrices : tensor of shape (num_rows, num_cols, num_queries, num_keys)
        4D tensor containing attention weight matrices to visualize
    xlabel : str
        Label for x-axis (typically "Keys")
    ylabel : str
        Label for y-axis (typically "Queries")
    titles : list of str, optional
        Titles for each subplot
    figsize : tuple
        Figure size for each subplot
    cmap : str
        Colormap name (default: 'Reds' for attention weights)
    """</code></pre>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0;"><strong>Key Insight:</strong> The function takes a <strong>4D tensor</strong> to allow visualizing multiple queries and different attention weight matrices in a grid layout</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 15: Code Implementation - Part 2 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}]'>
                    <h2 class="truncate-title">Implementation: Plotting Logic</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <pre><code class="python" style="font-size: 0.85em;">def show_heatmaps(matrices, xlabel, ylabel, titles=None,
                  figsize=(2.5, 2.5), cmap='Reds'):
    """Show heatmaps of matrices."""
    d2l.use_svg_display()
    num_rows, num_cols, _, _ = matrices.shape

    # Create subplots grid
    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                sharex=True, sharey=True, squeeze=False)

    # Iterate over rows and columns
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            # Plot heatmap
            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)

            # Add labels only on edges
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)
            if j == 0:
                ax.set_ylabel(ylabel)
            if titles:
                ax.set_title(titles[j])

    # Add colorbar
    fig.colorbar(pcm, ax=axes, shrink=0.6)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #2DD2C0;">Key Operations</strong>
                                    <ul style="margin: 8px 0 0 15px;">
                                        <li><code>imshow()</code> for heatmap</li>
                                        <li>Shared axes for clean grid</li>
                                        <li>Edge labels only</li>
                                    </ul>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #FFA05F;">Tensor Shape</strong>
                                    <p style="margin: 8px 0 0 0;"><code>(rows, cols, queries, keys)</code><br>
                                    Allows grid of multiple attention patterns</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 16: Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html"}, {"text": "Nadaraya (1964) & Watson (1964)", "url": "https://doi.org/10.1137/1109020"}]'>
                    <h2 class="truncate-title">Summary: Queries, Keys, and Values</h2>
                    <div style="font-size: 0.62em;">
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr; gap: 10px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 12px;">
                                    <h5 style="color: #10099F; margin-top: 0; margin-bottom: 6px;">üîë Core Concept</h5>
                                    <p style="font-size: 0.95em; margin: 0;">The attention mechanism allows us to <strong>aggregate data from many (key, value) pairs</strong> using a query, without compressing the database into a fixed-length vector</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 12px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0; margin-bottom: 6px;">üéØ Differentiability</h5>
                                    <p style="font-size: 0.95em; margin: 0;">Attention provides a <strong>differentiable means of control</strong> by which a neural network can select elements from a set and construct an associated weighted sum</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 12px;">
                                    <h5 style="color: #FFA05F; margin-top: 0; margin-bottom: 6px;">üìê Attention Pooling</h5>
                                    <p style="font-size: 0.95em; margin: 0;">The operation \(\textrm{Attention}(\mathbf{q}, \mathcal{D}) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i\) generates a <strong>linear combination of values</strong> based on query-key compatibility</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0; margin-bottom: 8px;">üîú Coming Up Next</h4>
                                <p style="font-size: 0.95em; margin: 0;">We'll explore a concrete example: the <strong>Nadaraya-Watson estimator</strong>, where attention is applied to regression problems. Queries correspond to locations where regression should be performed, and keys/values come from observed data points.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #4: Final Comprehensive Question -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statement best describes the advantage of attention mechanisms over compressing an entire input sequence into a single fixed-length vector?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Attention mechanisms are always faster to compute than compression",
                                "correct": false,
                                "explanation": "Incorrect. Computational speed is not the primary advantage. In fact, attention can be computationally expensive for long sequences."
                            },
                            {
                                "text": "Attention allows selective focus on different parts of the input without information loss from compression",
                                "correct": true,
                                "explanation": "Correct! Attention mechanisms can operate on the full input sequence (keys and values) and selectively focus (via attention weights) on relevant parts for each query, avoiding the information bottleneck of compressing everything into a single vector."
                            },
                            {
                                "text": "Attention mechanisms require fewer learnable parameters than compression-based approaches",
                                "correct": false,
                                "explanation": "Incorrect. Parameter count is not the key advantage, and attention mechanisms can actually have substantial parameters in the scoring function a(q, k)."
                            },
                            {
                                "text": "Attention weights are always interpretable and explain model decisions",
                                "correct": false,
                                "explanation": "Incorrect. The text explicitly warns that what attention weights mean and how they should be interpreted remains a hazy research topic. Interpretability claims should be made with caution."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- New Vertical Section: Attention Pooling by Similarity -->
            <section>
                <!-- Slide 1: Section Title -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.2 Attention Pooling by Similarity</h1>
                    <p>Nadaraya-Watson Regression as an Attention Mechanism</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Introduction to Kernel Regression -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">From Attention to Classical Regression</h2>
                    <div style="font-size: 0.85em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The Nadaraya-Watson Estimator</h4>
                            <p>Let's apply attention components to a classical problem: <span class="tooltip">kernel density estimation<span class="tooltiptext">A non-parametric method to estimate the probability density function of a random variable using kernels</span></span></p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Core Idea:</strong> Use similarity kernels Œ±(<strong>q</strong>, <strong>k</strong>) to relate queries <strong>q</strong> to keys <strong>k</strong></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <p><strong>Applications:</strong></p>
                            <ul style="font-size: 0.95em;">
                                <li><strong>Regression:</strong> Predict continuous values</li>
                                <li><strong>Classification:</strong> Predict discrete categories (with one-hot encoding)</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;">‚ú® <strong>Key Property:</strong> This method requires <em>no training</em> whatsoever!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Kernel Functions with Equations -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Common Similarity Kernels</h2>
                    <div style="font-size: 0.65em;">
                        <p>Different kernels provide different notions of similarity:</p>
                        <div style="margin-top: 15px;">
                            <div class="fragment">
                                <p><strong style="color: #10099F;">Gaussian Kernel:</strong></p>
                                <p>$$\alpha(\mathbf{q}, \mathbf{k}) = \exp\left(-\frac{1}{2} \|\mathbf{q} - \mathbf{k}\|^2 \right)$$</p>
                            </div>
                            <div class="fragment" style="margin-top: 10px;">
                                <p><strong style="color: #2DD2C0;">Boxcar Kernel:</strong></p>
                                <p>$$\alpha(\mathbf{q}, \mathbf{k}) = \mathbb{1}\{\|\mathbf{q} - \mathbf{k}\| \leq 1\}$$</p>
                                <p style="font-size: 0.9em; color: #666;">(1 if distance ‚â§ 1, otherwise 0)</p>
                            </div>
                            <div class="fragment" style="margin-top: 10px;">
                                <p><strong style="color: #FFA05F;">Epanechnikov Kernel:</strong></p>
                                <p>$$\alpha(\mathbf{q}, \mathbf{k}) = \max\left(0, 1 - \|\mathbf{q} - \mathbf{k}\|\right)$$</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="padding: 8px;">
                                <p style="margin: 0; font-size: 0.95em;">All kernels are <span class="tooltip">translation and rotation invariant<span class="tooltiptext">If we shift and rotate q and k in the same manner, the kernel value Œ± remains unchanged</span></span> ‚Äî they depend only on the distance between points</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Kernel Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Kernel Shapes</h2>
                    <div style="font-size: 0.85em;">
                        <p>Visual comparison of different kernel functions (centered at k = 0):</p>
                        <img src="images/kernels.svg" alt="Kernel Functions" style="max-width: 90%; margin-top: 20px;">
                        <div style="margin-top: 20px; font-size: 0.9em;">
                            <p><strong>Key Observations:</strong></p>
                            <ul style="text-align: left; display: inline-block;">
                                <li><strong>Gaussian:</strong> Smooth, decays gradually with distance</li>
                                <li><strong>Boxcar:</strong> Hard cutoff at distance 1</li>
                                <li><strong>Constant:</strong> Ignores distance entirely (baseline)</li>
                                <li><strong>Epanechnikov:</strong> Linear decay within range</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: The Attention Pooling Formula -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">The General Attention Pooling Formula</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>For any choice of kernel, the prediction is computed as:</p>
                            <p style="font-size: 1.1em; margin: 15px 0;">
                                $$f(\mathbf{q}) = \sum_i \mathbf{v}_i \frac{\alpha(\mathbf{q}, \mathbf{k}_i)}{\sum_j \alpha(\mathbf{q}, \mathbf{k}_j)}$$
                            </p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <p><strong>In the regression context:</strong></p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 8px;">
                                    <p style="margin: 5px 0;"><strong>Values:</strong></p>
                                    <p style="margin: 5px 0; font-family: monospace;">v<sub>i</sub> = y<sub>i</sub></p>
                                    <p style="margin: 5px 0; font-size: 0.85em; color: #666;">(scalar labels)</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 8px;">
                                    <p style="margin: 5px 0;"><strong>Keys:</strong></p>
                                    <p style="margin: 5px 0; font-family: monospace;">k<sub>i</sub> = x<sub>i</sub></p>
                                    <p style="margin: 5px 0; font-size: 0.85em; color: #666;">(feature vectors)</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFF9F0; border-left: 4px solid #FFA05F; padding: 8px;">
                                    <p style="margin: 5px 0;"><strong>Query:</strong></p>
                                    <p style="margin: 5px 0; font-family: monospace;">q</p>
                                    <p style="margin: 5px 0; font-size: 0.85em; color: #666;">(location to evaluate)</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 8px;">
                                <p style="margin: 0;"><strong>For classification:</strong> Use one-hot encoding for y<sub>i</sub> to obtain vector values <strong>v</strong><sub>i</sub></p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: MCQ on Kernel Properties -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does it mean for a kernel to be translation invariant?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The kernel value depends only on the absolute positions of q and k",
                                "correct": false,
                                "explanation": "Incorrect. Translation invariance means the kernel depends on relative position (distance), not absolute positions."
                            },
                            {
                                "text": "The kernel value remains unchanged if we shift both q and k by the same amount",
                                "correct": true,
                                "explanation": "Correct! Translation invariance means shifting both points together doesn\u2019t change the kernel value, which depends only on their relative distance."
                            },
                            {
                                "text": "The kernel can be translated along the x-axis without changing its shape",
                                "correct": false,
                                "explanation": "Incorrect. This describes a property of the function, not what translation invariance means for the kernel."
                            },
                            {
                                "text": "The kernel produces the same output regardless of input",
                                "correct": false,
                                "explanation": "Incorrect. This would describe a constant kernel, not translation invariance."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 7: Generating Synthetic Data -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Creating Test Data</h2>
                    <div style="font-size: 0.8em;">
                        <p>Let's create synthetic data to test Nadaraya-Watson estimation:</p>
                        <div class="fragment" style="margin-top: 20px;">
                            <p><strong style="color: #10099F;">True Function:</strong></p>
                            <p style="font-size: 1.1em; margin: 15px 0;">
                                $$y_i = 2\sin(x_i) + x_i + \epsilon$$
                            </p>
                            <p style="font-size: 0.9em; color: #666;">where Œµ ~ N(0, 1) is Gaussian noise</p>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <pre><code class="python" data-trim>
def f(x):
    return 2 * torch.sin(x) + x

# Generate 40 training examples
n = 40
x_train, _ = torch.sort(torch.rand(n) * 5)
y_train = f(x_train) + torch.randn(n)

# Create validation set
x_val = torch.arange(0, 5, 0.1)
y_val = f(x_val)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <p style="font-size: 0.9em;"><strong>Setup:</strong> 40 training points with noise, smooth validation curve</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: Nadaraya-Watson Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Implementing Nadaraya-Watson Regression</h2>
                    <div style="font-size: 0.75em;">
                        <pre><code class="python" data-trim data-line-numbers="|2-3|5|7|9|10">
def nadaraya_watson(x_train, y_train, x_val, kernel):
    # Compute pairwise distances
    dists = x_train.reshape((-1, 1)) - x_val.reshape((1, -1))

    # Apply kernel function
    k = kernel(dists).type(torch.float32)

    # Normalize over keys for each query (attention weights!)
    attention_w = k / k.sum(0)
    y_hat = y_train @ attention_w
    return y_hat, attention_w
                        </code></pre>
                        <div style="margin-top: 20px;">
                            <p><strong>Key Steps:</strong></p>
                            <ul style="text-align: left; display: inline-block; font-size: 0.95em;">
                                <li><strong>Line 2-3:</strong> Compute distance matrix (rows = training, cols = validation)</li>
                                <li><strong>Line 5:</strong> Apply kernel to get unnormalized similarities</li>
                                <li><strong>Line 7:</strong> Normalize columns to sum to 1 ‚Üí <strong>attention weights</strong></li>
                                <li><strong>Line 9:</strong> Weighted sum of labels using attention weights</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Slide 9: Connection to Attention Pooling -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">The Attention Parallel</h2>
                    <div style="font-size: 0.85em;">
                        <p>Nadaraya-Watson is <em>exactly</em> attention pooling!</p>
                        <div style="margin-top: 30px;">
                            <table style="width: 90%; margin: 0 auto; font-size: 0.9em; border-collapse: collapse;">
                                <thead>
                                    <tr style="background: #10099F; color: white;">
                                        <th style="padding: 12px; border: 1px solid #ddd;">Attention Mechanism</th>
                                        <th style="padding: 12px; border: 1px solid #ddd;">Nadaraya-Watson</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="fragment">
                                        <td style="padding: 10px; border: 1px solid #ddd; background: #F5F5FF;"><strong>Queries</strong></td>
                                        <td style="padding: 10px; border: 1px solid #ddd;">Validation features (x<sub>val</sub>)</td>
                                    </tr>
                                    <tr class="fragment">
                                        <td style="padding: 10px; border: 1px solid #ddd; background: #F5F5FF;"><strong>Keys</strong></td>
                                        <td style="padding: 10px; border: 1px solid #ddd;">Training features (x<sub>train</sub>)</td>
                                    </tr>
                                    <tr class="fragment">
                                        <td style="padding: 10px; border: 1px solid #ddd; background: #F5F5FF;"><strong>Values</strong></td>
                                        <td style="padding: 10px; border: 1px solid #ddd;">Training labels (y<sub>train</sub>)</td>
                                    </tr>
                                    <tr class="fragment">
                                        <td style="padding: 10px; border: 1px solid #ddd; background: #F5F5FF;"><strong>Attention Weights</strong></td>
                                        <td style="padding: 10px; border: 1px solid #ddd;">Normalized kernel weights</td>
                                    </tr>
                                    <tr class="fragment">
                                        <td style="padding: 10px; border: 1px solid #ddd; background: #F5F5FF;"><strong>Attention Pooling</strong></td>
                                        <td style="padding: 10px; border: 1px solid #ddd;">Weighted prediction ≈∑</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <p style="margin: 0;">The normalized kernel weights <strong>are</strong> the attention weights!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Regression Results -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Regression Results: Different Kernels</h2>
                    <div style="font-size: 0.8em;">
                        <img src="images/regression_results.svg" alt="Regression with Different Kernels" style="max-width: 95%; margin-top: 10px;">
                        <div style="margin-top: 15px;">
                            <p><strong>Observations:</strong></p>
                            <ul style="text-align: left; display: inline-block; font-size: 0.9em;">
                                <li><strong>Gaussian, Boxcar, Epanechnikov:</strong> Produce reasonable estimates close to the true function</li>
                                <li><strong>Constant kernel:</strong> Produces trivial estimate f(x) = (1/n)Œ£y<sub>i</sub> (mean of all labels)</li>
                                <li>Training points shown as circles, predictions as solid lines, true function as dashed magenta</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: MCQ on Regression Results -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does the constant kernel produce a poor estimate?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It assigns too much weight to distant points",
                                "correct": false,
                                "explanation": "Incorrect. The constant kernel assigns equal weight to all points, not just distant ones."
                            },
                            {
                                "text": "It assigns equal weight to all training points regardless of distance",
                                "correct": true,
                                "explanation": "Correct! The constant kernel Œ±(q,k) = 1 doesn\u2019t depend on distance, so all training points contribute equally, resulting in the mean prediction everywhere."
                            },
                            {
                                "text": "It has too many parameters to tune",
                                "correct": false,
                                "explanation": "Incorrect. The constant kernel has no parameters, which is part of the problem‚Äîit can\u2019t adapt to local structure."
                            },
                            {
                                "text": "It only considers the nearest neighbor",
                                "correct": false,
                                "explanation": "Incorrect. This would describe a very narrow kernel like a boxcar with small width, not the constant kernel."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 12: Attention Weight Heatmaps -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Visualizing Attention Weights</h2>
                    <div style="font-size: 0.8em;">
                        <p>Attention weight matrices for different kernels (darker = higher weight):</p>
                        <img src="images/attention_heatmaps.svg" alt="Attention Weight Heatmaps" style="max-width: 95%; margin-top: 15px;">
                        <div style="margin-top: 15px;">
                            <p><strong>Reading the heatmap:</strong></p>
                            <ul style="text-align: left; display: inline-block; font-size: 0.9em;">
                                <li><strong>Rows:</strong> Query points (validation features)</li>
                                <li><strong>Columns:</strong> Key points (training features)</li>
                                <li><strong>Color intensity:</strong> Attention weight magnitude</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box">
                                <p style="margin: 0;">Notice: Gaussian, Boxcar, and Epanechnikov have similar attention patterns despite different functional forms!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Effect of Kernel Width -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Tuning Kernel Width</h2>
                    <div style="font-size: 0.72em;">
                        <p>Gaussian kernels with different widths œÉ:</p>
                        <p style="font-size: 0.95em; margin: 10px 0;">
                            $$\alpha(\mathbf{q}, \mathbf{k}) = \exp\left(-\frac{1}{2\sigma^2} \|\mathbf{q} - \mathbf{k}\|^2 \right)$$
                        </p>
                        <img src="images/width_comparison.svg" alt="Width Comparison" style="max-width: 85%; margin: 0;">
                        <div style="margin-top: 10px;">
                            <p><strong>Trade-offs:</strong></p>
                            <ul style="text-align: left; display: inline-block; font-size: 0.9em;">
                                <li><strong>Narrow kernels (œÉ = 0.1, 0.2):</strong> Less smooth, better local adaptation, more sensitive to noise</li>
                                <li><strong>Wide kernels (œÉ = 0.5, 1.0):</strong> Smoother curves, less local detail, more robust to noise</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Slide 14: Width vs Attention Spread -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Kernel Width Affects Attention Focus</h2>
                    <div style="font-size: 0.8em;">
                        <p>Attention weight heatmaps for different œÉ values:</p>
                        <img src="images/width_attention.svg" alt="Width Attention Patterns" style="max-width: 95%; margin-top: 15px;">
                        <div style="margin-top: 15px;">
                            <p><strong>Key Insight:</strong></p>
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <p style="margin: 0;">Narrower kernel ‚Üí More focused attention (diagonal band)<br>
                                Wider kernel ‚Üí More distributed attention (broader influence)</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <p style="font-size: 0.9em; color: #666;"><em>Each query attends more strongly to nearby keys with narrow kernels</em></p>
                        </div>
                    </div>
                </section>

                <!-- Slide 15: MCQ on Kernel Width -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens when you use a very narrow Gaussian kernel (small œÉ)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The estimate becomes smoother and more robust to outliers",
                                "correct": false,
                                "explanation": "Incorrect. This describes the effect of a wider kernel, not a narrow one."
                            },
                            {
                                "text": "Each query attends almost exclusively to its nearest neighbors",
                                "correct": true,
                                "explanation": "Correct! A narrow kernel decays rapidly with distance, so only nearby training points receive significant attention weight."
                            },
                            {
                                "text": "All training points receive equal attention weight",
                                "correct": false,
                                "explanation": "Incorrect. This would describe a constant or very wide kernel, not a narrow one."
                            },
                            {
                                "text": "The model requires more training data",
                                "correct": false,
                                "explanation": "Incorrect. Nadaraya-Watson is non-parametric and doesn\u2019t require training. Width affects locality, not data requirements directly."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 16: Key Properties -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Properties of Nadaraya-Watson Estimation</h2>
                    <div style="font-size: 0.7em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 15px;">
                            <div class="fragment">
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; height: 100%; padding: 10px;">
                                    <h4 style="color: #2DD2C0; margin-top: 0;">‚úì No Training Required</h4>
                                    <p style="font-size: 0.95em;">Directly uses training data at inference time (<span class="tooltip">non-parametric<span class="tooltiptext">A non-parametric method doesn\u2019t assume a fixed functional form and grows in complexity with data size</span></span>)</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; height: 100%; padding: 10px;">
                                    <h4 style="color: #10099F; margin-top: 0;">‚úì Consistency Guarantees</h4>
                                    <p style="font-size: 0.95em;">With proper kernel narrowing as data grows, converges to optimal solution</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; height: 100%; padding: 10px;">
                                    <h4 style="color: #FFA05F; margin-top: 0;">‚úì Translation/Rotation Invariant</h4>
                                    <p style="font-size: 0.95em;">Kernels depend only on distances, not absolute positions or orientations</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="emphasis-box" style="background: #FFF0F0; border-left: 4px solid #FC8484; height: 100%; padding: 10px;">
                                    <h4 style="color: #FC8484; margin-top: 0;">‚öôÔ∏è Tunable Hyperparameters</h4>
                                    <p style="font-size: 0.95em;">Width can be adjusted globally or per-coordinate for better adaptation</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #F5F5F5; border-left: 4px solid #262626; padding: 8px;">
                                <p style="margin: 0;"><strong>Historical Note:</strong> Many kernel choice heuristics exist (Silverman 1986, Norelli et al. 2022)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 17: Limitations and Motivation for Learning -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">From Hand-Crafted to Learned Attention</h2>
                    <div style="font-size: 0.72em;">
                        <div class="fragment">
                            <h4 style="color: #FC8484;">‚ö†Ô∏è Limitations of Hand-Crafted Kernels</h4>
                            <ul style="font-size: 0.95em; margin-bottom: 15px;">
                                <li>Kernel choice is <strong>heuristic</strong> and problem-dependent</li>
                                <li>Width tuning requires manual adjustment or cross-validation</li>
                                <li>Simple distance-based similarity may not capture complex patterns</li>
                                <li>Works well for low-dimensional spaces, struggles in high dimensions</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #10099F;">üí° The Better Approach: Learn the Mechanism</h4>
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 8px;">
                                <p style="margin: 0; font-size: 1.05em;">Instead of hand-crafting Œ±(<strong>q</strong>, <strong>k</strong>), why not <strong>learn</strong> the representations for queries and keys?</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 8px;">
                                <p style="margin: 0;"><strong>Preview:</strong> Modern attention mechanisms learn Q, K, V transformations from data, allowing the network to discover task-specific similarity measures</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 18: Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"}]'>
                    <h2 class="truncate-title">Summary: Attention Pooling by Similarity</h2>
                    <div style="font-size: 0.72em;">
                        <div style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; margin-bottom: 8px; padding: 8px;">
                                <p style="margin: 5px 0;"><strong>1.</strong> Nadaraya-Watson kernel regression is an <strong>early precursor</strong> of modern attention mechanisms</p>
                            </div>
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; margin-bottom: 8px; padding: 8px;">
                                <p style="margin: 5px 0;"><strong>2.</strong> Attention weights are assigned based on <strong>similarity (or distance)</strong> between queries and keys</p>
                            </div>
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; margin-bottom: 8px; padding: 8px;">
                                <p style="margin: 5px 0;"><strong>3.</strong> Can be applied directly with <strong>no training</strong> for both regression and classification</p>
                            </div>
                            <div class="emphasis-box" style="background: #FFF9F0; border-left: 4px solid #FFA05F; margin-bottom: 8px; padding: 8px;">
                                <p style="margin: 5px 0;"><strong>4.</strong> The number of similar observations matters ‚Äî more data in a region ‚Üí better estimates</p>
                            </div>
                            <div class="emphasis-box" style="background: #FFF0F0; border-left: 4px solid #FC8484; padding: 8px;">
                                <p style="margin: 5px 0;"><strong>5.</strong> Limitations of hand-crafted kernels motivate <strong>learning</strong> query/key representations</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px; text-align: center;">
                            <p style="font-size: 1.1em; font-style: italic; color: #10099F;">Next: Learning parameterized attention functions!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 19: Final MCQ -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of learning attention mechanisms over using fixed kernels like Nadaraya-Watson?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Learned attention eliminates the need for training data",
                                "correct": false,
                                "explanation": "Incorrect. Learned attention requires training data to optimize the query/key/value transformations."
                            },
                            {
                                "text": "Learned attention can discover task-specific similarity measures from data",
                                "correct": true,
                                "explanation": "Correct! By learning the Q, K, V transformations, the network can discover similarity measures tailored to the specific task, rather than relying on hand-crafted distance metrics."
                            },
                            {
                                "text": "Learned attention always requires less computation",
                                "correct": false,
                                "explanation": "Incorrect. Learned attention mechanisms can be computationally expensive, especially with large sequence lengths."
                            },
                            {
                                "text": "Learned attention produces interpretable attention weights",
                                "correct": false,
                                "explanation": "Incorrect. As mentioned earlier, attention weight interpretability remains a research challenge."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- New Vertical Section: Attention Scoring Functions -->
            <section>
                <!-- Slide 1: Title Slide -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Section 11.3: Attention Scoring Functions", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.3 Attention Scoring Functions</h1>
                    <p>From Distance Metrics to Efficient Dot Products</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Motivation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"}]'>
                    <h2 class="truncate-title">Why Attention Scoring Functions Matter</h2>
                    <div style="font-size: 0.72em;">
                        <div class="fragment">
                            <p>In Section 11.2, we used distance-based kernels (like Gaussian) to model query-key interactions:</p>
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 8px;">
                                <p style="margin: 0;"><strong>Challenge:</strong> Distance functions are more expensive to compute than dot products</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <img src="images/attention-output.svg" alt="Attention Output Computation" style="max-width: 70%; height: auto;">
                            <p style="font-size: 0.85em; margin-top: 8px;">Computing attention output as weighted average: weights come from the <span class="tooltip">attention scoring function<span class="tooltiptext">Function a(q, k) that measures compatibility between query q and key k, used to compute attention weights via softmax</span></span> \(a\) and softmax</p>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 8px;">
                                <p style="margin: 0;"><strong>Goal:</strong> Design attention scoring functions that are simple, efficient, and effective</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Gaussian Kernel Revisited -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#dot-product-attention"}]'>
                    <h2 class="truncate-title">Gaussian Kernel Revisited: What Really Matters?</h2>
                    <div style="font-size: 0.8em;">
                        <p>Let's decompose the Gaussian kernel attention function (without exponentiation):</p>
                        <div class="fragment" style="margin: 25px 0;">
                            $$a(\mathbf{q}, \mathbf{k}_i) = -\frac{1}{2} \|\mathbf{q} - \mathbf{k}_i\|^2$$
                        </div>
                        <div class="fragment" style="margin: 25px 0;">
                            <p>Expanding the squared norm:</p>
                            $$a(\mathbf{q}, \mathbf{k}_i) = \color{#10099F}{\mathbf{q}^\top \mathbf{k}_i} \color{#FAC55B}{-\frac{1}{2} \|\mathbf{k}_i\|^2} \color{#FC8484}{-\frac{1}{2} \|\mathbf{q}\|^2}$$
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border: 2px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">Dot Product</h5>
                                    <p style="margin: 0;">$$\mathbf{q}^\top \mathbf{k}_i$$</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Measures similarity</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border: 2px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">Key Norm</h5>
                                    <p style="margin: 0;">$$-\frac{1}{2} \|\mathbf{k}_i\|^2$$</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Often constant (normalization)</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 15px; border-radius: 8px; border: 2px solid #FC8484;">
                                    <h5 style="color: #FC8484; margin-top: 0;">Query Norm</h5>
                                    <p style="margin: 0;">$$-\frac{1}{2} \|\mathbf{q}\|^2$$</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Same for all keys!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: MCQ on Kernel Simplification -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When computing attention weights using softmax normalization, why can we drop the query norm term ‚Äñq‚Äñ¬≤ from the Gaussian kernel?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because ‚Äñq‚Äñ¬≤ is always zero",
                                "correct": false,
                                "explanation": "Incorrect. The query norm is not zero; it represents the magnitude of the query vector."
                            },
                            {
                                "text": "Because ‚Äñq‚Äñ¬≤ is identical for all (q, k·µ¢) pairs and cancels out in softmax normalization",
                                "correct": true,
                                "explanation": "Correct! Since ‚Äñq‚Äñ¬≤ is the same for all keys, it becomes a constant factor that cancels out when normalizing attention weights to sum to 1."
                            },
                            {
                                "text": "Because ‚Äñq‚Äñ¬≤ makes computation slower",
                                "correct": false,
                                "explanation": "Incorrect. While efficiency matters, the mathematical reason is that it cancels out in normalization."
                            },
                            {
                                "text": "Because ‚Äñq‚Äñ¬≤ is too large and causes numerical instability",
                                "correct": false,
                                "explanation": "Incorrect. The term can be dropped due to normalization, not numerical stability concerns."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 5: Dot Product Derivation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#dot-product-attention"}]'>
                    <h2 class="truncate-title">Simplifying to Dot Product Attention</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Step 1: Drop the Query Norm</h4>
                            <p>After softmax normalization, \(\|\mathbf{q}\|^2\) is identical for all \((\mathbf{q}, \mathbf{k}_i)\) pairs:</p>
                            $$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(\mathbf{q}^\top \mathbf{k}_i - \frac{1}{2}\|\mathbf{k}_i\|^2 - \frac{1}{2}\|\mathbf{q}\|^2)}{\sum_j \exp(\mathbf{q}^\top \mathbf{k}_j - \frac{1}{2}\|\mathbf{k}_j\|^2 - \frac{1}{2}\|\mathbf{q}\|^2)}$$
                            <p style="margin-top: 10px;">The \(\exp(-\frac{1}{2}\|\mathbf{q}\|^2)\) terms cancel:</p>
                            $$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(\mathbf{q}^\top \mathbf{k}_i - \frac{1}{2}\|\mathbf{k}_i\|^2)}{\sum_j \exp(\mathbf{q}^\top \mathbf{k}_j - \frac{1}{2}\|\mathbf{k}_j\|^2)}$$
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #2DD2C0;">Step 2: Often Drop Key Norm Too</h4>
                            <p>With <span class="tooltip">batch/layer normalization<span class="tooltiptext">Normalization techniques that standardize activations, often resulting in constant or well-bounded norms</span></span>, \(\|\mathbf{k}_i\|\) becomes approximately constant</p>
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 8px;">
                                <p style="margin: 0;"><strong>Result:</strong> We can use just the dot product \(\mathbf{q}^\top \mathbf{k}_i\)!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: Scaling Problem -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#dot-product-attention"}]'>
                    <h2 class="truncate-title">The Scaling Problem: Variance Growth</h2>
                    <div style="font-size: 0.7em;">
                        <p>Consider queries and keys as random vectors: \(\mathbf{q}, \mathbf{k}_i \in \mathbb{R}^d\)</p>
                        <div class="fragment" style="margin: 12px 0;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 8px;">
                                <p style="margin: 0;"><strong>Assumption:</strong> Each element is i.i.d. with zero mean and unit variance</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin: 15px 0;">
                            <p><strong>Statistical Property:</strong></p>
                            <ul style="text-align: left; font-size: 0.9em;">
                                <li>Dot product: \(\mathbf{q}^\top \mathbf{k}_i = \sum_{j=1}^d q_j k_{ij}\)</li>
                                <li>Mean: \(\mathbb{E}[\mathbf{q}^\top \mathbf{k}_i] = 0\)</li>
                                <li class="fragment"><strong style="color: #FC8484;">Variance: \(\text{Var}[\mathbf{q}^\top \mathbf{k}_i] = d\)</strong></li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 8px;">
                                <p style="margin: 0; font-size: 0.95em;"><strong>Problem:</strong> As \(d\) increases, variance grows linearly! Leads to softmax <span class="tooltip">saturation<span class="tooltiptext">When softmax outputs become very close to 0 or 1, leading to vanishing gradients</span></span></p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: Scaled Dot Product Attention -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#dot-product-attention"}]'>
                    <h2 class="truncate-title">Solution: Scaled Dot Product Attention</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>To keep variance constant regardless of dimension, we scale by \(\frac{1}{\sqrt{d}}\):</p>
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin: 12px 0; border: 2px solid #10099F;">
                                <p style="margin: 0 0 10px 0; font-weight: bold; color: #10099F;">Scaled Dot Product Attention (Eq. 11.3.2):</p>
                                $$a(\mathbf{q}, \mathbf{k}_i) = \frac{\mathbf{q}^\top \mathbf{k}_i}{\sqrt{d}}$$
                            </div>
                        </div>
                        <div class="fragment" style="margin: 15px 0;">
                            <p>With softmax normalization (Eq. 11.3.3):</p>
                            $$\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(\mathbf{q}^\top \mathbf{k}_i / \sqrt{d})}{\sum_{j=1}^m \exp(\mathbf{q}^\top \mathbf{k}_j / \sqrt{d})}$$
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 8px;">
                                <p style="margin: 0;"><strong>Key Property:</strong> \(\text{Var}[\frac{\mathbf{q}^\top \mathbf{k}_i}{\sqrt{d}}] = 1\) regardless of \(d\)!</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 8px;">
                                <p style="margin: 0;">üìö <strong>Historical Note:</strong> This is the attention mechanism used in the famous <em>"Attention is All You Need"</em> paper (Vaswani et al., 2017) that introduced <span class="tooltip">Transformers<span class="tooltiptext">Architecture based entirely on attention mechanisms, without recurrence or convolution, that revolutionized NLP</span></span></p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: MCQ on Scaling Factor -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we scale the dot product by 1/‚àöd in scaled dot product attention?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the computation faster",
                                "correct": false,
                                "explanation": "Incorrect. The scaling does not improve computational speed; it is for statistical stability."
                            },
                            {
                                "text": "To keep the variance of the attention scores constant regardless of the dimension d",
                                "correct": true,
                                "explanation": "Correct! Without scaling, the variance of q^T k grows linearly with d. Scaling by 1/‚àöd ensures the variance remains 1, preventing softmax saturation and vanishing gradients."
                            },
                            {
                                "text": "To ensure attention weights sum to 1",
                                "correct": false,
                                "explanation": "Incorrect. The softmax operation ensures attention weights sum to 1, not the scaling factor."
                            },
                            {
                                "text": "To make the attention weights more interpretable",
                                "correct": false,
                                "explanation": "Incorrect. The scaling is for numerical stability, not interpretability."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 9: Convenience Functions Overview -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#convenience-functions"}]'>
                    <h2 class="truncate-title">Convenience Functions for Efficient Implementation</h2>
                    <div style="font-size: 0.85em;">
                        <p>To implement attention efficiently, we need two key utilities:</p>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #F5F5FF; padding: 20px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h4 style="color: #10099F; margin-top: 0;">1. Masked Softmax</h4>
                                    <p style="font-size: 0.9em;">Handle sequences of <strong>variable lengths</strong></p>
                                    <ul style="text-align: left; font-size: 0.85em; margin-top: 10px;">
                                        <li>Ignore padding tokens</li>
                                        <li>Essential for NLP tasks</li>
                                        <li>Prevent attention to <code>&lt;blank&gt;</code> tokens</li>
                                    </ul>
                                </div>
                                <div style="background: #F0FFF9; padding: 20px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h4 style="color: #2DD2C0; margin-top: 0;">2. Batch Matrix Multiplication</h4>
                                    <p style="font-size: 0.9em;">Efficiently process <strong>minibatches</strong></p>
                                    <ul style="text-align: left; font-size: 0.85em; margin-top: 10px;">
                                        <li>Parallel computation on GPU</li>
                                        <li>Essential for modern deep learning</li>
                                        <li>Optimize Q, K, V operations</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;"><strong>Why These Matter:</strong> Without these utilities, attention mechanisms would be impractical for real-world applications with variable-length sequences and large datasets</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Masked Softmax Motivation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.2.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#masked-softmax-operation"}]'>
                    <h2 class="truncate-title">Masked Softmax: Handling Variable-Length Sequences</h2>
                    <div style="font-size: 0.7em;">
                        <p>Consider three sentences in a minibatch with different lengths:</p>
                        <div class="fragment" style="margin: 15px 0;">
                            <div style="background: #F5F5F5; padding: 10px; border-radius: 8px; font-family: monospace;">
                                <div>Dive&nbsp;&nbsp;&nbsp;into&nbsp;&nbsp;&nbsp;Deep&nbsp;&nbsp;&nbsp;&nbsp;Learning</div>
                                <div>Learn&nbsp;&nbsp;to&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #FC8484;">&lt;blank&gt;</span></div>
                                <div>Hello&nbsp;&nbsp;world&nbsp;&nbsp;<span style="color: #FC8484;">&lt;blank&gt;</span>&nbsp;<span style="color: #FC8484;">&lt;blank&gt;</span></div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <p><strong>Problem:</strong> We don't want attention on the <code>&lt;blank&gt;</code> padding tokens!</p>
                            <p style="margin-top: 10px;">Instead of computing:</p>
                            $$\text{output} = \sum_{i=1}^n \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i$$
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <p>We need to limit the sum to the actual sequence length \(l \leq n\):</p>
                            $$\text{output} = \sum_{i=1}^{\color{#10099F}{l}} \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i$$
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 8px;">
                                <p style="margin: 0;"><strong>Solution:</strong> The <em>masked softmax operation</em> sets attention weights to zero for positions beyond the valid length</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Masked Softmax Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.2.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#masked-softmax-operation"}]'>
                    <h2 class="truncate-title">Masked Softmax: Implementation Strategy</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment">
                            <p><strong>Key Idea:</strong> Set masked positions to large negative values (e.g., \(-10^6\)) before softmax</p>
                            <div style="margin: 10px 0;">
                                $$\text{softmax}([-10^6, -10^6, x_1, x_2]) \approx [0, 0, \alpha_1, \alpha_2]$$
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <p><strong>PyTorch Implementation:</strong></p>
                            <pre style="font-size: 0.8em; max-height: 300px; overflow-y: auto;"><code class="language-python">def masked_softmax(X, valid_lens):
    """Perform softmax by masking elements on the last axis."""
    # X: 3D tensor (batch_size, num_queries, num_keys)
    # valid_lens: 1D or 2D tensor with valid lengths

    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            # Repeat for each query
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)

        # Mask: replace elements beyond valid length with -1e6
        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <p><strong>Example:</strong> With <code>valid_lens = [2, 3]</code> for a batch of 2 examples:</p>
                            <pre style="font-size: 0.8em; max-height: 120px; overflow-y: auto;"><code class="language-python">masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))
# Output shape: (2, 2, 4)
# First example:  weights for first 2 positions only
# Second example: weights for first 3 positions only</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: MCQ on Masked Softmax -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In masked softmax, why do we set masked positions to -10‚Å∂ instead of directly setting attention weights to 0?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To save memory",
                                "correct": false,
                                "explanation": "Incorrect. This approach does not save memory; it is about maintaining differentiability."
                            },
                            {
                                "text": "Because we need to apply the mask before softmax, and exp(-10‚Å∂) ‚âà 0 after softmax",
                                "correct": true,
                                "explanation": "Correct! We must mask before applying softmax. Setting values to a large negative number ensures they become approximately 0 after the exponential in softmax, while maintaining differentiability for backpropagation."
                            },
                            {
                                "text": "To make the code run faster on GPUs",
                                "correct": false,
                                "explanation": "Incorrect. While GPU efficiency is important, the primary reason is mathematical correctness."
                            },
                            {
                                "text": "To prevent numerical overflow",
                                "correct": false,
                                "explanation": "Incorrect. While numerical stability matters, the main reason is that we need to mask before softmax normalization."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 13: Batch Matrix Multiplication -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.2.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#batch-matrix-multiplication"}]'>
                    <h2 class="truncate-title">Batch Matrix Multiplication (BMM)</h2>
                    <div style="font-size: 0.7em;">
                        <p>When processing minibatches, we need to multiply multiple matrix pairs efficiently:</p>
                        <div class="fragment" style="margin: 15px 0;">
                            <p><strong>Input:</strong> Two batches of matrices</p>
                            $$\begin{align}
                            \mathbf{Q} &= [\mathbf{Q}_1, \mathbf{Q}_2, \ldots, \mathbf{Q}_n] \in \mathbb{R}^{n \times a \times b} \\
                            \mathbf{K} &= [\mathbf{K}_1, \mathbf{K}_2, \ldots, \mathbf{K}_n] \in \mathbb{R}^{n \times b \times c}
                            \end{align}$$
                        </div>
                        <div class="fragment" style="margin: 15px 0;">
                            <p><strong>Batch Matrix Multiplication:</strong> Element-wise matrix products</p>
                            $$\textrm{BMM}(\mathbf{Q}, \mathbf{K}) = [\mathbf{Q}_1 \mathbf{K}_1, \mathbf{Q}_2 \mathbf{K}_2, \ldots, \mathbf{Q}_n \mathbf{K}_n] \in \mathbb{R}^{n \times a \times c}$$
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin-top: 0;">PyTorch Example</h5>
                                    <pre style="margin: 0; font-size: 0.8em;"><code class="language-python">Q = torch.ones((2, 3, 4))
K = torch.ones((2, 4, 6))
result = torch.bmm(Q, K)
# Shape: (2, 3, 6)</code></pre>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Key Properties</h5>
                                    <ul style="text-align: left; margin: 5px 0; padding-left: 20px; font-size: 0.9em;">
                                        <li>Batch size \(n\) preserved</li>
                                        <li>Parallel on GPU</li>
                                        <li>Essential for attention</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;"><strong>In Attention:</strong> Used to compute \(\mathbf{Q}\mathbf{K}^\top\) and then multiply by \(\mathbf{V}\)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 14: Scaled Dot Product Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention"}]'>
                    <h2 class="truncate-title">Scaled Dot Product Attention: Complete Implementation</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment">
                            <p><strong>Matrix Form:</strong> For queries \(\mathbf{Q} \in \mathbb{R}^{n \times d}\), keys \(\mathbf{K} \in \mathbb{R}^{m \times d}\), values \(\mathbf{V} \in \mathbb{R}^{m \times v}\):</p>
                            <div style="background: #F5F5FF; padding: 10px; border-radius: 8px; margin: 8px 0; border: 2px solid #10099F;">
                                $$\mathrm{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}$$
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <p><strong>PyTorch Implementation:</strong></p>
                            <pre style="font-size: 0.75em; max-height: 320px; overflow-y: auto;"><code class="language-python">class DotProductAttention(nn.Module):
    """Scaled dot product attention."""
    def __init__(self, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens=None):
        # queries: (batch_size, num_queries, d)
        # keys:    (batch_size, num_keys, d)
        # values:  (batch_size, num_keys, value_dim)

        d = queries.shape[-1]

        # Compute attention scores: Q K^T / sqrt(d)
        # keys.transpose(1, 2) swaps last two dimensions
        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)

        # Apply masked softmax
        self.attention_weights = masked_softmax(scores, valid_lens)

        # Compute weighted sum of values
        return torch.bmm(self.dropout(self.attention_weights), values)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <p style="margin: 0;"><strong>Key Steps:</strong> (1) Compute scores with scaling, (2) Apply masked softmax, (3) Weight values with dropout</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 15: Interactive Demo - Dot Product Attention -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention"}]'>
                    <h2 class="truncate-title">Interactive Demo: Dot Product Attention Weights</h2>
                    <div style="font-size: 0.85em;">
                        <p>Visualize how attention weights are masked for different valid lengths:</p>
                        <div style="margin-top: 20px;">
                            <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 20px; margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                                <label style="display: flex; align-items: center; gap: 8px;">
                                    Example 1 valid length:
                                    <input type="range" id="valid-len-1" min="1" max="10" value="2" step="1" style="width: 150px;">
                                    <span id="valid-len-1-value" style="font-family: monospace; min-width: 30px;">2</span>
                                </label>
                                <label style="display: flex; align-items: center; gap: 8px;">
                                    Example 2 valid length:
                                    <input type="range" id="valid-len-2" min="1" max="10" value="6" step="1" style="width: 150px;">
                                    <span id="valid-len-2-value" style="font-family: monospace; min-width: 30px;">6</span>
                                </label>
                                <button id="reset-attention-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Reset</button>
                            </div>
                            <div id="attention-heatmap-demo" style="margin-top: 20px;"></div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Observation:</strong> Attention weights are zero beyond the valid length for each example in the batch</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 16: MCQ on Scaled Dot Product -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the DotProductAttention implementation, what is the purpose of keys.transpose(1, 2)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To change the batch size dimension",
                                "correct": false,
                                "explanation": "Incorrect. The transpose does not affect the batch dimension (dimension 0)."
                            },
                            {
                                "text": "To swap the last two dimensions so we can compute Q K^T using batch matrix multiplication",
                                "correct": true,
                                "explanation": "Correct! If keys have shape (batch, num_keys, d), then keys.transpose(1,2) gives shape (batch, d, num_keys), allowing torch.bmm(queries, keys.transpose(1,2)) to compute Q K^T correctly."
                            },
                            {
                                "text": "To apply the scaling factor 1/‚àöd",
                                "correct": false,
                                "explanation": "Incorrect. The scaling is done by dividing by math.sqrt(d), not by transposing."
                            },
                            {
                                "text": "To prepare for the dropout operation",
                                "correct": false,
                                "explanation": "Incorrect. The transpose is for matrix multiplication, not dropout."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 17: Additive Attention Motivation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#additive-attention"}]'>
                    <h2 class="truncate-title">Additive Attention: Different Dimensionalities</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p><strong>Challenge:</strong> What if queries and keys have different dimensions?</p>
                            <div style="margin: 8px 0;">
                                <p>Query: \(\mathbf{q} \in \mathbb{R}^q\) &nbsp;&nbsp;&nbsp; Key: \(\mathbf{k} \in \mathbb{R}^k\) &nbsp;&nbsp; where \(q \neq k\)</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <p><strong>Solutions:</strong></p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin-top: 8px;">
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0; margin-bottom: 5px; font-size: 1em;">Option 1: Use Matrix</h5>
                                    <p style="margin: 0; font-size: 0.95em;">$$\mathbf{q}^\top \mathbf{M} \mathbf{k}$$</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">where \(\mathbf{M} \in \mathbb{R}^{q \times k}\)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0; margin-bottom: 5px; font-size: 1em;">Option 2: Additive Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">More flexible</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Computationally efficient</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <p><strong>Additive Attention Scoring Function</strong> (Bahdanau et al., 2014):</p>
                            <div style="background: #F0FFF9; padding: 10px; border-radius: 8px; margin: 8px 0; border: 2px solid #2DD2C0;">
                                $$a(\mathbf{q}, \mathbf{k}) = \mathbf{w}_v^\top \tanh(\mathbf{W}_q \mathbf{q} + \mathbf{W}_k \mathbf{k}) \in \mathbb{R}$$
                            </div>
                            <p style="font-size: 0.9em; margin-top: 8px; margin-bottom: 5px;">Learnable parameters:</p>
                            <ul style="text-align: left; margin-left: 50px; font-size: 0.85em; line-height: 1.3;">
                                <li>\(\mathbf{W}_q \in \mathbb{R}^{h \times q}\): project queries to hidden dimension</li>
                                <li>\(\mathbf{W}_k \in \mathbb{R}^{h \times k}\): project keys to hidden dimension</li>
                                <li>\(\mathbf{w}_v \in \mathbb{R}^h\): compute scalar score from hidden representation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Slide 18: Additive Attention Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#additive-attention"}]'>
                    <h2 class="truncate-title">Additive Attention: Implementation Details</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p><strong>Key Insight:</strong> Equivalent to concatenating query and key, then feeding through MLP</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <pre style="font-size: 0.85em;"><code class="language-python">class AdditiveAttention(nn.Module):
    """Additive attention (Bahdanau attention)."""
    def __init__(self, num_hiddens, dropout):
        super().__init__()
        self.W_k = nn.LazyLinear(num_hiddens, bias=False)
        self.W_q = nn.LazyLinear(num_hiddens, bias=False)
        self.w_v = nn.LazyLinear(1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        # queries: (batch, num_queries, query_dim)
        # keys:    (batch, num_keys, key_dim)
        # values:  (batch, num_keys, value_dim)

        # Project to hidden dimension
        queries, keys = self.W_q(queries), self.W_k(keys)

        # Dimension expansion for broadcasting
        # queries: (batch, num_queries, 1, num_hiddens)
        # keys:    (batch, 1, num_keys, num_hiddens)
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)

        # Compute scores: (batch, num_queries, num_keys)
        scores = self.w_v(features).squeeze(-1)

        # Apply masked softmax and compute output
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;"><strong>Broadcasting:</strong> Using <code>unsqueeze</code> allows us to compute attention for all query-key pairs in parallel</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 19: Comparing Attention Mechanisms -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"}]'>
                    <h2 class="truncate-title">Dot Product vs Additive Attention: When to Use Each?</h2>
                    <div style="font-size: 0.65em;">
                        <table style="width: 100%; border-collapse: collapse; margin: 10px 0;">
                            <thead>
                                <tr style="background: #10099F; color: white;">
                                    <th style="padding: 8px; border: 1px solid #ddd;">Aspect</th>
                                    <th style="padding: 8px; border: 1px solid #ddd;">Scaled Dot Product</th>
                                    <th style="padding: 8px; border: 1px solid #ddd;">Additive Attention</th>
                                </tr>
                            </thead>
                            <tbody style="font-size: 0.85em;">
                                <tr>
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Formula</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">$$\frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}$$</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">$$\mathbf{w}_v^\top \tanh(\mathbf{W}_q\mathbf{q} + \mathbf{W}_k\mathbf{k})$$</td>
                                </tr>
                                <tr style="background: #f9f9f9;">
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Requirements</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Query and key same dimension</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Any dimensions (projected to hidden dim)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Parameters</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">None (parameter-free)</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">$$\mathbf{W}_q, \mathbf{W}_k, \mathbf{w}_v$$</td>
                                </tr>
                                <tr style="background: #f9f9f9;">
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Computation</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">‚úÖ Very efficient (matrix mult)</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Slightly more expensive</td>
                                </tr>
                                <tr>
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Complexity</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">$$O(nmd)$$</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">$$O(nmh)$$ + projection cost</td>
                                </tr>
                                <tr style="background: #f9f9f9;">
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Memory</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Lower (no params)</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Higher (learnable params)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 7px; border: 1px solid #ddd; font-weight: bold;">Used In</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">‚ú® Transformers, modern NLP</td>
                                    <td style="padding: 7px; border: 1px solid #ddd;">Seq2seq, machine translation</td>
                                </tr>
                            </tbody>
                        </table>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Modern Preference:</strong> Scaled dot product is the dominant choice due to its efficiency and effectiveness, especially with hardware optimization</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 20: Summary and Final MCQ -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.3.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#summary"}]'>
                    <h2 class="truncate-title">Summary: Attention Scoring Functions</h2>
                    <div style="font-size: 0.8em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 25px;">
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                <h4 style="color: #10099F; margin-top: 0;">Key Concepts</h4>
                                <ul style="text-align: left; font-size: 0.9em; margin: 0; padding-left: 20px;">
                                    <li>Dot products more efficient than distance functions</li>
                                    <li>Scaling by \(1/\sqrt{d}\) prevents variance growth</li>
                                    <li>Masked softmax handles variable-length sequences</li>
                                    <li>BMM enables efficient minibatch processing</li>
                                </ul>
                            </div>
                            <div style="background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                <h4 style="color: #2DD2C0; margin-top: 0;">Two Main Mechanisms</h4>
                                <ul style="text-align: left; font-size: 0.9em; margin: 0; padding-left: 20px;">
                                    <li><strong>Dot Product:</strong> Fast, parameter-free, used in Transformers</li>
                                    <li><strong>Additive:</strong> Handles different dimensions, more parameters</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <div style="margin-top: 25px;">
                        <h3 style="color: #10099F;">Test Your Understanding</h3>
                        <div data-mcq='{
                            "question": "Which statement best describes the relationship between dot product and additive attention?",
                            "type": "single",
                            "options": [
                                {
                                    "text": "Additive attention is always better because it has learnable parameters",
                                    "correct": false,
                                    "explanation": "Incorrect. More parameters do not guarantee better performance. Scaled dot product attention is preferred in modern architectures due to its efficiency and effectiveness."
                                },
                                {
                                    "text": "Dot product attention requires queries and keys to have the same dimension, while additive attention can handle different dimensions through learned projections",
                                    "correct": true,
                                    "explanation": "Correct! This is the key difference. Dot product requires matching dimensions for q^T k, while additive attention projects both to a common hidden dimension h, allowing flexibility in input dimensions."
                                },
                                {
                                    "text": "They are mathematically equivalent and produce identical results",
                                    "correct": false,
                                    "explanation": "Incorrect. They are different mechanisms with different mathematical formulations and produce different results."
                                },
                                {
                                    "text": "Additive attention is faster and more memory-efficient",
                                    "correct": false,
                                    "explanation": "Incorrect. Dot product attention is generally faster and more memory-efficient due to optimized matrix multiplication operations."
                                }
                            ]
                        }'></div>
                    </div>
                </section>

            </section>

            <!-- Single Vertical Section: Bahdanau Attention Mechanism -->
            <section>
                <!-- Title Slide -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Section 11.4: Bahdanau Attention", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"}, {"text": "Bahdanau et al. (2014) - Neural Machine Translation by Jointly Learning to Align and Translate", "url": "https://arxiv.org/abs/1409.0473"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.4 The Bahdanau Attention Mechanism</h1>
                    <p>Revolutionizing Sequence-to-Sequence Learning</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- The Fixed-State Problem -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"}]'>
                    <h2 class="truncate-title">The Bottleneck Problem in Sequence-to-Sequence Models</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>In traditional encoder-decoder architectures, the encoder must compress all information into a <strong>fixed-dimensional</strong> state vector.</p>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <img src="images/seq2seq-state.svg" alt="Seq2Seq Fixed State" style="max-width: 550px; height: auto;">
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFF5F5; border-left: 4px solid #FC8484;">
                            <h4 style="color: #FC8484; margin-top: 0;">The Problem</h4>
                            <ul style="margin: 10px 0; font-size: 0.95em;">
                                <li>Short sequences: Works reasonably well</li>
                                <li>Long sequences: Information bottleneck‚Äînot enough "space"</li>
                                <li>Complex sentences: Decoder fails to capture all nuances</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <p><strong>Key Insight:</strong> The state is treated as a <span class="tooltip">sufficient statistic<span class="tooltiptext">A statistic that captures all the information from data needed for a particular inference, discarding nothing relevant</span></span>, but for long sequences, it simply cannot encode everything.</p>
                        </div>
                    </div>
                </section>

                <!-- The Attention Solution -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"}]'>
                    <h2 class="truncate-title">The Attention Solution: Dynamic Context Variables</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p>Instead of a single fixed context $(\mathbf{c})$, Bahdanau attention computes a <strong>different context</strong> $(\mathbf{c}_{t'})$ at <strong>each decoding time step</strong> $(t')$.</p>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <h4 style="color: #10099F; margin-top: 0;">Context as Attention Pooling</h4>
                            <p style="font-size: 1.05em; text-align: center; margin: 15px 0;">
                                $$\mathbf{c}_{t'} = \sum_{t=1}^{T} \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_{t}) \mathbf{h}_{t}$$
                            </p>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 0.85em;">
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border: 2px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">Query</h5>
                                    <p style="margin: 0;">$(\mathbf{s}_{t'-1})$: Previous decoder state</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border: 2px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Keys & Values</h5>
                                    <p style="margin: 0;">$(\mathbf{h}_{t})$: All encoder hidden states</p>
                                </div>
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border: 2px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">Output</h5>
                                    <p style="margin: 0;">$(\mathbf{c}_{t'})$: Weighted context for step $(t')$</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <p><strong>Attention Weight Function:</strong> Computed using <span class="tooltip">additive attention<span class="tooltiptext">A scoring function that uses a learned MLP: Œ± = softmax(w^T tanh(W_q q + W_k k))</span></span> as defined in Section 11.3.2.</p>
                        </div>
                    </div>
                </section>

                <!-- Model Architecture -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"}]'>
                    <h2 class="truncate-title">RNN Encoder-Decoder with Bahdanau Attention</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <img src="images/seq2seq-details-attention.svg" alt="Seq2Seq with Attention Architecture" style="max-width: 550px; height: auto; margin: 10px 0;">
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div class="emphasis-box" style="background: #F5F5FF;">
                                    <h5 style="color: #10099F;">Encoder</h5>
                                    <ul style="margin: 5px 0; font-size: 0.9em;">
                                        <li>Processes source sequence</li>
                                        <li>Outputs hidden states $(\mathbf{h}_1, ..., \mathbf{h}_T)$</li>
                                        <li>Each $(\mathbf{h}_t)$ encodes context around position $(t)$</li>
                                    </ul>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9;">
                                    <h5 style="color: #2DD2C0;">Decoder with Attention</h5>
                                    <ul style="margin: 5px 0; font-size: 0.9em;">
                                        <li>At each step $(t')$, computes $(\mathbf{c}_{t'})$</li>
                                        <li>Attends to relevant encoder states</li>
                                        <li>Generates next token using context</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 10px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Key Point:</strong> The decoder can now "look back" at the entire source sequence, selectively focusing on the most relevant parts at each generation step.</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ 1: Understanding Attention Mechanism -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary advantage of Bahdanau attention over traditional seq2seq models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses fewer parameters and trains faster",
                                "correct": false,
                                "explanation": "Incorrect. Attention actually adds parameters (for the attention mechanism) and may increase training time slightly."
                            },
                            {
                                "text": "It computes a dynamic context vector at each decoding step, allowing the decoder to focus on relevant parts of the input",
                                "correct": true,
                                "explanation": "Correct! This is the key innovation. Instead of compressing everything into a fixed context, the decoder can selectively attend to different encoder states at each step."
                            },
                            {
                                "text": "It eliminates the need for recurrent connections in the encoder",
                                "correct": false,
                                "explanation": "Incorrect. Bahdanau attention still uses RNNs in both encoder and decoder; it adds attention on top of the RNN architecture."
                            },
                            {
                                "text": "It only works with short sequences and fails on long ones",
                                "correct": false,
                                "explanation": "Incorrect. Attention specifically addresses the problem of long sequences by avoiding the fixed-context bottleneck."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Decoder Implementation: Base Class -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Implementing the Decoder with Attention</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <p>We start by defining a base interface for attention-based decoders:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <pre><code class="language-python" data-trim>
class AttentionDecoder(d2l.Decoder):  #@save
    """The base attention-based decoder interface."""
    def __init__(self):
        super().__init__()

    @property
    def attention_weights(self):
        raise NotImplementedError
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF;">
                                <p style="margin: 0;"><strong>Key Feature:</strong> The <code>attention_weights</code> property allows us to extract and visualize which parts of the input sequence the decoder is focusing on at each time step.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Seq2SeqAttentionDecoder: Initialization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Seq2SeqAttentionDecoder: Architecture Components</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <pre><code class="language-python" data-trim>
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.LazyLinear(vocab_size)
        self.apply(d2l.init_seq2seq)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin: 5px 0;">Key Components</h5>
                                    <ul style="margin: 5px 0; font-size: 0.9em;">
                                        <li><strong>attention:</strong> Additive attention mechanism</li>
                                        <li><strong>embedding:</strong> Token to vector mapping</li>
                                        <li><strong>rnn:</strong> <span class="tooltip">GRU<span class="tooltiptext">Gated Recurrent Unit: A type of RNN that uses gating mechanisms to control information flow</span></span> decoder</li>
                                        <li><strong>dense:</strong> Output projection to vocabulary</li>
                                    </ul>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">Important Detail</h5>
                                    <p style="margin: 5px 0; font-size: 0.9em;">Notice the GRU input size: <code>embed_size + num_hiddens</code></p>
                                    <p style="margin: 5px 0; font-size: 0.85em;">This concatenates the embedding with the context vector!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- State Initialization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Initializing the Decoder State</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <pre><code class="language-python" data-trim>
def init_state(self, enc_outputs, enc_valid_lens):
    # Shape of outputs: (num_steps, batch_size, num_hiddens).
    # Shape of hidden_state: (num_layers, batch_size, num_hiddens)
    outputs, hidden_state = enc_outputs
    return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin-top: 0;">Decoder State Components</h4>
                                <div style="font-size: 0.9em;">
                                    <ol style="margin: 10px 0;">
                                        <li><strong>outputs:</strong> All encoder hidden states‚Äîused as keys and values
                                            <ul style="font-size: 0.9em; color: #666;">
                                                <li>Shape: <code>(batch_size, num_steps, num_hiddens)</code></li>
                                            </ul>
                                        </li>
                                        <li><strong>hidden_state:</strong> Final encoder state‚Äîinitializes decoder
                                            <ul style="font-size: 0.9em; color: #666;">
                                                <li>Shape: <code>(num_layers, batch_size, num_hiddens)</code></li>
                                            </ul>
                                        </li>
                                        <li><strong>enc_valid_lens:</strong> Actual lengths (for <span class="tooltip">masking padding<span class="tooltiptext">Preventing the attention mechanism from attending to padding tokens in sequences of varying lengths</span></span>)</li>
                                    </ol>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Forward Pass Part 1 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Forward Pass: Step-by-Step Decoding</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <pre><code class="language-python" data-trim data-line-numbers="1-11">
def forward(self, X, state):
    # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).
    # Shape of hidden_state: (num_layers, batch_size, num_hiddens)
    enc_outputs, hidden_state, enc_valid_lens = state

    # Shape of the output X: (num_steps, batch_size, embed_size)
    X = self.embedding(X).permute(1, 0, 2)
    outputs, self._attention_weights = [], []

    for x in X:
        # Process each decoder time step...
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #F5F5FF;">
                                <h5 style="color: #10099F; margin: 5px 0;">Setup Phase</h5>
                                <ul style="margin: 5px 0; font-size: 0.95em;">
                                    <li>Extract encoder outputs (keys/values) and hidden state from decoder state</li>
                                    <li>Embed input tokens and transpose to <code>(num_steps, batch_size, embed_size)</code></li>
                                    <li>Initialize lists to collect outputs and attention weights</li>
                                    <li>Loop through each decoder time step</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Forward Pass Part 2 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Forward Pass: Computing Context at Each Step</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <pre><code class="language-python" data-trim data-line-numbers="1-12">
for x in X:
    # Shape of query: (batch_size, 1, num_hiddens)
    query = torch.unsqueeze(hidden_state[-1], dim=1)

    # Shape of context: (batch_size, 1, num_hiddens)
    context = self.attention(
        query, enc_outputs, enc_outputs, enc_valid_lens)

    # Concatenate on the feature dimension
    x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)

    # Reshape x as (1, batch_size, embed_size + num_hiddens)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.85em;">
                                <div style="background: #FFFBF0; padding: 10px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">1. Query Formation</h5>
                                    <p style="margin: 3px 0; font-size: 0.9em;">Use last layer's hidden state as query</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin: 5px 0;">2. Attention</h5>
                                    <p style="margin: 3px 0; font-size: 0.9em;">Compute weighted context from encoder</p>
                                </div>
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin: 5px 0;">3. Concatenation</h5>
                                    <p style="margin: 3px 0; font-size: 0.9em;">Combine context + input embedding</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 10px; border-radius: 8px;">
                                    <h5 style="color: #FC8484; margin: 5px 0;">4. RNN Step</h5>
                                    <p style="margin: 3px 0; font-size: 0.9em;">Process through GRU (next slide)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Forward Pass Part 3 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#defining-the-decoder-with-attention"}]'>
                    <h2 class="truncate-title">Forward Pass: RNN Processing and Output</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <pre><code class="language-python" data-trim data-line-numbers="1-13">
    # (Continuing from previous slide...)
    out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
    outputs.append(out)
    self._attention_weights.append(self.attention.attention_weights)

# After fully connected layer transformation, shape of outputs:
# (num_steps, batch_size, vocab_size)
outputs = self.dense(torch.cat(outputs, dim=0))
return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                  enc_valid_lens]

@property
def attention_weights(self):
    return self._attention_weights
                            </code></pre>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <h5 style="color: #10099F; margin-top: 0;">Complete Forward Pass Summary</h5>
                            <ol style="margin: 8px 0; font-size: 0.9em;">
                                <li>Process concatenated input through GRU, updating hidden state</li>
                                <li>Store RNN output and attention weights for this step</li>
                                <li>After all steps: project outputs to vocabulary size via dense layer</li>
                                <li>Return predictions and updated state (for next forward pass)</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <!-- MCQ 2: Decoder Implementation -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the Seq2SeqAttentionDecoder, what serves as the query in the attention mechanism at each decoding step?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The current input token embedding",
                                "correct": false,
                                "explanation": "Incorrect. The input embedding is concatenated with the context after attention is computed, not used as the query."
                            },
                            {
                                "text": "The hidden state of the decoder from the previous time step",
                                "correct": true,
                                "explanation": "Correct! Specifically, the hidden state from the last layer at the previous time step (hidden_state[-1]) is used as the query to compute attention over encoder outputs."
                            },
                            {
                                "text": "All previous decoder hidden states concatenated together",
                                "correct": false,
                                "explanation": "Incorrect. Only the most recent hidden state is used as the query at each step."
                            },
                            {
                                "text": "The final encoder hidden state",
                                "correct": false,
                                "explanation": "Incorrect. The encoder states serve as keys and values, not the query. The query comes from the decoder."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Training Configuration -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#training"}]'>
                    <h2 class="truncate-title">Training the Model</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <p>Training setup uses the same approach as standard seq2seq, but with the attention-enhanced decoder:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <pre><code class="language-python" data-trim>
data = d2l.MTFraEng(batch_size=128)
embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2

encoder = d2l.Seq2SeqEncoder(
    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)

model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                    lr=0.005)
trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
trainer.fit(model, data)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin: 5px 0;">Hyperparameters</h5>
                                    <ul style="margin: 5px 0; font-size: 0.9em;">
                                        <li>Embedding: 256 dims</li>
                                        <li>Hidden: 256 dims</li>
                                        <li>Layers: 2</li>
                                        <li>Dropout: 0.2</li>
                                    </ul>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">Training Details</h5>
                                    <ul style="margin: 5px 0; font-size: 0.9em;">
                                        <li>Dataset: English‚ÜíFrench</li>
                                        <li>Batch size: 128</li>
                                        <li>Learning rate: 0.005</li>
                                        <li>Gradient clipping: 1.0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Training Results -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#training"}]'>
                    <h2 class="truncate-title">Training Progress</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <img src="images/bahdanau-training.svg" alt="Training and Validation Loss" style="max-width: 600px; height: auto; margin: 20px auto; display: block;">
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <h5 style="color: #2DD2C0; margin-top: 0;">Observations</h5>
                                <ul style="margin: 8px 0; font-size: 0.9em;">
                                    <li><strong>Training loss:</strong> Decreases steadily, converging near 0</li>
                                    <li><strong>Validation loss:</strong> Stable around 1.5-2.0</li>
                                    <li><strong>Gap:</strong> Indicates some overfitting, but model generalizes reasonably</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Translation Examples -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#training"}]'>
                    <h2 class="truncate-title">Translation Results with BLEU Scores</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <p>Let's evaluate the model on a few English sentences:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <pre><code class="language-python" data-trim>
engs = ['go .', 'i lost .', "he's calm .", "i'm home ."]
fras = ['va !', "j'ai perdu .", 'il est calme .', 'je suis chez moi .']
preds, _ = model.predict_step(
    data.build(engs, fras), d2l.try_gpu(), data.num_steps)
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; font-family: monospace; font-size: 0.85em;">
                                <div style="margin: 8px 0;">
                                    <span style="color: #10099F;">go .</span> => <span style="color: #2DD2C0;">['va', '!']</span>,
                                    <span style="color: #FAC55B;">BLEU: 1.000</span> ‚úì
                                </div>
                                <div style="margin: 8px 0;">
                                    <span style="color: #10099F;">i lost .</span> => <span style="color: #2DD2C0;">["j'ai", 'perdu', '.']</span>,
                                    <span style="color: #FAC55B;">BLEU: 1.000</span> ‚úì
                                </div>
                                <div style="margin: 8px 0;">
                                    <span style="color: #10099F;">he's calm .</span> => <span style="color: #FC8484;">['il', 'court', '.']</span>,
                                    <span style="color: #FC8484;">BLEU: 0.000</span> ‚úó
                                </div>
                                <div style="margin: 8px 0;">
                                    <span style="color: #10099F;">i'm home .</span> => <span style="color: #2DD2C0;">['je', 'suis', 'chez', 'moi', '.']</span>,
                                    <span style="color: #FAC55B;">BLEU: 1.000</span> ‚úì
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <p style="font-size: 0.9em; color: #666;"><strong>Note:</strong> <span class="tooltip">BLEU score<span class="tooltiptext">Bilingual Evaluation Understudy: A metric that measures how similar the machine translation is to reference translations, based on n-gram overlap</span></span> of 1.000 indicates perfect match with reference translation</p>
                        </div>
                    </div>
                </section>

                <!-- Attention Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#training"}]'>
                    <h2 class="truncate-title">Visualizing Attention: "I'm home"</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <p>Let's examine which source words the decoder attends to when translating <em>"i'm home ."</em>:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <img src="images/bahdanau-attention-heatmap.svg" alt="Attention Heatmap" style="max-width: 550px; height: auto; margin: 15px auto; display: block;">
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <h5 style="color: #10099F; margin-top: 0;">Key Observations</h5>
                                <ul style="margin: 8px 0; font-size: 0.9em;">
                                    <li><strong>Non-uniform weights:</strong> Each query focuses on different source positions</li>
                                    <li><strong>Selective aggregation:</strong> Different parts of input are weighted differently</li>
                                    <li><strong>Alignment learning:</strong> Model learns to align source and target words</li>
                                    <li><strong>Dynamic context:</strong> Each decoder step gets a custom-weighted context</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ 3: Overall Understanding -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "How does Bahdanau attention address the bottleneck problem in sequence-to-sequence models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "By using a larger fixed-dimensional state vector",
                                "correct": false,
                                "explanation": "Incorrect. Simply increasing the state size does not fundamentally solve the bottleneck‚Äîit just postpones it for longer sequences."
                            },
                            {
                                "text": "By computing a weighted sum of all encoder hidden states at each decoder step, with weights determined by the current decoder state",
                                "correct": true,
                                "explanation": "Correct! This is the essence of Bahdanau attention. The context variable is dynamically computed as a weighted combination of encoder states, where weights depend on relevance to the current decoder state."
                            },
                            {
                                "text": "By removing the encoder and processing the source sequence directly in the decoder",
                                "correct": false,
                                "explanation": "Incorrect. Bahdanau attention still uses an encoder-decoder architecture; it enhances it with attention, not replaces it."
                            },
                            {
                                "text": "By limiting translations to short sequences only",
                                "correct": false,
                                "explanation": "Incorrect. The goal of attention is to handle long sequences better, not to avoid them."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Key Takeaways -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.4.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html#summary"}]'>
                    <h2 class="truncate-title">Key Takeaways: Bahdanau Attention</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; margin-bottom: 20px;">
                            <h4 style="color: #10099F; margin-top: 0;">Core Concept</h4>
                            <p style="margin: 0;">The decoder doesn't use a single fixed context‚Äîit computes a <strong>dynamic context</strong> $(\mathbf{c}_{t'})$ at each step by attending to all encoder states.</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.85em;">
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin: 5px 0;">Mechanism</h5>
                                    <ul style="margin: 5px 0;">
                                        <li><strong>Query:</strong> Decoder hidden state</li>
                                        <li><strong>Keys/Values:</strong> Encoder states</li>
                                        <li><strong>Scoring:</strong> Additive attention</li>
                                    </ul>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">Benefits</h5>
                                    <ul style="margin: 5px 0;">
                                        <li>Handles long sequences</li>
                                        <li>Selective information use</li>
                                        <li>Interpretable alignments</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #FFF5F5; border-left: 4px solid #FC8484;">
                            <h4 style="color: #FC8484; margin-top: 0;">Historical Impact</h4>
                            <p style="margin: 0;">This mechanism became one of the <strong>most influential ideas</strong> of the past decade, laying the foundation for <strong>Transformers</strong> and modern NLP architectures.</p>
                        </div>
                    </div>
                </section>

            </section>

            <!-- Multi-Head Attention Section (11.5) -->
            <section>
                <!-- Slide 1: Section Title -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Section 11.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.5 Multi-Head Attention</h1>
                    <p>Attending from Multiple Perspectives</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Motivation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html"}]'>
                    <h2 class="truncate-title">Why Multiple Attention Heads?</h2>
                    <div style="font-size: 0.8em;">
                        <div class="fragment">
                            <p>Given the same queries, keys, and values, we may want our model to capture knowledge from <strong>different behaviors</strong> of the attention mechanism:</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.9em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin: 5px 0;">üéØ Different Ranges</h5>
                                    <p style="margin: 5px 0; font-size: 0.95em;">Some heads capture <strong>short-range</strong> dependencies (nearby words)</p>
                                    <p style="margin: 5px 0; font-size: 0.95em;">Other heads capture <strong>long-range</strong> dependencies (distant context)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin: 5px 0;">üîç Different Features</h5>
                                    <p style="margin: 5px 0; font-size: 0.95em;">Some heads focus on <strong>syntactic</strong> relationships (grammar)</p>
                                    <p style="margin: 5px 0; font-size: 0.95em;">Other heads focus on <strong>semantic</strong> relationships (meaning)</p>
                                </div>
                            </div>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Benefit:</strong> By combining knowledge from <strong>\\(h\\) different representation subspaces</strong>, the model can learn richer and more nuanced representations than single-head attention.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Architecture Overview -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html"}]'>
                    <h2 class="truncate-title">Multi-Head Attention Architecture</h2>
                    <div style="font-size: 0.75em;">
                        <div style="text-align: center; margin: 20px 0;">
                            <img src="images/multi-head-attention.svg" alt="Multi-head attention architecture" style="max-width: 65%; height: auto;">
                        </div>

                        <div class="fragment">
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin-top: 15px;">
                                <h5 style="color: #10099F; margin: 5px 0;">Architecture Flow</h5>
                                <ol style="margin: 8px 0; font-size: 0.95em; line-height: 1.6;">
                                    <li><strong>Linear Projections:</strong> Transform Q, K, V with $(h)$ independent <span class="tooltip">learned projections<span class="tooltiptext">Linear transformations applied via weight matrices that map inputs to lower-dimensional subspaces</span></span></li>
                                    <li><strong>Parallel Attention:</strong> Feed $(h)$ projected versions into attention pooling <strong>in parallel</strong></li>
                                    <li><strong>Concatenation:</strong> Combine the $(h)$ attention outputs</li>
                                    <li><strong>Output Projection:</strong> Apply final learned linear transformation</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Mathematical Formulation - Part 1 (Individual Head) -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#model"}]'>
                    <h2 class="truncate-title">Mathematical Formulation: Individual Head</h2>
                    <div style="font-size: 0.68em;">
                        <div>
                            <p>Given a query $(\mathbf{q} \in \mathbb{R}^{d_q})$, a key $(\mathbf{k} \in \mathbb{R}^{d_k})$, and a value $(\mathbf{v} \in \mathbb{R}^{d_v})$, each attention head $(\mathbf{h}_i)$ ($(i = 1, \ldots, h)$) is computed as:</p>
                        </div>

                        <div class="fragment" style="background: #F5F5FF; padding: 12px; border-radius: 8px; margin: 12px 0;">
                            <p style="font-size: 1.3em; text-align: center; margin: 0;">
                                $$\mathbf{h}_i = f(\color{#10099F}{\mathbf{W}_i^{(q)}\mathbf{q}}\color{black}{,} \color{#2DD2C0}{\mathbf{W}_i^{(k)}\mathbf{k}}\color{black}{,} \color{#FFA05F}{\mathbf{W}_i^{(v)}\mathbf{v}}\color{black}{)} \in \mathbb{R}^{p_v}$$
                            </p>
                        </div>

                        <div class="fragment">
                            <p><strong>Components:</strong></p>
                            <ul style="font-size: 0.95em; line-height: 1.5;">
                                <li><span style="color: #10099F;">$(\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q})$</span>: <span class="tooltip">Learnable projection<span class="tooltiptext">A weight matrix that is learned during training through backpropagation</span></span> for queries</li>
                                <li><span style="color: #2DD2C0;">$(\mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k})$</span>: Learnable projection for keys</li>
                                <li><span style="color: #FFA05F;">$(\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v})$</span>: Learnable projection for values</li>
                                <li>$(f)$: <span class="tooltip">Attention pooling function<span class="tooltiptext">Could be additive attention, scaled dot-product attention, or other attention mechanisms</span></span> (e.g., scaled dot-product attention)</li>
                            </ul>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 10px; background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                            <p style="margin: 0; font-size: 0.95em;"><strong>Key Point:</strong> Each head $(i)$ has its <strong>own set of learnable parameters</strong>, allowing it to focus on different aspects of the input.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: Mathematical Formulation - Part 2 (Multi-Head Output) -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#model"}]'>
                    <h2 class="truncate-title">Mathematical Formulation: Multi-Head Output</h2>
                    <div style="font-size: 0.7em;">
                        <div style="display: grid; grid-template-columns: 0.4fr 0.6fr; gap: 20px; align-items: start;">
                            <div>
                                <p>The multi-head attention output is obtained by <span class="tooltip">concatenating<span class="tooltiptext">Stacking vectors end-to-end to form a longer vector</span></span> all $(h)$ heads and applying another linear transformation:</p>
                                <div class="fragment" style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin: 12px 0;">
                                    <p style="font-size: 1.25em; text-align: center; margin: 0;">
                                        $$\mathbf{W}_o \begin{bmatrix}\mathbf{h}_1\\\vdots\\\mathbf{h}_h\end{bmatrix} \in \mathbb{R}^{p_o}$$
                                    </p>
                                </div>
                            </div>
                            <div>
                                <div class="fragment">
                                    <p><strong>Where:</strong></p>
                                    <ul style="font-size: 0.95em; line-height: 1.5;">
                                        <li>$(\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v})$: Output projection matrix</li>
                                        <li>The concatenated vector has dimension $(h p_v)$ ($(h)$ heads √ó $(p_v)$ dimensions per head)</li>
                                        <li>Final output has dimension $(p_o)$</li>
                                    </ul>
                                </div>
                                <div class="fragment emphasis-box" style="margin-top: 10px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                    <p style="margin: 0; font-size: 0.95em;"><strong>Expressiveness:</strong> Based on this design, each head may attend to <strong>different parts of the input</strong>. More sophisticated functions than simple weighted averages can be expressed!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: MCQ - Understanding Multi-Head Concept -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary benefit of using multiple attention heads instead of a single head?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It reduces the computational cost of attention",
                                "correct": false,
                                "explanation": "Incorrect. Multi-head attention actually increases computational cost since we compute multiple attention functions in parallel."
                            },
                            {
                                "text": "It allows the model to jointly attend to information from different representation subspaces",
                                "correct": true,
                                "explanation": "Correct! Multiple heads enable the model to capture different types of relationships (e.g., short-range vs. long-range, syntactic vs. semantic) simultaneously."
                            },
                            {
                                "text": "It eliminates the need for learned weight matrices",
                                "correct": false,
                                "explanation": "Incorrect. Multi-head attention actually requires more weight matrices‚Äîeach head has its own set of learnable projections."
                            },
                            {
                                "text": "It guarantees that the model will converge faster during training",
                                "correct": false,
                                "explanation": "Incorrect. While multi-head attention can improve model performance, it does not guarantee faster convergence."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 7: Implementation Details -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Implementation: Dimension Choices</h2>
                    <div style="font-size: 0.68em;">
                        <div>
                            <p>To avoid significant growth of computational and <span class="tooltip">parametrization costs<span class="tooltiptext">The number of learnable parameters in the model that need to be stored and trained</span></span>, we make strategic dimension choices:</p>
                        </div>

                        <div class="fragment" style="background: #F5F5FF; padding: 12px; border-radius: 8px; margin: 12px 0;">
                            <p style="font-size: 1.15em; text-align: center; margin: 0;">
                                $$p_q = p_k = p_v = \frac{p_o}{h}$$
                            </p>
                        </div>

                        <div class="fragment">
                            <p><strong>Implementation Strategy:</strong></p>
                            <ul style="font-size: 0.95em; line-height: 1.5;">
                                <li>Set the number of outputs of linear transformations to $(p_q h = p_k h = p_v h = p_o)$</li>
                                <li>This allows $(h)$ heads to be computed <strong>in parallel</strong> efficiently</li>
                                <li>Each head operates on $(\frac{p_o}{h})$ dimensions</li>
                                <li>In the implementation, $(p_o)$ is specified via <code>num_hiddens</code></li>
                            </ul>
                        </div>

                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 12px; font-size: 0.85em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin: 5px 0;">‚úì Benefits</h5>
                                    <ul style="margin: 5px 0; font-size: 0.95em;">
                                        <li>Constant parameter count</li>
                                        <li>Parallel computation</li>
                                        <li>Efficient GPU utilization</li>
                                    </ul>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">‚öôÔ∏è Attention Function</h5>
                                    <p style="margin: 5px 0; font-size: 0.95em;">We use <strong>scaled dot-product attention</strong> for each head</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: Code Walkthrough - Class Definition -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Implementation: Class Definition</h2>
                    <div style="font-size: 0.65em;">
                        <p>The <code>MultiHeadAttention</code> class initializes the key components:</p>
                        <pre style="margin: 15px 0;"><code class="language-python" style="max-height: 400px;">class MultiHeadAttention(d2l.Module):
    """Multi-head attention."""
    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):
        super().__init__()
        self.num_heads = num_heads
        # Use scaled dot-product attention for each head
        self.attention = d2l.DotProductAttention(dropout)
        # Linear transformations for queries, keys, and values
        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)
        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)
        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)
        # Output projection
        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)</code></pre>

                        <div class="fragment">
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin-top: 15px;">
                                <h5 style="color: #10099F; margin: 5px 0;">Key Components</h5>
                                <ul style="margin: 8px 0; font-size: 0.95em; line-height: 1.6;">
                                    <li><code>num_heads</code>: Number of parallel attention heads $(h)$</li>
                                    <li><code>W_q, W_k, W_v</code>: Linear layers for projecting Q, K, V (all output <code>num_hiddens</code> dimensions)</li>
                                    <li><code>W_o</code>: Output projection layer</li>
                                    <li><code>attention</code>: The attention pooling mechanism (scaled dot-product)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 9: Code Walkthrough - Forward Pass Part 1 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Implementation: Forward Pass (Part 1)</h2>
                    <div style="font-size: 0.62em;">
                        <pre style="margin: 10px 0;"><code class="language-python" style="max-height: 420px;">def forward(self, queries, keys, values, valid_lens):
    # Shape of queries, keys, or values:
    # (batch_size, no. of queries or key-value pairs, num_hiddens)
    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)

    # After transposing, shape of output queries, keys, or values:
    # (batch_size * num_heads, no. of queries or key-value pairs,
    # num_hiddens / num_heads)
    queries = self.transpose_qkv(self.W_q(queries))
    keys = self.transpose_qkv(self.W_k(keys))
    values = self.transpose_qkv(self.W_v(values))

    if valid_lens is not None:
        # On axis 0, copy the first item (scalar or vector) for num_heads
        # times, then copy the next item, and so on
        valid_lens = torch.repeat_interleave(
            valid_lens, repeats=self.num_heads, dim=0)</code></pre>

                        <div class="fragment" style="background: #F0FFF9; padding: 15px; border-radius: 8px; margin-top: 15px;">
                            <h5 style="color: #2DD2C0; margin: 5px 0;">What's Happening?</h5>
                            <ol style="margin: 8px 0; font-size: 0.95em; line-height: 1.6;">
                                <li>Apply linear transformations: <code>W_q(queries)</code>, <code>W_k(keys)</code>, <code>W_v(values)</code></li>
                                <li>Reshape tensors for parallel head computation via <code>transpose_qkv</code></li>
                                <li>Replicate <code>valid_lens</code> for each head</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Code Walkthrough - Forward Pass Part 2 -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Implementation: Forward Pass (Part 2)</h2>
                    <div style="font-size: 0.65em;">
                        <pre style="margin: 10px 0;"><code class="language-python" style="max-height: 420px;">    # Shape of output: (batch_size * num_heads, no. of queries,
    # num_hiddens / num_heads)
    output = self.attention(queries, keys, values, valid_lens)

    # Shape of output_concat: (batch_size, no. of queries, num_hiddens)
    output_concat = self.transpose_output(output)

    return self.W_o(output_concat)</code></pre>

                        <div class="fragment" style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin-top: 15px;">
                            <h5 style="color: #10099F; margin: 5px 0;">Final Steps</h5>
                            <ol style="margin: 8px 0; font-size: 0.95em; line-height: 1.6;">
                                <li>Compute attention for all heads in parallel: <code>self.attention(...)</code></li>
                                <li>Reshape output back and concatenate heads: <code>transpose_output</code></li>
                                <li>Apply final linear transformation: <code>W_o</code></li>
                            </ol>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0; font-size: 0.95em;"><strong>Result:</strong> Output shape is <code>(batch_size, num_queries, num_hiddens)</code>‚Äîsame as input!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Code Walkthrough - Transpose Operations -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Implementation: Tensor Reshaping</h2>
                    <div style="font-size: 0.6em;">
                        <p>Two helper methods enable parallel computation of multiple heads:</p>
                        <pre style="margin: 10px 0;"><code class="language-python" style="max-height: 400px;">def transpose_qkv(self, X):
    """Transposition for parallel computation of multiple attention heads."""
    # Shape of input X: (batch_size, no. of queries or key-value pairs, num_hiddens)
    # Shape of output X: (batch_size, no. of queries or key-value pairs,
    #                     num_heads, num_hiddens / num_heads)
    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)
    # Shape of output X: (batch_size, num_heads, no. of queries or key-value pairs,
    #                     num_hiddens / num_heads)
    X = X.permute(0, 2, 1, 3)
    # Shape of output: (batch_size * num_heads, no. of queries or key-value pairs,
    #                   num_hiddens / num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])

def transpose_output(self, X):
    """Reverse the operation of transpose_qkv."""
    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)</code></pre>

                        <div class="fragment" style="background: #F0FFF9; padding: 15px; border-radius: 8px; margin-top: 15px;">
                            <p style="margin: 0; font-size: 0.95em;"><strong>Purpose:</strong> <code>transpose_qkv</code> reshapes to enable parallel processing, <code>transpose_output</code> reverses the operation and concatenates heads.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: Interactive Demo -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation"}]'>
                    <h2 class="truncate-title">Interactive Demo: Multi-Head Attention</h2>
                    <div style="font-size: 0.75em;">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 20px; margin-bottom: 20px; padding: 12px; background: #f9f9f9; border-radius: 8px; z-index: 100; position: relative;">
                            <label style="display: flex; align-items: center; gap: 10px;">
                                <span style="font-weight: 600;">Number of Heads:</span>
                                <input type="range" id="num-heads-slider" min="1" max="4" value="2" step="1" style="width: 180px;">
                                <span id="num-heads-value" style="font-family: monospace; font-weight: bold; color: #10099F; min-width: 30px;">2</span>
                            </label>
                            <button id="reset-multihead-btn" style="background: #10099F; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; font-weight: 600;">Reset</button>
                        </div>

                        <div id="multihead-attention-demo" style="display: flex; justify-content: center; align-items: center; min-height: 280px;"></div>

                        <div style="margin-top: 15px; font-size: 0.85em; text-align: center; color: #666;">
                            <p style="margin: 5px 0;">Each heatmap shows attention weights from a <strong>different head</strong></p>
                            <p style="margin: 5px 0;">Notice how different heads learn to focus on different positions!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: MCQ - Implementation Details -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the multi-head attention implementation, why do we set p_q = p_k = p_v = p_o / h?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make each head learn the same representation",
                                "correct": false,
                                "explanation": "Incorrect. Each head learns different representations through its own weight matrices, not because of dimension choices."
                            },
                            {
                                "text": "To avoid significant growth in computational and parametrization costs while enabling parallel computation",
                                "correct": true,
                                "explanation": "Correct! This dimension choice keeps the total parameter count and computational cost similar to single-head attention while enabling h heads to be computed in parallel."
                            },
                            {
                                "text": "To ensure that attention weights sum to 1",
                                "correct": false,
                                "explanation": "Incorrect. Attention weights summing to 1 is ensured by the softmax function, not by dimension choices."
                            },
                            {
                                "text": "To eliminate the need for the output projection matrix W_o",
                                "correct": false,
                                "explanation": "Incorrect. We still need W_o to transform the concatenated heads to the desired output dimension."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 14: Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.5.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#summary"}]'>
                    <h2 class="truncate-title">Summary: Multi-Head Attention</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; margin-bottom: 12px;">
                            <h4 style="color: #10099F; margin-top: 0;">Core Concept</h4>
                            <p style="margin: 0;">Multi-head attention combines knowledge from <strong>different representation subspaces</strong> of queries, keys, and values through $(h)$ parallel attention functions.</p>
                        </div>

                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.85em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin: 5px 0;">‚úì Key Benefits</h5>
                                    <ul style="margin: 8px 0; line-height: 1.5;">
                                        <li><strong>Multiple perspectives:</strong> Capture diverse relationships</li>
                                        <li><strong>Parallel processing:</strong> Efficient computation</li>
                                        <li><strong>Richer representations:</strong> Better than single-head</li>
                                    </ul>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #FAC55B; margin: 5px 0;">‚öôÔ∏è Implementation</h5>
                                    <ul style="margin: 8px 0; line-height: 1.5;">
                                        <li>Each head: \\(p_o / h\\) dimensions</li>
                                        <li>Scaled dot-product attention per head</li>
                                        <li><span class="tooltip">Tensor reshaping<span class="tooltiptext">Using reshape and permute operations to arrange data for parallel computation</span></span> enables parallelization</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFF5F5; border-left: 4px solid #FC8484;">
                            <h4 style="color: #FC8484; margin-top: 0;">Looking Ahead</h4>
                            <p style="margin: 0;">Multi-head attention is a <strong>cornerstone of the Transformer architecture</strong>, which has revolutionized natural language processing and beyond!</p>
                        </div>
                    </div>
                </section>

            </section>

            <!-- Section: Self-Attention and Positional Encoding -->
            <section>

                <!-- Slide 1: Title Slide -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.6 Self-Attention and Positional Encoding</h1>
                    <p>Understanding How Sequences Attend to Themselves</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Self-Attention Concept -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Self-Attention: Every Token Attends to Every Token</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>In deep learning, we often use CNNs or RNNs to encode sequences. With attention mechanisms, we can do something different:</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin-top: 0;">üîÑ Self-Attention Mechanism</h4>
                                <p style="margin: 0;">Each token in a sequence can attend to <strong>all other tokens</strong> in the same sequence, including itself!</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <p><strong>Key Idea:</strong> At every step, each token has its own:</p>
                            <ul style="font-size: 0.95em; margin-top: 10px;">
                                <li><strong style="color: #10099F;">Query vector</strong> - what information it's looking for</li>
                                <li><strong style="color: #2DD2C0;">Key vector</strong> - what information it contains</li>
                                <li><strong style="color: #FC8484;">Value vector</strong> - the actual information to retrieve</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #F9F9F9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                <p style="margin: 0;"><strong>Also called:</strong> <span class="tooltip">self-attention<span class="tooltiptext">Architecture where queries, keys, and values all come from the same sequence</span></span> or <span class="tooltip">intra-attention<span class="tooltiptext">Alternative name emphasizing attention within a single sequence</span></span></p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Self-Attention Formula -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Self-Attention: Mathematical Formulation</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>Given a sequence of input tokens \(\mathbf{x}_1, \ldots, \mathbf{x}_n\) where \(\mathbf{x}_i \in \mathbb{R}^d\):</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #F5F5FF; padding: 20px; border-radius: 8px; border: 2px solid #10099F;">
                                <p style="font-size: 1.1em; margin-bottom: 15px;"><strong>Self-attention outputs:</strong></p>
                                <p style="font-size: 1.2em; text-align: center; margin: 20px 0;">
                                    $$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.9em;">
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">üì• Input</h5>
                                    <p style="margin: 0;">Sequence of \(n\) tokens, each \(d\)-dimensional</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #FC8484; margin-top: 0;">üì§ Output</h5>
                                    <p style="margin: 0;">Sequence of \(n\) tokens, same \(d\)-dimensional</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Notice:</strong> Both keys and values come from the <strong>same sequence</strong> \(\mathbf{x}_1, \ldots, \mathbf{x}_n\)</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Self-Attention in Practice -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Using Multi-Head Attention for Self-Attention</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p>We can use multi-head attention for self-attention by feeding the <strong>same input</strong> as queries, keys, and values:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <pre><code class="language-python" style="font-size: 0.85em;">num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_heads, dropout=0.5)

batch_size, num_queries = 2, 4
valid_lens = torch.tensor([3, 2])  # Sequence lengths
X = torch.ones((batch_size, num_queries, num_hiddens))

# Self-attention: X is used for queries, keys, AND values!
output = attention(X, X, X, valid_lens)
# Output shape: (batch_size, num_queries, num_hiddens)
# Same shape as input!</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin-top: 0;">üéØ Key Insight</h4>
                                <p style="margin: 0;">The input tensor <code style="background: white; padding: 2px 6px; border-radius: 3px;">X</code> serves <strong>triple duty</strong>: it's transformed into queries, keys, and values through learned linear projections</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: Architecture Comparison Diagram -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Comparing CNNs, RNNs, and Self-Attention</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <p>How do different architectures process sequences of \(n\) tokens with \(d\)-dimensional representations?</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <img src="images/cnn-rnn-self-attention.svg" alt="CNN RNN Self-Attention Comparison" style="max-width: 90%; max-height: 450px;">
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0; font-size: 0.9em;"><strong>Key Question:</strong> What are the trade-offs in computational complexity, parallelization, and ability to model long-range dependencies?</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: Detailed Comparison Table -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Architectural Trade-offs: Detailed Analysis</h2>
                    <div style="font-size: 0.55em;">
                        <table style="width: 100%; border-collapse: collapse; margin-top: 20px;">
                            <thead>
                                <tr style="background: #10099F; color: white;">
                                    <th style="padding: 12px; border: 1px solid #ddd; text-align: left;">Architecture</th>
                                    <th style="padding: 12px; border: 1px solid #ddd;">Computational Complexity</th>
                                    <th style="padding: 12px; border: 1px solid #ddd;">Sequential Operations</th>
                                    <th style="padding: 12px; border: 1px solid #ddd;">Maximum Path Length</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr style="background: #F5F5FF;">
                                    <td style="padding: 12px; border: 1px solid #ddd;"><strong style="color: #10099F;">CNN</strong></td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(knd^2)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(1)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(n/k)\)</td>
                                </tr>
                                <tr style="background: #F0FFF9;">
                                    <td style="padding: 12px; border: 1px solid #ddd;"><strong style="color: #2DD2C0;">RNN</strong></td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(nd^2)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(n)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(n)\)</td>
                                </tr>
                                <tr style="background: #FFF5F5;">
                                    <td style="padding: 12px; border: 1px solid #ddd;"><strong style="color: #FC8484;">Self-Attention</strong></td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(n^2d)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(1)\)</td>
                                    <td style="padding: 12px; border: 1px solid #ddd; text-align: center;">\(\mathcal{O}(1)\)</td>
                                </tr>
                            </tbody>
                        </table>
                        <div style="margin-top: 20px; font-size: 1.1em;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 6px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin: 0 0 8px 0;">CNN</h5>
                                    <p style="margin: 0; font-size: 0.9em;"><strong>\(k\)</strong> = kernel size</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Hierarchical, needs multiple layers for long dependencies</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 6px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin: 0 0 8px 0;">RNN</h5>
                                    <p style="margin: 0; font-size: 0.9em;">Cannot parallelize across time</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Sequential processing of \(n\) steps</p>
                                </div>
                                <div style="background: #FFF5F5; padding: 12px; border-radius: 6px; border-left: 4px solid #FC8484;">
                                    <h5 style="color: #FC8484; margin: 0 0 8px 0;">Self-Attention</h5>
                                    <p style="margin: 0; font-size: 0.9em;">Direct connections between all positions</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">‚ö†Ô∏è Quadratic in sequence length!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: Complexity Explanation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Understanding the Complexity Trade-offs</h2>
                    <div style="font-size: 0.6em;">
                        <div style="display: grid; grid-template-columns: 1fr; gap: 10px;">
                            <div class="fragment" style="background: #F5F5FF; padding: 10px; border-radius: 8px; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin-top: 0;">üî∑ CNN: \(\mathcal{O}(knd^2)\)</h4>
                                <p style="margin: 0 0 8px 0;">Convolves kernel of size \(k\) over \(n\) positions, each with \(d \times d\) weight matrix</p>
                                <p style="margin: 0; font-size: 0.85em;"><strong>‚úì Pros:</strong> Constant parallel operations, efficient for local patterns</p>
                                <p style="margin: 5px 0 0 0; font-size: 0.85em;"><strong>‚úó Cons:</strong> Needs \(\mathcal{O}(n/k)\) layers to connect distant positions</p>
                            </div>

                            <div class="fragment" style="background: #F0FFF9; padding: 10px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                <h4 style="color: #2DD2C0; margin-top: 0;">üî∑ RNN: \(\mathcal{O}(nd^2)\)</h4>
                                <p style="margin: 0 0 8px 0;">Processes \(n\) steps sequentially, each multiplying \(d \times d\) weight matrix by \(d\)-dimensional hidden state</p>
                                <p style="margin: 0; font-size: 0.85em;"><strong>‚úì Pros:</strong> Linear in sequence length, handles variable lengths naturally</p>
                                <p style="margin: 5px 0 0 0; font-size: 0.85em;"><strong>‚úó Cons:</strong> Sequential (no parallelization), gradients must flow through \(n\) steps</p>
                            </div>

                            <div class="fragment" style="background: #FFF5F5; padding: 10px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <h4 style="color: #FC8484; margin-top: 0;">üî∑ Self-Attention: \(\mathcal{O}(n^2d)\)</h4>
                                <p style="margin: 0 0 8px 0;">Computes attention between all \(n \times n\) pairs: \(\mathbf{Q}\mathbf{K}^T\) is \((n \times d) \times (d \times n) = n \times n\), then multiply by \(\mathbf{V}\) (\(n \times d\))</p>
                                <p style="margin: 0; font-size: 0.85em;"><strong>‚úì Pros:</strong> Fully parallel, direct paths between any positions (best for long-range dependencies)</p>
                                <p style="margin: 5px 0 0 0; font-size: 0.85em;"><strong>‚úó Cons:</strong> Quadratic complexity makes it slow for very long sequences</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 10px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Summary:</strong> CNNs and self-attention can parallelize, but self-attention has the shortest path between any pair of positions. However, the \(n^2\) complexity is problematic for long sequences!</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #1: Architecture Comparison -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which architecture has the best ability to model long-range dependencies (shortest maximum path length)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Convolutional Neural Networks (CNNs)",
                                "correct": false,
                                "explanation": "CNNs have a maximum path length of O(n/k), requiring multiple layers to connect distant positions. The receptive field grows with depth."
                            },
                            {
                                "text": "Recurrent Neural Networks (RNNs)",
                                "correct": false,
                                "explanation": "RNNs have a maximum path length of O(n), as information must flow sequentially through all intermediate steps."
                            },
                            {
                                "text": "Self-Attention",
                                "correct": true,
                                "explanation": "Correct! Self-attention has a maximum path length of O(1) - every position is directly connected to every other position in a single layer."
                            },
                            {
                                "text": "All three architectures have the same maximum path length",
                                "correct": false,
                                "explanation": "This is incorrect. They have very different maximum path lengths: CNN O(n/k), RNN O(n), Self-Attention O(1)."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 8: Positional Encoding Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">The Problem: Self-Attention Loses Sequence Order</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div style="background: #FFF5F5; padding: 20px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <h4 style="color: #FC8484; margin-top: 0;">‚ö†Ô∏è Critical Issue</h4>
                                <p style="margin: 0;">Self-attention processes all tokens <strong>in parallel</strong> - it doesn't know the <strong>order</strong> of the sequence!</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <p><strong>Example:</strong> These two sentences would be processed identically:</p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 15px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                    <p style="margin: 0; font-style: italic;">"The cat chased the dog"</p>
                                </div>
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px;">
                                    <p style="margin: 0; font-style: italic;">"The dog chased the cat"</p>
                                </div>
                            </div>
                            <p style="margin-top: 15px; font-size: 0.95em;">Without positional information, self-attention treats these as the same set of words!</p>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <h4 style="color: #2DD2C0; margin-top: 0;">üí° Solution: Positional Encoding</h4>
                                <p style="margin: 0;">Add <span class="tooltip">positional information<span class="tooltiptext">Additional input features that encode the position of each token in the sequence</span></span> to each token's representation</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 9: Positional Encoding Types -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Positional Encoding: Learned vs Fixed</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>Positional encodings can be either <strong>learned</strong> during training or <strong>fixed</strong> using a predetermined function:</p>
                        </div>
                        <div class="fragment" style="margin-top: 25px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #F5F5FF; padding: 20px; border-radius: 8px; border: 2px solid #10099F;">
                                    <h4 style="color: #10099F; margin-top: 0;">üìö Learned Positional Encodings</h4>
                                    <p style="margin: 0 0 10px 0; font-size: 0.95em;">Treated as parameters optimized during training</p>
                                    <p style="margin: 0; font-size: 0.9em;"><strong>‚úì</strong> Can adapt to data</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;"><strong>‚úó</strong> Limited to training sequence length</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 20px; border-radius: 8px; border: 2px solid #2DD2C0;">
                                    <h4 style="color: #2DD2C0; margin-top: 0;">üî¢ Fixed Positional Encodings</h4>
                                    <p style="margin: 0 0 10px 0; font-size: 0.95em;">Based on sine and cosine functions</p>
                                    <p style="margin: 0; font-size: 0.9em;"><strong>‚úì</strong> Works for any sequence length</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;"><strong>‚úì</strong> Enables relative position learning</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 25px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0;"><strong>Focus:</strong> We'll examine the fixed <span class="tooltip">sinusoidal positional encoding<span class="tooltiptext">Positional encoding scheme using sine and cosine functions at different frequencies</span></span> scheme introduced in the original Transformer paper</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Positional Encoding Formula -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Sinusoidal Positional Encoding Formula</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <p>Suppose input representation \(\mathbf{X} \in \mathbb{R}^{n \times d}\) contains \(d\)-dimensional embeddings for \(n\) tokens.</p>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border: 2px solid #10099F;">
                                <p style="margin: 0 0 10px 0; font-size: 1.1em;"><strong>Positional encoding adds matrix \(\mathbf{P} \in \mathbb{R}^{n \times d}\):</strong></p>
                                <p style="text-align: center; font-size: 1.15em; margin: 12px 0;">
                                    $$\text{Output} = \mathbf{X} + \mathbf{P}$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <p><strong>Elements of \(\mathbf{P}\) are defined using sine and cosine functions:</strong></p>
                            <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border: 2px solid #2DD2C0; margin-top: 10px;">
                                <p style="text-align: center; font-size: 1.2em; margin: 0;">
                                    $$p_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)$$
                                </p>
                                <p style="text-align: center; font-size: 1.2em; margin: 10px 0 0 0;">
                                    $$p_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 8px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 6px;">
                                    <p style="margin: 0;"><strong style="color: #10099F;">\(i\)</strong> = position in sequence (row)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 6px;">
                                    <p style="margin: 0;"><strong style="color: #2DD2C0;">\(j\)</strong> = dimension index</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 10px; border-radius: 6px;">
                                    <p style="margin: 0;"><strong style="color: #FAC55B;">\(d\)</strong> = embedding dimension</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 10px; background: #FFF5F5; border-left: 4px solid #FC8484;">
                            <p style="margin: 0;"><strong>Key property:</strong> Even dimensions use <span style="color: #10099F;">sine</span>, odd dimensions use <span style="color: #2DD2C0;">cosine</span>. Frequency decreases with dimension index.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Positional Encoding Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Implementation: PositionalEncoding Class</h2>
                    <div style="font-size: 0.55em;">
                        <div class="fragment">
                            <p>Here's how to implement sinusoidal positional encoding in PyTorch:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <pre><code class="language-python" style="font-size: 0.75em;">class PositionalEncoding(nn.Module):
    """Positional encoding using sine and cosine functions."""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        # Create a long enough positional encoding matrix P
        self.P = torch.zeros((1, max_len, num_hiddens))

        # X contains position indices scaled by frequencies
        # Shape: (max_len, num_hiddens//2)
        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) \
            / torch.pow(10000, torch.arange(0, num_hiddens, 2,
                                           dtype=torch.float32) / num_hiddens)

        # Even dimensions (0, 2, 4, ...) get sine
        self.P[:, :, 0::2] = torch.sin(X)
        # Odd dimensions (1, 3, 5, ...) get cosine
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        # Add positional encoding to input (only up to sequence length)
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)</code></pre>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0;"><strong>Usage:</strong> \(\mathbf{P}\) is precomputed once and reused. At runtime, we slice it to match the input sequence length and add it element-wise.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: Positional Encoding Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Visualizing Positional Encoding Frequencies</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p>Different dimensions oscillate at different frequencies. Lower dimensions (columns) have higher frequencies:</p>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <img src="images/positional-encoding-frequency.svg" alt="Positional Encoding Frequencies" style="max-width: 85%; max-height: 300px;">
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin-top: 0;">üìä Columns 6 & 7</h5>
                                    <p style="margin: 0;">Higher frequency oscillations - capture fine-grained positional differences</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">üìä Columns 8 & 9</h5>
                                    <p style="margin: 0;">Lower frequency oscillations - capture coarser positional patterns</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Insight:</strong> The alternating sine/cosine at each frequency creates paired dimensions. Together, they encode position uniquely.</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Absolute Positional Information - Binary Analogy -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Absolute Positional Information: Binary Analogy</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>To understand how decreasing frequency relates to position encoding, consider binary representations:</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #F5F5FF; padding: 20px; border-radius: 8px; font-family: monospace; font-size: 0.95em;">
                                <pre style="margin: 0; line-height: 1.6;">0 in binary is 000
1 in binary is 001
2 in binary is 010
3 in binary is 011
4 in binary is 100
5 in binary is 101
6 in binary is 110
7 in binary is 111</pre>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px; font-size: 0.9em;">
                                <div style="background: #FFF5F5; padding: 12px; border-radius: 6px; border-left: 4px solid #FC8484;">
                                    <p style="margin: 0;"><strong>Lowest bit:</strong> Alternates every <strong>1</strong> number</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 6px; border-left: 4px solid #FAC55B;">
                                    <p style="margin: 0;"><strong>Middle bit:</strong> Alternates every <strong>2</strong> numbers</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 6px; border-left: 4px solid #2DD2C0;">
                                    <p style="margin: 0;"><strong>Highest bit:</strong> Alternates every <strong>4</strong> numbers</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <h4 style="color: #10099F; margin-top: 0;">üîó Connection to Positional Encoding</h4>
                            <p style="margin: 0;">Similarly, <strong>lower encoding dimensions have higher frequencies</strong> (like lower-order bits), while <strong>higher dimensions have lower frequencies</strong> (like higher-order bits). This provides a unique encoding for each position!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 14: Positional Encoding Heatmap -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Positional Encoding Matrix Heatmap</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p>The complete positional encoding matrix \(\mathbf{P}\) visualized as a heatmap (rows = positions, columns = dimensions):</p>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <img src="images/positional-encoding-heatmap.svg" alt="Positional Encoding Heatmap" style="max-width: 45%; max-height: 320px;">
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #10099F; margin-top: 0;">üìê Vertical (Rows)</h5>
                                    <p style="margin: 0;">Each row represents a position in the sequence (0 to 59)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">üìè Horizontal (Columns)</h5>
                                    <p style="margin: 0;">Each column is an encoding dimension (frequency decreases left to right)</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFF5F5; border-left: 4px solid #FC8484;">
                            <p style="margin: 0;"><strong>Pattern:</strong> Continuous representations (floats) are more space-efficient than binary. Each position gets a unique \(d\)-dimensional encoding.</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #2: Positional Encoding Properties -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does the positional encoding use trigonometric functions with decreasing frequencies?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the computation faster",
                                "correct": false,
                                "explanation": "While sine and cosine are relatively fast to compute, speed is not the primary reason for this design."
                            },
                            {
                                "text": "To encode absolute positional information similar to how different bit positions in binary encode numbers",
                                "correct": true,
                                "explanation": "Correct! Just like binary bits alternate at different frequencies (every 1, 2, 4, 8... positions), the trigonometric functions at different frequencies create unique encodings for each position."
                            },
                            {
                                "text": "To ensure all positions have the same magnitude",
                                "correct": false,
                                "explanation": "Actually, sine and cosine already output values in [-1, 1] regardless of frequency. The varying frequencies serve a different purpose."
                            },
                            {
                                "text": "To make sure positions far apart have similar encodings",
                                "correct": false,
                                "explanation": "This is the opposite of what we want! We need positions to have unique encodings, especially nearby positions."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 15: Relative Positional Information -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Relative Positional Information: The Key Property</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <p>Beyond absolute position, the sinusoidal encoding has a remarkable property:</p>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <h4 style="color: #10099F; margin-top: 0;">üéØ Key Property</h4>
                            <p style="margin: 0;">For any <strong>fixed offset \(\delta\)</strong>, the positional encoding at position \(i + \delta\) can be represented as a <strong>linear projection</strong> of the encoding at position \(i\)</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <p><strong>Why is this important?</strong></p>
                            <ul style="font-size: 0.95em; margin-top: 10px;">
                                <li>The model can easily learn to <span class="tooltip">attend by relative positions<span class="tooltiptext">Pay attention to tokens based on their distance rather than absolute position</span></span></li>
                                <li>The same <span class="tooltip">projection matrix<span class="tooltiptext">A 2√ó2 rotation matrix that depends only on the offset Œ¥</span></span> works for all positions \(i\)</li>
                                <li>Enables <strong>generalization</strong> to longer sequences than seen during training</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border: 2px solid #2DD2C0;">
                                <p style="margin: 0 0 10px 0;"><strong>Mathematical formulation:</strong></p>
                                <p style="margin: 0;">Define \(\omega_j = \frac{1}{10000^{2j/d}}\). Then any pair \((p_{i, 2j}, p_{i, 2j+1})\) can be linearly projected to \((p_{i+\delta, 2j}, p_{i+\delta, 2j+1})\)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 16: Relative Position Mathematics -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Mathematical Proof: Linear Projection Property</h2>
                    <div style="font-size: 0.52em;">
                        <div class="fragment">
                            <p>The projection uses a <strong>2D rotation matrix</strong> that depends only on \(\delta\) (not on \(i\)):</p>
                        </div>
                        <div class="fragment" style="margin-top: 8px;">
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border: 2px solid #10099F;">
                                <p style="text-align: center; font-size: 1.15em; margin: 0;">
                                    $$\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\ -\sin(\delta \omega_j) & \cos(\delta \omega_j) \end{bmatrix} \begin{bmatrix} p_{i, 2j} \\ p_{i, 2j+1} \end{bmatrix}$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 8px;">
                            <p>Expanding using <span class="tooltip">sine addition formulas<span class="tooltiptext">sin(A+B) = sin(A)cos(B) + cos(A)sin(B) and cos(A+B) = cos(A)cos(B) - sin(A)sin(B)</span></span>:</p>
                            <div style="background: #F0FFF9; padding: 10px; border-radius: 8px; margin-top: 8px;">
                                <p style="font-size: 1.05em; margin: 0;">
                                    $$= \begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\ -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \end{bmatrix}$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 8px;">
                            <p>Simplifying using trigonometric identities:</p>
                            <div style="background: #FFF5F5; padding: 10px; border-radius: 8px; margin-top: 8px; border: 2px solid #FC8484;">
                                <p style="font-size: 1.1em; margin: 0;">
                                    $$= \begin{bmatrix} \sin((i+\delta) \omega_j) \\ \cos((i+\delta) \omega_j) \end{bmatrix} = \begin{bmatrix} p_{i+\delta, 2j} \\ p_{i+\delta, 2j+1} \end{bmatrix}$$
                                </p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 8px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Conclusion:</strong> The 2√ó2 projection matrix depends only on \(\delta\) and \(j\), <strong>not on \(i\)</strong>. This means the relationship between positions is <strong>translation-invariant</strong>!</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #3: Relative Positional Encoding -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does it mean that positional encoding at position i+Œ¥ can be represented as a linear projection of the encoding at position i?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The encoding uses matrix multiplication to transform positions",
                                "correct": false,
                                "explanation": "While technically true, this misses the key insight about what this property enables."
                            },
                            {
                                "text": "The model can learn relative position relationships that generalize across all positions",
                                "correct": true,
                                "explanation": "Correct! Since the projection matrix depends only on the offset Œ¥ (not absolute position i), the model can learn patterns based on relative distances that work consistently throughout the sequence."
                            },
                            {
                                "text": "All positions have identical encodings",
                                "correct": false,
                                "explanation": "This is incorrect. Each position has a unique encoding. The linear projection property relates encodings at different positions."
                            },
                            {
                                "text": "The encoding requires less memory than other methods",
                                "correct": false,
                                "explanation": "Memory efficiency is not the primary benefit of this property. The key advantage is the ability to model relative position relationships."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 17: Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Section 11.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"}]'>
                    <h2 class="truncate-title">Summary: Self-Attention and Positional Encoding</h2>
                    <div style="font-size: 0.6em;">
                        <div style="display: grid; grid-template-columns: 1fr; gap: 10px;">
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin-top: 0;">üîÑ Self-Attention</h4>
                                <p style="margin: 0;">Queries, keys, and values all come from the same sequence. Each token attends to all other tokens.</p>
                            </div>

                            <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                <h4 style="color: #2DD2C0; margin-top: 0;">‚öñÔ∏è Architectural Trade-offs</h4>
                                <ul style="margin: 8px 0 0 0; padding-left: 20px;">
                                    <li><strong>CNNs & Self-Attention:</strong> Parallel computation (\(\mathcal{O}(1)\) sequential ops)</li>
                                    <li><strong>Self-Attention:</strong> Shortest maximum path length (\(\mathcal{O}(1)\))</li>
                                    <li><strong>Challenge:</strong> Quadratic complexity \(\mathcal{O}(n^2d)\) limits very long sequences</li>
                                </ul>
                            </div>

                            <div style="background: #FFF5F5; padding: 12px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <h4 style="color: #FC8484; margin-top: 0;">üìç Positional Encoding</h4>
                                <p style="margin: 0 0 8px 0;">Required because self-attention doesn't preserve sequence order</p>
                                <p style="margin: 0;"><strong>Sinusoidal approach:</strong> Uses sine and cosine at different frequencies to encode position</p>
                            </div>

                            <div style="background: #FFFBF0; padding: 12px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                <h4 style="color: #FAC55B; margin-top: 0;">üéØ Two Types of Positional Information</h4>
                                <ul style="margin: 8px 0 0 0; padding-left: 20px;">
                                    <li><strong>Absolute:</strong> Unique encoding for each position (like binary representation)</li>
                                    <li><strong>Relative:</strong> Linear projection property enables learning position offsets</li>
                                </ul>
                            </div>
                        </div>
                        <div style="margin-top: 12px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; border-radius: 8px; text-align: center;">
                            <p style="margin: 0; font-size: 1.1em;"><strong>üöÄ These components form the foundation of the Transformer architecture!</strong></p>
                        </div>
                    </div>
                </section>

            </section>

            <!-- New Vertical Section: The Transformer Architecture -->
            <section>
                <!-- Title Slide for Transformer Section -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}, {"text": "Vaswani et al. (2017) - Attention is All You Need", "url": "https://arxiv.org/abs/1706.03762"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.7 The Transformer Architecture</h1>
                    <p>The Architecture That Changed Deep Learning</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 1: Motivation for Transformers -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}]'>
                    <h2 class="truncate-title">Motivation: Why Pure Attention?</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Comparison of Architectures</h4>
                            <p>Recall from Section 11.6.2 the comparison between CNNs, RNNs, and self-attention:</p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px; margin: 20px 0; font-size: 0.9em;">
                                <div style="background: #FFF5F5; padding: 15px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                    <h5 style="color: #FC8484; margin-top: 0;">RNNs</h5>
                                    <p style="margin: 0;">‚úÖ Capture sequence order<br>
                                    ‚ùå Sequential computation<br>
                                    ‚ùå Long path lengths</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">CNNs</h5>
                                    <p style="margin: 0;">‚úÖ Parallel computation<br>
                                    ‚ùå Limited receptive field<br>
                                    ‚ùå Needs many layers</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Self-Attention ‚≠ê</h5>
                                    <p style="margin: 0;">‚úÖ Parallel computation<br>
                                    ‚úÖ Shortest path length<br>
                                    ‚úÖ Direct dependencies</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; margin-top: 15px;">
                            <p style="margin: 0;"><strong>Key Insight:</strong> Self-attention combines the best of both worlds ‚Äî parallel computation AND short path lengths!</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 2: Prior Work -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}, {"text": "Cheng et al. (2016)", "url": "https://arxiv.org/abs/1601.06733"}, {"text": "Lin et al. (2017)", "url": "https://arxiv.org/abs/1703.03130"}]'>
                    <h2 class="truncate-title">Evolution: From RNN+Attention to Pure Attention</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Prior Self-Attention Models</h4>
                            <p>Earlier models used self-attention but still relied on RNNs:</p>
                            <div style="background: #F5F5FF; padding: 18px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #10099F;">
                                <ul style="margin: 0; padding-left: 25px;">
                                    <li><strong>Cheng et al. (2016):</strong> Long Short-Term Memory-Networks for Machine Reading</li>
                                    <li><strong>Lin et al. (2017):</strong> A Structured Self-Attentive Sentence Embedding</li>
                                    <li><strong>Paulus et al. (2017):</strong> A Deep Reinforced Model for Abstractive Summarization</li>
                                </ul>
                                <p style="margin: 10px 0 0 0; font-size: 0.95em;">All used RNNs for input representations, then applied self-attention</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 20px; border-radius: 8px; margin-top: 15px;">
                                <h4 style="color: white; margin-top: 0;">üéØ The Transformer Breakthrough (Vaswani et al., 2017)</h4>
                                <p style="margin: 0; font-size: 1.05em;"><strong>"Attention is All You Need"</strong> ‚Äî The first architecture based <em>solely</em> on attention mechanisms, with NO convolutional or recurrent layers!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Architecture Overview -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}]'>
                    <h2 class="truncate-title">The Transformer Architecture: High-Level Overview</h2>
                    <div style="font-size: 0.65em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div>
                                <img src="images/transformer_architecture.svg" alt="Transformer Architecture" style="max-width: 100%; height: auto;">
                            </div>
                            <div>
                                <div class="fragment">
                                    <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin-bottom: 12px; border-left: 4px solid #10099F;">
                                        <h5 style="color: #10099F; margin-top: 0;">üîµ Encoder (Left)</h5>
                                        <ul style="margin: 0; padding-left: 20px; font-size: 0.95em;">
                                            <li>Multi-head self-attention</li>
                                            <li>Positionwise FFN</li>
                                            <li>Add & Norm after each sublayer</li>
                                            <li>Repeated \(N\) times</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="fragment">
                                    <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; margin-bottom: 12px; border-left: 4px solid #2DD2C0;">
                                        <h5 style="color: #2DD2C0; margin-top: 0;">üî∑ Decoder (Right)</h5>
                                        <ul style="margin: 0; padding-left: 20px; font-size: 0.95em;">
                                            <li><strong>Masked</strong> multi-head self-attention</li>
                                            <li>Multi-head <span class="tooltip">encoder-decoder attention<span class="tooltiptext">Cross-attention where queries come from decoder, keys and values from encoder</span></span></li>
                                            <li>Positionwise FFN</li>
                                            <li>Add & Norm after each sublayer</li>
                                            <li>Repeated \(N\) times</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="fragment">
                                    <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 12px;">
                                        <p style="margin: 0;">üìç <strong>Positional encoding</strong> added to both encoder and decoder inputs</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #1 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key innovation of the Transformer architecture compared to earlier attention-based models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It was the first model to use attention mechanisms",
                                "correct": false,
                                "explanation": "Incorrect. Attention mechanisms had been used before the Transformer, particularly in seq2seq models with RNNs."
                            },
                            {
                                "text": "It is based solely on attention mechanisms, without any convolutional or recurrent layers",
                                "correct": true,
                                "explanation": "Correct! The Transformer was the first architecture to rely entirely on attention mechanisms, eliminating the need for RNNs or CNNs for sequence processing."
                            },
                            {
                                "text": "It introduces positional encoding for the first time",
                                "correct": false,
                                "explanation": "While positional encoding is important in the Transformer, it was not the key innovation. The innovation was using pure attention without RNNs or CNNs."
                            },
                            {
                                "text": "It can only be used for machine translation tasks",
                                "correct": false,
                                "explanation": "Incorrect. While originally proposed for sequence-to-sequence learning, Transformers have been applied to a wide range of tasks in NLP, vision, and beyond."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 4: Positionwise Feed-Forward Networks -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#positionwise-feed-forward-networks"}]'>
                    <h2 class="truncate-title">Positionwise Feed-Forward Networks</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; padding: 15px;">
                                <h4 style="color: #10099F; margin-top: 0;">üéØ Key Idea</h4>
                                <p style="margin: 0;">The <span class="tooltip">positionwise<span class="tooltiptext">Applied independently to each position in the sequence</span></span> FFN transforms representations at <strong>all sequence positions</strong> using the <strong>same MLP</strong></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #10099F;">Architecture</h4>
                            <div style="background: white; padding: 15px; border-radius: 8px; border: 2px solid #EEEEEE;">
                                <p style="margin: 0 0 10px 0;">Two linear transformations with a ReLU activation in between:</p>
                                <div style="text-align: center; font-size: 1.1em; margin: 10px 0;">
                                    $$\text{FFN}(\mathbf{X}) = \max(0, \mathbf{X}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$$
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #10099F;">PyTorch Implementation</h4>
                            <pre><code class="python" style="font-size: 0.85em;">class PositionWiseFFN(nn.Module):
    """The positionwise feed-forward network."""
    def __init__(self, ffn_num_hiddens, ffn_num_outputs):
        super().__init__()
        self.dense1 = nn.LazyLinear(ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.LazyLinear(ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: Input/Output Shapes -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#positionwise-feed-forward-networks"}]'>
                    <h2 class="truncate-title">Positionwise FFN: Shape Transformations</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Input Shape</h4>
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; margin-bottom: 10px;">
                                <p style="margin: 0; font-family: monospace; font-size: 1.05em;">
                                    <strong>X:</strong> (batch_size, num_steps, num_hiddens)
                                </p>
                                <p style="margin: 10px 0 0 0; font-size: 0.95em;">
                                    ‚Ä¢ <code>batch_size</code>: Number of sequences<br>
                                    ‚Ä¢ <code>num_steps</code>: Sequence length<br>
                                    ‚Ä¢ <code>num_hiddens</code>: Feature dimension
                                </p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Transformation Process</h4>
                            <div style="background: white; padding: 12px; border-radius: 8px; border: 2px solid #EEEEEE; margin-bottom: 10px;">
                                <ol style="margin: 0; padding-left: 25px;">
                                    <li><strong>Dense1:</strong> (batch_size, num_steps, num_hiddens) ‚Üí (batch_size, num_steps, ffn_num_hiddens)</li>
                                    <li><strong>ReLU:</strong> Shape unchanged, element-wise activation</li>
                                    <li><strong>Dense2:</strong> (batch_size, num_steps, ffn_num_hiddens) ‚Üí (batch_size, num_steps, ffn_num_outputs)</li>
                                </ol>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 12px;">
                                <h5 style="color: #2DD2C0; margin-top: 0;">üí° Key Property</h5>
                                <p style="margin: 0;">The <strong>same MLP parameters</strong> are applied to each position independently. When inputs at all positions are the same, outputs are also identical!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: Residual Connections and Layer Normalization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization"}, {"text": "Ba et al. (2016) - Layer Normalization", "url": "https://arxiv.org/abs/1607.06450"}]'>
                    <h2 class="truncate-title">Residual Connections and Layer Normalization</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The "Add & Norm" Component</h4>
                            <p>Both are <strong>key to training very deep models</strong>:</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ <span class="tooltip">Residual Connection<span class="tooltiptext">Skip connection that allows gradients to flow directly, enabling deeper networks</span></span></h5>
                                    <div style="background: white; padding: 10px; border-radius: 5px; margin: 10px 0; font-size: 0.95em; text-align: center;">
                                        $$\mathbf{x} + \text{sublayer}(\mathbf{x})$$
                                    </div>
                                    <p style="margin: 0; font-size: 0.95em;">Adds input directly to output of sublayer</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ <span class="tooltip">Layer Normalization<span class="tooltiptext">Normalizes across the feature dimension for each example independently</span></span></h5>
                                    <div style="background: white; padding: 10px; border-radius: 5px; margin: 10px 0; font-size: 0.85em; text-align: center;">
                                        $$\text{LayerNorm}(\mathbf{x} + \text{sublayer}(\mathbf{x}))$$
                                    </div>
                                    <p style="margin: 0; font-size: 0.95em;">Normalizes the result across features</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 15px;">
                                <h5 style="color: #FAC55B; margin-top: 0;">‚ö†Ô∏è Shape Requirement</h5>
                                <p style="margin: 0;">For residual connections to work: $$\text{sublayer}(\mathbf{x}) \in \mathbb{R}^d \text{ whenever } \mathbf{x} \in \mathbb{R}^d$$</p>
                                <p style="margin: 8px 0 0 0; font-size: 0.95em;">The sublayer must preserve the dimensionality!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: Layer Norm vs Batch Norm -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization"}]'>
                    <h2 class="truncate-title">Layer Normalization vs Batch Normalization</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="margin-bottom: 15px;">Layer normalization is preferred over batch normalization for NLP tasks:</p>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 15px;">
                                <div style="background: #FFF5F5; padding: 15px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                    <h5 style="color: #FC8484; margin-top: 0;">Batch Normalization</h5>
                                    <ul style="margin: 8px 0; padding-left: 20px; font-size: 0.95em;">
                                        <li>Normalizes across the <strong>batch dimension</strong></li>
                                        <li>For position \(i\): uses statistics from all examples at position \(i\)</li>
                                        <li>‚ùå Depends on batch size</li>
                                        <li>‚ùå Issues with variable-length sequences</li>
                                    </ul>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Layer Normalization ‚≠ê</h5>
                                    <ul style="margin: 8px 0; padding-left: 20px; font-size: 0.95em;">
                                        <li>Normalizes across the <strong>feature dimension</strong></li>
                                        <li>For each example: uses statistics from all features</li>
                                        <li>‚úÖ Batch size independent</li>
                                        <li>‚úÖ Better for variable-length sequences</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 10px;">Comparison Example</h4>
                            <pre><code class="python" style="font-size: 0.8em;">X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)

# Layer norm: normalizes across features (dim=1) for each example
# Output: [[-1, 1], [-1, 1]]

# Batch norm: normalizes across batch (dim=0) for each feature
# Output: [[-1, -1], [1, 1]]</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: AddNorm Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization"}]'>
                    <h2 class="truncate-title">The AddNorm Component: Implementation</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; margin-bottom: 15px;">
                                <p style="margin: 0;"><strong>AddNorm = Residual Connection + Layer Normalization + Dropout</strong></p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">PyTorch Implementation</h4>
                            <pre><code class="python" style="font-size: 0.85em;">class AddNorm(nn.Module):
    """The residual connection followed by layer normalization."""
    def __init__(self, norm_shape, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(norm_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #10099F;">How It Works</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 0.9em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #10099F;">Input</strong>
                                    <p style="margin: 8px 0 0 0;"><code>X</code>: Original input<br><code>Y</code>: Sublayer output</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #2DD2C0;">Process</strong>
                                    <p style="margin: 8px 0 0 0;">1. Apply dropout to Y<br>2. Add X (residual)<br>3. Layer normalize</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #FAC55B;">Output</strong>
                                    <p style="margin: 8px 0 0 0;">Same shape as X and Y</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #2 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is layer normalization preferred over batch normalization in Transformer models for NLP?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Layer normalization is computationally faster than batch normalization",
                                "correct": false,
                                "explanation": "While efficiency matters, this is not the primary reason. The main advantages are related to handling variable-length sequences and batch size independence."
                            },
                            {
                                "text": "Layer normalization is independent of batch size and better handles variable-length sequences",
                                "correct": true,
                                "explanation": "Correct! Layer normalization normalizes across the feature dimension for each example independently, making it batch size independent and more suitable for variable-length sequences common in NLP."
                            },
                            {
                                "text": "Batch normalization cannot be used with residual connections",
                                "correct": false,
                                "explanation": "Incorrect. Batch normalization can be used with residual connections (e.g., in ResNet). The issue is specifically about sequence modeling in NLP."
                            },
                            {
                                "text": "Layer normalization was invented specifically for Transformers",
                                "correct": false,
                                "explanation": "Incorrect. Layer normalization (Ba et al., 2016) was proposed before Transformers (Vaswani et al., 2017), though it became popular through its use in Transformers."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 9: Transformer Encoder Block -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#encoder"}]'>
                    <h2 class="truncate-title">Transformer Encoder Block</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Structure: Two Sublayers</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 15px;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ Multi-Head Self-Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Queries, keys, and values all from the <strong>same sequence</strong> (encoder output)</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ Positionwise Feed-Forward Network</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Same MLP applied to each position independently</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">PyTorch Implementation</h4>
                            <pre><code class="python" style="font-size: 0.75em;">class TransformerEncoderBlock(nn.Module):
    """The Transformer encoder block."""
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout):
        super().__init__()
        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(num_hiddens, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(num_hiddens, dropout)

    def forward(self, X, valid_lens):
        # Self-attention with Add & Norm
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        # Feed-forward with Add & Norm
        return self.addnorm2(Y, self.ffn(Y))</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 12px;">
                                <p style="margin: 0;">üìä <strong>Shape Preservation:</strong> No layer changes the shape ‚Äî input and output have the same dimensions!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 10: Full Transformer Encoder -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#encoder"}]'>
                    <h2 class="truncate-title">Complete Transformer Encoder</h2>
                    <div style="font-size: 0.58em;">
                        <div class="fragment">
                            <p style="margin-bottom: 12px;">The full encoder stacks <code>num_blks</code> encoder blocks:</p>
                        </div>
                        <div class="fragment">
                            <pre><code class="python" style="font-size: 0.8em;">class TransformerEncoder(d2l.Encoder):
    """The Transformer encoder."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
                 num_heads, num_blks, dropout):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module("block"+str(i),
                TransformerEncoderBlock(num_hiddens, ffn_num_hiddens,
                                       num_heads, dropout))

    def forward(self, X, valid_lens):
        # Embedding * sqrt(d_model) + positional encoding
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 12px;">
                                <h5 style="color: #2DD2C0; margin-top: 0;">üîë Key Detail: Embedding Scaling</h5>
                                <p style="margin: 0;">Embeddings are multiplied by \(\sqrt{d_{\text{model}}}\) before adding positional encoding. This rescaling is necessary because positional encodings are between -1 and 1.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Decoder Architecture Intro -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#decoder"}]'>
                    <h2 class="truncate-title">Transformer Decoder: Overview</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p style="margin-bottom: 15px;">The decoder has the same structure as the encoder, but with an <strong>additional sublayer</strong>:</p>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr; gap: 12px;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ <span class="tooltip">Masked<span class="tooltiptext">Prevents attending to future positions, preserving causality</span></span> Multi-Head Self-Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Queries, keys, values from <strong>previous decoder layer</strong><br>
                                    <strong>Masked</strong> to preserve <span class="tooltip">autoregressive property<span class="tooltiptext">Prediction at position t can only depend on positions before t</span></span></p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ Encoder-Decoder Attention (Cross-Attention)</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Queries from <strong>decoder</strong><br>
                                    Keys and values from <strong>encoder output</strong></p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">3Ô∏è‚É£ Positionwise Feed-Forward Network</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Same as in the encoder</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 15px; margin-top: 15px;">
                            <p style="margin: 0;">‚ö†Ô∏è Each sublayer is followed by <strong>Add & Norm</strong>, just like in the encoder</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: Masked Self-Attention -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#decoder"}]'>
                    <h2 class="truncate-title">Masked Self-Attention: Preserving Autoregression</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; margin-bottom: 15px;">
                                <h4 style="color: white; margin-top: 0;">üéØ The Problem</h4>
                                <p style="margin: 0;">During training, we have the full target sequence. But at test time, we generate one token at a time. How do we train the model to match this behavior?</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">The Solution: Masked Attention</h4>
                            <div style="background: #F5F5FF; padding: 18px; border-radius: 8px; margin-bottom: 12px;">
                                <p style="margin: 0 0 10px 0;">Each position in the decoder can <strong>only attend to positions up to and including itself</strong>:</p>
                                <ul style="margin: 0; padding-left: 25px;">
                                    <li>Position 1 attends to: position 1 only</li>
                                    <li>Position 2 attends to: positions 1, 2</li>
                                    <li>Position 3 attends to: positions 1, 2, 3</li>
                                    <li>Position \(t\) attends to: positions \(1, \ldots, t\)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Implementation</h4>
                            <div style="background: white; padding: 15px; border-radius: 8px; border: 2px solid #EEEEEE;">
                                <p style="margin: 0 0 8px 0; font-size: 0.95em;">During training, create <code>dec_valid_lens</code> as:</p>
                                <pre style="margin: 0;"><code class="python" style="font-size: 0.85em;">dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device)
                               .repeat(batch_size, 1)
# Result: [[1, 2, 3, ..., num_steps],
#          [1, 2, 3, ..., num_steps],
#          ...]  (batch_size rows)</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Transformer Decoder Block Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#decoder"}]'>
                    <h2 class="truncate-title">Transformer Decoder Block: Implementation</h2>
                    <div style="font-size: 0.52em;">
                        <pre><code class="python" style="font-size: 0.85em;">class TransformerDecoderBlock(nn.Module):
    # The i-th block in the Transformer decoder
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):
        super().__init__()
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(num_hiddens, dropout)
        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(num_hiddens, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm3 = AddNorm(num_hiddens, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]

        # Decoder self-attention with caching for inference
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), dim=1)
        state[2][self.i] = key_values

        # Masked self-attention
        if self.training:
            batch_size, num_steps, _ = X.shape
            dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device)
                                   .repeat(batch_size, 1)
        else:
            dec_valid_lens = None

        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)

        # Encoder-decoder attention
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)

        return self.addnorm3(Z, self.ffn(Z)), state</code></pre>
                    </div>
                </section>

                <!-- MCQ #3 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is masked self-attention necessary in the Transformer decoder?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To reduce computational complexity during training",
                                "correct": false,
                                "explanation": "Incorrect. Masking does not reduce complexity; it ensures that predictions only depend on previous tokens, preserving the autoregressive property."
                            },
                            {
                                "text": "To preserve the autoregressive property, ensuring predictions at position t only depend on positions before t",
                                "correct": true,
                                "explanation": "Correct! Masked attention prevents the decoder from attending to future positions, ensuring that generation at test time (one token at a time) matches the training condition."
                            },
                            {
                                "text": "To allow the decoder to focus more on the encoder outputs",
                                "correct": false,
                                "explanation": "Incorrect. Masking is applied to decoder self-attention, not encoder-decoder attention. It controls what the decoder can see in its own sequence."
                            },
                            {
                                "text": "To handle variable-length sequences more efficiently",
                                "correct": false,
                                "explanation": "Incorrect. Variable-length sequences are handled by valid_lens parameters. Masking specifically prevents looking ahead in the sequence."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 14: Complete Transformer Decoder -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#decoder"}]'>
                    <h2 class="truncate-title">Complete Transformer Decoder</h2>
                    <div style="font-size: 0.55em;">
                        <pre><code class="python" style="font-size: 0.8em;">class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
                 num_heads, num_blks, dropout):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.num_blks = num_blks
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module("block"+str(i),
                TransformerDecoderBlock(num_hiddens, ffn_num_hiddens,
                                       num_heads, dropout, i))
        self.dense = nn.LazyLinear(vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens):
        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # Store attention weights for visualization
            self._attention_weights[0][i] = blk.attention1.attention.attention_weights
            self._attention_weights[1][i] = blk.attention2.attention.attention_weights
        return self.dense(X), state</code></pre>
                        <div class="fragment emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0; padding: 12px; margin-top: 12px;">
                            <p style="margin: 0;">üìä <strong>Output:</strong> Linear layer projects to vocabulary size for next-token prediction</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 15: Training Setup -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Training the Transformer</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Hyperparameters</h4>
                            <div style="background: #F5F5FF; padding: 18px; border-radius: 8px; margin-bottom: 15px;">
                                <pre style="margin: 0;"><code class="python">batch_size = 128
num_hiddens = 256      # Embedding dimension
num_blks = 2           # Number of encoder/decoder layers
dropout = 0.2
ffn_num_hiddens = 64   # Hidden size in FFN
num_heads = 4          # Number of attention heads
learning_rate = 0.001
max_epochs = 30</code></pre>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Task: English-French Translation</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">Dataset</h5>
                                    <p style="margin: 0; font-size: 0.95em;">MTFraEng ‚Äî Machine translation dataset with English-French sentence pairs</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">Optimization</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Adam optimizer with gradient clipping (max_norm=1.0)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 16: Training Results -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Training Results: Loss Curves</h2>
                    <div style="font-size: 0.65em;">
                        <div style="text-align: center; margin-bottom: 12px;">
                            <img src="https://d2l.ai/_images/output_transformer_3f197a_198_0.svg" alt="Training and Validation Loss" style="max-width: 450px; width: 100%;">
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">Training Loss</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Decreases steadily from ~4 to near 0, showing the model is learning the training data well</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">Validation Loss</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Stabilizes around 2, indicating good generalization without severe overfitting</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 17: Translation Examples and BLEU -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Translation Examples and BLEU Scores</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F; margin-bottom: 12px;">Sample Translations</h4>
                            <div style="background: white; padding: 15px; border-radius: 8px; border: 2px solid #EEEEEE; font-family: monospace; font-size: 0.85em; margin-bottom: 15px;">
                                <table style="width: 100%; border-collapse: collapse;">
                                    <tr style="background: #F5F5F5;">
                                        <th style="padding: 8px; text-align: left; border-bottom: 2px solid #10099F;">English</th>
                                        <th style="padding: 8px; text-align: left; border-bottom: 2px solid #10099F;">Prediction</th>
                                        <th style="padding: 8px; text-align: left; border-bottom: 2px solid #10099F;">BLEU</th>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;">go .</td>
                                        <td style="padding: 8px;">va !</td>
                                        <td style="padding: 8px; color: #2DD2C0; font-weight: bold;">1.000</td>
                                    </tr>
                                    <tr style="background: #F9F9F9;">
                                        <td style="padding: 8px;">i lost .</td>
                                        <td style="padding: 8px;">j'ai perdu .</td>
                                        <td style="padding: 8px; color: #2DD2C0; font-weight: bold;">1.000</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;">he's calm .</td>
                                        <td style="padding: 8px;">il est mouill√© .</td>
                                        <td style="padding: 8px; color: #FAC55B; font-weight: bold;">0.658</td>
                                    </tr>
                                    <tr style="background: #F9F9F9;">
                                        <td style="padding: 8px;">i'm home .</td>
                                        <td style="padding: 8px;">je suis chez moi .</td>
                                        <td style="padding: 8px; color: #2DD2C0; font-weight: bold;">1.000</td>
                                    </tr>
                                </table>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; padding: 15px;">
                                <h5 style="color: #FAC55B; margin-top: 0;">üìä <span class="tooltip">BLEU Score<span class="tooltiptext">Bilingual Evaluation Understudy: measures n-gram overlap between prediction and reference. 1.0 is perfect match.</span></span></h5>
                                <p style="margin: 0;">The model achieves high BLEU scores on simple sentences, with some errors on more ambiguous translations (e.g., "calm" ‚Üí "mouill√©" / wet)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 18: Attention Visualization Intro -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Visualizing Transformer Attention</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p style="margin-bottom: 15px;">The Transformer stores attention weights during forward pass, allowing us to visualize what the model attends to:</p>
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <h5 style="color: #10099F; margin-top: 0;">üîµ Encoder Self-Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Shows which input positions each token attends to</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <h5 style="color: #2DD2C0; margin-top: 0;">üî∑ Decoder Self-Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Shows causal masking ‚Äî only attending to past</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                    <h5 style="color: #FAC55B; margin-top: 0;">üîó Encoder-Decoder Attention</h5>
                                    <p style="margin: 0; font-size: 0.95em;">Shows which source tokens each target token attends to</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 18px;">
                                <p style="margin: 0;">üìä For a 2-layer, 4-head Transformer, we get:<br>
                                ‚Ä¢ <strong>Encoder:</strong> 2 layers √ó 4 heads = 8 attention weight matrices<br>
                                ‚Ä¢ <strong>Decoder:</strong> 2 layers √ó 4 heads √ó 2 types = 16 attention weight matrices</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 19: Encoder Self-Attention Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Encoder Self-Attention Patterns</h2>
                    <div style="font-size: 0.65em;">
                        <div style="text-align: center; margin-bottom: 10px;">
                            <img src="https://d2l.ai/_images/output_transformer_3f197a_243_0.svg" alt="Encoder Self-Attention" style="max-width: 550px; width: 100%;">
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Key Observations</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <strong style="color: #10099F;">Different Patterns per Head</strong>
                                    <p style="margin: 8px 0 0 0;">Each attention head learns to focus on different aspects of the input</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">No Future Leakage</strong>
                                    <p style="margin: 8px 0 0 0;">With <code>valid_lens</code>, padding tokens are not attended to</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 20: Decoder Self-Attention Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Decoder Self-Attention: Causal Masking</h2>
                    <div style="font-size: 0.65em;">
                        <div style="text-align: center; margin-bottom: 10px;">
                            <img src="https://d2l.ai/_images/output_transformer_3f197a_273_0.svg" alt="Decoder Self-Attention" style="max-width: 550px; width: 100%;">
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; padding: 12px;">
                                <h5 style="color: #FC8484; margin-top: 0;">üî∫ Triangular Pattern</h5>
                                <p style="margin: 0;">Notice the lower-triangular structure ‚Äî each query position only attends to itself and previous positions. This is the <strong>causal mask</strong> in action, preserving the autoregressive property!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 21: Encoder-Decoder Attention Visualization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#training"}]'>
                    <h2 class="truncate-title">Encoder-Decoder Attention: Cross-Attention Patterns</h2>
                    <div style="font-size: 0.65em;">
                        <div style="text-align: center; margin-bottom: 10px;">
                            <img src="https://d2l.ai/_images/output_transformer_3f197a_288_0.svg" alt="Encoder-Decoder Attention" style="max-width: 550px; width: 100%;">
                        </div>
                        <div class="fragment">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #10099F;">
                                    <strong style="color: #10099F;">Query from Decoder</strong>
                                    <p style="margin: 8px 0 0 0;">Each output position queries the encoder representations</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                    <strong style="color: #2DD2C0;">Keys/Values from Encoder</strong>
                                    <p style="margin: 8px 0 0 0;">The decoder attends to all encoder positions (respecting valid_lens)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #4 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the encoder-decoder attention of the Transformer, where do the queries, keys, and values come from?",
                        "type": "single",
                        "options": [
                            {
                                "text": "All three come from the encoder output",
                                "correct": false,
                                "explanation": "Incorrect. If all came from the encoder, it would be encoder self-attention, not cross-attention."
                            },
                            {
                                "text": "Queries from the decoder, keys and values from the encoder",
                                "correct": true,
                                "explanation": "Correct! This is cross-attention: queries come from the decoder output, while keys and values come from the encoder output, allowing the decoder to attend to the source sequence."
                            },
                            {
                                "text": "All three come from the decoder output",
                                "correct": false,
                                "explanation": "Incorrect. This would be decoder self-attention, not encoder-decoder attention."
                            },
                            {
                                "text": "Queries from the encoder, keys and values from the decoder",
                                "correct": false,
                                "explanation": "Incorrect. The direction is reversed ‚Äî decoder queries the encoder representations."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 22: Architecture Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#summary"}]'>
                    <h2 class="truncate-title">Transformer Architecture: Summary</h2>
                    <div style="font-size: 0.6em;">
                        <div style="display: grid; grid-template-columns: 1fr; gap: 8px;">
                            <div class="fragment" style="background: #F5F5FF; padding: 12px; border-radius: 8px; border-left: 4px solid #10099F;">
                                <h5 style="color: #10099F; margin-top: 0;">üèóÔ∏è Overall Architecture</h5>
                                <p style="margin: 0;">The Transformer is an <strong>encoder-decoder architecture</strong>, though either component can be used individually for different tasks</p>
                            </div>

                            <div class="fragment" style="background: #F0FFF9; padding: 12px; border-radius: 8px; border-left: 4px solid #2DD2C0;">
                                <h5 style="color: #2DD2C0; margin-top: 0;">üéØ Key Components</h5>
                                <ul style="margin: 0; padding-left: 25px;">
                                    <li><strong>Multi-head self-attention:</strong> For representing input and output sequences</li>
                                    <li><strong>Masked attention:</strong> Decoder uses masked version to preserve autoregressive property</li>
                                    <li><strong>Residual connections & layer norm:</strong> Critical for training deep models</li>
                                    <li><strong>Positionwise FFN:</strong> Transforms representations using same MLP at all positions</li>
                                </ul>
                            </div>

                            <div class="fragment" style="background: #FFFBF0; padding: 12px; border-radius: 8px; border-left: 4px solid #FAC55B;">
                                <h5 style="color: #FAC55B; margin-top: 0;">üìä Shape Invariance</h5>
                                <p style="margin: 0;">Throughout the encoder and decoder, the hidden dimension remains constant, enabling clean residual connections</p>
                            </div>

                            <div class="fragment" style="background: #FFF5F5; padding: 12px; border-radius: 8px; border-left: 4px solid #FC8484;">
                                <h5 style="color: #FC8484; margin-top: 0;">üåç Applications</h5>
                                <p style="margin: 0;">Originally for sequence-to-sequence learning, Transformers are now pervasive in language, vision, speech, and reinforcement learning</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 23: Practical Considerations -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}]'>
                    <h2 class="truncate-title">Practical Considerations and Variations</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Common Use Cases</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px; margin-bottom: 15px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #10099F;">Encoder-Only</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">BERT, RoBERTa<br>For classification tasks</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #2DD2C0;">Decoder-Only</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">GPT, LLaMA<br>For generation tasks</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; text-align: center;">
                                    <strong style="color: #FAC55B;">Encoder-Decoder</strong>
                                    <p style="margin: 8px 0 0 0; font-size: 0.9em;">T5, BART<br>For seq2seq tasks</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">Challenges and Solutions</h4>
                            <div style="background: #FFF5F5; padding: 18px; border-radius: 8px; border-left: 4px solid #FC8484; margin-bottom: 12px;">
                                <h5 style="color: #FC8484; margin-top: 0;">‚ö†Ô∏è Quadratic Complexity</h5>
                                <p style="margin: 0 0 8px 0;">Self-attention is \(\mathcal{O}(n^2d)\) where \(n\) is sequence length</p>
                                <p style="margin: 0; font-size: 0.95em;"><strong>Solutions:</strong> Sparse attention (Longformer), Linear attention, Chunked processing</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 18px;">
                                <p style="margin: 0;"><strong>Modern Scale:</strong> Contemporary Transformers use \(d_{\text{model}} = 1024\) or higher, \(N = 24\) or more layers, and are trained on billions of tokens!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ #5 (Final) -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which of the following statements about the Transformer architecture is TRUE?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The encoder and decoder must always be used together",
                                "correct": false,
                                "explanation": "Incorrect. While originally proposed as an encoder-decoder architecture, modern models often use just the encoder (e.g., BERT) or just the decoder (e.g., GPT) depending on the task."
                            },
                            {
                                "text": "Layer normalization and residual connections are critical for training deep Transformer models",
                                "correct": true,
                                "explanation": "Correct! Residual connections allow gradients to flow directly through the network, and layer normalization stabilizes training. Both are essential for training deep Transformers effectively."
                            },
                            {
                                "text": "The positionwise FFN applies different parameters to each position in the sequence",
                                "correct": false,
                                "explanation": "Incorrect. The positionwise FFN applies the SAME MLP parameters to all positions independently. This is what makes it positionwise."
                            },
                            {
                                "text": "Self-attention has better computational complexity than RNNs for long sequences",
                                "correct": false,
                                "explanation": "Incorrect. Self-attention is O(n¬≤d) which is worse than RNNs O(nd¬≤) for long sequences. However, self-attention offers parallelization and shorter path lengths."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Final Summary Slide -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.7", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html"}]'>
                    <h2 class="truncate-title">The Transformer Revolution</h2>
                    <div style="font-size: 0.65em;">
                        <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; border-radius: 12px; margin-bottom: 12px;">
                            <h4 style="color: white; margin-top: 0; margin-bottom: 10px;">‚ú® Key Takeaways</h4>
                            <ul style="margin: 0; padding-left: 25px; line-height: 1.6;">
                                <li>First architecture based <strong>purely on attention</strong> ‚Äî no RNNs or CNNs needed</li>
                                <li><strong>Parallel computation</strong> and <strong>short path lengths</strong> enable efficient training on long sequences</li>
                                <li><strong>Residual connections + layer normalization</strong> enable training very deep models</li>
                                <li><strong>Masked attention</strong> in decoder preserves autoregressive generation</li>
                                <li>Modular design: encoder-only, decoder-only, or full encoder-decoder</li>
                            </ul>
                        </div>
                        <div class="fragment">
                            <div style="text-align: center; padding: 18px; background: #F5F5FF; border-radius: 12px;">
                                <p style="font-size: 1.15em; color: #10099F; font-weight: bold; margin: 0 0 8px 0;">"Attention is All You Need"</p>
                                <p style="margin: 0; font-size: 0.95em; color: #262626;">‚Äî Vaswani et al., 2017</p>
                                <p style="margin: 10px 0 0 0; font-size: 0.9em; color: #666;">This architecture fundamentally changed deep learning, leading to BERT, GPT, T5, and countless other state-of-the-art models across all domains.</p>
                            </div>
                        </div>
                    </div>
                </section>

            </section>

            <!-- Single Vertical Section: Vision Transformers -->
            <section>
                <!-- Slide 1: Title Slide - Transformers for Vision -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html"}, {"text": "Dosovitskiy et al. (2021) - An Image is Worth 16x16 Words", "url": "https://arxiv.org/abs/2010.11929"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.8 Transformers for Vision</h1>
                    <p>Applying Attention Mechanisms to Computer Vision</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Architecture Overview -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#model"}]'>
                    <h2 class="truncate-title">Vision Transformer Architecture</h2>
                    <div style="font-size: 0.7em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; align-items: start;">
                            <div>
                                <img src="images/vit_architecture.svg" alt="Vision Transformer Architecture" style="width: 100%; max-width: 450px;">
                            </div>
                            <div>
                                <div class="fragment">
                                    <h4 style="color: #10099F; margin-top: 0;">Three Main Components</h4>
                                    <div style="display: grid; gap: 8px; font-size: 0.9em;">
                                        <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                            <strong style="color: #10099F;">1. Stem (Patch Embedding)</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.95em;">Splits image into patches and embeds them</p>
                                        </div>
                                        <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                            <strong style="color: #2DD2C0;">2. Body (Transformer Encoder)</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.95em;">Stacks of multi-head attention + MLP blocks</p>
                                        </div>
                                        <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484;">
                                            <strong style="color: #FC8484;">3. Head (Classifier)</strong>
                                            <p style="margin: 5px 0 0 0; font-size: 0.95em;">Transforms &lt;cls&gt; token to output label</p>
                                        </div>
                                    </div>
                                </div>
                                <div class="fragment" style="margin-top: 12px;">
                                    <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                        <p style="margin: 0;"><strong>Key Insight:</strong> An image is treated as a <em>sequence of patches</em>, just like words in a sentence!</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Patch Embedding Concept -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#patch-embedding"}]'>
                    <h2 class="truncate-title">Splitting Images into Patches</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Mathematical Formulation</h4>
                            <p>Given an input image with dimensions:</p>
                            <ul style="font-size: 0.95em; line-height: 1.5;">
                                <li>Height: $h$, Width: $w$, Channels: $c$</li>
                                <li><span class="tooltip">Patch size<span class="tooltiptext">The height and width of each square patch extracted from the image</span></span>: $p \times p$</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                <p style="margin: 0 0 8px 0; font-size: 1.05em;"><strong>Number of patches:</strong></p>
                                <p style="text-align: center; font-size: 1.2em; margin: 8px 0;">$$m = \frac{hw}{p^2}$$</p>
                                <p style="margin: 8px 0 0 0; font-size: 0.95em;"><strong>Each patch is flattened to:</strong> $cp^2$ dimensions</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <h4 style="color: #2DD2C0;">Example</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.9em;">
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <p style="margin: 0;"><strong>Image:</strong> 96√ó96 RGB</p>
                                    <p style="margin: 5px 0 0 0;">$h=96, w=96, c=3$</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                    <p style="margin: 0;"><strong>Patch size:</strong> 16√ó16</p>
                                    <p style="margin: 5px 0 0 0;">$p=16$</p>
                                </div>
                            </div>
                            <div style="background: #E6F7FF; padding: 12px; border-radius: 8px; margin-top: 12px;">
                                <p style="margin: 0;"><strong>Result:</strong> $m = \frac{96 \times 96}{16^2} = 36$ patches</p>
                                <p style="margin: 5px 0 0 0;">Each patch: $3 \times 16^2 = 768$ dimensions</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Patch Embedding Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.2", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#patch-embedding"}]'>
                    <h2 class="truncate-title">Implementing Patch Embedding</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; margin-bottom: 15px;">
                                <p style="margin: 0;"><strong>Clever Implementation:</strong> Use a single <span class="tooltip">convolution operation<span class="tooltiptext">Convolution with kernel_size and stride both equal to patch_size effectively splits and projects the image</span></span> where kernel size = stride = patch size</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <pre><code class="python" data-line-numbers="|2-4|6-9|11-13">class PatchEmbedding(nn.Module):
    def __init__(self, img_size=96, patch_size=16, num_hiddens=512):
        super().__init__()
        # Helper function to handle single values or tuples
        def _make_tuple(x):
            if not isinstance(x, (list, tuple)):
                return (x, x)
            return x

        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)
        # Calculate number of patches
        self.num_patches = (img_size[0] // patch_size[0]) * (
            img_size[1] // patch_size[1])

        # Convolution does both splitting and linear projection
        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,
                                  stride=patch_size)

    def forward(self, X):
        # Output shape: (batch_size, num_patches, num_hiddens)
        return self.conv(X).flatten(2).transpose(1, 2)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #10099F;">Input:</strong> (batch, 3, 96, 96)
                                </div>
                                <div style="background: #F0FFF9; padding: 12px; border-radius: 8px;">
                                    <strong style="color: #2DD2C0;">Output:</strong> (batch, 36, 512)
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 5: MCQ on Patch Embedding -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "If you have a 224√ó224 RGB image and use 32√ó32 patches, how many patches will you get?",
                        "type": "single",
                        "options": [
                            {
                                "text": "7 patches",
                                "correct": false,
                                "explanation": "This would be the number along one dimension (224/32=7), but you need to consider both height and width."
                            },
                            {
                                "text": "14 patches",
                                "correct": false,
                                "explanation": "This is 7√ó2, but remember we need to multiply the number of patches in both dimensions."
                            },
                            {
                                "text": "49 patches",
                                "correct": true,
                                "explanation": "Correct! m = hw/p¬≤ = (224√ó224)/(32¬≤) = 50176/1024 = 49. Or equivalently: (224/32) √ó (224/32) = 7 √ó 7 = 49 patches."
                            },
                            {
                                "text": "196 patches",
                                "correct": false,
                                "explanation": "This would be correct for 16√ó16 patches (14√ó14), but with 32√ó32 patches we get fewer patches."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 6: CLS Token and Positional Embeddings -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#model"}]'>
                    <h2 class="truncate-title">Special Tokens and Position Information</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">1. The &lt;cls&gt; Token</h4>
                            <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F; margin-bottom: 12px;">
                                <p style="margin: 0 0 6px 0;"><strong>Purpose:</strong> Special learnable token prepended to the sequence</p>
                                <p style="margin: 0; font-size: 0.95em;">‚Ä¢ Attends to all image patches via self-attention</p>
                                <p style="margin: 0; font-size: 0.95em;">‚Ä¢ Its final representation is used for classification</p>
                                <p style="margin: 6px 0 0 0; font-size: 0.95em;"><strong>Sequence:</strong> [&lt;cls&gt;, patch‚ÇÅ, patch‚ÇÇ, ..., patch<sub>m</sub>] ‚Üí <strong>m+1 tokens</strong></p>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #2DD2C0;">2. Positional Embeddings</h4>
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <p style="margin: 0 0 6px 0;"><strong>Problem:</strong> Self-attention is <span class="tooltip">permutation-invariant<span class="tooltiptext">The attention mechanism treats the input as an unordered set - it doesn't know which patch comes from where in the image</span></span></p>
                                <p style="margin: 0 0 6px 0;"><strong>Solution:</strong> Add learnable positional embeddings to each token</p>
                                <p style="margin: 0; font-size: 0.95em;">‚Ä¢ Unlike sinusoidal encodings in original Transformer</p>
                                <p style="margin: 0; font-size: 0.95em;">‚Ä¢ <strong>m+1</strong> learnable vectors (one for each token including &lt;cls&gt;)</p>
                                <p style="margin: 0; font-size: 0.95em;">‚Ä¢ Allows the model to learn spatial relationships</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="text-align: center; background: #FFFBF0; padding: 12px; border-radius: 8px;">
                                <p style="margin: 0; font-size: 1.05em;"><strong>Final Input:</strong> Token Embeddings + Positional Embeddings + Dropout</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: ViT MLP Component -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#vision-transformer-encoder"}]'>
                    <h2 class="truncate-title">Vision Transformer MLP</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Differences from Standard Transformer FFN</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <p style="margin: 0;"><strong>1. GELU Activation</strong></p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;"><span class="tooltip">Gaussian Error Linear Unit<span class="tooltiptext">GELU(x) = x¬∑Œ¶(x) where Œ¶ is the cumulative distribution function of the standard normal distribution. It's a smoother version of ReLU that allows small negative values.</span></span></p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em; color: #666;">Smoother than ReLU</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <p style="margin: 0;"><strong>2. Dropout after each layer</strong></p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">Regularization</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em; color: #666;">Prevents overfitting</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <pre><code class="python" data-line-numbers="|2-6|8-10">class ViTMLP(nn.Module):
    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):
        super().__init__()
        self.dense1 = nn.LazyLinear(mlp_num_hiddens)
        self.gelu = nn.GELU()
        self.dropout1 = nn.Dropout(dropout)
        self.dense2 = nn.LazyLinear(mlp_num_outputs)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        return self.dropout2(self.dense2(self.dropout1(self.gelu(
            self.dense1(x)))))</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 10px;">
                            <div style="text-align: center; background: #E6F7FF; padding: 10px; border-radius: 8px;">
                                <p style="margin: 0; font-size: 0.95em;">Input ‚Üí Dense ‚Üí GELU ‚Üí Dropout ‚Üí Dense ‚Üí Dropout ‚Üí Output</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: Pre-Normalization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#vision-transformer-encoder"}]'>
                    <h2 class="truncate-title">Pre-Normalization vs Post-Normalization</h2>
                    <div style="font-size: 0.65em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div class="fragment">
                                <h4 style="color: #FC8484; text-align: center;">Post-Norm (Original Transformer)</h4>
                                <div style="background: #FFF5F5; padding: 12px; border-radius: 8px; border: 2px solid #FC8484;">
                                    <div style="font-family: monospace; font-size: 0.85em; line-height: 1.6;">
                                        <div>X</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #FFE6E6; padding: 6px; border-radius: 4px; margin: 4px 0;">Multi-Head Attention</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #FFD6D6; padding: 6px; border-radius: 4px; margin: 4px 0;">Add & Norm</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #FFE6E6; padding: 6px; border-radius: 4px; margin: 4px 0;">MLP</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #FFD6D6; padding: 6px; border-radius: 4px; margin: 4px 0;">Add & Norm</div>
                                    </div>
                                </div>
                                <p style="margin-top: 8px; font-size: 0.9em; text-align: center; color: #666;">Norm applied <strong>after</strong> residual</p>
                            </div>
                            <div class="fragment">
                                <h4 style="color: #10099F; text-align: center;">Pre-Norm (ViT)</h4>
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 8px; border: 2px solid #10099F;">
                                    <div style="font-family: monospace; font-size: 0.85em; line-height: 1.6;">
                                        <div>X</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #D6E4FF; padding: 6px; border-radius: 4px; margin: 4px 0;">Norm</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #E6F0FF; padding: 6px; border-radius: 4px; margin: 4px 0;">Multi-Head Attention</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #F5F8FF; padding: 6px; border-radius: 4px; margin: 4px 0;">Add (Residual)</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #D6E4FF; padding: 6px; border-radius: 4px; margin: 4px 0;">Norm</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #E6F0FF; padding: 6px; border-radius: 4px; margin: 4px 0;">MLP</div>
                                        <div style="margin-left: 10px;">‚Üì</div>
                                        <div style="background: #F5F8FF; padding: 6px; border-radius: 4px; margin: 4px 0;">Add (Residual)</div>
                                    </div>
                                </div>
                                <p style="margin-top: 8px; font-size: 0.9em; text-align: center; color: #666;">Norm applied <strong>before</strong> sub-layer</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #E6F7FF 0%, #F0FFF9 100%); border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Advantage of Pre-Norm:</strong> More effective and efficient training for Transformers. Better gradient flow and training stability.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 9: MCQ on Pre-Normalization -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main benefit of pre-normalization (used in ViT) compared to post-normalization (used in the original Transformer)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It reduces the number of parameters in the model",
                                "correct": false,
                                "explanation": "Pre-normalization and post-normalization use the same number of parameters - the difference is in when the normalization is applied, not how many parameters are used."
                            },
                            {
                                "text": "It leads to more effective and efficient training with better gradient flow",
                                "correct": true,
                                "explanation": "Correct! Pre-normalization provides better training stability and gradient flow, making it easier to train deeper Transformer models effectively."
                            },
                            {
                                "text": "It makes the model run faster during inference",
                                "correct": false,
                                "explanation": "The inference speed is essentially the same - pre-norm vs post-norm is about training dynamics, not inference speed."
                            },
                            {
                                "text": "It eliminates the need for residual connections",
                                "correct": false,
                                "explanation": "Both pre-norm and post-norm architectures still use residual connections. The difference is where normalization happens relative to the residual path."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 10: ViT Block Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.3", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#vision-transformer-encoder"}]'>
                    <h2 class="truncate-title">Vision Transformer Encoder Block</h2>
                    <div style="font-size: 0.68em;">
                        <div class="fragment">
                            <pre><code class="python" data-line-numbers="|2-8|10-12">class ViTBlock(nn.Module):
    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,
                 num_heads, dropout, use_bias=False):
        super().__init__()
        self.ln1 = nn.LayerNorm(norm_shape)
        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                dropout, use_bias)
        self.ln2 = nn.LayerNorm(norm_shape)
        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)

    def forward(self, X, valid_lens=None):
        # Pre-norm + attention + residual
        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)
        # Pre-norm + MLP + residual
        return X + self.mlp(self.ln2(X))</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.95em;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <p style="margin: 0;"><strong style="color: #10099F;">Line 12:</strong></p>
                                    <p style="margin: 5px 0 0 0; font-family: monospace; font-size: 0.85em;">self.ln1(X)</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">Normalize, then use as Q, K, V</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <p style="margin: 0;"><strong style="color: #2DD2C0;">Line 14:</strong></p>
                                    <p style="margin: 5px 0 0 0; font-family: monospace; font-size: 0.85em;">self.ln2(X)</p>
                                    <p style="margin: 5px 0 0 0; font-size: 0.9em;">Normalize before MLP</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;"><strong>Important:</strong> The encoder block does <em>not</em> change the shape of its input. If input is (batch, tokens, hidden), output is also (batch, tokens, hidden).</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 11: Complete ViT Architecture (Part 1) -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#putting-it-all-together"}]'>
                    <h2 class="truncate-title">Complete Vision Transformer: Initialization</h2>
                    <div style="font-size: 0.7em;">
                        <pre><code class="python" data-line-numbers="|2-5|7-9|10-12|13-16|17-21">class ViT(d2l.Classifier):
    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,
                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,
                 use_bias=False, num_classes=10):
        super().__init__()
        self.save_hyperparameters()

        # 1. Patch embedding
        self.patch_embedding = PatchEmbedding(
            img_size, patch_size, num_hiddens)

        # 2. Special <cls> token (learnable parameter)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))

        # 3. Positional embeddings (learnable)
        num_steps = self.patch_embedding.num_patches + 1  # +1 for cls token
        self.pos_embedding = nn.Parameter(
            torch.randn(1, num_steps, num_hiddens))

        # 4. Dropout for embeddings
        self.dropout = nn.Dropout(emb_dropout)

        # (continued on next slide...)</code></pre>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 0.9em;">
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #10099F;">Patches</strong>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Split & embed image</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #2DD2C0;">&lt;cls&gt; Token</strong>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">For classification</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 10px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #FAC55B;">Positions</strong>
                                    <p style="margin: 5px 0 0 0; font-size: 0.85em;">Spatial information</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 12: Complete ViT Architecture (Part 2) -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.4", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#putting-it-all-together"}]'>
                    <h2 class="truncate-title">Complete Vision Transformer: Body & Head</h2>
                    <div style="font-size: 0.72em;">
                        <pre><code class="python" data-line-numbers="|2-6|8-10|12-18">    # (continued from previous slide...)

    # 5. Transformer encoder blocks
    self.blks = nn.Sequential()
    for i in range(num_blks):
        self.blks.add_module(f"{i}", ViTBlock(
            num_hiddens, num_hiddens, mlp_num_hiddens,
            num_heads, blk_dropout, use_bias))

    # 6. Classification head
    self.head = nn.Sequential(nn.LayerNorm(num_hiddens),
                              nn.Linear(num_hiddens, num_classes))

def forward(self, X):
    # Patch embedding
    X = self.patch_embedding(X)
    # Prepend <cls> token
    X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)
    # Add positional embeddings and apply dropout
    X = self.dropout(X + self.pos_embedding)
    # Pass through Transformer encoder blocks
    for blk in self.blks:
        X = blk(X)
    # Extract <cls> token and classify
    return self.head(X[:, 0])</code></pre>
                        <div class="fragment" style="margin-top: 15px;">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #E6F7FF 0%, #F0FFF9 100%); border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Key Line:</strong> <code>X[:, 0]</code> extracts the &lt;cls&gt; token representation, which has attended to all patches and contains global image information.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 13: Training Example -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.5", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#training"}]'>
                    <h2 class="truncate-title">Training Vision Transformer</h2>
                    <div style="font-size: 0.7em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; align-items: start;">
                            <div>
                                <div class="fragment">
                                    <h4 style="color: #10099F;">Training Code</h4>
                                    <pre><code class="python" style="font-size: 0.85em;">img_size, patch_size = 96, 16
num_hiddens = 512
mlp_num_hiddens = 2048
num_heads, num_blks = 8, 2
emb_dropout = 0.1
blk_dropout = 0.1
lr = 0.1

model = ViT(img_size, patch_size,
    num_hiddens, mlp_num_hiddens,
    num_heads, num_blks,
    emb_dropout, blk_dropout, lr)

trainer = d2l.Trainer(
    max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(
    batch_size=128,
    resize=(img_size, img_size))

trainer.fit(model, data)</code></pre>
                                </div>
                            </div>
                            <div>
                                <div class="fragment">
                                    <h4 style="color: #2DD2C0;">Training Curves</h4>
                                    <img src="images/vit_training_pytorch.svg" alt="ViT Training on Fashion-MNIST" style="width: 100%; max-width: 350px;">
                                </div>
                                <div class="fragment" style="margin-top: 10px;">
                                    <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                        <p style="margin: 0; font-size: 0.95em;"><strong>Note:</strong> Training converges well, but on small datasets like Fashion-MNIST, ViT doesn't outperform ResNet. More on this later!</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 14: MCQ on ViT Architecture -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does Vision Transformer add a special <cls> token to the sequence of image patches?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To reduce the computational complexity of self-attention",
                                "correct": false,
                                "explanation": "Adding a token actually increases the sequence length slightly. The <cls> token is not about reducing complexity."
                            },
                            {
                                "text": "To provide a global representation that attends to all patches for classification",
                                "correct": true,
                                "explanation": "Correct! The <cls> token attends to all image patches through self-attention, creating a global representation that is then used for classification."
                            },
                            {
                                "text": "To replace the need for positional embeddings",
                                "correct": false,
                                "explanation": "The <cls> token does not replace positional embeddings - both are used. Positional embeddings are still needed to encode spatial information."
                            },
                            {
                                "text": "To mark the beginning of each image in a batch",
                                "correct": false,
                                "explanation": "The <cls> token is added to each image separately, not to mark batch boundaries. Its purpose is to aggregate information for classification."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 15: Scalability - The Key Advantage -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#summary-and-discussion"}]'>
                    <h2 class="truncate-title">The Scalability Advantage of Vision Transformers</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484; margin-bottom: 12px;">
                                <p style="margin: 0; font-size: 1.05em;"><strong>Surprising Result:</strong> On Fashion-MNIST and even ImageNet (1.2M images), ViT does <em>not</em> outperform ResNet!</p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="margin: 12px 0;">
                                <h4 style="color: #10099F; text-align: center;">Performance vs Dataset Size</h4>
                                <div id="scalability-viz" style="width: 100%; height: 280px;"></div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #E6F7FF 0%, #F0FFF9 100%); border-left: 4px solid #10099F;">
                                <p style="margin: 0 0 6px 0; font-size: 1.05em;"><strong>The Turning Point:</strong></p>
                                <p style="margin: 0; font-size: 0.95em;">When trained on <strong>300 million images</strong>, Vision Transformers outperform ResNets by a large margin!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 16: Inductive Biases -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#summary-and-discussion"}]'>
                    <h2 class="truncate-title">Why? Inductive Biases Explained</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B; margin-bottom: 12px;">
                                <p style="margin: 0;"><strong><span class="tooltip">Inductive Bias<span class="tooltiptext">Built-in assumptions about the structure of the problem that help learning with limited data but may limit flexibility</span></span>:</strong> Built-in assumptions that help learning with less data</p>
                            </div>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div class="fragment">
                                <div style="background: #F0FFF9; padding: 15px; border-radius: 10px; border: 2px solid #2DD2C0; height: 100%;">
                                    <h4 style="color: #2DD2C0; margin-top: 0; text-align: center;">CNNs (Strong Biases)</h4>
                                    <div style="font-size: 0.9em;">
                                        <p style="margin: 8px 0;"><strong>‚úì <span class="tooltip">Translation Invariance<span class="tooltiptext">A cat in the top-left is the same as a cat in the bottom-right - same features everywhere</span></span></strong></p>
                                        <p style="margin: 0 0 0 20px; font-size: 0.85em; color: #666;">Features learned in one location apply everywhere</p>

                                        <p style="margin: 12px 0 0 0;"><strong>‚úì <span class="tooltip">Locality<span class="tooltiptext">Nearby pixels are more related than distant pixels - convolution focuses on local neighborhoods</span></span></strong></p>
                                        <p style="margin: 0 0 0 20px; font-size: 0.85em; color: #666;">Nearby pixels are more important than distant ones</p>

                                        <div style="background: #E6F7F7; padding: 10px; border-radius: 8px; margin-top: 10px;">
                                            <p style="margin: 0; font-size: 0.9em;"><strong>Result:</strong> Data efficient, but less flexible at scale</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="background: #F5F5FF; padding: 15px; border-radius: 10px; border: 2px solid #10099F; height: 100%;">
                                    <h4 style="color: #10099F; margin-top: 0; text-align: center;">Transformers (Weak Biases)</h4>
                                    <div style="font-size: 0.9em;">
                                        <p style="margin: 8px 0;"><strong>‚úó No translation invariance</strong></p>
                                        <p style="margin: 0 0 0 20px; font-size: 0.85em; color: #666;">Must learn that objects are the same everywhere</p>

                                        <p style="margin: 12px 0 0 0;"><strong>‚úó No locality</strong></p>
                                        <p style="margin: 0 0 0 20px; font-size: 0.85em; color: #666;">Must learn which pixels are related</p>

                                        <div style="background: #E6EDFF; padding: 10px; border-radius: 8px; margin-top: 10px;">
                                            <p style="margin: 0; font-size: 0.9em;"><strong>Result:</strong> Needs more data, but superior scalability!</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <div class="emphasis-box" style="background: linear-gradient(135deg, #E6F7FF 0%, #F0FFF9 100%); border-left: 4px solid #10099F;">
                                <p style="margin: 0;"><strong>Key Insight:</strong> Transformers learn everything from data. With enough data, they can learn better representations than hand-crafted architectural biases!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 17: MCQ on Inductive Biases -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do Vision Transformers require more training data than CNNs to achieve good performance?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because Transformers have more parameters than CNNs",
                                "correct": false,
                                "explanation": "While model size can matter, this is not the main reason. Even with similar parameter counts, ViTs need more data due to their lack of inductive biases."
                            },
                            {
                                "text": "Because Transformers lack built-in assumptions like translation invariance and locality that CNNs have",
                                "correct": true,
                                "explanation": "Correct! CNNs have built-in inductive biases (translation invariance, locality) that help them learn efficiently with less data. Transformers must learn these patterns from scratch, requiring more training examples."
                            },
                            {
                                "text": "Because the attention mechanism is less powerful than convolution",
                                "correct": false,
                                "explanation": "Actually, attention is more flexible than convolution. The issue is not power but the lack of built-in assumptions about image structure."
                            },
                            {
                                "text": "Because patch embedding discards too much spatial information",
                                "correct": false,
                                "explanation": "Patch embedding preserves spatial information through positional embeddings. The data requirement is about learning visual patterns, not losing information."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 18: Beyond Basic ViT -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#summary-and-discussion"}]'>
                    <h2 class="truncate-title">Modern Developments: Beyond Basic ViT</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 10px; margin-bottom: 12px; border-left: 4px solid #10099F;">
                                <h4 style="color: #10099F; margin: 0 0 8px 0;">DeiT: Data-Efficient Image Transformers (2021)</h4>
                                <p style="margin: 0; font-size: 0.95em;">Achieved strong results on ImageNet using:</p>
                                <ul style="margin: 6px 0 0 20px; font-size: 0.9em;">
                                    <li><span class="tooltip">Knowledge distillation<span class="tooltiptext">Training technique where a student model learns from a teacher model's predictions, transferring knowledge efficiently</span></span> from CNNs</li>
                                    <li>Data augmentation strategies</li>
                                    <li>Regularization techniques</li>
                                </ul>
                                <p style="margin: 8px 0 0 0; font-size: 0.85em; color: #666;"><em>Made ViT practical without massive datasets</em></p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="background: #F0FFF9; padding: 12px; border-radius: 10px; margin-bottom: 12px; border-left: 4px solid #2DD2C0;">
                                <h4 style="color: #2DD2C0; margin: 0 0 8px 0;">Swin Transformer: Hierarchical Vision Transformer (2021)</h4>
                                <p style="margin: 0; font-size: 0.95em;">Key innovations:</p>
                                <ul style="margin: 6px 0 0 20px; font-size: 0.9em;">
                                    <li><strong><span class="tooltip">Shifted windows<span class="tooltiptext">Attention computed within local windows that shift between layers, reducing quadratic complexity</span></span>:</strong> Reduces <span class="tooltip">quadratic complexity<span class="tooltiptext">O(n¬≤) computation where n is the number of tokens - problematic for high-resolution images</span></span> of self-attention</li>
                                    <li><strong>Hierarchical structure:</strong> Multi-scale feature maps like CNNs</li>
                                    <li><strong>Locality reintroduced:</strong> Local attention windows</li>
                                </ul>
                                <p style="margin: 8px 0 0 0; font-size: 0.85em; color: #666;"><em>Achieves state-of-the-art across multiple CV tasks: detection, segmentation, etc.</em></p>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                <p style="margin: 0;"><strong>The Evolution:</strong> From pure Transformer (ViT) ‚Üí more efficient variants (Swin) ‚Üí general-purpose backbone for computer vision</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 19: Impact and Summary -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.8.6", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#summary-and-discussion"}]'>
                    <h2 class="truncate-title">Vision Transformers: A Paradigm Shift in Computer Vision</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Key Takeaways</h4>
                            <div style="display: grid; gap: 10px; margin-bottom: 15px;">
                                <div class="emphasis-box" style="background: #F5F5FF; border-left: 4px solid #10099F;">
                                    <p style="margin: 0;"><strong>1. Images as Sequences:</strong> ViT treats images as sequences of patches, enabling pure Transformer architecture for vision</p>
                                </div>
                                <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                    <p style="margin: 0;"><strong>2. Scalability Over Efficiency:</strong> Transformers show superior scaling properties with larger datasets (300M+ images)</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFFBF0; border-left: 4px solid #FAC55B;">
                                    <p style="margin: 0;"><strong>3. Trade-off:</strong> Less data-efficient than CNNs on small datasets due to lack of inductive biases, but better at scale</p>
                                </div>
                                <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484;">
                                    <p style="margin: 0;"><strong>4. Rapid Evolution:</strong> DeiT, Swin, and others addressed initial limitations, making Transformers practical for various vision tasks</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #E6F7FF 0%, #F0FFF9 100%); padding: 15px; border-radius: 12px; border: 2px solid #10099F;">
                                <p style="font-size: 1.1em; color: #10099F; font-weight: bold; margin: 0 0 10px 0; text-align: center;">The Landscape Has Changed</p>
                                <p style="margin: 0; text-align: center; font-size: 0.95em;">Just as Transformers became the dominant architecture in NLP, they are now a game-changer in computer vision. The era of CNN dominance has ended.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 20: Final MCQ -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main architectural difference that allows Vision Transformers to scale better than CNNs on very large datasets?",
                        "type": "single",
                        "options": [
                            {
                                "text": "ViT uses larger batch sizes during training",
                                "correct": false,
                                "explanation": "Batch size is a training hyperparameter, not an architectural property. Both CNNs and ViTs can use various batch sizes."
                            },
                            {
                                "text": "ViT learns all patterns from data without built-in structural assumptions",
                                "correct": true,
                                "explanation": "Correct! Unlike CNNs with built-in biases (locality, translation invariance), Transformers learn all patterns from data. This flexibility allows them to discover more optimal representations when given sufficient training data."
                            },
                            {
                                "text": "ViT has fewer parameters than comparable CNNs",
                                "correct": false,
                                "explanation": "ViT does not necessarily have fewer parameters. The scaling advantage comes from architectural flexibility, not parameter efficiency."
                            },
                            {
                                "text": "ViT uses patch embedding which is more efficient than convolution",
                                "correct": false,
                                "explanation": "Patch embedding is actually just a convolution operation. The scaling advantage comes from the global self-attention mechanism and lack of hard-coded biases, not from patch embedding itself."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- New Vertical Section: Large-Scale Pretraining with Transformers (11.9) -->
            <section>
                <!-- Slide 1: Title Slide -->
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Chapter 11.9", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">11.9 Large-Scale Pretraining with Transformers</h1>
                    <p>From Task-Specific Models to Foundation Models</p>
                    <p class="mt-lg">
                        <small>Instructor: Hafsteinn Einarsson</small><br>
                        <small>University of Iceland</small>
                    </p>
                </section>

                <!-- Slide 2: Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.9", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"}]'>
                    <h2 class="truncate-title">The Shift from Specific Experts to Generalists</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <div class="emphasis-box" style="background: #FFF5F5; border-left: 4px solid #FC8484;">
                                <h4 style="color: #FC8484; margin-top: 0;">‚ùå The Old Paradigm</h4>
                                <p style="margin-bottom: 8px;">Models trained <strong>from scratch</strong> on specific tasks (e.g., English‚ÜíFrench translation)</p>
                                <p style="margin: 0; font-size: 0.9em;">Each model becomes a <span class="tooltip">specific expert<span class="tooltiptext">A model specialized for one task, sensitive to even slight distribution shifts</span></span>, sensitive to data distribution shifts</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div class="emphasis-box" style="background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                                <h4 style="color: #2DD2C0; margin-top: 0;">‚úì The New Paradigm</h4>
                                <p style="margin-bottom: 8px;"><strong>Pretraining</strong> on large data ‚Üí <strong>more competent generalists</strong></p>
                                <p style="margin: 0; font-size: 0.9em;">Models can perform multiple tasks with or without adaptation</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4 style="color: #10099F;">Key Insight: Scalability</h4>
                            <p>Transformers demonstrate superior <strong>scaling behavior</strong>: performance improves as a <span class="tooltip">power law<span class="tooltiptext">A relationship where performance scales as L ‚àù N^(-Œ±), where N is model size and Œ± is a constant</span></span> with:</p>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px; font-size: 0.9em; margin-top: 12px;">
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #10099F;">Model<br>Parameters</strong>
                                </div>
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #10099F;">Training<br>Tokens</strong>
                                </div>
                                <div style="background: #F5F5FF; padding: 12px; border-radius: 6px; text-align: center;">
                                    <strong style="color: #10099F;">Training<br>Compute</strong>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 3: Multimodal Example - Gato -->
                <section data-sources='[{"text": "Reed et al. (2022) - A Generalist Agent", "url": "https://arxiv.org/abs/2205.06175"}]'>
                    <h2 class="truncate-title">Example: Gato - A True Generalist Model</h2>
                    <div style="font-size: 0.75em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 20px; border-radius: 10px;">
                                <h4 style="color: white; margin-top: 0;">ü§ñ Gato: One Model, Many Modalities</h4>
                                <p style="margin: 0;">A single Transformer that can play Atari, caption images, chat, and act as a robot!</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4 style="color: #10099F;">The Key Innovation</h4>
                            <p>All multimodal data (text, images, joint torques, button presses) is <strong>serialized into a flat sequence of tokens</strong></p>
                            <div style="background: #FFFBF0; padding: 15px; border-radius: 8px; margin-top: 12px;">
                                <p style="margin: 0; font-size: 0.95em;">This allows processing akin to:</p>
                                <ul style="margin: 10px 0 0 20px; text-align: left;">
                                    <li>Text tokens (like in GPT)</li>
                                    <li>Image patches (like in Vision Transformers)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #F5F5FF; border-left: 4px solid #10099F;">
                            <p style="margin: 0;"><strong>Takeaway:</strong> Transformers scale well across diverse modalities when pretrained on large, diverse data</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 4: Three Transformer Modes Overview -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.9", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"}]'>
                    <h2 class="truncate-title">Three Ways to Use Transformers</h2>
                    <div style="font-size: 0.7em;">
                        <div class="fragment">
                            <p>The Transformer architecture can be used in three different modes:</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px;">
                                <div style="background: #F5F5FF; padding: 20px; border-radius: 10px; border: 2px solid #10099F;">
                                    <h4 style="color: #10099F; margin-top: 0;">1Ô∏è‚É£ Encoder-Only</h4>
                                    <p style="font-size: 0.95em; margin-bottom: 10px;">For encoding tasks</p>
                                    <div style="background: white; padding: 10px; border-radius: 6px; font-size: 0.85em;">
                                        <strong>Example:</strong> BERT
                                    </div>
                                    <p style="margin: 10px 0 0 0; font-size: 0.9em;">All tokens attend to each other</p>
                                </div>
                                <div style="background: #F0FFF9; padding: 20px; border-radius: 10px; border: 2px solid #2DD2C0;">
                                    <h4 style="color: #2DD2C0; margin-top: 0;">2Ô∏è‚É£ Encoder-Decoder</h4>
                                    <p style="font-size: 0.95em; margin-bottom: 10px;">For seq2seq tasks</p>
                                    <div style="background: white; padding: 10px; border-radius: 6px; font-size: 0.85em;">
                                        <strong>Examples:</strong> T5, BART
                                    </div>
                                    <p style="margin: 10px 0 0 0; font-size: 0.9em;">Original Transformer design</p>
                                </div>
                                <div style="background: #FFFBF0; padding: 20px; border-radius: 10px; border: 2px solid #FAC55B;">
                                    <h4 style="color: #FFA05F; margin-top: 0;">3Ô∏è‚É£ Decoder-Only</h4>
                                    <p style="font-size: 0.95em; margin-bottom: 10px;">For generation tasks</p>
                                    <div style="background: white; padding: 10px; border-radius: 6px; font-size: 0.85em;">
                                        <strong>Examples:</strong> GPT series
                                    </div>
                                    <p style="margin: 10px 0 0 0; font-size: 0.9em;">Causal attention only</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: #E6F7FF; padding: 15px;">
                            <p style="margin: 0;"><strong>This section:</strong> We'll review each mode and explain scalability in pretraining Transformers</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #1 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of pretraining Transformers on large data compared to training from scratch?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Pretrained models train faster on downstream tasks",
                                "correct": false,
                                "explanation": "While this can be true, the main advantage is that pretrained models become more competent generalists that can perform multiple tasks with or without adaptation, rather than being task-specific experts."
                            },
                            {
                                "text": "Models become generalists that can perform multiple tasks with or without adaptation",
                                "correct": true,
                                "explanation": "Correct! Pretraining on large data allows models to learn general representations that transfer to multiple tasks, making them more robust and versatile compared to task-specific models trained from scratch."
                            },
                            {
                                "text": "Pretrained models require less memory",
                                "correct": false,
                                "explanation": "Pretrained models are typically larger and require more memory. The advantage is in their generalization capabilities, not memory efficiency."
                            },
                            {
                                "text": "They eliminate the need for task-specific data",
                                "correct": false,
                                "explanation": "While pretrained models can perform some tasks with few examples (few-shot) or no examples (zero-shot), many tasks still benefit from task-specific fine-tuning on relevant data."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Slide 5: Encoder-Only - Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.9.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html#encoder-only"}]'>
                    <h2 class="truncate-title">Mode 1: Encoder-Only Transformers</h2>
                    <div style="font-size: 0.72em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">What is Encoder-Only?</h4>
                            <p>A sequence of input tokens is converted into the <strong>same number</strong> of representations</p>
                            <div style="background: #F5F5FF; padding: 15px; border-radius: 8px; margin-top: 12px;">
                                <p style="margin: 0; font-size: 0.95em;"><strong>Key characteristic:</strong> Self-attention layers where <strong>all input tokens attend to each other</strong></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4 style="color: #10099F;">Example: Vision Transformers</h4>
                            <div style="background: #E6F7FF; padding: 15px; border-radius: 8px;">
                                <p style="margin-bottom: 10px;">Convert image patches ‚Üí representations of special "&lt;cls&gt;" token ‚Üí project to classification labels</p>
                                <p style="margin: 0; font-size: 0.9em; color: #666;">The &lt;cls&gt; representation depends on <strong>all input tokens</strong></p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px; background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px;">
                            <p style="margin: 0;"><strong>Inspiration:</strong> This design was inspired by <strong>BERT</strong>, an encoder-only Transformer pretrained on text</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 6: BERT Pretraining -->
                <section data-sources='[{"text": "Devlin et al. (2018) - BERT", "url": "https://arxiv.org/abs/1810.04805"}]'>
                    <h2 class="truncate-title">Pretraining BERT with Masked Language Modeling</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The Training Objective</h4>
                            <p>Input text with <strong>randomly masked tokens</strong> ‚Üí predict the masked tokens</p>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <div style="background: #F5F5FF; padding: 20px; border-radius: 10px;">
                                <h5 style="color: #10099F; margin-top: 0;">Example</h5>
                                <div style="display: flex; align-items: center; justify-content: center; gap: 8px; margin: 15px 0; font-size: 1.1em;">
                                    <span style="background: #E6F7FF; padding: 8px 12px; border-radius: 5px;">&lt;cls&gt;</span>
                                    <span style="background: #E6F7FF; padding: 8px 12px; border-radius: 5px;">I</span>
                                    <span style="background: #FC8484; color: white; padding: 8px 12px; border-radius: 5px; font-weight: bold;">&lt;mask&gt;</span>
                                    <span style="background: #E6F7FF; padding: 8px 12px; border-radius: 5px;">this</span>
                                    <span style="background: #E6F7FF; padding: 8px 12px; border-radius: 5px;">red</span>
                                    <span style="background: #E6F7FF; padding: 8px 12px; border-radius: 5px;">car</span>
                                </div>
                                <div style="text-align: center; font-size: 1.2em; margin: 10px 0;">‚Üì</div>
                                <p style="text-align: center; font-size: 1.05em; margin: 0;">Predict: <span style="background: #2DD2C0; color: white; padding: 6px 12px; border-radius: 5px; font-weight: bold;">"love"</span></p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 15px;">
                            <h4 style="color: #10099F;">Why "Bidirectional"?</h4>
                            <div style="background: #FFFBF0; padding: 15px; border-radius: 8px;">
                                <p style="margin: 0;">All tokens attend to each other ‚Üí prediction of "love" depends on tokens <strong>before AND after</strong> it</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 15px; background: #F0FFF9; border-left: 4px solid #2DD2C0;">
                            <p style="margin: 0;"><strong>Data source:</strong> Large-scale text from books and Wikipedia (no manual labeling needed!)</p>
                        </div>
                    </div>
                </section>

                <!-- Slide 7: BERT Fine-Tuning -->
                <section data-sources='[{"text": "Devlin et al. (2018) - BERT", "url": "https://arxiv.org/abs/1810.04805"}]'>
                    <h2 class="truncate-title">Fine-Tuning BERT for Downstream Tasks</h2>
                    <div style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">The Fine-Tuning Process</h4>
                            <div style="background: #F5F5FF; padding: 12px; border-radius: 8px;">
                                <ol style="margin: 0; text-align: left; padding-left: 25px;">
                                    <li style="margin-bottom: 6px;">Take pretrained BERT</li>
                                    <li style="margin-bottom: 6px;">Add task-specific layers (with randomized parameters)</li>
                                    <li>Update <strong>all parameters</strong> using downstream task data</li>
                                </ol>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 12px;">
                            <h4 style="color: #10099F;">Example: Sentiment Analysis</h4>
                            <div style="background: #E6F7FF; padding: 12px; border-radius: 10px; text-align: center;">
                                <div style="margin-bottom: 10px;">
                                    <div style="background: white; padding: 10px; border-radius: 8px; display: inline-block;">
                                        <strong>Input:</strong> "This show is not bad"
                                    </div>
                                </div>
                                <div style="margin: 10px 0;">‚Üì</div>
                                <div style="background: #10099F; color: white; padding: 10px; border-radius: 8px; margin: 10px 0;">
                                    <strong>Pretrained BERT</strong><br>
                                    <span style="font-size: 0.9em;">(350M parameters)</span>
                                </div>
                                <div style="margin: 10px 0;">‚Üì Extract &lt;cls&gt; representation</div>
                                <div style="background: #2DD2C0; color: white; padding: 10px; border-radius: 8px; margin: 10px 0;">
                                    <strong>Fully Connected Layer</strong><br>
                                    <span style="font-size: 0.9em;">(trained from scratch)</span>
                                </div>
                                <div style="margin: 10px 0;">‚Üì</div>
                                <div style="background: white; padding: 10px; border-radius: 8px; display: inline-block; border: 2px solid #2DD2C0;">
                                    <strong>Output:</strong> <span style="color: #2DD2C0;">Positive ‚úì</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Slide 8: BERT Impact and Variants -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 11.9.1", "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html#encoder-only"}]'>
                    <h2 class="truncate-title">BERT's Impact and Derivatives</h2>
                    <div style="font-size: 0.6em;">
                        <div class="fragment">
                            <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 12px; border-radius: 10px; margin-bottom: 12px;">
                                <h4 style="color: white; margin-top: 0;">üèÜ BERT's Achievement</h4>
                                <p style="margin: 0;">350M parameters trained on 250B tokens ‚Üí advanced state-of-the-art for:</p>
                                <ul style="margin: 8px 0 0 20px; text-align: left;">
                                    <li>Single text classification</li>
                                    <li>Text pair classification/regression</li>
                                    <li>Text tagging</li>
                                    <li>Question answering</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #10099F;">BERT Variants and Improvements</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; font-size: 0.9em;">
                                <div style="background: #F5F5FF; padding: 10px; border-radius: 8px;">
                                    <strong style="color: #10099F;">RoBERTa</strong> (Liu et al., 2019)<br>
                                    <span style="font-size: 0.85em;">Trained on 2000B tokens</span>
                                </div>
                                <div style="background: #F0FFF9; padding: 10px; border-radius: 8px;">
                                    <strong style="color: #2DD2C0;">ALBERT</strong> (Lan et al., 2019)<br>
                                    <span style="font-size: 0.85em;">Parameter sharing</span>
                                </div>
                                <div style="background: #FFFBF0; padding: 10px; border-radius: 8px;">
                                    <strong style="color: #FAC55B;">SpanBERT</strong> (Joshi et al., 2020)<br>
                                    <span style="font-size: 0.85em;">Predict spans of text</span>
                                </div>
                                <div style="background: #FFF5F5; padding: 10px; border-radius: 8px;">
                                    <strong style="color: #FC8484;">DistilBERT</strong> (Sanh et al., 2019)<br>
                                    <span style="font-size: 0.85em;">Lightweight via distillation</span>
                                </div>
                                <div style="background: #E6F7FF; padding: 10px; border-radius: 8px;">
                                    <strong style="color: #10099F;">ELECTRA</strong> (Clark et al., 2020)<br>
                                    <span style="font-size: 0.85em;">Replaced token detection</span>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 12px; background: #FFFBF0; border-left: 4px solid #FAC55B;">
                            <p style="margin: 0;"><strong>Beyond NLP:</strong> BERT inspired Transformer pretraining in computer vision (Vision Transformers, Swin Transformers, MAE)</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ #2 -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes BERT a \"bidirectional\" encoder?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It processes text from left to right and then from right to left",
                                "correct": false,
                                "explanation": "BERT does not process text in two passes. The term bidirectional refers to the attention pattern, not the processing direction."
                            },
                            {
                                "text": "All tokens can attend to each other, so predictions depend on context from both before and after",
                                "correct": true,
                                "explanation": "Correct! In BERT, there is no constraint in the attention pattern - all tokens can attend to each other. This means when predicting a masked token, the model can use context from both before and after it in the sequence."
                            },
                            {
                                "text": "It has two encoders working in parallel",
                                "correct": false,
                                "explanation": "BERT has a single Transformer encoder. The bidirectional aspect refers to the attention mechanism, not multiple encoders."
                            },
                            {
                                "text": "It can translate text in both directions",
                                "correct": false,
                                "explanation": "BERT is not designed for translation. It is an encoder-only model for encoding tasks, not sequence-to-sequence tasks like translation."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            </section>
        </div>
    </div>

    <!-- Reveal.js core -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>

    <!-- Shared utilities -->
    <script src="../shared/js/title-handler.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/multiple-choice.js"></script>

    <!-- D3.js for visualization -->
    <script src="https://d3js.org/d3.v7.min.js"></script>

    <!-- Initialize Reveal.js -->
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            slideNumber: 'c/t',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes ]
        });

        // Attention Heatmap Visualization
        Reveal.on('slidechanged', event => {
            // Check if we're on the attention heatmap slide
            if (event.currentSlide.querySelector('#attention-heatmap')) {
                initializeAttentionHeatmap();
            }
            // Check if we're on the scalability visualization slide
            if (event.currentSlide.querySelector('#scalability-viz')) {
                initializeScalabilityViz();
            }
        });

        function initializeAttentionHeatmap() {
            const container = d3.select('#attention-heatmap');
            container.selectAll('*').remove();  // Clear previous content

            const size = 10;  // 10x10 matrix
            const cellSize = 20;
            const margin = {top: 35, right: 70, bottom: 35, left: 45};
            const width = cellSize * size + margin.left + margin.right;
            const height = cellSize * size + margin.top + margin.bottom;

            // Create SVG
            const svg = container.append('svg')
                .attr('width', width)
                .attr('height', height);

            const g = svg.append('g')
                .attr('transform', `translate(${margin.left},${margin.top})`);

            // Color scale
            const colorScale = d3.scaleSequential(d3.interpolateReds)
                .domain([0, 1]);

            // Function to generate different matrix patterns
            function generateMatrix(pattern) {
                const matrix = [];
                for (let i = 0; i < size; i++) {
                    matrix[i] = [];
                    for (let j = 0; j < size; j++) {
                        if (pattern === 'identity') {
                            matrix[i][j] = (i === j) ? 1 : 0;
                        } else if (pattern === 'uniform') {
                            matrix[i][j] = 1 / size;
                        } else if (pattern === 'random') {
                            matrix[i][j] = Math.random();
                        }
                    }
                    // Normalize row to sum to 1
                    if (pattern === 'random') {
                        const rowSum = matrix[i].reduce((a, b) => a + b, 0);
                        matrix[i] = matrix[i].map(val => val / rowSum);
                    }
                }
                return matrix;
            }

            // Function to update the heatmap
            function updateHeatmap(pattern) {
                const matrix = generateMatrix(pattern);

                // Flatten matrix for D3
                const data = [];
                for (let i = 0; i < size; i++) {
                    for (let j = 0; j < size; j++) {
                        data.push({row: i, col: j, value: matrix[i][j]});
                    }
                }

                // Bind data to cells
                const cells = g.selectAll('.cell')
                    .data(data, d => `${d.row}-${d.col}`);

                // Enter
                cells.enter().append('rect')
                    .attr('class', 'cell')
                    .attr('x', d => d.col * cellSize)
                    .attr('y', d => d.row * cellSize)
                    .attr('width', cellSize)
                    .attr('height', cellSize)
                    .attr('stroke', '#fff')
                    .attr('stroke-width', 1)
                    .merge(cells)
                    .transition()
                    .duration(500)
                    .attr('fill', d => colorScale(d.value));

                cells.exit().remove();
            }

            // Add axes labels
            g.append('text')
                .attr('x', cellSize * size / 2)
                .attr('y', -15)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('font-weight', 'bold')
                .style('fill', '#10099F')
                .text('Keys');

            g.append('text')
                .attr('transform', 'rotate(-90)')
                .attr('x', -cellSize * size / 2)
                .attr('y', -40)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('font-weight', 'bold')
                .style('fill', '#10099F')
                .text('Queries');

            // Add axis tick labels
            const tickIndices = [0, 2, 4, 6, 8, 9];
            tickIndices.forEach(i => {
                g.append('text')
                    .attr('x', i * cellSize + cellSize / 2)
                    .attr('y', size * cellSize + 20)
                    .attr('text-anchor', 'middle')
                    .style('font-size', '10px')
                    .text(i);

                g.append('text')
                    .attr('x', -10)
                    .attr('y', i * cellSize + cellSize / 2 + 5)
                    .attr('text-anchor', 'end')
                    .style('font-size', '10px')
                    .text(i);
            });

            // Add colorbar
            const colorbarHeight = cellSize * size;
            const colorbarWidth = 20;
            const colorbarX = cellSize * size + 40;

            const colorbarScale = d3.scaleLinear()
                .domain([0, 1])
                .range([colorbarHeight, 0]);

            const colorbarAxis = d3.axisRight(colorbarScale)
                .ticks(5)
                .tickFormat(d3.format('.2f'));

            const defs = svg.append('defs');
            const gradient = defs.append('linearGradient')
                .attr('id', 'colorbar-gradient')
                .attr('x1', '0%')
                .attr('x2', '0%')
                .attr('y1', '100%')
                .attr('y2', '0%');

            gradient.append('stop')
                .attr('offset', '0%')
                .attr('stop-color', colorScale(0));

            gradient.append('stop')
                .attr('offset', '100%')
                .attr('stop-color', colorScale(1));

            g.append('rect')
                .attr('x', colorbarX)
                .attr('y', 0)
                .attr('width', colorbarWidth)
                .attr('height', colorbarHeight)
                .style('fill', 'url(#colorbar-gradient)');

            g.append('g')
                .attr('transform', `translate(${colorbarX + colorbarWidth}, 0)`)
                .call(colorbarAxis);

            // Initialize with identity pattern
            updateHeatmap('identity');

            // Add event listener to dropdown
            d3.select('#attention-pattern').on('change', function() {
                updateHeatmap(this.value);
            });
        }

        // Variance Growth Visualization (Slide 6)
        Reveal.on('slidechanged', event => {
            if (event.currentSlide.querySelector('#variance-demo')) {
                initializeVarianceDemo();
            }
            if (event.currentSlide.querySelector('#attention-heatmap-demo')) {
                initializeAttentionHeatmapDemo();
            }
            if (event.currentSlide.querySelector('#multihead-attention-demo')) {
                initializeMultiHeadAttentionDemo();
            }
        });

        function initializeVarianceDemo() {
            const container = d3.select('#variance-demo');
            container.selectAll('*').remove();

            const margin = {top: 30, right: 30, bottom: 50, left: 60};
            const width = 700 - margin.left - margin.right;
            const height = 300 - margin.top - margin.bottom;

            const svg = container.append('svg')
                .attr('width', width + margin.left + margin.right)
                .attr('height', height + margin.top + margin.bottom)
                .append('g')
                .attr('transform', `translate(${margin.left},${margin.top})`);

            // Scales
            const xScale = d3.scaleLinear().domain([2, 512]).range([0, width]);
            const yScale = d3.scaleLinear().domain([0, 550]).range([height, 0]);

            // Axes
            svg.append('g')
                .attr('transform', `translate(0,${height})`)
                .call(d3.axisBottom(xScale).ticks(8))
                .style('font-size', '12px');

            svg.append('g')
                .call(d3.axisLeft(yScale).ticks(6))
                .style('font-size', '12px');

            // Axis labels
            svg.append('text')
                .attr('x', width / 2)
                .attr('y', height + 40)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('fill', '#10099F')
                .text('Dimension d');

            svg.append('text')
                .attr('transform', 'rotate(-90)')
                .attr('x', -height / 2)
                .attr('y', -45)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('fill', '#10099F')
                .text('Variance of q^T k');

            // Line for variance growth
            const line = d3.line()
                .x(d => xScale(d))
                .y(d => yScale(d));

            const dims = d3.range(2, 513, 5);

            svg.append('path')
                .datum(dims)
                .attr('fill', 'none')
                .attr('stroke', '#FC8484')
                .attr('stroke-width', 3)
                .attr('d', line);

            // Scaled variance line (always 1)
            svg.append('line')
                .attr('x1', 0)
                .attr('x2', width)
                .attr('y1', yScale(1))
                .attr('y2', yScale(1))
                .attr('stroke', '#2DD2C0')
                .attr('stroke-width', 3)
                .attr('stroke-dasharray', '5,5');

            // Current dimension marker
            const marker = svg.append('circle')
                .attr('r', 6)
                .attr('fill', '#10099F')
                .attr('stroke', 'white')
                .attr('stroke-width', 2);

            // Legend
            const legend = svg.append('g')
                .attr('transform', `translate(${width - 200}, 20)`);

            legend.append('line')
                .attr('x1', 0).attr('x2', 30)
                .attr('y1', 0).attr('y2', 0)
                .attr('stroke', '#FC8484')
                .attr('stroke-width', 3);
            legend.append('text')
                .attr('x', 35).attr('y', 5)
                .style('font-size', '12px')
                .text('Unscaled: Var = d');

            legend.append('line')
                .attr('x1', 0).attr('x2', 30)
                .attr('y1', 25).attr('y2', 25)
                .attr('stroke', '#2DD2C0')
                .attr('stroke-width', 3)
                .attr('stroke-dasharray', '5,5');
            legend.append('text')
                .attr('x', 35).attr('y', 30)
                .style('font-size', '12px')
                .text('Scaled: Var = 1');

            // Update function
            function updateMarker(dim) {
                marker
                    .attr('cx', xScale(dim))
                    .attr('cy', yScale(dim));

                d3.select('#dim-value').text(dim);
            }

            // Initial position
            updateMarker(64);

            // Event listener
            d3.select('#dim-slider').on('input', function() {
                updateMarker(+this.value);
            });
        }

        // Attention Heatmap Demo (Slide 15)
        function initializeAttentionHeatmapDemo() {
            const container = d3.select('#attention-heatmap-demo');
            container.selectAll('*').remove();

            const margin = {top: 50, right: 100, bottom: 50, left: 80};
            const cellSize = 35;
            const rows = 2;  // 2 queries
            const cols = 10; // 10 keys
            const width = cellSize * cols + margin.left + margin.right;
            const height = cellSize * rows + margin.top + margin.bottom;

            const svg = container.append('svg')
                .attr('width', width)
                .attr('height', height)
                .append('g')
                .attr('transform', `translate(${margin.left},${margin.top})`);

            // Color scale
            const colorScale = d3.scaleSequential(d3.interpolateBlues)
                .domain([0, 1]);

            function generateAttentionWeights(validLen) {
                // Generate random attention weights
                const weights = [];
                for (let i = 0; i < validLen; i++) {
                    weights.push(Math.random());
                }
                // Pad with zeros
                while (weights.length < cols) {
                    weights.push(0);
                }
                // Normalize valid positions to sum to 1
                const sum = weights.slice(0, validLen).reduce((a, b) => a + b, 0);
                for (let i = 0; i < validLen; i++) {
                    weights[i] /= sum;
                }
                return weights;
            }

            function updateHeatmap() {
                const validLen1 = +d3.select('#valid-len-1').property('value');
                const validLen2 = +d3.select('#valid-len-2').property('value');

                d3.select('#valid-len-1-value').text(validLen1);
                d3.select('#valid-len-2-value').text(validLen2);

                const data = [
                    generateAttentionWeights(validLen1),
                    generateAttentionWeights(validLen2)
                ];

                // Flatten for D3
                const flatData = [];
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        flatData.push({
                            row: i,
                            col: j,
                            value: data[i][j]
                        });
                    }
                }

                // Draw cells
                const cells = svg.selectAll('rect')
                    .data(flatData, d => `${d.row}-${d.col}`);

                cells.enter()
                    .append('rect')
                    .attr('x', d => d.col * cellSize)
                    .attr('y', d => d.row * cellSize)
                    .attr('width', cellSize - 1)
                    .attr('height', cellSize - 1)
                    .attr('fill', d => colorScale(d.value))
                    .attr('stroke', '#fff')
                    .attr('stroke-width', 1)
                    .merge(cells)
                    .transition()
                    .duration(300)
                    .attr('fill', d => colorScale(d.value));

                // Add text values
                const texts = svg.selectAll('text.cell-value')
                    .data(flatData, d => `${d.row}-${d.col}`);

                texts.enter()
                    .append('text')
                    .attr('class', 'cell-value')
                    .attr('x', d => d.col * cellSize + cellSize / 2)
                    .attr('y', d => d.row * cellSize + cellSize / 2)
                    .attr('text-anchor', 'middle')
                    .attr('dominant-baseline', 'central')
                    .style('font-size', '10px')
                    .style('fill', d => d.value > 0.5 ? 'white' : '#333')
                    .text(d => d.value > 0 ? d.value.toFixed(2) : '0.00')
                    .merge(texts)
                    .transition()
                    .duration(300)
                    .text(d => d.value > 0 ? d.value.toFixed(2) : '0.00')
                    .style('fill', d => d.value > 0.5 ? 'white' : '#333');
            }

            // Add labels
            svg.append('text')
                .attr('x', -10)
                .attr('y', cellSize / 2)
                .attr('text-anchor', 'end')
                .style('font-size', '12px')
                .text('Query 1');

            svg.append('text')
                .attr('x', -10)
                .attr('y', cellSize * 1.5)
                .attr('text-anchor', 'end')
                .style('font-size', '12px')
                .text('Query 2');

            // Column labels
            for (let i = 0; i < cols; i++) {
                svg.append('text')
                    .attr('x', i * cellSize + cellSize / 2)
                    .attr('y', -10)
                    .attr('text-anchor', 'middle')
                    .style('font-size', '11px')
                    .text(`K${i}`);
            }

            // Title
            svg.append('text')
                .attr('x', cellSize * cols / 2)
                .attr('y', -30)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('font-weight', 'bold')
                .style('fill', '#10099F')
                .text('Attention Weights (masked by valid length)');

            // Event listeners
            d3.select('#valid-len-1').on('input', updateHeatmap);
            d3.select('#valid-len-2').on('input', updateHeatmap);
            d3.select('#reset-attention-btn').on('click', () => {
                d3.select('#valid-len-1').property('value', 2);
                d3.select('#valid-len-2').property('value', 6);
                updateHeatmap();
            });

            // Initial render
            updateHeatmap();
        }

        // Multi-Head Attention Demo (Slide 12 in Multi-Head Attention section)
        function initializeMultiHeadAttentionDemo() {
            const container = d3.select('#multihead-attention-demo');
            container.selectAll('*').remove();

            let numHeads = 2;
            const seqLength = 6;  // Sequence length
            const cellSize = 30;
            const headSpacing = 15;
            const margin = {top: 5, right: 5, bottom: 30, left: 60};

            function generateAttentionWeights(seed) {
                // Generate different attention patterns for different heads
                const weights = [];
                for (let i = 0; i < seqLength; i++) {
                    const row = [];
                    // Create different patterns based on seed (head index)
                    for (let j = 0; j < seqLength; j++) {
                        let weight;
                        if (seed === 0) {
                            // Head 0: Focus on nearby positions (local attention)
                            weight = Math.exp(-Math.abs(i - j) * 0.8);
                        } else if (seed === 1) {
                            // Head 1: Focus on distant positions
                            weight = Math.exp(-Math.pow(i - j, 2) * 0.1) * (Math.abs(i - j) > 1 ? 1.5 : 0.3);
                        } else if (seed === 2) {
                            // Head 2: Focus on beginning of sequence
                            weight = Math.exp(-j * 0.4);
                        } else {
                            // Head 3: Focus on end of sequence
                            weight = Math.exp(-(seqLength - 1 - j) * 0.4);
                        }
                        row.push(weight);
                    }
                    // Normalize to sum to 1 (softmax-like)
                    const sum = row.reduce((a, b) => a + b, 0);
                    weights.push(row.map(w => w / sum));
                }
                return weights;
            }

            function render(n) {
                container.selectAll('*').remove();

                const totalWidth = n * (cellSize * seqLength + margin.left + margin.right + headSpacing);
                const height = cellSize * seqLength + margin.top + margin.bottom;

                const svg = container.append('svg')
                    .attr('width', totalWidth)
                    .attr('height', height);

                const colorScale = d3.scaleSequential(d3.interpolateBlues)
                    .domain([0, 1]);

                // Draw each head
                for (let h = 0; h < n; h++) {
                    const offsetX = h * (cellSize * seqLength + margin.left + margin.right + headSpacing);
                    const g = svg.append('g')
                        .attr('transform', `translate(${offsetX + margin.left},${margin.top})`);

                    const weights = generateAttentionWeights(h);

                    // Draw heatmap
                    for (let i = 0; i < seqLength; i++) {
                        for (let j = 0; j < seqLength; j++) {
                            g.append('rect')
                                .attr('x', j * cellSize)
                                .attr('y', i * cellSize)
                                .attr('width', cellSize - 1)
                                .attr('height', cellSize - 1)
                                .attr('fill', colorScale(weights[i][j]))
                                .attr('stroke', '#fff')
                                .attr('stroke-width', 1);

                            // Add text for values
                            g.append('text')
                                .attr('x', j * cellSize + cellSize / 2)
                                .attr('y', i * cellSize + cellSize / 2)
                                .attr('text-anchor', 'middle')
                                .attr('dominant-baseline', 'central')
                                .style('font-size', '9px')
                                .style('fill', weights[i][j] > 0.5 ? 'white' : '#333')
                                .text(weights[i][j].toFixed(2));
                        }
                    }

                    // Row labels
                    for (let i = 0; i < seqLength; i++) {
                        g.append('text')
                            .attr('x', -10)
                            .attr('y', i * cellSize + cellSize / 2)
                            .attr('text-anchor', 'end')
                            .attr('dominant-baseline', 'central')
                            .style('font-size', '11px')
                            .text(`Q${i}`);
                    }

                    // Column labels
                    for (let j = 0; j < seqLength; j++) {
                        g.append('text')
                            .attr('x', j * cellSize + cellSize / 2)
                            .attr('y', seqLength * cellSize + 15)
                            .attr('text-anchor', 'middle')
                            .style('font-size', '11px')
                            .text(`K${j}`);
                    }

                    // Head label
                    g.append('text')
                        .attr('x', cellSize * seqLength / 2)
                        .attr('y', -15)
                        .attr('text-anchor', 'middle')
                        .style('font-size', '13px')
                        .style('font-weight', 'bold')
                        .style('fill', '#10099F')
                        .text(`Head ${h + 1}`);
                }
            }

            // Initial render
            render(numHeads);

            // Event listeners
            d3.select('#num-heads-slider').on('input', function() {
                numHeads = +this.value;
                d3.select('#num-heads-value').text(numHeads);
                render(numHeads);
            });

            d3.select('#reset-multihead-btn').on('click', () => {
                numHeads = 2;
                d3.select('#num-heads-slider').property('value', 2);
                d3.select('#num-heads-value').text(2);
                render(numHeads);
            });
        }

        // Scalability Visualization (ViT vs CNN Performance)
        function initializeScalabilityViz() {
            const container = d3.select('#scalability-viz');
            container.selectAll('*').remove();

            const margin = {top: 30, right: 120, bottom: 50, left: 60};
            const width = 700 - margin.left - margin.right;
            const height = 320 - margin.top - margin.bottom;

            const svg = container.append('svg')
                .attr('width', width + margin.left + margin.right)
                .attr('height', height + margin.top + margin.bottom)
                .append('g')
                .attr('transform', `translate(${margin.left},${margin.top})`);

            // Data: dataset size (in millions) vs accuracy
            const data = {
                cnn: [
                    {size: 0.06, acc: 72},    // Fashion-MNIST
                    {size: 1.2, acc: 76},     // ImageNet
                    {size: 14, acc: 80},      // ImageNet-21k
                    {size: 100, acc: 83},     // Larger dataset
                    {size: 300, acc: 84}      // JFT-300M
                ],
                vit: [
                    {size: 0.06, acc: 70},    // Fashion-MNIST
                    {size: 1.2, acc: 74},     // ImageNet
                    {size: 14, acc: 79},      // ImageNet-21k
                    {size: 100, acc: 84},     // Larger dataset
                    {size: 300, acc: 88}      // JFT-300M
                ]
            };

            // Scales
            const xScale = d3.scaleLog()
                .domain([0.05, 400])
                .range([0, width]);

            const yScale = d3.scaleLinear()
                .domain([65, 90])
                .range([height, 0]);

            // Axes
            const xAxis = d3.axisBottom(xScale)
                .tickValues([0.1, 1, 10, 100])
                .tickFormat(d => d < 1 ? `${(d*1000).toFixed(0)}K` : `${d}M`);

            const yAxis = d3.axisLeft(yScale)
                .tickFormat(d => `${d}%`);

            svg.append('g')
                .attr('transform', `translate(0,${height})`)
                .call(xAxis)
                .style('font-size', '12px');

            svg.append('g')
                .call(yAxis)
                .style('font-size', '12px');

            // Axis labels
            svg.append('text')
                .attr('x', width / 2)
                .attr('y', height + 40)
                .attr('text-anchor', 'middle')
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('Training Dataset Size (Images)');

            svg.append('text')
                .attr('transform', 'rotate(-90)')
                .attr('x', -height / 2)
                .attr('y', -45)
                .attr('text-anchor', 'middle')
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('Accuracy');

            // Line generators
            const line = d3.line()
                .x(d => xScale(d.size))
                .y(d => yScale(d.acc))
                .curve(d3.curveMonotoneX);

            // CNN line
            svg.append('path')
                .datum(data.cnn)
                .attr('fill', 'none')
                .attr('stroke', '#2DD2C0')
                .attr('stroke-width', 3)
                .attr('d', line);

            // ViT line
            svg.append('path')
                .datum(data.vit)
                .attr('fill', 'none')
                .attr('stroke', '#10099F')
                .attr('stroke-width', 3)
                .attr('d', line);

            // Points for CNN
            svg.selectAll('.cnn-point')
                .data(data.cnn)
                .enter()
                .append('circle')
                .attr('class', 'cnn-point')
                .attr('cx', d => xScale(d.size))
                .attr('cy', d => yScale(d.acc))
                .attr('r', 5)
                .attr('fill', '#2DD2C0')
                .attr('stroke', 'white')
                .attr('stroke-width', 2);

            // Points for ViT
            svg.selectAll('.vit-point')
                .data(data.vit)
                .enter()
                .append('circle')
                .attr('class', 'vit-point')
                .attr('cx', d => xScale(d.size))
                .attr('cy', d => yScale(d.acc))
                .attr('r', 5)
                .attr('fill', '#10099F')
                .attr('stroke', 'white')
                .attr('stroke-width', 2);

            // Legend
            const legend = svg.append('g')
                .attr('transform', `translate(${width - 100}, 20)`);

            legend.append('line')
                .attr('x1', 0)
                .attr('x2', 30)
                .attr('y1', 0)
                .attr('y2', 0)
                .attr('stroke', '#2DD2C0')
                .attr('stroke-width', 3);

            legend.append('text')
                .attr('x', 35)
                .attr('y', 4)
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('CNN (ResNet)');

            legend.append('line')
                .attr('x1', 0)
                .attr('x2', 30)
                .attr('y1', 25)
                .attr('y2', 25)
                .attr('stroke', '#10099F')
                .attr('stroke-width', 3);

            legend.append('text')
                .attr('x', 35)
                .attr('y', 29)
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('ViT');

            // Annotation for crossover point
            const crossoverX = xScale(100);
            const crossoverY = yScale(83.5);

            svg.append('line')
                .attr('x1', crossoverX)
                .attr('y1', crossoverY - 10)
                .attr('x2', crossoverX)
                .attr('y2', crossoverY - 40)
                .attr('stroke', '#FAC55B')
                .attr('stroke-width', 2)
                .attr('stroke-dasharray', '4,4');

            svg.append('text')
                .attr('x', crossoverX)
                .attr('y', crossoverY - 45)
                .attr('text-anchor', 'middle')
                .style('font-size', '11px')
                .style('font-weight', 'bold')
                .style('fill', '#FAC55B')
                .text('ViT overtakes CNN');
        }

        // Scaling Law Visualization
        function initializeScalingLawViz() {
            const container = d3.select('#scaling-law-viz');
            container.selectAll('*').remove();

            const margin = {top: 30, right: 60, bottom: 50, left: 60};
            const width = 700 - margin.left - margin.right;
            const height = 400 - margin.top - margin.bottom;

            const svg = container.append('svg')
                .attr('width', width + margin.left + margin.right)
                .attr('height', height + margin.top + margin.bottom)
                .append('g')
                .attr('transform', `translate(${margin.left},${margin.top})`);

            // Get current slider values
            const modelSize = +d3.select('#model-size-slider').property('value');
            const datasetSize = +d3.select('#dataset-size-slider').property('value');

            // Power law function: L = a * N^(-alpha)
            // Where L is loss, N is size, alpha is scaling exponent
            const alpha = 0.076; // Empirical scaling exponent for model size
            const beta = 0.095;  // Empirical scaling exponent for dataset size

            // Calculate loss based on both factors
            // Base loss at reference point (100M params, 100B tokens)
            const baseLoss = 2.5;
            const refModel = 100;
            const refData = 100;

            const loss = baseLoss * Math.pow(refModel / modelSize, alpha) * Math.pow(refData / datasetSize, beta);

            // Scales
            const xScale = d3.scaleLog()
                .domain([1, 1000])
                .range([0, width]);

            const yScale = d3.scaleLinear()
                .domain([0.5, 5])
                .range([height, 0]);

            // Axes
            svg.append('g')
                .attr('transform', `translate(0,${height})`)
                .call(d3.axisBottom(xScale)
                    .tickValues([1, 10, 100, 1000])
                    .tickFormat(d => d + 'M'))
                .style('font-size', '12px');

            svg.append('text')
                .attr('x', width / 2)
                .attr('y', height + 40)
                .attr('text-anchor', 'middle')
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('Model Size (Parameters)');

            svg.append('g')
                .call(d3.axisLeft(yScale))
                .style('font-size', '12px');

            svg.append('text')
                .attr('transform', 'rotate(-90)')
                .attr('x', -height / 2)
                .attr('y', -45)
                .attr('text-anchor', 'middle')
                .style('font-size', '13px')
                .style('fill', '#262626')
                .text('Loss');

            // Generate power law curve data
            const curveData = [];
            for (let size = 1; size <= 1000; size *= 1.1) {
                const curveLoss = baseLoss * Math.pow(refModel / size, alpha) * Math.pow(refData / datasetSize, beta);
                curveData.push({size, loss: curveLoss});
            }

            // Line generator
            const line = d3.line()
                .x(d => xScale(d.size))
                .y(d => yScale(d.loss));

            // Draw power law curve
            svg.append('path')
                .datum(curveData)
                .attr('fill', 'none')
                .attr('stroke', '#10099F')
                .attr('stroke-width', 3)
                .attr('d', line);

            // Draw current point
            svg.append('circle')
                .attr('cx', xScale(modelSize))
                .attr('cy', yScale(loss))
                .attr('r', 8)
                .attr('fill', '#FC8484')
                .attr('stroke', 'white')
                .attr('stroke-width', 2);

            // Add current loss value
            svg.append('text')
                .attr('x', xScale(modelSize))
                .attr('y', yScale(loss) - 15)
                .attr('text-anchor', 'middle')
                .style('font-size', '14px')
                .style('font-weight', 'bold')
                .style('fill', '#FC8484')
                .text(`Loss: ${loss.toFixed(3)}`);

            // Add formula
            svg.append('text')
                .attr('x', width - 10)
                .attr('y', 20)
                .attr('text-anchor', 'end')
                .style('font-size', '12px')
                .style('fill', '#666')
                .html('L ‚àù N<tspan baseline-shift="super" font-size="9px">-Œ±</tspan> √ó D<tspan baseline-shift="super" font-size="9px">-Œ≤</tspan>');

            // Dataset size indicator
            svg.append('text')
                .attr('x', 10)
                .attr('y', 20)
                .style('font-size', '11px')
                .style('fill', '#666')
                .text(`Dataset: ${datasetSize}B tokens`);
        }

        // Initialize on slide load
        Reveal.on('slidechanged', event => {
            if (event.currentSlide.querySelector('#scaling-law-viz')) {
                initializeScalingLawViz();
            }
        });

        // Update visualization when sliders change
        d3.select('#model-size-slider').on('input', function() {
            d3.select('#model-size-value').text(this.value);
            if (Reveal.getCurrentSlide().querySelector('#scaling-law-viz')) {
                initializeScalingLawViz();
            }
        });

        d3.select('#dataset-size-slider').on('input', function() {
            d3.select('#dataset-size-value').text(this.value);
            if (Reveal.getCurrentSlide().querySelector('#scaling-law-viz')) {
                initializeScalingLawViz();
            }
        });

        d3.select('#reset-scaling-law-btn').on('click', () => {
            d3.select('#model-size-slider').property('value', 10);
            d3.select('#model-size-value').text(10);
            d3.select('#dataset-size-slider').property('value', 10);
            d3.select('#dataset-size-value').text(10);
            if (Reveal.getCurrentSlide().querySelector('#scaling-law-viz')) {
                initializeScalingLawViz();
            }
        });
    </script>

</body>
</html>
