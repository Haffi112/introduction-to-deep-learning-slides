<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recurrent Neural Networks - Introduction to Deep Learning</title>

    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    <link rel="stylesheet" href="css/rnn-custom.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Recurrent Neural Networks</h1>
                <p>Chapter 9: Introduction</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>

            <!-- Section 1: From Fixed to Variable Length Data (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 9", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html"}]'>
                    <h2 class="truncate-title">From Fixed-Length to Sequential Data</h2>
                    <div class="content-container">
                        <div class="fragment">
                            <h4>What We've Covered So Far</h4>
                            <ul style="font-size: 0.8em;">
                                <li><strong>Linear/Logistic Regression & MLPs:</strong> Fixed feature vectors $$\mathbf{x}_i = [x_1, x_2, ..., x_d]$$</li>
                                <li><strong>Tabular Data:</strong> Arranged in tables with fixed columns</li>
                                <li><strong>CNNs for Images:</strong> Fixed dimensions (e.g., 28×28 Fashion-MNIST)</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>The Challenge:</strong> What about sequences of images (video) or generating sequential outputs (image captioning)?</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Sequential Learning Tasks</h2>
                    <div class="sequential-tasks" style="display: flex; justify-content: space-between; gap: 20px; font-size: 0.65em;">
                        <div class="fragment" style="flex: 1;">
                            <h4>Tasks Requiring Sequential Outputs</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Image captioning</li>
                                <li>Speech synthesis</li>
                                <li>Music generation</li>
                            </ul>
                        </div>
                        <div class="fragment" style="flex: 1;">
                            <h4>Tasks Learning from Sequential Inputs</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Time series prediction</li>
                                <li>Video analysis</li>
                                <li>Musical information retrieval</li>
                            </ul>
                        </div>
                        <div class="fragment" style="flex: 1;">
                            <h4>Tasks with Both Sequential Input and Output</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Machine translation</li>
                                <li>Dialogue systems</li>
                                <li>Robot control</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statement best describes the limitation of the models covered before RNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They could only work with numerical data",
                                "correct": false,
                                "explanation": "MLPs and CNNs can work with various data types, not just numerical."
                            },
                            {
                                "text": "They required fixed-length inputs and produced single predictions",
                                "correct": true,
                                "explanation": "Correct! Previous models like MLPs and CNNs work with fixed-size inputs (fixed feature vectors or fixed image dimensions) and output single predictions."
                            },
                            {
                                "text": "They were too slow for practical applications",
                                "correct": false,
                                "explanation": "Speed was not the primary limitation; the issue was handling variable-length sequential data."
                            },
                            {
                                "text": "They could not learn complex patterns",
                                "correct": false,
                                "explanation": "CNNs and MLPs can learn very complex patterns; they just struggle with sequential, variable-length data."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 2: Recurrent Neural Networks (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 9", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html"}]'>
                    <h2 class="truncate-title">Recurrent Neural Networks</h2>
                    <div class="rnn-definition" style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 0.9em;">RNNs are deep learning models that capture the dynamics of sequences via <span class="tooltip">recurrent connections<span class="tooltiptext">Connections that can be thought of as cycles in the network of nodes</span></span>.</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Key Characteristics</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Unrolled across time steps</strong> with the same parameters at each step</li>
                                <li><strong>Standard connections:</strong> Applied synchronously within the same time step</li>
                                <li><strong>Recurrent connections:</strong> Dynamic, passing information across adjacent time steps</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>RNNs can be thought of as feedforward networks where parameters are <strong>shared across time steps</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Unfolding Through Time</h2>
                    <div class="unfolding-viz">
                        <h4>Figure 9.1: RNN Unfolding</h4>
                        <img src="images/unfolded-rnn.svg" alt="Unfolded RNN" style="max-width: 90%; margin: 20px auto; display: block;">
                        <div class="fragment mt-lg" style="font-size: 0.8em;">
                            <p><strong>Left:</strong> Recurrent connections depicted as cyclic edges</p>
                            <p><strong>Right:</strong> RNN unfolded over time steps</p>
                            <ul style="margin-top: 10px;">
                                <li>Recurrent edges span adjacent time steps</li>
                                <li>Conventional connections computed synchronously</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Graves et al. (2008)", "url": "https://d2l.ai/chapter_references/zreferences.html#id97"}, {"text": "Sutskever et al. (2014)", "url": "https://d2l.ai/chapter_references/zreferences.html#id273"}, {"text": "Lipton et al. (2016)", "url": "https://d2l.ai/chapter_references/zreferences.html#id172"}]'>
                    <h2 class="truncate-title">RNN History and Applications</h2>
                    <div class="rnn-history">
                        <div class="fragment">
                            <h4>Origins</h4>
                            <p style="font-size: 0.85em;">RNNs originated as models of the brain in cognitive science, later adopted by machine learning</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Rise to Prominence in the 2010s</h4>
                            <ul style="font-size: 0.8em;">
                                <li><strong>Handwriting recognition</strong> (Graves et al., 2008)</li>
                                <li><strong>Machine translation</strong> (Sutskever et al., 2014)</li>
                                <li><strong>Medical diagnoses</strong> (Lipton et al., 2016)</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Current Status</h4>
                            <p style="font-size: 0.85em;">While RNNs have ceded market share to Transformers, they remain staple models for sequential modeling</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes RNN parameters \"shared\" across time steps?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Different parameters are averaged together",
                                "correct": false,
                                "explanation": "RNNs dont average different parameters; they reuse the same ones."
                            },
                            {
                                "text": "The same underlying parameters are applied at each time step",
                                "correct": true,
                                "explanation": "Correct! When RNNs are unrolled across time steps, the same weight matrices and biases are used at every step."
                            },
                            {
                                "text": "Parameters are copied and modified at each step",
                                "correct": false,
                                "explanation": "Parameters are not copied or modified; the exact same parameters are reused."
                            },
                            {
                                "text": "Each time step randomly samples from a parameter pool",
                                "correct": false,
                                "explanation": "There is no random sampling; the same fixed parameters are consistently applied."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 3: Key Insight (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 9", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html"}]'>
                    <h2 class="truncate-title">The Key Insight</h2>
                    <div class="key-insight" style="font-size: 0.65em;">
                        <div class="fragment">
                            <div class="insight-box">
                                <p>While inputs and targets for many tasks cannot be represented as fixed-length vectors, they can be represented as <strong>varying-length sequences of fixed-length vectors</strong></p>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Examples</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Documents:</strong> Sequences of words</li>
                                <li><strong>Medical records:</strong> Sequences of events (encounters, medications, procedures, lab tests, diagnoses)</li>
                                <li><strong>Videos:</strong> Varying-length sequences of still images</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>This representation enables RNNs to handle diverse sequential data types</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Sequentiality Beyond RNNs</h2>
                    <div class="beyond-rnns" style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 0.9em;">Sequentiality is not unique to RNNs:</p>
                        </div>
                        <div class="fragment mt-lg">
                            <ul style="font-size: 0.85em;">
                                <li><strong>CNNs</strong> can be adapted for varying length data (e.g., images of varying resolution)</li>
                                <li><strong>Transformer models</strong> (covered in chapter 11) have gained significant market share</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">However, RNNs rose to prominence as the <strong>default models</strong> for handling complex sequential structure in deep learning</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>This chapter is as much about the <strong>ABCs of sequence modeling</strong> as it is about RNNs</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key insight that enables RNNs to handle diverse data types?",
                        "type": "single",
                        "options": [
                            {
                                "text": "All data can be converted to images",
                                "correct": false,
                                "explanation": "Not all sequential data can or should be represented as images."
                            },
                            {
                                "text": "Complex data can be represented as varying-length sequences of fixed-length vectors",
                                "correct": true,
                                "explanation": "Correct! Documents become sequences of word vectors, videos become sequences of frame vectors, etc. The vectors are fixed-length but sequences can vary."
                            },
                            {
                                "text": "RNNs can process any data type directly",
                                "correct": false,
                                "explanation": "RNNs need data to be properly represented as sequences of vectors first."
                            },
                            {
                                "text": "All sequences must be padded to the same length",
                                "correct": false,
                                "explanation": "The key insight is that sequences can have varying lengths while vectors at each step are fixed."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 4: Focus on Language (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 9", "url": "https://d2l.ai/chapter_recurrent-neural-networks/index.html"}]'>
                    <h2 class="truncate-title">Focus on Natural Language Processing</h2>
                    <div class="nlp-focus" style="font-size: 0.65em;">
                        <div class="fragment">
                            <p style="font-size: 0.9em;">While sequence models appear in numerous applications, basic research has been driven predominantly by advances in <strong>natural language processing</strong></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Chapter Approach</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Focus exposition and examples on <strong>text data</strong></li>
                                <li>Principles apply to other data modalities</li>
                                <li>Master text examples → Apply to other domains</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>If you understand the text examples, applying models to other data should be <strong>relatively straightforward</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Chapter Roadmap</h2>
                    <div class="roadmap">
                        <h4>What's Coming Next</h4>
                        <ol style="font-size: 0.7em;">
                            <li class="fragment"><strong>Basic notation</strong> for sequences</li>
                            <li class="fragment"><strong>Evaluation measures</strong> for sequentially structured outputs</li>
                            <li class="fragment"><strong>Language models</strong> and their basic concepts</li>
                            <li class="fragment"><strong>First RNN models</strong> motivated by language modeling</li>
                            <li class="fragment"><strong>Gradient calculation</strong> through backpropagation in RNNs</li>
                            <li class="fragment"><strong>Training challenges</strong> and their solutions</li>
                            <li class="fragment"><strong>Modern RNN architectures</strong> (Chapter 10)</li>
                        </ol>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does this chapter focus primarily on text data?",
                        "type": "single",
                        "options": [
                            {
                                "text": "RNNs can only process text",
                                "correct": false,
                                "explanation": "RNNs work with any sequential data: video, audio, time series, etc."
                            },
                            {
                                "text": "Text is easier to process than other data types",
                                "correct": false,
                                "explanation": "Text processing has its own complexities; its not necessarily easier."
                            },
                            {
                                "text": "Basic research in sequence modeling has been driven by NLP advances",
                                "correct": true,
                                "explanation": "Correct! The text states that advances in natural language processing have predominantly driven basic research in sequence modeling."
                            },
                            {
                                "text": "Other data types dont have sequential structure",
                                "correct": false,
                                "explanation": "Many data types have sequential structure: video, audio, sensor data, etc."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- PART 2: WORKING WITH SEQUENCES -->
            <!-- Section 5: Working with Sequences Introduction (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Working with Sequences</h2>
                    <div class="content-container" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>From Single Vectors to Sequences</h4>
                            <p style="font-size: 0.85em;">Previously: Single feature vector $$\mathbf{x} \in \mathbb{R}^d$$</p>
                            <p style="font-size: 0.85em;">Now: Ordered list of feature vectors $$\mathbf{x}_1, \dots, \mathbf{x}_T$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Each feature vector $\mathbf{x}_t$ is indexed by <span class="tooltip">time step<span class="tooltiptext">A discrete point in the sequence, denoted as t ∈ ℤ⁺</span></span> $t \in \mathbb{Z}^+$ and lies in $\mathbb{R}^d$</p>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p>Key change: Focus shifts from individual inputs to <strong>sequences of inputs</strong></p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Types of Sequential Datasets</h2>
                    <div class="sequential-datasets">
                        <div class="fragment">
                            <h4>Single Massive Sequences</h4>
                            <p style="font-size: 0.85em;">Example: Long streams of sensor readings for climate science</p>
                            <p style="font-size: 0.85em;">Approach: Randomly sample subsequences of predetermined length</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Collections of Sequences</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Documents:</strong> Each with its own sequence of words, length $T_i$</li>
                                <li><strong>Patient records:</strong> Hospital stays with varying numbers of events</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Sequences can have <strong>different lengths</strong> within the same dataset</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Dependencies in Sequences</h2>
                    <div class="dependencies">
                        <div class="fragment">
                            <h4>Not Independent!</h4>
                            <p style="font-size: 0.85em;">While entire sequences are sampled independently, elements within a sequence are <strong>not independent</strong></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Examples of Dependencies</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Words later in a document depend on earlier words</li>
                                <li>Day 10 medication depends on days 1-9 of hospital stay</li>
                                <li>Customer preferences evolve through interactions</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Why Model as Sequences?</h4>
                            <p style="font-size: 0.85em;">Auto-fill features work because we can predict continuations better than random</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key difference between sequence data and the data we used with MLPs/CNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Sequence data is always text-based",
                                "correct": false,
                                "explanation": "Sequences can be any type of data: text, sensor readings, video frames, etc."
                            },
                            {
                                "text": "Sequence data consists of ordered lists of feature vectors with potential dependencies",
                                "correct": true,
                                "explanation": "Correct! Sequences are ordered lists of vectors x₁, ..., xₜ where elements may depend on each other, unlike fixed single vectors."
                            },
                            {
                                "text": "Sequence data must have the same length for all examples",
                                "correct": false,
                                "explanation": "Sequences can have different lengths Tᵢ in the same dataset, which is one of the challenges."
                            },
                            {
                                "text": "Sequence data elements are always independent",
                                "correct": false,
                                "explanation": "Elements in a sequence typically have dependencies - thats why we model them as sequences!"
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 6: Types of Sequence Tasks (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Types of Sequence Prediction Tasks</h2>
                    <div class="task-types">
                        <div class="fragment">
                            <h4>1. Fixed Input → Fixed Output</h4>
                            <p style="font-size: 0.85em;">Traditional ML (not sequence modeling)</p>
                        </div>
                        <div class="fragment">
                            <h4>2. Sequential Input → Fixed Output</h4>
                            <p style="font-size: 0.85em;">Example: Sentiment classification from movie review</p>
                        </div>
                        <div class="fragment">
                            <h4>3. Fixed Input → Sequential Output</h4>
                            <p style="font-size: 0.85em;">Example: Image captioning</p>
                        </div>
                        <div class="fragment">
                            <h4>4. Sequential Input → Sequential Output</h4>
                            <p style="font-size: 0.85em;">Examples: Machine translation, video captioning</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Sequence-to-Sequence Task Types</h2>
                    <div class="seq2seq-types">
                        <div class="fragment">
                            <h4>Aligned Sequences</h4>
                            <p style="font-size: 0.85em;">Input at each time step aligns with corresponding target</p>
                            <p style="font-size: 0.85em;">Example: <strong>Part-of-speech tagging</strong></p>
                            <pre style="font-size: 0.7em;">
Input:  The  cat  sat  on  the  mat
Output: DET  NOUN VERB PREP DET  NOUN</pre>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Unaligned Sequences</h4>
                            <p style="font-size: 0.85em;">No step-for-step correspondence between input and target</p>
                            <p style="font-size: 0.85em;">Example: <strong>Machine translation</strong></p>
                            <pre style="font-size: 0.7em;">
Input:  How are you doing?
Output: Hvað er að frétta af þér?</pre>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html"}]'>
                    <h2 class="truncate-title">Unsupervised Sequence Modeling</h2>
                    <div class="unsupervised">
                        <div class="fragment">
                            <h4>The Most Straightforward Problem</h4>
                            <p style="font-size: 0.85em;">Before handling targets, we start with <span class="tooltip">density modeling<span class="tooltiptext">Estimating the probability distribution of sequences without labels</span></span></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Goal</h4>
                            <p style="font-size: 0.85em;">Estimate the probability mass function:</p>
                            <p>$$p(\mathbf{x}_1, \ldots, \mathbf{x}_T)$$</p>
                            <p style="font-size: 0.85em;">This tells us how likely we are to see any given sequence</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Also called <strong>sequence modeling</strong> or <strong>language modeling</strong> (for text)</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which task would be considered an aligned sequence-to-sequence problem?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Machine translation from English to French",
                                "correct": false,
                                "explanation": "Translation is unaligned - different languages have different word orders and counts."
                            },
                            {
                                "text": "Named entity recognition (tagging each word as person/place/org/none)",
                                "correct": true,
                                "explanation": "Correct! Each input word gets exactly one corresponding tag, making it aligned."
                            },
                            {
                                "text": "Generating a summary from an article",
                                "correct": false,
                                "explanation": "Summarization is unaligned - the summary is typically much shorter than the input."
                            },
                            {
                                "text": "Image captioning",
                                "correct": false,
                                "explanation": "This is fixed input (image) to sequential output (caption), not sequence-to-sequence."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 7: Autoregressive Models (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">Autoregressive Models</h2>
                    <div class="autoregressive-intro">
                        <div class="fragment">
                            <h4>Example: Stock Market Prediction</h4>
                            <img src="images/ftse100.png" alt="FTSE 100 index" style="width: 70%; margin: 10px auto; display: block;">
                            <p style="font-size: 0.75em;">FTSE 100 index over about 30 years</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">A trader wants to predict the next price based on history</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">The Autoregressive Problem</h2>
                    <div class="ar-problem" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Goal: Estimate the Distribution</h4>
                            <p>$$P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
                            <p style="font-size: 0.85em;">Probability of next value given all previous values</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Or at least key statistics:</h4>
                            <p>$$\mathbb{E}[(x_t \mid x_{t-1}, \ldots, x_1)]$$</p>
                            <p style="font-size: 0.85em;">Expected value of next observation</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><span class="tooltip">Autoregressive<span class="tooltiptext">Models that regress a signal on its own previous values</span></span>: Predicting future values from past values of the <strong>same signal</strong></p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">Connection to Unsupervised Sequence Modeling</h2>
                    <div class="connection-to-unsupervised" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Recall: Unsupervised Sequence Modeling</h4>
                            <p style="font-size: 0.85em;">We want to estimate the probability mass function:</p>
                            <div class="equation-box">
                                $$p(\mathbf{x}_1, \ldots, \mathbf{x}_T)$$
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Chain Rule Decomposition</h4>
                            <p style="font-size: 0.85em;">Using the chain rule of probability:</p>
                            <div class="equation-box">
                                $$p(\mathbf{x}_1, \ldots, \mathbf{x}_T) = \prod_{t=1}^T p(x_t \mid x_{t-1}, \ldots, x_1)$$
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Insight:</strong> Autoregressive modeling solves each factor $p(x_t \mid x_{t-1}, \ldots, x_1)$ in this decomposition!</p>
                        </div>
                        <div class="fragment mt-md">
                            <p style="font-size: 0.85em;">Once we can predict each next token, we can model entire sequences</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">The Variable-Length Challenge</h2>
                    <div class="var-length">
                        <div class="fragment">
                            <h4>The Problem</h4>
                            <p style="font-size: 0.85em;">Number of inputs $x_{t-1}, \ldots, x_1$ varies with $t$</p>
                            <ul style="font-size: 0.85em;">
                                <li>At $t=2$: Only 1 input ($x_1$)</li>
                                <li>At $t=100$: 99 inputs ($x_1, \ldots, x_{99}$)</li>
                                <li>At $t=1000$: 999 inputs!</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Why is this problematic?</h4>
                            <p style="font-size: 0.85em;">Neural networks require <strong>fixed-size inputs</strong></p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">Two Key Strategies</h2>
                    <div class="strategies">
                        <div class="fragment">
                            <h4>Strategy 1: Fixed Window (τ-order Markov)</h4>
                            <p style="font-size: 0.85em;">Only use last $\tau$ observations: $x_{t-1}, \ldots, x_{t-\tau}$</p>
                            <ul style="font-size: 0.85em;">
                                <li>✓ Fixed input size</li>
                                <li>✓ Works with standard neural networks</li>
                                <li>✗ Ignores older history</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Strategy 2: Latent Autoregressive Models</h4>
                            <div style="display: flex; align-items: center; gap: 30px;">
                                <div style="flex: 1;">
                                    <img src="images/sequence-model.svg" alt="Latent autoregressive model" style="width: 100%; max-width: 300px;">
                                </div>
                                <div style="flex: 1;">
                                    <p style="font-size: 0.85em;">Maintain summary $h_t$ of past observations</p>
                                    <ul style="font-size: 0.85em;">
                                        <li>$\hat{x}_t = P(x_t \mid h_t)$</li>
                                        <li>$h_t = g(h_{t-1}, x_{t-1})$</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#autoregressive-models"}]'>
                    <h2 class="truncate-title">Stationarity Assumption</h2>
                    <div class="stationarity">
                        <div class="fragment">
                            <h4>What is <span class="tooltip">Stationarity<span class="tooltiptext">The assumption that the dynamics generating the data don't change over time</span></span>?</h4>
                            <p style="font-size: 0.85em;">The dynamics of sequence generation don't change over time</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">While specific values $x_t$ change, the <strong>process</strong> generating them remains constant</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Training Data Construction</h4>
                            <p style="font-size: 0.85em;">Create examples by sampling windows randomly from historical data</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Allows us to use past data to learn patterns for future prediction</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is the variable input length a problem for autoregressive models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the model too slow to train",
                                "correct": false,
                                "explanation": "Speed is not the primary issue - the fundamental problem is architectural."
                            },
                            {
                                "text": "Neural networks require fixed-size input vectors",
                                "correct": true,
                                "explanation": "Correct! Standard neural networks need a fixed number of input features, but x_{t-1},...,x_1 grows with t."
                            },
                            {
                                "text": "Longer sequences always have more noise",
                                "correct": false,
                                "explanation": "Noise is not inherently related to sequence length."
                            },
                            {
                                "text": "The model becomes too complex",
                                "correct": false,
                                "explanation": "Model complexity is fixed; the issue is handling varying numbers of inputs."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 8: Sequence Models & Language Models (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#sequence-models"}]'>
                    <h2 class="truncate-title">Sequence Models & Language Models</h2>
                    <div class="seq-models">
                        <div class="fragment">
                            <h4>Goal: Estimate Joint Probability</h4>
                            <p>$$p(x_1, \ldots, x_T)$$</p>
                            <p style="font-size: 0.85em;">How likely is this entire sequence?</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Common with Discrete Tokens</h4>
                            <p style="font-size: 0.85em;">Especially important for words in natural language</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Terminology</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Sequence models:</strong> General term</li>
                                <li><strong>Language models:</strong> When applied to text</li>
                                <li>Often used interchangeably due to NLP dominance</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#sequence-models"}]'>
                    <h2 class="truncate-title">Why Language Models Matter</h2>
                    <div class="lm-importance">
                        <div class="fragment">
                            <h4>1. Evaluate Likelihood</h4>
                            <p style="font-size: 0.85em;">Compare naturalness of candidate outputs</p>
                            <ul style="font-size: 0.8em;">
                                <li>Machine translation: Which translation is more natural?</li>
                                <li>Speech recognition: Which transcription is more likely?</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>2. Sample Sequences</h4>
                            <p style="font-size: 0.85em;">Generate new text that follows learned patterns</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>3. Optimize Sequences</h4>
                            <p style="font-size: 0.85em;">Find the most likely sequence given constraints</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#sequence-models"}]'>
                    <h2 class="truncate-title">From Joint to Conditional Probabilities</h2>
                    <div class="decomposition" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>The Chain Rule of Probability</h4>
                            <p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>What this means:</h4>
                            <ul style="font-size: 0.85em;">
                                <li><span style="color: #10099F;">$P(x_1)$</span>: Probability of first element</li>
                                <li><span style="color: #2DD2C0;">$P(x_2 \mid x_1)$</span>: Probability of second given first</li>
                                <li><span style="color: #FC8484;">$P(x_3 \mid x_2, x_1)$</span>: Probability of third given first two</li>
                                <li>... and so on</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Language modeling <strong>is</strong> autoregressive prediction!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#sequence-models"}]'>
                    <h2 class="truncate-title">Output for Discrete Sequences</h2>
                    <div class="discrete-output">
                        <div class="fragment">
                            <h4>For Words/Discrete Tokens</h4>
                            <p style="font-size: 0.85em;">Model must output <strong>full probability distribution</strong> over vocabulary</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Example: Next Word Prediction</h4>
                            <p style="font-size: 0.85em;">Given: "The cat sat on the..."</p>
                            <pre style="font-size: 0.7em;">
Output probabilities:
  "mat":    0.35
  "floor":  0.25
  "table":  0.15
  "chair":  0.10
  ...       ...</pre>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Model acts as a <strong>probabilistic classifier</strong> at each step</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "According to the chain rule decomposition, P(x₁, x₂, x₃) equals:",
                        "type": "single",
                        "options": [
                            {
                                "text": "P(x₁) + P(x₂) + P(x₃)",
                                "correct": false,
                                "explanation": "Probabilities of joint events multiply, not add."
                            },
                            {
                                "text": "P(x₁) × P(x₂) × P(x₃)",
                                "correct": false,
                                "explanation": "This assumes independence. The chain rule accounts for dependencies."
                            },
                            {
                                "text": "P(x₁) × P(x₂|x₁) × P(x₃|x₂,x₁)",
                                "correct": true,
                                "explanation": "Correct! Each term is conditioned on all previous elements in the sequence."
                            },
                            {
                                "text": "P(x₃|x₂) × P(x₂|x₁) × P(x₁)",
                                "correct": false,
                                "explanation": "Close, but x₃ should be conditioned on both x₂ AND x₁."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9: Markov Models (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">Markov Models</h2>
                    <div class="markov-intro">
                        <div class="fragment">
                            <h4>The <span class="tooltip">Markov Condition<span class="tooltiptext">The future is conditionally independent of the past, given the recent history</span></span></h4>
                            <p style="font-size: 0.85em;">Only condition on $\tau$ previous time steps</p>
                            <p style="font-size: 0.85em;">$x_{t-1}, \ldots, x_{t-\tau}$ instead of $x_{t-1}, \ldots, x_1$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Key Assumption</h4>
                            <p style="font-size: 0.85em;">The future is conditionally independent of the distant past, given recent history</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Assumption:</strong> Throwing away history beyond $\tau$ steps causes <strong>no loss</strong> in predictive power</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">Orders of Markov Models</h2>
                    <div class="markov-orders">
                        <div class="fragment">
                            <h4>First-Order Markov ($\tau = 1$)</h4>
                            <p>$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1})$</p>
                            <p style="font-size: 0.85em;">Only depends on immediate previous state</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>k-th Order Markov ($\tau = k$)</h4>
                            <p style="font-size: 0.85em;">Depends on previous $k$ states</p>
                            <p style="font-size: 0.85em;">Example (2nd order): $P(x_t \mid x_{t-1}, x_{t-2})$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Trade-off</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Higher $k$: More context, better accuracy</li>
                                <li>Lower $k$: Less computation, easier to train</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">Markov Models in Practice</h2>
                    <div class="markov-practice">
                        <div class="fragment">
                            <h4>Often Approximately True</h4>
                            <p style="font-size: 0.85em;">Real text gains information from more context, but gains <strong>diminish rapidly</strong></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Computational Benefits</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Fixed input size for neural networks</li>
                                <li>Reduced memory requirements</li>
                                <li>Faster training and inference</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Modern Models</h4>
                            <p style="font-size: 0.85em;">Even massive RNN and Transformer models seldom use more than <strong>thousands of words</strong> of context</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">Discrete Markov Models: Counting Approach</h2>
                    <div class="discrete-markov-counting" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Simple Counting Approach</h4>
                            <p style="font-size: 0.85em;">For discrete data (like words), we can estimate probabilities directly:</p>
                            <ol style="font-size: 0.85em;">
                                <li>Count occurrences of each word in each context</li>
                                <li>Compute relative frequencies</li>
                                <li>Use as estimate of $P(x_t \mid x_{t-1})$</li>
                            </ol>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Example: First-Order Model</h4>
                            <div class="equation-box">
                                $$P(\text{cat} \mid \text{the}) = \frac{\text{count}(\text{the cat})}{\text{count}(\text{the})}$$
                            </div>
                            <p style="font-size: 0.85em;">If "the" appears 1000 times and "the cat" appears 50 times, then $P(\text{cat} \mid \text{the}) = 0.05$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Challenges</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Sparsity:</strong> Many word combinations never seen in training</li>
                                <li><strong>Storage:</strong> Need to store counts for all possible contexts</li>
                                <li><strong>Smoothing:</strong> Handle zero counts with techniques like <span class="tooltip">Laplace smoothing<span class="tooltiptext">A technique that adds a small constant (usually 1) to all counts to avoid zero probabilities. For example, if "the cat" never appears, instead of P=0/1000=0, we get P=(0+1)/(1000+V) where V is vocabulary size</span></span></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">From Counting to Optimal Sequences</h2>
                    <div class="sequence-modeling-connection" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Beyond Just Counting: Finding the Best Sequence</h4>
                            <p style="font-size: 0.85em;">Discrete Markov models don't just estimate probabilities - they help us find optimal sequences</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>The Sequence Modeling Challenge</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Given:</strong> A sequence of observations (words, sounds, etc.)</li>
                                <li><strong>Find:</strong> The most likely underlying sequence of states</li>
                                <li><strong>Example:</strong> Speech recognition - audio → words</li>
                                <li><strong>Example:</strong> Part-of-speech tagging - words → grammatical tags</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>The Computational Problem</h4>
                            <p style="font-size: 0.85em;">With $|S|$ possible states and $T$ time steps, there are $|S|^T$ possible sequences!</p>
                            <div class="equation-box">
                                $$\text{Find: } \arg\max_{s_1, \ldots, s_T} P(s_1, \ldots, s_T \mid x_1, \ldots, x_T)$$
                            </div>
                            <p style="font-size: 0.85em;">We need a smarter approach than checking every possibility...</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models"}]'>
                    <h2 class="truncate-title">Dynamic Programming for Sequence Decoding</h2>
                    <div class="discrete-markov-dp" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4><span class="tooltip">Dynamic Programming<span class="tooltiptext">An optimization technique that breaks problems into overlapping subproblems</span></span></h4>
                            <p style="font-size: 0.85em;">Find the most likely sequence efficiently using the <strong>Viterbi algorithm</strong></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Key Insight: Optimal Substructure</h4>
                            <p style="font-size: 0.85em;">The optimal path to state $s_t$ only depends on optimal paths to states at $s_{t-1}$</p>
                            <div class="equation-box">
                                $$\delta_t(s) = \max_{s'} [\delta_{t-1}(s') \cdot P(s \mid s') \cdot P(x_t \mid s)]$$
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Complexity Reduction</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Brute force:</strong> $O(|S|^T)$ - exponential in sequence length</li>
                                <li><strong>Viterbi:</strong> $O(T \cdot |S|^2)$ - linear in sequence length</li>
                                <li><strong>Example:</strong> For 10 states, 100 time steps: $10^{100}$ vs $10^5$ operations!</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <div class="note-box" style="background: #10099F20; border: 1px solid #10099F; padding: 10px; border-radius: 5px;">
                                <p style="font-size: 0.85em;">This principle extends to modern neural sequence models - we can decode efficiently even with complex probability distributions</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key advantage of using a k-th order Markov assumption?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the model more accurate by using all historical data",
                                "correct": false,
                                "explanation": "Markov models explicitly limit history to k steps, not use all of it."
                            },
                            {
                                "text": "It provides fixed-size inputs while approximating full dependencies",
                                "correct": true,
                                "explanation": "Correct! By limiting context to k steps, we get fixed-size inputs for neural networks while still capturing most important dependencies."
                            },
                            {
                                "text": "It eliminates all dependencies between time steps",
                                "correct": false,
                                "explanation": "Markov models preserve dependencies up to k steps back."
                            },
                            {
                                "text": "It only works with continuous data",
                                "correct": false,
                                "explanation": "Markov models work well with both discrete and continuous data."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 10: Order of Decoding (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding"}]'>
                    <h2 class="truncate-title">The Order of Decoding</h2>
                    <div class="decoding-order">
                        <div class="fragment">
                            <h4>Left-to-Right Factorization</h4>
                            <p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Right-to-Left Factorization (Also Valid!)</h4>
                            <p>$$P(x_1, \ldots, x_T) = P(x_T) \prod_{t=T-1}^1 P(x_t \mid x_{t+1}, \ldots, x_T)$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Both are mathematically valid factorizations</p>
                            <p style="font-size: 0.85em;">So why do we prefer left-to-right?</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding"}]'>
                    <h2 class="truncate-title">Why Left-to-Right? Reason 1</h2>
                    <div class="reason1">
                        <div class="fragment">
                            <h4>Natural Human Intuition</h4>
                            <p style="font-size: 0.85em;">We read text left-to-right (in most languages)</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">We naturally predict what comes <strong>next</strong>, not what came <strong>before</strong></p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Example</h4>
                            <p style="font-size: 0.85em;">"The weather today is..."</p>
                            <p style="font-size: 0.85em;">Easy to predict: → "sunny", "cloudy", "rainy"</p>
                            <p style="font-size: 0.85em;">Harder to predict: "??? ??? ??? is beautiful" ← </p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>We have better intuitions for forward prediction</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding"}]'>
                    <h2 class="truncate-title">Why Left-to-Right? Reason 2</h2>
                    <div class="reason2">
                        <div class="fragment">
                            <h4>Extensibility to Arbitrary Lengths</h4>
                            <p style="font-size: 0.85em;">Can extend sequences naturally:</p>
                        </div>
                        <div class="fragment mt-lg" style="font-size: 0.85em;">
                            <p>$$P(x_{t+1}, x_t, \ldots, x_1) = P(x_t, \ldots, x_1) \cdot P(x_{t+1} \mid x_t, \ldots, x_1)$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Just multiply by one more conditional probability!</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Practical Benefit</h4>
                            <p style="font-size: 0.85em;">Same model handles sequences of any length</p>
                            <p style="font-size: 0.85em;">Can generate text of arbitrary length</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding"}]'>
                    <h2 class="truncate-title">Why Left-to-Right? Reason 3</h2>
                    <div class="reason3" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Causal Structure</h4>
                            <p style="font-size: 0.85em;">Future events cannot influence the past</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">If we change $x_t$:</p>
                            <ul style="font-size: 0.85em;">
                                <li>✓ Can affect $x_{t+1}, x_{t+2}, ...$ (future)</li>
                                <li>✗ Cannot affect $x_{t-1}, x_{t-2}, ...$ (past)</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Mathematical Consequence</h4>
                            <p style="font-size: 0.85em;">Sometimes: $x_{t+1} = f(x_t) + \epsilon$ (additive noise)</p>
                            <p style="font-size: 0.85em;">But the reverse is often not true!</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Forward prediction aligns with <strong>causality</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which is NOT a reason we prefer left-to-right factorization for language modeling?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It aligns with how humans naturally read and predict text",
                                "correct": false,
                                "explanation": "This IS a valid reason - we have better intuitions for forward prediction."
                            },
                            {
                                "text": "It makes the mathematics simpler than right-to-left",
                                "correct": true,
                                "explanation": "Correct! Both factorizations are equally valid mathematically. The preference is based on practical, not mathematical reasons."
                            },
                            {
                                "text": "It allows extending sequences to arbitrary lengths easily",
                                "correct": false,
                                "explanation": "This IS a valid reason - we can extend by multiplying by P(x_{t+1}|history)."
                            },
                            {
                                "text": "It aligns with causal structure of many phenomena",
                                "correct": false,
                                "explanation": "This IS a valid reason - future cannot influence past."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 11: Training with Synthetic Data (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#training"}]'>
                    <h2 class="truncate-title">Training: Synthetic Data Example</h2>
                    <div class="synthetic-intro">
                        <div class="fragment">
                            <h4>Before Text: Continuous-Valued Data</h4>
                            <p style="font-size: 0.85em;">Let's start with a simple synthetic example</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Data Generation</h4>
                            <pre><code class="python">T = 1000  # Total time steps
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.randn(T) * 0.2</code></pre>
                            <p style="font-size: 0.85em;">Sine wave with additive noise</p>
                        </div>
                        <div class="fragment mt-lg">
                            <div id="synthetic-data-viz" style="width: 100%; height: 300px;"></div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#training"}]'>
                    <h2 class="truncate-title">Creating Training Examples</h2>
                    <div class="training-examples">
                        <div class="fragment">
                            <h4>τ-th Order Markov Assumption</h4>
                            <p style="font-size: 0.85em;">Use past $\tau$ observations to predict next value</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Example with τ = 4</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Features:</strong> $\mathbf{x}_t = [x_{t-4}, x_{t-3}, x_{t-2}, x_{t-1}]$</li>
                                <li><strong>Label:</strong> $y = x_t$</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Dataset Size</h4>
                            <p style="font-size: 0.85em;">With $T = 1000$ and $\tau = 4$:</p>
                            <p style="font-size: 0.85em;">We get $1000 - 4 = 996$ examples</p>
                            <p style="font-size: 0.85em;">(Can't predict $y_1, y_2, y_3, y_4$ without history)</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#training"}]'>
                    <h2 class="truncate-title">Data Preparation Code</h2>
                    <div class="data-prep">
                        <div class="fragment">
                            <h4>Creating Features and Labels</h4>
                            <pre><code class="python">class Data(d2l.DataModule):
    def __init__(self, batch_size=16, T=1000,
                 num_train=600, tau=4):
        self.save_hyperparameters()
        self.time = torch.arange(1, T + 1, dtype=torch.float32)
        self.x = torch.sin(0.01 * self.time) + \
                 torch.randn(T) * 0.2

    def get_dataloader(self, train):
        # Create sliding windows
        features = [self.x[i : self.T-self.tau+i]
                   for i in range(self.tau)]
        self.features = torch.stack(features, 1)
        self.labels = self.x[self.tau:].reshape((-1, 1))

        # Split train/test
        i = slice(0, self.num_train) if train else \
            slice(self.num_train, None)
        return self.get_tensorloader([self.features,
                                     self.labels], train, i)</code></pre>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#training"}]'>
                    <h2 class="truncate-title">Training a Linear Model</h2>
                    <div class="training-model">
                        <div class="fragment">
                            <h4>Simple Linear Regression</h4>
                            <pre><code class="python">model = d2l.LinearRegression(lr=0.01)
trainer = d2l.Trainer(max_epochs=5)
trainer.fit(model, data)</code></pre>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>What the Model Learns</h4>
                            <p style="font-size: 0.85em;">Linear combination of past τ values:</p>
                            <p>$$\hat{x}_t = w_1 x_{t-4} + w_2 x_{t-3} + w_3 x_{t-2} + w_4 x_{t-1} + b$$</p>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Model learns weights that best predict next value</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "With T=1000 time steps and τ=4, why do we get 996 training examples?",
                        "type": "single",
                        "options": [
                            {
                                "text": "We randomly drop 4 examples for validation",
                                "correct": false,
                                "explanation": "The reduction is not random - its systematic based on the need for history."
                            },
                            {
                                "text": "We cannot create features for the first τ time steps",
                                "correct": true,
                                "explanation": "Correct! For y₁, y₂, y₃, y₄, we dont have enough previous observations (need x₋₃ to x₀ which dont exist)."
                            },
                            {
                                "text": "The last 4 time steps are reserved for testing",
                                "correct": false,
                                "explanation": "The split between train and test is separate from the τ reduction."
                            },
                            {
                                "text": "Each example uses 4 time steps, so 1000/4 ≈ 996",
                                "correct": false,
                                "explanation": "Examples use sliding windows, not non-overlapping chunks."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 12: Prediction (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">One-Step-Ahead Prediction</h2>
                    <div class="one-step" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Evaluating the Model</h4>
                            <pre><code class="python">onestep_preds = model(data.features).detach().numpy()
d2l.plot(data.time[data.tau:],
         [data.labels, onestep_preds],
         'time', 'x',
         legend=['labels', '1-step preds'])</code></pre>
                        </div>
                        <div class="fragment mt-lg">
                            <div id="onestep-viz" style="width: 100%; height: 350px;"></div>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em;">Predictions look good, even at t=1000!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">Multi-Step Prediction Challenge</h2>
                    <div class="multi-step-challenge">
                        <div class="fragment">
                            <h4>The Problem</h4>
                            <p style="font-size: 0.85em;">What if we only observed up to time 604?</p>
                            <p style="font-size: 0.85em;">Want to predict time 609 (5 steps ahead)</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>We Don't Have the Inputs!</h4>
                            <p style="font-size: 0.85em;">To predict $\hat{x}_{609}$, we need $x_{605}, x_{606}, x_{607}, x_{608}$</p>
                            <p style="font-size: 0.85em;">But we haven't observed these yet!</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Solution: Use Our Own Predictions</h4>
                            <p style="font-size: 0.85em;">Feed predictions back as inputs</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">k-Step-Ahead Prediction</h2>
                    <div class="k-step">
                        <div class="fragment">
                            <h4>The Recursive Process</h4>
                            <p style="font-size: 0.75em;">
                            $$\begin{aligned}
                            \hat{x}_{605} &= f(x_{601}, x_{602}, x_{603}, x_{604}) \\
                            \hat{x}_{606} &= f(x_{602}, x_{603}, x_{604}, \color{#FC8484}{\hat{x}_{605}}) \\
                            \hat{x}_{607} &= f(x_{603}, x_{604}, \color{#FC8484}{\hat{x}_{605}, \hat{x}_{606}}) \\
                            \hat{x}_{608} &= f(x_{604}, \color{#FC8484}{\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}}) \\
                            \hat{x}_{609} &= f(\color{#FC8484}{\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}})
                            \end{aligned}$$
                            </p>
                            <p style="font-size: 0.85em;"><span style="color: #FC8484;">Red</span> = predicted values used as input</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Each prediction depends on previous predictions!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">Multi-Step Prediction Results</h2>
                    <div class="multi-results" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Implementation</h4>
                            <pre><code class="python" style="font-size: 0.7em;">multistep_preds = torch.zeros(data.T)
multistep_preds[:] = data.x
for i in range(data.num_train + data.tau, data.T):
    multistep_preds[i] = model(
        multistep_preds[i - data.tau:i].reshape((1, -1)))</code></pre>
                        </div>
                        <div class="fragment mt-lg">
                            <div id="multistep-viz" style="width: 100%; height: 350px;"></div>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em; color: #FC8484;">Predictions decay to a constant quickly!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">Error Accumulation</h2>
                    <div class="error-accumulation">
                        <div class="fragment">
                            <h4>Why Performance Degrades</h4>
                            <p style="font-size: 0.85em;">Errors compound at each step:</p>
                        </div>
                        <div class="fragment mt-lg">
                            <ul style="font-size: 0.85em;">
                                <li>Step 1: Error $\epsilon_1 = \bar{\epsilon}$</li>
                                <li>Step 2: Error $\epsilon_2 = \bar{\epsilon} + c\epsilon_1$</li>
                                <li>Step 3: Error $\epsilon_3 = \bar{\epsilon} + c\epsilon_2$</li>
                                <li>...</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Real-World Example</h4>
                            <p style="font-size: 0.85em;">Weather forecasts:</p>
                            <ul style="font-size: 0.85em;">
                                <li>24 hours: Pretty accurate ✓</li>
                                <li>7 days: Much less accurate ✗</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#prediction"}]'>
                    <h2 class="truncate-title">k-Step Predictions Comparison</h2>
                    <div class="k-step-comparison">
                        <div class="fragment">
                            <h4>Comparing Different k Values</h4>
                            <div id="kstep-comparison-viz" style="width: 100%; height: 400px;"></div>
                        </div>
                        <div class="fragment mt-lg" style="font-size: 0.5em;">
                            <p style="font-size: 0.85em;">Quality degrades as k increases:</p>
                            <ul style="font-size: 0.85em;">
                                <li>1-step: Excellent</li>
                                <li>4-step: Still good</li>
                                <li>16-step: Getting worse</li>
                                <li>64-step: Almost useless</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do multi-step predictions degrade in quality?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The model forgets the training data",
                                "correct": false,
                                "explanation": "The model parameters dont change; the issue is in how predictions are made."
                            },
                            {
                                "text": "Prediction errors accumulate as we use predictions as inputs",
                                "correct": true,
                                "explanation": "Correct! Each prediction has some error, and when we use these imperfect predictions as inputs for the next step, errors compound exponentially."
                            },
                            {
                                "text": "The sine function becomes more complex over time",
                                "correct": false,
                                "explanation": "The underlying function remains the same throughout."
                            },
                            {
                                "text": "The model was not trained on future data",
                                "correct": false,
                                "explanation": "While true, the model was trained on the same type of data throughout the sequence."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 13: Summary & Exercises -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.1.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#summary"}]'>
                    <h2 class="truncate-title">Chapter 9.1 Summary</h2>
                    <div class="chapter-summary" style="font-size: 0.75em;">
                        <div class="fragment">
                            <h4>Key Concepts</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Sequences: Ordered lists of feature vectors</li>
                                <li>Autoregressive models: Predict from past values</li>
                                <li>Language models: Estimate sequence probabilities</li>
                                <li>Markov assumption: Limited context window</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Major Challenges</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Variable-length inputs</li>
                                <li>Error accumulation in multi-step prediction</li>
                                <li>Interpolation vs extrapolation</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Important Insights</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Respect temporal order in training</li>
                                <li>Never train on future data</li>
                                <li>Prediction quality degrades with horizon</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the fundamental difference between interpolation and extrapolation in sequence prediction?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Interpolation is faster than extrapolation",
                                "correct": false,
                                "explanation": "Speed is not the fundamental difference."
                            },
                            {
                                "text": "Interpolation fills gaps in known data, extrapolation predicts beyond it",
                                "correct": true,
                                "explanation": "Correct! Interpolation works within the range of observed data (filling gaps), while extrapolation extends beyond what weve seen, which is much harder and prone to error accumulation."
                            },
                            {
                                "text": "Interpolation uses neural networks, extrapolation uses statistics",
                                "correct": false,
                                "explanation": "Both can use either approach; the difference is in the task."
                            },
                            {
                                "text": "There is no difference - they are the same task",
                                "correct": false,
                                "explanation": "They are fundamentally different challenges with different difficulty levels."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 14: Text Processing Pipeline (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html"}]'>
                    <h2 class="truncate-title">Converting Raw Text into Sequence Data</h2>
                    <div class="text-processing-intro" style="font-size: 0.7em;">
                        <h3>The Text Processing Pipeline</h3>
                        <p style="font-size: 0.9em;">Transforming raw text into model-ready sequences</p>
                        <div class="pipeline-steps mt-lg">
                            <div class="fragment">
                                <div style="background: linear-gradient(135deg, #10099F 0%, #2DD2C0 100%); color: white; padding: 15px; border-radius: 8px; margin: 10px;">
                                    <strong>Step 1: Load Text</strong>
                                    <p style="font-size: 0.8em; margin: 5px 0;">Read raw text as strings into memory</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="background: linear-gradient(135deg, #2DD2C0 0%, #00FFBA 100%); color: #262626; padding: 15px; border-radius: 8px; margin: 10px;">
                                    <strong>Step 2: Tokenize</strong>
                                    <p style="font-size: 0.8em; margin: 5px 0;">Split strings into tokens (words/characters)</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="background: linear-gradient(135deg, #FAC55B 0%, #FFA05F 100%); color: #262626; padding: 15px; border-radius: 8px; margin: 10px;">
                                    <strong>Step 3: Build Vocabulary</strong>
                                    <p style="font-size: 0.8em; margin: 5px 0;">Map tokens to numerical indices</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div style="background: linear-gradient(135deg, #FC8484 0%, #FFA05F 100%); color: white; padding: 15px; border-radius: 8px; margin: 10px;">
                                    <strong>Step 4: Convert to Indices</strong>
                                    <p style="font-size: 0.8em; margin: 5px 0;">Transform text into numerical sequences</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the correct order of steps in the text processing pipeline?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Tokenize → Build Vocabulary → Load Text → Convert to Indices",
                                "correct": false,
                                "explanation": "Loading text must come first before any processing can occur."
                            },
                            {
                                "text": "Load Text → Tokenize → Build Vocabulary → Convert to Indices",
                                "correct": true,
                                "explanation": "Correct! We must first load text, then split it into tokens, create a vocabulary mapping, and finally convert to numerical indices."
                            },
                            {
                                "text": "Build Vocabulary → Load Text → Tokenize → Convert to Indices",
                                "correct": false,
                                "explanation": "We cannot build a vocabulary before loading and tokenizing the text."
                            },
                            {
                                "text": "Load Text → Convert to Indices → Tokenize → Build Vocabulary",
                                "correct": false,
                                "explanation": "We need to tokenize and build vocabulary before converting to indices."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 15: Reading Raw Text Data (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#reading-the-dataset"}]'>
                    <h2 class="truncate-title">Reading the Dataset</h2>
                    <div class="dataset-intro">
                        <h3>The Time Machine Dataset</h3>
                        <p style="font-size: 0.85em;">H.G. Wells' classic novel (~30,000 words)</p>
                        <div class="fragment mt-lg">
                            <h4>Implementation: TimeMachine Class</h4>
                            <pre><code class="python" style="font-size: 0.65em;">class TimeMachine(d2l.DataModule):
    """The Time Machine dataset."""
    def _download(self):
        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,
                           '090b5e7e70c295757f55df93cb0a180b9691891a')
        with open(fname) as f:
            return f.read()

data = TimeMachine()
raw_text = data._download()
raw_text[:60]
# Output: 'The Time Machine, by H. G. Wells [1898]\n\n\n\n\nI\n\n\nThe Time Tra'</code></pre>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#reading-the-dataset"}]'>
                    <h2 class="truncate-title">Text Preprocessing</h2>
                    <div class="preprocessing">
                        <h4>Cleaning the Raw Text</h4>
                        <p style="font-size: 0.85em;">Remove punctuation and convert to lowercase</p>
                        <div class="fragment mt-lg">
                            <pre><code class="python" style="font-size: 0.65em;">@d2l.add_to_class(TimeMachine)
def _preprocess(self, text):
    return re.sub('[^A-Za-z]+', ' ', text).lower()

text = data._preprocess(raw_text)
text[:60]
# Output: 'the time machine by h g wells i the time traveller for so it'</code></pre>
                        </div>
                        <div class="fragment mt-lg emphasis-box">
                            <p><strong>Regex Pattern:</strong> <code>[^A-Za-z]+</code></p>
                            <p style="font-size: 0.8em;">Replaces all non-alphabetic characters with spaces</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#reading-the-dataset"}]'>
                    <h2 class="truncate-title">Interactive Text Preprocessing Demo</h2>
                    <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                        <label style="font-size: 0.9em;">
                            Sample Text:
                            <input type="text" id="preprocess-input" value="Hello, World! 123" style="padding: 5px; width: 200px;">
                        </label>
                        <button id="preprocess-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Preprocess</button>
                    </div>
                    <div id="preprocess-output" style="margin-top: 20px; padding: 20px; background: #f5f5f5; border-radius: 8px;">
                        <h4>Original:</h4>
                        <p id="original-text" style="font-family: monospace; color: #FC8484;"></p>
                        <h4>Preprocessed:</h4>
                        <p id="processed-text" style="font-family: monospace; color: #2DD2C0;"></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does the preprocessing regex pattern [^A-Za-z]+ do?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Keeps only alphabetic characters and replaces everything else with spaces",
                                "correct": true,
                                "explanation": "Correct! The ^ inside brackets means NOT, so it matches everything that is NOT a letter."
                            },
                            {
                                "text": "Removes all alphabetic characters",
                                "correct": false,
                                "explanation": "The ^ inside brackets means NOT, so it matches non-alphabetic characters."
                            },
                            {
                                "text": "Converts text to uppercase",
                                "correct": false,
                                "explanation": "The regex replaces characters; case conversion is done by .lower()."
                            },
                            {
                                "text": "Splits text into sentences",
                                "correct": false,
                                "explanation": "This regex replaces non-alphabetic characters with spaces, not split into sentences."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 16: Tokenization Strategies (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#tokenization"}]'>
                    <h2 class="truncate-title">Tokenization</h2>
                    <div class="tokenization-intro">
                        <h3>What are <span class="tooltip">Tokens<span class="tooltiptext">The atomic (indivisible) units of text that serve as input to models</span></span>?</h3>
                        <p style="font-size: 0.85em;">Atomic units of text - the building blocks for sequences</p>
                        <div class="fragment mt-lg">
                            <h4>Example Sentence</h4>
                            <p style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px;">
                                "Baby needs a new pair of shoes"
                            </p>
                        </div>
                        <div class="fragment mt-lg">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                    <h5>Word Tokens (7 tokens)</h5>
                                    <p style="font-size: 0.75em; font-family: monospace;">["Baby", "needs", "a", "new", "pair", "of", "shoes"]</p>
                                    <p style="font-size: 0.7em; color: #666;">Large vocabulary (10K-100K+ words)</p>
                                </div>
                                <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                    <h5>Character Tokens (30 tokens)</h5>
                                    <p style="font-size: 0.75em; font-family: monospace;">['B','a','b','y',' ','n','e','e','d','s',...]</p>
                                    <p style="font-size: 0.7em; color: #666;">Small vocabulary (256 ASCII chars)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#tokenization"}]'>
                    <h2 class="truncate-title">Character-Level Tokenization</h2>
                    <div class="char-tokenization">
                        <h4>Implementation</h4>
                        <pre><code class="python" style="font-size: 0.65em;">@d2l.add_to_class(TimeMachine)
def _tokenize(self, text):
    return list(text)

tokens = data._tokenize(text)
','.join(tokens[:30])
# Output: 't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '</code></pre>
                        <div class="fragment mt-lg">
                            <h4>Trade-offs</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2DD2C0;">✓ Advantages</h5>
                                    <ul style="font-size: 0.75em;">
                                        <li>Small vocabulary size</li>
                                        <li>No unknown tokens</li>
                                        <li>Can handle any text</li>
                                    </ul>
                                </div>
                                <div style="background: #ffebee; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #FC8484;">✗ Disadvantages</h5>
                                    <ul style="font-size: 0.75em;">
                                        <li>Longer sequences</li>
                                        <li>Harder to capture word meaning</li>
                                        <li>More computation needed</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#tokenization"}]'>
                    <h2 class="truncate-title">Interactive Tokenization Comparison</h2>
                    <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                        <label style="font-size: 0.9em;">
                            Text:
                            <input type="text" id="token-input" value="The quick brown fox" style="padding: 5px; width: 200px;">
                        </label>
                        <button id="tokenize-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Tokenize</button>
                    </div>
                    <div id="tokenization-comparison" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                        <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                            <h4>Character Tokens</h4>
                            <p id="char-tokens" style="font-family: monospace; font-size: 0.8em; color: #10099F;"></p>
                            <p id="char-count" style="font-size: 0.75em; color: #666;"></p>
                        </div>
                        <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                            <h4>Word Tokens</h4>
                            <p id="word-tokens" style="font-family: monospace; font-size: 0.8em; color: #2DD2C0;"></p>
                            <p id="word-count" style="font-size: 0.75em; color: #666;"></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statement about tokenization strategies is correct?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Character tokenization results in shorter sequences than word tokenization",
                                "correct": false,
                                "explanation": "Character tokenization creates longer sequences because each character becomes a token."
                            },
                            {
                                "text": "Word tokenization requires a larger vocabulary than character tokenization",
                                "correct": true,
                                "explanation": "Correct! Word vocabularies typically have 10K-100K+ entries, while character vocabularies only need ~256 for ASCII."
                            },
                            {
                                "text": "Character tokenization cannot handle unknown words",
                                "correct": false,
                                "explanation": "Character tokenization can handle any text since all characters are in the vocabulary."
                            },
                            {
                                "text": "Word tokenization is always better than character tokenization",
                                "correct": false,
                                "explanation": "Each approach has trade-offs; the best choice depends on the specific task and constraints."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 17: Building Vocabulary (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#vocabulary"}]'>
                    <h2 class="truncate-title">Building Vocabulary</h2>
                    <div class="vocabulary-intro">
                        <h3>The Vocab Class</h3>
                        <p style="font-size: 0.85em;">Mapping tokens to numerical indices</p>
                        <div class="fragment mt-lg">
                            <h4>Key Components</h4>
                            <ul style="font-size: 0.8em;">
                                <li><strong>Token Frequencies:</strong> Count occurrences of each token</li>
                                <li><strong>Index Mapping:</strong> Assign unique indices to tokens</li>
                                <li><strong>Unknown Token:</strong> Special <code>&lt;unk&gt;</code> for unseen tokens</li>
                                <li><strong>Reserved Tokens:</strong> Special tokens for specific purposes</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg emphasis-box">
                            <p>Why numerical indices? Neural networks require numerical input!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#vocabulary"}]'>
                    <h2 class="truncate-title">Vocabulary Implementation</h2>
                    <pre><code class="python" style="font-size: 0.55em;">class Vocab:
    """Vocabulary for text."""
    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):
        # Flatten a 2D list if needed
        if tokens and isinstance(tokens[0], list):
            tokens = [token for line in tokens for token in line]

        # Count token frequencies
        counter = collections.Counter(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)

        # The list of unique tokens
        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [
            token for token, freq in self.token_freqs if freq >= min_freq])))

        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if hasattr(indices, '__len__') and len(indices) > 1:
            return [self.idx_to_token[int(index)] for index in indices]
        return self.idx_to_token[indices]

    @property
    def unk(self):  # Index for the unknown token
        return self.token_to_idx['<unk>']</code></pre>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#vocabulary"}]'>
                    <h2 class="truncate-title">Using the Vocabulary</h2>
                    <div class="vocab-usage">
                        <h4>Example: Token to Index Conversion</h4>
                        <pre><code class="python" style="font-size: 0.65em;">vocab = Vocab(tokens)
indices = vocab[tokens[:10]]
print('indices:', indices)
print('words:', vocab.to_tokens(indices))

# Output:
# indices: [21, 9, 6, 0, 21, 10, 14, 6, 0, 14]
# words: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm']</code></pre>
                        <div class="fragment mt-lg">
                            <h4>Key Features</h4>
                            <ul style="font-size: 0.8em;">
                                <li><strong>Frequency Filtering:</strong> <code>min_freq</code> parameter removes rare tokens</li>
                                <li><strong>Bidirectional Mapping:</strong> Token ↔ Index conversion</li>
                                <li><strong>Unknown Handling:</strong> Unseen tokens → <code>&lt;unk&gt;</code> index</li>
                                <li><strong>Sorted Storage:</strong> Tokens sorted by frequency for efficiency</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#vocabulary"}]'>
                    <h2 class="truncate-title">Interactive Vocabulary Builder</h2>
                    <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                        <label style="font-size: 0.9em;">
                            Text:
                            <input type="text" id="vocab-input" value="the cat sat on the mat" style="padding: 5px; width: 250px;">
                        </label>
                        <label style="font-size: 0.9em;">
                            Min Freq:
                            <input type="number" id="min-freq" value="1" min="1" style="padding: 5px; width: 60px;">
                        </label>
                        <button id="build-vocab-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Build Vocab</button>
                    </div>
                    <div id="vocab-output" style="margin-top: 20px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <h4>Token Frequencies</h4>
                                <div id="token-freqs" style="font-size: 0.75em; font-family: monospace;"></div>
                            </div>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <h4>Token → Index Mapping</h4>
                                <div id="token-mapping" style="font-size: 0.75em; font-family: monospace;"></div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the purpose of the <unk> token in vocabulary?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To mark the end of a sentence",
                                "correct": false,
                                "explanation": "The <unk> token represents unknown/unseen tokens, not sentence boundaries."
                            },
                            {
                                "text": "To represent tokens that were not seen during vocabulary construction",
                                "correct": true,
                                "explanation": "Correct! <unk> is used for out-of-vocabulary tokens encountered during inference."
                            },
                            {
                                "text": "To separate different documents",
                                "correct": false,
                                "explanation": "<unk> handles unknown tokens, not document separation."
                            },
                            {
                                "text": "To indicate the most frequent token",
                                "correct": false,
                                "explanation": "<unk> represents unknown tokens, regardless of frequency."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 18: Exploratory Language Statistics (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Exploratory Language Statistics</h2>
                    <div class="language-stats">
                        <h3>Word Frequency Analysis</h3>
                        <p style="font-size: 0.85em;">Top 10 most frequent words in <em>The Time Machine</em></p>
                        <div class="fragment mt-lg">
                            <pre><code class="python" style="font-size: 0.6em;">words = text.split()
vocab = Vocab(words)
vocab.token_freqs[:10]

# Output:
[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816),
 ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]</code></pre>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Observation: <span class="tooltip">Stop Words<span class="tooltiptext">Common words that serve syntactic roles but carry little semantic meaning (the, a, in, to, etc.)</span></span></h4>
                            <p style="font-size: 0.8em;">Most frequent words are articles, pronouns, and prepositions</p>
                            <ul style="font-size: 0.75em;">
                                <li>Not particularly descriptive</li>
                                <li>Common across all texts</li>
                                <li>Important for grammar but not unique content</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Zipf's Law</h2>
                    <div class="zipfs-law" style="font-size: 0.7em;">
                        <h3><span class="tooltip">Zipf's Law<span class="tooltiptext">A power law stating that the frequency of a word is inversely proportional to its rank in the frequency table</span></span></h3>
                        <p style="font-size: 0.85em;">The frequency of the <em>i</em>-th most frequent word:</p>
                        <div class="fragment mt-lg">
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 1.1em; text-align: center;">
                                    $$n_i \propto \frac{1}{i^\alpha}$$
                                </p>
                                <p style="font-size: 0.8em; text-align: center; color: #666;">
                                    where α is the exponent characterizing the distribution
                                </p>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Log-Log Representation</h4>
                            <p style="font-size: 0.9em; text-align: center;">
                                $$\log n_i = -\alpha \log i + c$$
                            </p>
                            <p style="font-size: 0.75em; text-align: center; color: #666;">
                                Forms a straight line on log-log plot
                            </p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Visualizing Zipf's Law</h2>
                    <div class="zipf-visualization">
                        <h4>Word Frequency Distribution</h4>
                        <img src="images/output_text-sequence_e0a8c3_110_0.svg" alt="Zipf's Law visualization" style="width: 40%; margin: 20px auto; display: block;">
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.8em;">Key observations:</p>
                            <ul style="font-size: 0.75em;">
                                <li>Frequency decays quickly: 10th word is &lt;1/5 as common as the 1st</li>
                                <li>Power law distribution holds across the range</li>
                                <li>Few very common words, many rare words</li>
                                <li>Straight line on log-log plot confirms Zipf's Law</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "According to Zipfs Law, if the most frequent word appears 1000 times, approximately how many times would the 10th most frequent word appear (assuming α = 1)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "About 100 times",
                                "correct": true,
                                "explanation": "Correct! With α = 1, frequency is proportional to 1/rank, so the 10th word appears ~1000/10 = 100 times."
                            },
                            {
                                "text": "About 500 times",
                                "correct": false,
                                "explanation": "This would be true for the 2nd most frequent word (1000/2 = 500)."
                            },
                            {
                                "text": "About 10 times",
                                "correct": false,
                                "explanation": "This underestimates the frequency; it would be 1000/10 = 100 times."
                            },
                            {
                                "text": "About 900 times",
                                "correct": false,
                                "explanation": "Frequency decays much faster; the 10th word is much less common."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 19: N-gram Analysis (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">N-gram Analysis</h2>
                    <div class="ngram-intro">
                        <h3>Beyond Single Words: <span class="tooltip">N-grams<span class="tooltiptext">Contiguous sequences of n items (words or characters) from a text</span></span></h3>
                        <div class="fragment mt-lg">
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px;">
                                <div style="background: linear-gradient(135deg, #10099F, #2DD2C0); color: white; padding: 15px; border-radius: 8px;">
                                    <h4>Unigrams</h4>
                                    <p style="font-size: 0.75em;">Single words</p>
                                    <code style="font-size: 0.7em;">["the", "cat", "sat"]</code>
                                </div>
                                <div style="background: linear-gradient(135deg, #2DD2C0, #00FFBA); color: #262626; padding: 15px; border-radius: 8px;">
                                    <h4>Bigrams</h4>
                                    <p style="font-size: 0.75em;">Two consecutive words</p>
                                    <code style="font-size: 0.7em;">["the cat", "cat sat"]</code>
                                </div>
                                <div style="background: linear-gradient(135deg, #FAC55B, #FFA05F); color: #262626; padding: 15px; border-radius: 8px;">
                                    <h4>Trigrams</h4>
                                    <p style="font-size: 0.75em;">Three consecutive words</p>
                                    <code style="font-size: 0.7em;">["the cat sat"]</code>
                                </div>
                            </div>
                        </div>
                        <div class="fragment mt-lg emphasis-box">
                            <p>N-grams capture local context and word relationships</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Bigram Frequencies</h2>
                    <div class="bigram-analysis">
                        <h4>Top 10 Bigrams</h4>
                        <pre><code class="python" style="font-size: 0.6em;">bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]
bigram_vocab = Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]

# Output:
[('of--the', 309), ('in--the', 169), ('i--had', 130), ('i--was', 112),
 ('and--the', 109), ('the--time', 102), ('it--was', 99), ('to--the', 85),
 ('as--i', 78), ('of--a', 73)]</code></pre>
                        <div class="fragment mt-lg">
                            <h4>Observation</h4>
                            <p style="font-size: 0.8em;">9 out of 10 most frequent bigrams contain only stop words!</p>
                            <p style="font-size: 0.75em; color: #10099F;">Only "the--time" is specific to the book's content</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Trigram Frequencies</h2>
                    <div class="trigram-analysis">
                        <h4>Top 10 Trigrams</h4>
                        <pre><code class="python" style="font-size: 0.6em;">trigram_tokens = ['--'.join(triple) for triple in zip(
    words[:-2], words[1:-1], words[2:])]
trigram_vocab = Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]

# Output:
[('the--time--traveller', 59), ('the--time--machine', 30),
 ('the--medical--man', 24), ('it--seemed--to', 16), ('it--was--a', 15),
 ('here--and--there', 15), ('seemed--to--me', 14), ('i--did--not', 14),
 ('i--saw--the', 13), ('i--began--to', 13)]</code></pre>
                        <div class="fragment mt-lg">
                            <h4>Better Content Capture</h4>
                            <p style="font-size: 0.8em;">Trigrams reveal more book-specific content:</p>
                            <ul style="font-size: 0.75em;">
                                <li>"the time traveller" - main character</li>
                                <li>"the time machine" - central object</li>
                                <li>"the medical man" - supporting character</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Comparing N-gram Distributions</h2>
                    <div class="ngram-comparison">
                        <h4>Frequency Distributions Follow Zipf's Law</h4>
                        <img src="images/output_text-sequence_e0a8c3_155_0.svg" alt="N-gram frequency comparison" style="width: 40%; margin: 20px auto; display: block;">
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.8em;">Key insights:</p>
                            <ul style="font-size: 0.75em;">
                                <li>All n-grams follow Zipf's law (power law distribution)</li>
                                <li>Smaller α (flatter slope) for longer n-grams</li>
                                <li>Number of distinct n-grams is limited</li>
                                <li>Many n-grams occur very rarely</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#exploratory-language-statistics"}]'>
                    <h2 class="truncate-title">Interactive N-gram Generator</h2>
                    <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                        <label style="font-size: 0.9em;">
                            Text:
                            <input type="text" id="ngram-input" value="the quick brown fox jumps over the lazy dog" style="padding: 5px; width: 300px;">
                        </label>
                        <label style="font-size: 0.9em;">
                            N:
                            <select id="ngram-n">
                                <option value="1">Unigrams</option>
                                <option value="2" selected>Bigrams</option>
                                <option value="3">Trigrams</option>
                            </select>
                        </label>
                        <button id="generate-ngrams" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Generate</button>
                    </div>
                    <div id="ngram-output" style="margin-top: 20px;">
                        <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                            <h4>Generated N-grams</h4>
                            <div id="ngram-list" style="font-family: monospace; font-size: 0.8em; color: #10099F;"></div>
                            <p id="ngram-stats" style="font-size: 0.75em; color: #666; margin-top: 10px;"></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do trigrams capture more content-specific information than unigrams?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Trigrams are always longer words",
                                "correct": false,
                                "explanation": "Trigrams are sequences of three words, not longer individual words."
                            },
                            {
                                "text": "Trigrams capture local context and word relationships",
                                "correct": true,
                                "explanation": "Correct! Trigrams preserve word order and capture phrases that are specific to the content."
                            },
                            {
                                "text": "Trigrams occur more frequently than unigrams",
                                "correct": false,
                                "explanation": "Individual words (unigrams) occur more frequently than specific three-word sequences."
                            },
                            {
                                "text": "Trigrams have smaller vocabulary size",
                                "correct": false,
                                "explanation": "The number of possible trigrams is much larger than the number of unique words."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 20: Complete Implementation (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#putting-it-all-together"}]'>
                    <h2 class="truncate-title">Putting It All Together</h2>
                    <div class="complete-implementation" style="font-size: 0.7em;">
                        <h3>Complete Text Processing Pipeline</h3>
                        <p style="font-size: 0.85em;">The <code>build</code> method combines all components</p>
                        <div class="fragment mt-lg">
                            <pre><code class="python" style="font-size: 0.6em;">@d2l.add_to_class(TimeMachine)
def build(self, raw_text, vocab=None):
    tokens = self._tokenize(self._preprocess(raw_text))
    if vocab is None:
        vocab = Vocab(tokens)
    corpus = [vocab[token] for token in tokens]
    return corpus, vocab

corpus, vocab = data.build(raw_text)
len(corpus), len(vocab)
# Output: (170580, 28)</code></pre>
                        </div>
                        <div class="fragment mt-lg" style="font-size: 0.75em;">
                            <h4>Pipeline Flow</h4>
                            <ol style="font-size: 0.8em;">
                                <li><strong>Preprocess:</strong> Clean and normalize text</li>
                                <li><strong>Tokenize:</strong> Split into character tokens</li>
                                <li><strong>Build Vocab:</strong> Create token-to-index mapping</li>
                                <li><strong>Convert:</strong> Transform tokens to indices</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#putting-it-all-together"}]'>
                    <h2 class="truncate-title">Key Implementation Details</h2>
                    <div class="implementation-details" style="font-size: 0.65em;">
                        <h4>Design Choices</h4>
                        <ul style="font-size: 0.8em;">
                            <li><strong>Character-level tokenization:</strong> Simplifies training</li>
                            <li><strong>Single list corpus:</strong> Not split by sentences/paragraphs</li>
                            <li><strong>Vocabulary size:</strong> 28 unique characters (26 letters + space + &lt;unk&gt;)</li>
                            <li><strong>Corpus length:</strong> 170,580 character tokens</li>
                        </ul>
                        <div class="fragment mt-lg emphasis-box">
                            <p><strong>Result:</strong> A sequence of 170,580 numerical indices representing the entire book</p>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Ready for Model Input</h4>
                            <p style="font-size: 0.8em;">The corpus can now be:</p>
                            <ul style="font-size: 0.75em;">
                                <li>Split into training/validation sets</li>
                                <li>Batched for efficient processing</li>
                                <li>Fed into neural network models</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the complete pipeline, why is character-level tokenization chosen over word-level?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It produces shorter sequences",
                                "correct": false,
                                "explanation": "Character-level tokenization actually produces longer sequences than word-level."
                            },
                            {
                                "text": "It simplifies training with a small, fixed vocabulary",
                                "correct": true,
                                "explanation": "Correct! Character-level uses only ~28 tokens vs thousands for word-level, simplifying the model."
                            },
                            {
                                "text": "It captures meaning better",
                                "correct": false,
                                "explanation": "Word-level tokenization typically captures semantic meaning better than character-level."
                            },
                            {
                                "text": "It requires less memory",
                                "correct": false,
                                "explanation": "Character sequences are longer, potentially requiring more memory for processing."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 21: Summary and Exercises -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.2.6", "url": "https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html#summary"}]'>
                    <h2 class="truncate-title">Chapter 9.2 Summary</h2>
                    <div class="chapter-summary" style="font-size: 0.5em;">
                        <h3>Key Takeaways</h3>
                        <div class="fragment">
                            <h4>Text Processing Pipeline</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Load → Tokenize → Build Vocabulary → Convert to Indices</li>
                                <li>Characters vs words as tokens: trade-offs in vocabulary size and sequence length</li>
                                <li>Unknown tokens handled with special &lt;unk&gt; symbol</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <h4>Language Statistics</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Word frequencies follow <strong>Zipf's Law</strong>: $$n_i \propto \frac{1}{i^\alpha}$$</li>
                                <li>Stop words dominate frequency lists</li>
                                <li>N-grams also follow power law distributions</li>
                                <li>Longer n-grams capture more content-specific patterns</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg emphasis-box">
                            <p>Understanding text statistics is crucial for effective NLP model design</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test your understanding</h2>
                    <div data-mcq='{
                        "question": "Which statement best describes the relationship between Zipfs Law and n-gram distributions?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Only unigrams follow Zipfs Law; n-grams do not",
                                "correct": false,
                                "explanation": "All n-grams (unigrams, bigrams, trigrams) follow Zipfs Law with different exponents."
                            },
                            {
                                "text": "All n-grams follow Zipfs Law, with flatter distributions for longer n-grams",
                                "correct": true,
                                "explanation": "Correct! Longer n-grams have smaller α values, resulting in flatter power law distributions."
                            },
                            {
                                "text": "N-grams follow a normal distribution, not Zipfs Law",
                                "correct": false,
                                "explanation": "N-gram frequencies follow power law (Zipfs) distributions, not normal distributions."
                            },
                            {
                                "text": "The distribution type depends on the text genre",
                                "correct": false,
                                "explanation": "Zipfs Law is remarkably consistent across different texts and languages."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 22: Introduction to Language Models (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html"}]'>
                    <h2 class="truncate-title">Language Models</h2>
                    <p>Estimating the joint probability of text sequences</p>
                    <div class="emphasis-box" style="font-size: 0.75em;">
                        <p>Goal: Model the probability of observing a sequence of tokens</p>
                        <p>$$P(x_1, x_2, \ldots, x_T)$$</p>
                    </div>
                    <p class="fragment">Essential for:</p>
                    <ul class="fragment" style="font-size: 0.75em;">
                        <li>Text generation</li>
                        <li>Speech recognition</li>
                        <li>Machine translation</li>
                        <li>Text completion</li>
                    </ul>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html"}]'>
                    <h3 class="truncate-title">Joint Probability Decomposition</h3>
                    <p>Using the chain rule of probability:</p>
                    <div class="math-box">
                        $$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, \ldots, x_{t-1})$$
                    </div>
                    <p class="fragment">Example with four words:</p>
                    <div class="fragment math-box">
                        $$\begin{aligned}
                        &P(\text{deep}, \text{learning}, \text{is}, \text{fun}) \\
                        =&P(\text{deep}) \cdot P(\text{learning} \mid \text{deep}) \\
                        &\cdot P(\text{is} \mid \text{deep}, \text{learning}) \\
                        &\cdot P(\text{fun} \mid \text{deep}, \text{learning}, \text{is})
                        \end{aligned}$$
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html"}]'>
                    <h3 class="truncate-title">Applications of Language Models</h3>
                    <div class="comparison-grid" style="font-size: 0.7em;">
                        <div>
                            <h4>Speech Recognition</h4>
                            <p>Disambiguate similar-sounding phrases:</p>
                            <ul>
                                <li>"to recognize speech" ✓</li>
                                <li>"to wreck a nice beach" ✗</li>
                            </ul>
                        </div>
                        <div>
                            <h4>Text Generation</h4>
                            <p>Generate coherent text by sampling:</p>
                            <p>$$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="font-size: 0.7em;">
                        <p>Language models help distinguish between grammatically correct but semantically unlikely sequences</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html"}]'>
                    <h3 class="truncate-title">Challenge: Conditional Complexity</h3>
                    <p>The conditional probability becomes increasingly complex:</p>
                    <div class="math-box">
                        $$P(x_t \mid x_1, \ldots, x_{t-1})$$
                    </div>
                    <p class="fragment">Problems with long contexts:</p>
                    <ul class="fragment">
                        <li>Exponentially growing parameter space</li>
                        <li>Sparse data for long sequences</li>
                        <li>Computational complexity</li>
                    </ul>
                    <p class="fragment">Solution: <span class="tooltip">Markov assumptions<span class="tooltiptext">Assumption that the future depends only on a limited history, not the entire past</span></span></p>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "What is the primary goal of language models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To translate between languages",
                                "correct": false,
                                "explanation": "Translation is an application, but not the primary goal."
                            },
                            {
                                "text": "To estimate the joint probability of token sequences",
                                "correct": true,
                                "explanation": "Correct! Language models estimate P(x₁, x₂, ..., xₜ) for sequences."
                            },
                            {
                                "text": "To compress text data",
                                "correct": false,
                                "explanation": "Compression is a byproduct, not the primary goal."
                            },
                            {
                                "text": "To identify grammatical errors",
                                "correct": false,
                                "explanation": "Grammar checking is an application, not the core goal."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 23: Markov Models and N-grams (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#markov-models-and-n-grams"}]'>
                    <h2 class="truncate-title">Markov Models and N-grams</h2>
                    <p>Simplifying assumptions for tractable language modeling</p>
                    <div class="emphasis-box">
                        <p><span class="tooltip">Markov Property<span class="tooltiptext">The future depends only on the current state, not the entire history</span></span>: Limited context dependency</p>
                    </div>
                    <p class="fragment">Different approximation orders:</p>
                    <ul class="fragment">
                        <li><strong>Unigram:</strong> No dependencies</li>
                        <li><strong>Bigram:</strong> Depends on previous word</li>
                        <li><strong>Trigram:</strong> Depends on two previous words</li>
                        <li><strong>N-gram:</strong> Depends on n-1 previous words</li>
                    </ul>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#markov-models-and-n-grams"}]'>
                    <h3 class="truncate-title">N-gram Model Formulations</h3>
                    <p>Different orders of approximation:</p>

                    <div class="math-box" style="font-size: 0.75em;">
                        <p><strong>Unigram (Independence):</strong></p>
                        $$P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2) P(x_3) P(x_4)$$
                    </div>

                    <div class="fragment math-box" style="font-size: 0.75em;">
                        <p><strong>Bigram (First-order Markov):</strong></p>
                        $$P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2 \mid x_1) P(x_3 \mid x_2) P(x_4 \mid x_3)$$
                    </div>

                    <div class="fragment math-box" style="font-size: 0.75em;">
                        <p><strong>Trigram (Second-order Markov):</strong></p>
                        $$P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2 \mid x_1) P(x_3 \mid x_1, x_2) P(x_4 \mid x_2, x_3)$$
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#markov-models-and-n-grams"}]'>
                    <h3 class="truncate-title">Interactive N-gram Generator</h3>
                    <p>Explore how different n-gram models capture text patterns</p>

                    <div class="demo-controls" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px;">
                        <div style="margin-bottom: 10px;">
                            <label>Input Text:</label><br>
                            <input type="text" id="ngram-model-input" value="the quick brown fox jumps over the lazy dog" style="width: 100%; padding: 5px;">
                        </div>
                        <div style="display: flex; gap: 10px; align-items: center;">
                            <label>N-gram Size:
                                <select id="ngram-model-size">
                                    <option value="1">Unigram</option>
                                    <option value="2" selected>Bigram</option>
                                    <option value="3">Trigram</option>
                                </select>
                            </label>
                            <button id="generate-ngram-model" style="padding: 8px 16px; background: #10099F; color: white; border: none; border-radius: 5px;">Generate</button>
                        </div>
                    </div>

                    <div id="ngram-model-output" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                        <div>
                            <h4>N-grams Generated:</h4>
                            <div id="ngram-model-list" style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px; max-height: 200px; overflow-y: auto;"></div>
                        </div>
                        <div>
                            <h4>Frequency Count:</h4>
                            <div id="ngram-model-freq" style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px; max-height: 200px; overflow-y: auto;"></div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#markov-models-and-n-grams"}]'>
                    <h3 class="truncate-title">Trade-offs in N-gram Models</h3>
                    <div class="comparison-grid">
                        <div>
                            <h4>Lower-order (Unigram, Bigram)</h4>
                            <ul>
                                <li>✓ More training data per pattern</li>
                                <li>✓ Better generalization</li>
                                <li>✗ Less context awareness</li>
                                <li>✗ Poor long-range dependencies</li>
                            </ul>
                        </div>
                        <div>
                            <h4>Higher-order (Trigram+)</h4>
                            <ul>
                                <li>✓ Captures more context</li>
                                <li>✓ Better local coherence</li>
                                <li>✗ Data sparsity issues</li>
                                <li>✗ Exponentially more parameters</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "Which n-gram model assumes complete independence between words?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Bigram",
                                "correct": false,
                                "explanation": "Bigram models depend on the previous word."
                            },
                            {
                                "text": "Unigram",
                                "correct": true,
                                "explanation": "Correct! Unigram models assume P(x₁,x₂,x₃) = P(x₁)P(x₂)P(x₃)."
                            },
                            {
                                "text": "Trigram",
                                "correct": false,
                                "explanation": "Trigram models depend on two previous words."
                            },
                            {
                                "text": "All n-gram models",
                                "correct": false,
                                "explanation": "Only unigram models assume complete independence."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 24: Word Frequency and Challenges (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#word-frequency"}]'>
                    <h2 class="truncate-title">Word Frequency Estimation</h2>
                    <p>Computing language model parameters from data</p>
                    <div class="math-box">
                        <p>Estimate conditional probability:</p>
                        $$\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})}$$
                    </div>
                    <p class="fragment">Where:</p>
                    <ul class="fragment">
                        <li>$n(x)$ = count of word $x$</li>
                        <li>$n(x, x')$ = count of word pair $(x, x')$</li>
                    </ul>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#word-frequency"}]'>
                    <h3 class="truncate-title">The Sparsity Problem</h3>
                    <p>Challenges with frequency-based estimation:</p>

                    <div class="emphasis-box" style="font-size: 0.7em;">
                        <p>As n-gram order increases, most combinations never appear in training data</p>
                    </div>

                    <div class="fragment" style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-top: 20px;">
                        <div style="text-align: center; padding: 10px; background: #f5f5f5; border-radius: 5px;">
                            <h4>Unigram</h4>
                            <p style="font-size: 2em; color: #10099F;">10K</p>
                            <p>unique patterns</p>
                        </div>
                        <div style="text-align: center; padding: 10px; background: #f5f5f5; border-radius: 5px;">
                            <h4>Bigram</h4>
                            <p style="font-size: 2em; color: #2DD2C0;">100M</p>
                            <p>possible patterns</p>
                        </div>
                        <div style="text-align: center; padding: 10px; background: #f5f5f5; border-radius: 5px;">
                            <h4>Trigram</h4>
                            <p style="font-size: 2em; color: #FC8484;">1T</p>
                            <p>possible patterns</p>
                        </div>
                    </div>

                    <p class="fragment">Most trigrams will have zero counts!</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#word-frequency"}]'>
                    <h3 class="truncate-title">Interactive Frequency Counter</h3>
                    <p>Observe how word frequency changes with context</p>

                    <div class="demo-controls" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px;">
                        <div style="margin-bottom: 10px;">
                            <label>Sample Text:</label><br>
                            <textarea id="freq-counter-input" style="width: 100%; height: 60px; padding: 5px;">The cat sat on the mat. The cat was happy. The dog sat on the floor. The dog was tired.</textarea>
                        </div>
                        <button id="compute-frequencies" style="padding: 8px 16px; background: #10099F; color: white; border: none; border-radius: 5px;">Compute Frequencies</button>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; font-size: 0.4em;">
                        <div>
                            <h4>Unigram Counts</h4>
                            <div id="unigram-counts" style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px; height: 150px; overflow-y: auto; font-size: 0.9em;"></div>
                        </div>
                        <div>
                            <h4>Bigram Counts</h4>
                            <div id="bigram-counts" style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 5px; height: 150px; overflow-y: auto; font-size: 0.9em;"></div>
                        </div>
                        <div>
                            <h4>Statistics</h4>
                            <div id="freq-statistics" style="background: #f5f5f5; padding: 10px; border-radius: 5px; height: 150px;">
                                <p>Total words: <span id="total-words">0</span></p>
                                <p>Unique unigrams: <span id="unique-unigrams">0</span></p>
                                <p>Unique bigrams: <span id="unique-bigrams">0</span></p>
                                <p>Sparsity ratio: <span id="sparsity-ratio">0%</span></p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#word-frequency"}]'>
                    <h3 class="truncate-title">Zero Probability Problem</h3>
                    <p>What happens with unseen n-grams?</p>

                    <div class="math-box" style="font-size: 0.75em;">
                        <p>If $n(\text{word}_1, \text{word}_2) = 0$:</p>
                        $$P(\text{word}_2 \mid \text{word}_1) = \frac{0}{n(\text{word}_1)} = 0$$
                    </div>

                    <div class="fragment emphasis-box" style="background: #fff3cd; border-left: 4px solid #FAC55B; font-size: 0.7em;">
                        <p>⚠️ Zero probability for any word makes the entire sequence probability zero!</p>
                    </div>

                    <p class="fragment">This leads to:</p>
                    <ul class="fragment" style="font-size: 0.75em;">
                        <li>Unable to handle novel but valid phrases</li>
                        <li>Poor generalization to new text</li>
                        <li>Need for smoothing techniques</li>
                    </ul>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "What is the main challenge with using raw frequency counts for language modeling?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Counting takes too much time",
                                "correct": false,
                                "explanation": "Computational efficiency is not the main challenge."
                            },
                            {
                                "text": "Many valid n-grams have zero counts in training data",
                                "correct": true,
                                "explanation": "Correct! Data sparsity means many valid combinations never appear in training."
                            },
                            {
                                "text": "Frequencies are always the same",
                                "correct": false,
                                "explanation": "Frequencies vary significantly across different n-grams."
                            },
                            {
                                "text": "Words appear too frequently",
                                "correct": false,
                                "explanation": "High frequency is not a problem; zero frequency is."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 25: Laplace Smoothing (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#laplace-smoothing"}]'>
                    <h2 class="truncate-title">Laplace Smoothing</h2>
                    <p>Addressing the zero probability problem</p>
                    <div class="emphasis-box">
                        <p><span class="tooltip">Laplace Smoothing<span class="tooltiptext">Add a small constant to all counts to avoid zero probabilities</span></span>: Add pseudocounts to prevent zeros</p>
                    </div>
                    <p class="fragment">Key idea: Pretend we saw every possible n-gram at least $\epsilon$ times</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#laplace-smoothing"}]'>
                    <h3 class="truncate-title">Smoothing Formulas</h3>
                    <p>Adding pseudocounts at different levels:</p>

                    <div class="math-box" style="font-size: 0.75em;">
                        <p><strong>Unigram smoothing:</strong></p>
                        $$\hat{P}(x) = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}$$
                    </div>

                    <div class="fragment math-box" style="font-size: 0.75em;">
                        <p><strong>Bigram smoothing:</strong></p>
                        $$\hat{P}(x' \mid x) = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}$$
                    </div>

                    <div class="fragment math-box" style="font-size: 0.75em;">
                        <p><strong>Trigram smoothing:</strong></p>
                        $$\hat{P}(x'' \mid x,x') = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}$$
                    </div>

                    <p class="fragment">Where: $n$ = total words, $m$ = vocabulary size</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#laplace-smoothing"}]'>
                    <h3 class="truncate-title">Interactive Smoothing Demo</h3>
                    <p>See how smoothing parameters affect probability estimates</p>

                    <div class="demo-controls" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px; font-size: 0.5em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div>
                                <label>Word count (n): <span id="word-count-value">100</span></label><br>
                                <input type="range" id="word-count-slider" min="10" max="1000" value="100" style="width: 100%;">
                            </div>
                            <div>
                                <label>Smoothing (ε): <span id="epsilon-value">1.0</span></label><br>
                                <input type="range" id="epsilon-slider" min="0" max="10" step="0.1" value="1" style="width: 100%;">
                            </div>
                        </div>
                        <div style="margin-top: 10px;">
                            <label>Vocabulary size (m): <span id="vocab-size-value">1000</span></label><br>
                            <input type="range" id="vocab-size-slider" min="100" max="10000" value="1000" style="width: 100%;">
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                        <div>
                            <h4>Unsmoothed Probability</h4>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 5px; text-align: center;">
                                <p>Seen word (n=10):</p>
                                <p style="font-size: 1.5em; color: #10099F;" id="unsmoothed-seen">0.100</p>
                                <p>Unseen word (n=0):</p>
                                <p style="font-size: 1.5em; color: #FC8484;" id="unsmoothed-unseen">0.000</p>
                            </div>
                        </div>
                        <div>
                            <h4>Smoothed Probability</h4>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 5px; text-align: center;">
                                <p>Seen word (n=10):</p>
                                <p style="font-size: 1.5em; color: #10099F;" id="smoothed-seen">0.099</p>
                                <p>Unseen word (n=0):</p>
                                <p style="font-size: 1.5em; color: #2DD2C0;" id="smoothed-unseen">0.001</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.1.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#laplace-smoothing"}]'>
                    <h3 class="truncate-title">Limitations of Laplace Smoothing</h3>
                    <p>Why we need better approaches:</p>

                    <div class="comparison-grid" style="font-size: 0.6em;">
                        <div>
                            <h4>Problems</h4>
                            <ul>
                                <li>Assigns too much probability to unseen events</li>
                                <li>Doesn't consider word similarity</li>
                                <li>Ignores semantic relationships</li>
                                <li>Still requires storing all counts</li>
                            </ul>
                        </div>
                        <div>
                            <h4>Neural Solutions</h4>
                            <ul>
                                <li>Learn distributed representations</li>
                                <li>Share parameters across similar contexts</li>
                                <li>Capture semantic similarities</li>
                                <li>Generalize to novel combinations</li>
                            </ul>
                        </div>
                    </div>

                    <div class="fragment emphasis-box" style="font-size: 0.7em;">
                        <p>This motivates using neural networks for language modeling!</p>
                    </div>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "What happens to P(x) as ε approaches infinity in Laplace smoothing?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Approaches 0",
                                "correct": false,
                                "explanation": "As ε increases, probabilities become more uniform, not zero."
                            },
                            {
                                "text": "Approaches 1",
                                "correct": false,
                                "explanation": "Individual probabilities dont approach 1; they spread uniformly."
                            },
                            {
                                "text": "Approaches uniform distribution 1/m",
                                "correct": true,
                                "explanation": "Correct! As ε→∞, all words get equal probability 1/m."
                            },
                            {
                                "text": "Remains unchanged",
                                "correct": false,
                                "explanation": "The smoothing parameter ε directly affects the probability."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 26: Perplexity (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity"}]'>
                    <h2 class="truncate-title">Perplexity</h2>
                    <p>Measuring language model quality</p>
                    <div class="emphasis-box">
                        <p><span class="tooltip">Perplexity<span class="tooltiptext">Exponential of the average cross-entropy loss; measures how surprised the model is by the test data</span></span>: How well can the model predict the next token?</p>
                    </div>
                    <p class="fragment">Consider these continuations of "It is raining":</p>
                    <ol class="fragment">
                        <li>✓ "It is raining outside"</li>
                        <li>? "It is raining banana tree"</li>
                        <li>✗ "It is raining piouw;kcj pwepoiut"</li>
                    </ol>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity"}]'>
                    <h3 class="truncate-title">Mathematical Definition</h3>
                    <p>From cross-entropy to perplexity:</p>

                    <div class="math-box">
                        <p><strong>Average cross-entropy loss:</strong></p>
                        $$\mathcal{L} = \frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)$$
                    </div>

                    <div class="fragment math-box">
                        <p><strong>Perplexity:</strong></p>
                        $$\text{PPL} = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)$$
                    </div>

                    <p class="fragment">Or simply: $\text{PPL} = \exp(\mathcal{L})$</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity"}]'>
                    <h3 class="truncate-title">Interpreting Perplexity</h3>
                    <p>Perplexity as the effective number of choices:</p>

                    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0; font-size: 0.7em;">
                        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #10099F, #2DD2C0); color: white; border-radius: 10px;">
                            <h4>Best Case</h4>
                            <p style="font-size: 2.5em; margin: 10px 0;">PPL = 1</p>
                            <p>Perfect prediction</p>
                            <p style="font-size: 0.9em;">P(correct) = 1.0</p>
                        </div>
                        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #FAC55B, #FFA05F); color: white; border-radius: 10px;">
                            <h4>Baseline</h4>
                            <p style="font-size: 2.5em; margin: 10px 0;">PPL = |V|</p>
                            <p>Uniform distribution</p>
                            <p style="font-size: 0.9em;">Random guessing</p>
                        </div>
                        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #FC8484, #FF6B6B); color: white; border-radius: 10px;">
                            <h4>Worst Case</h4>
                            <p style="font-size: 2.5em; margin: 10px 0;">PPL = ∞</p>
                            <p>Always wrong</p>
                            <p style="font-size: 0.9em;">P(correct) = 0</p>
                        </div>
                    </div>

                    <p class="fragment">Lower perplexity = Better model</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity"}]'>
                    <h3 class="truncate-title">Interactive Perplexity Calculator</h3>
                    <p>See how prediction accuracy affects perplexity</p>

                    <div class="demo-controls" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px;">
                        <div>
                            <label>Sequence: "the cat sat on the"</label>
                        </div>
                        <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 10px; margin-top: 10px;">
                            <div>
                                <label>"the": <span id="prob1-value">0.10</span></label><br>
                                <input type="range" id="prob1" min="0.01" max="1" step="0.01" value="0.10" style="width: 100%;">
                            </div>
                            <div>
                                <label>"cat": <span id="prob2-value">0.05</span></label><br>
                                <input type="range" id="prob2" min="0.01" max="1" step="0.01" value="0.05" style="width: 100%;">
                            </div>
                            <div>
                                <label>"sat": <span id="prob3-value">0.08</span></label><br>
                                <input type="range" id="prob3" min="0.01" max="1" step="0.01" value="0.08" style="width: 100%;">
                            </div>
                            <div>
                                <label>"on": <span id="prob4-value">0.15</span></label><br>
                                <input type="range" id="prob4" min="0.01" max="1" step="0.01" value="0.15" style="width: 100%;">
                            </div>
                            <div>
                                <label>"the": <span id="prob5-value">0.20</span></label><br>
                                <input type="range" id="prob5" min="0.01" max="1" step="0.01" value="0.20" style="width: 100%;">
                            </div>
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                        <div style="background: #f5f5f5; padding: 15px; border-radius: 5px;">
                            <h4>Cross-Entropy Loss</h4>
                            <p style="font-size: 1.2em;">Individual losses:</p>
                            <div id="individual-losses" style="font-family: monospace; font-size: 0.9em;"></div>
                            <p style="margin-top: 10px;">Average: <span id="avg-loss" style="font-size: 1.5em; color: #10099F;">2.30</span></p>
                        </div>
                        <div style="background: #f5f5f5; padding: 15px; border-radius: 5px;">
                            <h4>Perplexity</h4>
                            <p style="text-align: center; margin-top: 30px;">
                                <span style="font-size: 3em; color: #2DD2C0;" id="perplexity-value">10.0</span>
                            </p>
                            <p style="text-align: center;">Effective choices per position</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity"}]'>
                    <h3 class="truncate-title">Perplexity in Practice</h3>
                    <p>Typical perplexity values for different models:</p>

                    <div id="perplexity-comparison" style="height: 300px; margin: 20px 0;"></div>

                    <div class="fragment emphasis-box" style="font-size: 0.75em;">
                        <p>Key insights:</p>
                        <ul>
                            <li>Perplexity is comparable across different text lengths</li>
                            <li>Lower values indicate better compression and prediction</li>
                            <li>Domain-specific models achieve lower perplexity on their domain</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "A language model has perplexity of 50 on a test set. What does this mean?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The model is 50% accurate",
                                "correct": false,
                                "explanation": "Perplexity is not a percentage accuracy measure."
                            },
                            {
                                "text": "On average, the model is as confused as if choosing uniformly from 50 options",
                                "correct": true,
                                "explanation": "Correct! Perplexity represents the effective number of equally likely choices."
                            },
                            {
                                "text": "The model makes 50 errors",
                                "correct": false,
                                "explanation": "Perplexity is not an error count."
                            },
                            {
                                "text": "The vocabulary size is 50",
                                "correct": false,
                                "explanation": "Perplexity can be different from vocabulary size."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 27: Partitioning Sequences (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h2 class="truncate-title">Partitioning Sequences</h2>
                    <p>Preparing data for language model training</p> 
                    <div class="emphasis-box">
                        <p>Goal: Create overlapping subsequences of fixed length for efficient training</p>
                    </div>
                    <p class="fragment">Key concepts:</p>
                    <ul class="fragment">
                        <li>Fixed sequence length $n$</li>
                        <li>Random offset $d \in [0, n)$ for each epoch</li>
                        <li>Target is input shifted by one token</li>
                    </ul>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h3 class="truncate-title">Partitioning Mathematics</h3>
                    <p>Given corpus of length $T$ and subsequence length $n$:</p>

                    <div class="math-box" style="font-size: 0.7em;">
                        <p>1. Random offset: $d \sim \text{Uniform}(0, n-1)$</p>
                        <p>2. Number of subsequences: $m = \lfloor (T-d)/n \rfloor$</p>
                        <p>3. Subsequences: $\mathbf{x}_d, \mathbf{x}_{d+n}, \ldots, \mathbf{x}_{d+n(m-1)}$</p>
                    </div>

                    <div class="fragment" style="font-size: 0.7em;">
                        <p>Where each subsequence:</p>
                        <div class="math-box">
                            $$\mathbf{x}_t = [x_t, x_{t+1}, \ldots, x_{t+n-1}]$$
                        </div>
                    </div>

                    <div class="fragment" style="font-size: 0.7em;">
                        <p>Input-target pairs:</p>
                        <div class="math-box">
                            <p>Input: $\mathbf{x}_t = [x_t, \ldots, x_{t+n-1}]$</p>
                            <p>Target: $\mathbf{x}_{t+1} = [x_{t+1}, \ldots, x_{t+n}]$</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h3 class="truncate-title">Visualization of Sequence Partitioning</h3>
                    <p>Example with sequence length n=5 and offset d=2</p>

                    <div style="text-align: center; margin: 20px 0;">
                        <img src="images/lang-model-data.svg" alt="Sequence Partitioning Diagram" style="max-width: 90%; height: auto;">
                    </div>

                    <p>Each input sequence predicts the next token at each position</p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h3 class="truncate-title">Interactive Sequence Partitioner</h3>
                    <p>Explore how sequences are partitioned for training</p>

                    <div class="demo-controls" style="margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px; font-size: 0.5em;">
                        <div style="margin-bottom: 10px;">
                            <label>Text corpus:</label><br>
                            <input type="text" id="partition-text" value="abcdefghijklmnopqrstuvwxyz" style="width: 100%; padding: 5px; font-family: monospace;">
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px;">
                            <div>
                                <label>Sequence length (n): <span id="seq-length-value">5</span></label><br>
                                <input type="range" id="seq-length" min="2" max="10" value="5" style="width: 100%;">
                            </div>
                            <div>
                                <label>Offset (d): <span id="offset-value">2</span></label><br>
                                <input type="range" id="offset" min="0" max="4" value="2" style="width: 100%;">
                            </div>
                            <div style="display: flex; align-items: flex-end;">
                                <button id="partition-btn" style="width: 100%; padding: 8px; background: #10099F; color: white; border: none; border-radius: 5px;">Partition</button>
                            </div>
                        </div>
                    </div>

                    <div style="background: #f5f5f5; padding: 15px; border-radius: 5px;">
                        <h4>Generated Sequences:</h4>
                        <div id="partition-output" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div>
                                <h5>Input Sequences:</h5>
                                <div id="input-sequences" style="font-family: monospace; white-space: pre-line;"></div>
                            </div>
                            <div>
                                <h5>Target Sequences:</h5>
                                <div id="target-sequences" style="font-family: monospace; white-space: pre-line;"></div>
                            </div>
                        </div>
                        <p style="margin-top: 10px;">Number of sequences: <span id="num-sequences">0</span></p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h3 class="truncate-title">Data Loading Code</h3>
                    <p>Implementation for creating minibatches:</p>

                    <pre><code class="language-python">def __init__(self, batch_size, num_steps):
    corpus, self.vocab = self.build(self._download())
    # Create all possible subsequences
    array = torch.tensor([corpus[i:i+num_steps+1]
                          for i in range(len(corpus)-num_steps)])
    # Split into input and target
    self.X, self.Y = array[:,:-1], array[:,1:]

def get_dataloader(self, train):
    # Random sampling of subsequences
    idx = slice(0, self.num_train) if train else
          slice(self.num_train, self.num_train + self.num_val)
    return self.get_tensorloader([self.X, self.Y], train, idx)</code></pre>

                    <div class="fragment emphasis-box">
                        <p>Key: Target is input shifted by one position!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences"}]'>
                    <h3 class="truncate-title">Example Output</h3>
                    <p>Sample minibatch with batch_size=2, num_steps=10:</p>

                    <pre><code class="language-python"># Input sequences (X)
tensor([[10,  4,  2, 21, 10, 16, 15,  0, 20,  2],
        [21,  9,  6, 19,  0, 24,  2, 26,  0, 16]])

# Target sequences (Y) - shifted by 1
tensor([[ 4,  2, 21, 10, 16, 15,  0, 20,  2, 10],
        [ 9,  6, 19,  0, 24,  2, 26,  0, 16,  9]])</code></pre>

                    <div class="fragment">
                        <p>Notice: Y[i] = X[i+1] for each position</p>
                        <ul>
                            <li>This allows predicting the next token at each step</li>
                            <li>Efficient use of data: each token is both input and target</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h3 class="truncate-title">Test Your Understanding</h3>
                    <div data-mcq='{
                        "question": "Why do we use a random offset d when partitioning sequences?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make training faster",
                                "correct": false,
                                "explanation": "Random offset doesnt speed up training."
                            },
                            {
                                "text": "To ensure we see different subsequences each epoch",
                                "correct": true,
                                "explanation": "Correct! Random offset ensures variety in training data across epochs."
                            },
                            {
                                "text": "To reduce memory usage",
                                "correct": false,
                                "explanation": "Offset doesnt affect memory usage."
                            },
                            {
                                "text": "To make sequences longer",
                                "correct": false,
                                "explanation": "Offset doesnt change sequence length."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 28: Summary and Exercises (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.3.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#summary-and-discussion"}]'>
                    <h2 class="truncate-title">Summary: Language Models</h2>

                    <div class="key-concepts" style="font-size: 0.75em;">
                        <h3>Key Takeaways</h3>
                        <ul>
                            <li><strong>Language models</strong> estimate joint probability of text sequences</li>
                            <li><strong>N-gram models</strong> use Markov assumptions to simplify dependencies</li>
                            <li><strong>Data sparsity</strong> is a fundamental challenge for count-based methods</li>
                            <li><strong>Laplace smoothing</strong> addresses zero probabilities but has limitations</li>
                            <li><strong>Perplexity</strong> measures model quality (lower is better)</li>
                            <li><strong>Neural models</strong> overcome limitations of count-based approaches</li>
                        </ul>
                    </div>

                    <div class="fragment emphasis-box" style="font-size: 0.75em;">
                        <p>Modern large language models scale up with data, model size, and compute to achieve state-of-the-art performance across diverse tasks</p>
                    </div>
                </section>

                <section>
                    <h3 class="truncate-title">Test your understanding</h3>
                    <div data-mcq='{
                        "question": "Which statement best describes the relationship between perplexity and cross-entropy loss?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Perplexity is the logarithm of cross-entropy",
                                "correct": false,
                                "explanation": "Perplexity is the exponential, not logarithm, of cross-entropy."
                            },
                            {
                                "text": "Perplexity is the exponential of average cross-entropy loss",
                                "correct": true,
                                "explanation": "Correct! PPL = exp(L) where L is average cross-entropy loss."
                            },
                            {
                                "text": "They are the same value",
                                "correct": false,
                                "explanation": "They are related but not equal; perplexity is exp(cross-entropy)."
                            },
                            {
                                "text": "Perplexity is cross-entropy squared",
                                "correct": false,
                                "explanation": "Perplexity uses exponential, not squaring."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.4: Recurrent Neural Networks (Main) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html"}]'>
                    <h2 class="truncate-title">Section 9.4: Recurrent Neural Networks</h2>
                    <div class="content-container">
                        <div class="fragment">
                            <h4>From n-grams to Neural Networks</h4>
                            <p style="font-size: 0.85em;">Recall: n-gram models suffer from exponential parameter growth</p>
                            <ul style="font-size: 0.8em;">
                                <li>For vocabulary $|\mathcal{V}|$ and n-grams: $|\mathcal{V}|^n$ parameters</li>
                                <li>Example: 10,000 words, 4-grams = $10^{16}$ parameters!</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Solution:</strong> Use a <span class="tooltip">latent variable<span class="tooltiptext">A hidden representation that captures the essential information from the observed data</span></span> model</p>
                            <p style="font-size: 0.9em;">$$P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})$$</p>
                            <p style="font-size: 0.8em;">where $h_{t-1}$ is a <span class="tooltip">hidden state<span class="tooltiptext">A vector that stores the sequence information up to the current time step</span></span></p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html"}]'>
                    <h2 class="truncate-title">Hidden State Computation</h2>
                    <div class="content-container" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>General Form of Hidden State Update</h4>
                            <p style="font-size: 0.9em;">$$h_t = f(x_t, h_{t-1})$$</p>
                            <p style="font-size: 0.8em;">The hidden state at time $t$ depends on:</p>
                            <ul style="font-size: 0.8em;">
                                <li>Current input $x_t$</li>
                                <li>Previous hidden state $h_{t-1}$</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <div class="emphasis-box">
                                <p style="font-size: 0.85em;"><strong>Key Insight:</strong> For a sufficiently powerful function $f$, this is not an approximation—$h_t$ could store all observed data</p>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.85em; color: #666;">⚠️ <strong>Note:</strong> Hidden layers vs. Hidden states</p>
                            <ul style="font-size: 0.75em; color: #666;">
                                <li>Hidden layers: Layers between input and output</li>
                                <li>Hidden states: Inputs that depend on previous time steps</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Section 9.4.1: Neural Networks without Hidden States (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#neural-networks-without-hidden-states"}]'>
                    <h2 class="truncate-title">9.4.1: Neural Networks without Hidden States</h2>
                    <div class="content-container" style="font-size: 0.7em;">
                        <h4>Standard MLP Review</h4>
                        <p style="font-size: 0.85em;">Let's first consider an MLP with a single hidden layer:</p>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.9em; color: #10099F;"><strong>Hidden Layer:</strong></p>
                            <p style="font-size: 0.95em;">$$\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)$$</p>
                            <ul style="font-size: 0.75em;">
                                <li>$\mathbf{X} \in \mathbb{R}^{n \times d}$: minibatch of $n$ examples, $d$ features</li>
                                <li>$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$: weight matrix</li>
                                <li>$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$: bias</li>
                                <li>$\phi$: activation function</li>
                            </ul>
                        </div>
                        <div class="fragment mt-lg">
                            <p style="font-size: 0.9em; color: #10099F;"><strong>Output Layer:</strong></p>
                            <p style="font-size: 0.95em;">$$\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q$$</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.1", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#neural-networks-without-hidden-states"}]'>
                    <h2 class="truncate-title">Matrix Operations Example</h2>
                    <div class="content-container" style="font-size: 0.7em;">
                        <h4>Code Demonstration</h4>
                        <pre><code class="python">import torch

# Example dimensions
n, d, h, q = 2, 3, 4, 2  # batch, input, hidden, output

# Random inputs and parameters
X = torch.randn(n, d)
W_xh = torch.randn(d, h)
b_h = torch.randn(1, h)
W_hq = torch.randn(h, q)
b_q = torch.randn(1, q)

# Forward pass
H = torch.tanh(torch.matmul(X, W_xh) + b_h)
O = torch.matmul(H, W_hq) + b_q

print(f"Input shape: {X.shape}")
print(f"Hidden shape: {H.shape}")
print(f"Output shape: {O.shape}")</code></pre>
                        <div class="fragment">
                            <pre style="background: #f5f5f5; padding: 10px; border-radius: 5px;"><code>Input shape: torch.Size([2, 3])
Hidden shape: torch.Size([2, 4])
Output shape: torch.Size([2, 2])</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In a standard MLP without hidden states, what determines the output at each forward pass?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Only the current input X",
                                "correct": true,
                                "explanation": "Correct! In a standard MLP, the output depends only on the current input, with no memory of previous inputs."
                            },
                            {
                                "text": "The current input and all previous inputs",
                                "correct": false,
                                "explanation": "MLPs without hidden states have no mechanism to remember previous inputs."
                            },
                            {
                                "text": "A hidden state from the previous time step",
                                "correct": false,
                                "explanation": "Standard MLPs do not maintain hidden states across time steps."
                            },
                            {
                                "text": "Random initialization at each step",
                                "correct": false,
                                "explanation": "The output is deterministically computed from the input, not randomly initialized."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.4.2: Recurrent Neural Networks with Hidden States (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states"}]'>
                    <h2 class="truncate-title">9.4.2: RNNs with Hidden States</h2>
                    <div class="content-container">
                        <h4>Adding Memory to Neural Networks</h4>
                        <p style="font-size: 0.85em;">RNNs introduce <span class="tooltip">recurrent computation<span class="tooltiptext">Computation that uses outputs from previous time steps as inputs to the current time step</span></span> for hidden states:</p>

                        <div class="fragment mt-lg">
                            <p style="font-size: 0.9em; color: #10099F;"><strong>Hidden State Update:</strong></p>
                            <p style="font-size: 0.95em;">$$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)$$</p>

                            <ul style="font-size: 0.75em;">
                                <li>$\mathbf{X}_t$: Input at time $t$</li>
                                <li>$\mathbf{H}_{t-1}$: Hidden state from time $t-1$</li>
                                <li>$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$: Hidden-to-hidden weights</li>
                            </ul>
                        </div>

                        <div class="fragment mt-lg">
                            <p style="font-size: 0.9em; color: #10099F;"><strong>Output Computation:</strong></p>
                            <p style="font-size: 0.95em;">$$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q$$</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states"}]'>
                    <h2 class="truncate-title">RNN Computational Graph</h2>
                    <div class="content-container">
                        <div id="rnn-computation-viz" style="width: 100%; height: 400px;"></div>
                        <div class="demo-controls mt-lg" style="display: flex; align-items: center; justify-content: center; gap: 15px;">
                            <button id="rnn-animate-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Animate Forward Pass</button>
                            <button id="rnn-reset-btn" style="background: #666; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Reset</button>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states"}]'>
                    <h2 class="truncate-title">Code: Recurrent Computation</h2>
                    <div class="content-container">
                        <pre><code class="python"># Method 1: Concatenation
def rnn_concat(inputs, state, params):
    W_xh, W_hh, b_h, W_hq, b_q = params
    H = []
    for X in inputs:
        # Concatenate input and previous state
        combined = torch.cat((X, state), dim=1)
        W = torch.cat((W_xh, W_hh), dim=0)
        state = torch.tanh(torch.matmul(combined, W) + b_h)
        H.append(state)
    return H, state

# Method 2: Separate computations
def rnn_separate(inputs, state, params):
    W_xh, W_hh, b_h, W_hq, b_q = params
    H = []
    for X in inputs:
        state = torch.tanh(
            torch.matmul(X, W_xh) +
            torch.matmul(state, W_hh) + b_h
        )
        H.append(state)
    return H, state</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes RNNs different from standard feedforward networks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "RNNs have more layers",
                                "correct": false,
                                "explanation": "The key difference is not the number of layers, but the recurrent connections."
                            },
                            {
                                "text": "RNNs maintain a hidden state that carries information across time steps",
                                "correct": true,
                                "explanation": "Correct! RNNs use hidden states to maintain memory of previous inputs, enabling sequence processing."
                            },
                            {
                                "text": "RNNs use different activation functions",
                                "correct": false,
                                "explanation": "RNNs can use the same activation functions as feedforward networks."
                            },
                            {
                                "text": "RNNs require more training data",
                                "correct": false,
                                "explanation": "Data requirements depend on the task, not the architecture type."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.4.3: RNN-Based Character-Level Language Models (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models"}]'>
                    <h2 class="truncate-title">9.4.3: Character-Level Language Models</h2>
                    <div class="content-container">
                        <h4>Using RNNs for Text Generation</h4>
                        <p style="font-size: 0.65em;">Character-level models predict the next character based on previous characters</p>

                        <div class="fragment mt-lg">
                            <img src="images/rnn-char-model.jpg" alt="RNN Character Model" style="width: 40%; max-width: 600px;">
                            <p style="font-size: 0.75em; color: #666;">Input: "machin" → Target: "achine"</p>
                        </div>

                        <div class="fragment mt-lg">
                            <p style="font-size: 0.6em;">Training Process:</p>
                            <ul style="font-size: 0.5em;">
                                <li>Apply softmax at each time step</li>
                                <li>Use <span class="tooltip">cross-entropy loss<span class="tooltiptext">A loss function that measures the difference between predicted and actual probability distributions</span></span></li>
                                <li>Loss at time $t$ depends on $P(\text{"h"} \mid \text{"m", "a", "c"})$</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models"}]'>
                    <h2 class="truncate-title">Interactive Character Prediction</h2>
                    <div class="content-container">
                        <div id="char-prediction-demo" style="padding: 20px; background: #f9f9f9; border-radius: 8px;">
                            <h4>Character-Level RNN Demo</h4>
                            <div style="margin: 20px 0;">
                                <label style="display: block; margin-bottom: 10px;">
                                    Input Text:
                                    <input type="text" id="char-input" value="machin" style="padding: 5px; width: 200px; font-family: monospace;">
                                </label>
                                <button id="predict-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Predict Next Character</button>
                            </div>
                            <div id="prediction-output" style="margin-top: 20px; min-height: 100px;"></div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models"}]'>
                    <h2 class="truncate-title">Implementation Details</h2>
                    <div class="content-container">
                        <h4>Practical Considerations</h4>
                        <ul style="font-size: 0.85em;">
                            <li class="fragment">Each token represented as $d$-dimensional vector</li>
                            <li class="fragment">Batch size $n > 1$ for efficiency</li>
                            <li class="fragment">Input $\mathbf{X}_t$ at time $t$: $n \times d$ matrix</li>
                            <li class="fragment">Output $\mathbf{O}_t$ determines next character probabilities</li>
                        </ul>

                        <div class="fragment emphasis-box mt-lg">
                            <p style="font-size: 0.85em;"><strong>Key Advantage:</strong> Number of parameters doesn't grow with sequence length!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In a character-level RNN language model, what does the model predict at each time step?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The entire remaining sequence",
                                "correct": false,
                                "explanation": "The model predicts one character at a time, not the entire sequence."
                            },
                            {
                                "text": "The probability distribution over the next character",
                                "correct": true,
                                "explanation": "Correct! The RNN outputs a probability distribution over all possible characters for the next position."
                            },
                            {
                                "text": "The hidden state value",
                                "correct": false,
                                "explanation": "The hidden state is internal; the model outputs character probabilities."
                            },
                            {
                                "text": "The loss value",
                                "correct": false,
                                "explanation": "The loss is computed during training but not predicted by the model."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.4.4: Summary -->
            <section data-sources='[{"text": "Dive into Deep Learning - Section 9.4.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#summary"}]'>
                <h2 class="truncate-title">9.4.4: Summary</h2>
                <div class="content-container" style="font-size: 0.7em;">
                    <h3>Key Takeaways</h3>
                    <ul style="font-size: 0.9em;">
                        <li class="fragment">RNNs use <strong>recurrent computation</strong> to maintain hidden states</li>
                        <li class="fragment">Hidden states capture <strong>historical information</strong> from the sequence</li>
                        <li class="fragment">Model parameters remain <strong>constant</strong> across time steps</li>
                        <li class="fragment">Number of parameters <strong>doesn't grow</strong> with sequence length</li>
                        <li class="fragment">Can be used for <strong>character-level language modeling</strong></li>
                    </ul>

                    <div class="fragment emphasis-box mt-lg">
                        <p style="font-size: 0.85em;">RNNs provide a powerful framework for processing sequential data with temporal dependencies</p>
                    </div>
                </div>
            </section>

            <!-- Section 9.5: Implementing RNNs from Scratch -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.5", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html"}]'>
                    <h2 class="truncate-title">9.5: Implementing RNNs from Scratch</h2>
                    <div class="content-container">
                        <h3>Building an RNN from the Ground Up</h3>

                        <div class="fragment">
                            <p>We'll implement:</p>
                            <ul style="font-size: 0.9em;">
                                <li><strong>RNN model architecture</strong> with recurrent connections</li>
                                <li><strong>One-hot encoding</strong> for text input</li>
                                <li><strong>Forward propagation</strong> through time</li>
                                <li><strong>Gradient clipping</strong> to prevent exploding gradients</li>
                                <li><strong>Character-level language modeling</strong></li>
                            </ul>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p style="font-size: 0.85em;">Understanding the implementation details helps us appreciate both the power and limitations of RNNs</p>
                        </div>
                    </div>
                </section>

                <!-- Section 9.5.1: RNN Model Architecture -->
                <section data-sources='[{"text": "Dive into Deep Learning - RNN Architecture", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html"}]'>
                    <h2 class="truncate-title">9.5.1: RNN Model Architecture</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>Mathematical Formulation</h3>

                        <div class="math-section">
                            <p>Hidden state update:</p>
                            <p>$$\mathbf{H}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)$$</p>

                            <div class="fragment">
                                <p>Output computation:</p>
                                <p>$$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q$$</p>
                            </div>
                        </div>

                        <div class="fragment mt-lg">
                            <h4>Parameter Dimensions</h4>
                            <ul style="font-size: 0.85em;">
                                <li>$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$ - Input to hidden</li>
                                <li>$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$ - Hidden to hidden</li>
                                <li>$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$ - Hidden to output</li>
                                <li>$\mathbf{b}_h \in \mathbb{R}^{h}$, $\mathbf{b}_q \in \mathbb{R}^{q}$ - Biases</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">RNN Implementation in PyTorch</h2>
                    <pre><code class="python" data-trim data-noescape>
class RNNScratch(d2l.Module):
    """The RNN model implemented from scratch."""
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W_xh = nn.Parameter(
            torch.randn(num_inputs, num_hiddens) * sigma)
        self.W_hh = nn.Parameter(
            torch.randn(num_hiddens, num_hiddens) * sigma)
        self.b_h = nn.Parameter(torch.zeros(num_hiddens))

    def forward(self, inputs, state=None):
        if state is None:
            state = torch.zeros((inputs.shape[1], self.num_hiddens),
                              device=inputs.device)
        outputs = []
        for X in inputs:  # Shape: (num_steps, batch_size, num_inputs)
            state = torch.tanh(torch.matmul(X, self.W_xh) +
                             torch.matmul(state, self.W_hh) + self.b_h)
            outputs.append(state)
        return outputs, state
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: RNN Architecture</h2>
                    <div data-mcq='{
                        "question": "In the RNN forward pass, what role does the hidden state H_{t-1} play?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It stores the final output of the network",
                                "correct": false,
                                "explanation": "The hidden state is internal; outputs are computed separately from hidden states."
                            },
                            {
                                "text": "It carries information from previous time steps to the current step",
                                "correct": true,
                                "explanation": "Correct! The hidden state acts as the memory of the RNN, carrying information forward through time."
                            },
                            {
                                "text": "It is only used during backpropagation",
                                "correct": false,
                                "explanation": "The hidden state is essential during forward propagation to process sequences."
                            },
                            {
                                "text": "It represents the input at the previous time step",
                                "correct": false,
                                "explanation": "The hidden state is a learned representation, not the raw input."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.2: One-Hot Encoding -->
                <section data-sources='[{"text": "Dive into Deep Learning - One-Hot Encoding", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#one-hot-encoding"}]'>
                    <h2 class="truncate-title">9.5.2: One-Hot Encoding</h2>
                    <div class="content-container">
                        <h3>Representing Discrete Tokens</h3>

                        <p>One-hot encoding converts tokens to binary vectors:</p>
                        <ul style="font-size: 0.9em;">
                            <li>Vector length = vocabulary size</li>
                            <li>Single 1 at token's index, 0s elsewhere</li>
                            <li>Enables mathematical operations on text</li>
                        </ul>

                        <div class="fragment mt-lg">
                            <h4>Implementation</h4>
                            <pre><code class="python" data-trim data-noescape>
def one_hot(X, num_classes):
    # X shape: (batch_size, num_steps)
    # Output shape: (num_steps, batch_size, vocab_size)
    return F.one_hot(X.T, num_classes).type(torch.float32)
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">One-Hot Encoding Interactive Demo</h2>
                    <div class="demo-container">
                        <div style="margin-bottom: 20px;">
                            <label style="font-size: 1.1em;">Enter text:
                                <input type="text" id="onehot-input" value="hello" style="padding: 8px; font-size: 1em; width: 200px;">
                            </label>
                            <button id="encode-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; margin-left: 10px; cursor: pointer;">
                                Encode
                            </button>
                        </div>
                        <div id="onehot-visualization" style="width: 100%; height: 400px;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: One-Hot Encoding</h2>
                    <div data-mcq='{
                        "question": "If our vocabulary has 1000 unique tokens, what is the dimension of a one-hot encoded vector for a single token?",
                        "type": "single",
                        "options": [
                            {
                                "text": "1",
                                "correct": false,
                                "explanation": "One-hot vectors have the same dimension as the vocabulary size."
                            },
                            {
                                "text": "1000",
                                "correct": true,
                                "explanation": "Correct! Each token is represented by a vector of length equal to the vocabulary size."
                            },
                            {
                                "text": "Depends on the token frequency",
                                "correct": false,
                                "explanation": "One-hot encoding dimension is fixed and equals vocabulary size, regardless of frequency."
                            },
                            {
                                "text": "Equal to the sequence length",
                                "correct": false,
                                "explanation": "The vector dimension equals vocabulary size, not sequence length."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.3: Parameter Initialization -->
                <section data-sources='[{"text": "Dive into Deep Learning - Parameter Initialization", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html"}]'>
                    <h2 class="truncate-title">9.5.3: Parameter Initialization</h2>
                    <div class="content-container" style="font-size: 0.65em;">
                        <h3>Initializing RNN Parameters</h3>

                        <div class="code-section">
                            <h4>Weight Initialization Strategy</h4>
                            <pre><code class="python" data-trim data-noescape>
def init_params(self):
    # Input to hidden weights
    self.W_xh = nn.Parameter(
        torch.randn(num_inputs, num_hiddens) * sigma)

    # Hidden to hidden weights
    self.W_hh = nn.Parameter(
        torch.randn(num_hiddens, num_hiddens) * sigma)

    # Hidden to output weights
    self.W_hq = nn.Parameter(
        torch.randn(num_hiddens, vocab_size) * sigma)

    # Biases initialized to zero
    self.b_h = nn.Parameter(torch.zeros(num_hiddens))
    self.b_q = nn.Parameter(torch.zeros(vocab_size))
                            </code></pre>
                        </div>

                        <div class="fragment">
                            <p style="font-size: 0.9em;"><strong>Key points:</strong></p>
                            <ul style="font-size: 0.85em;">
                                <li>Small random weights (scaled by σ = 0.01)</li>
                                <li>Prevents <span class="tooltip">saturation<span class="tooltiptext">When activation functions output extreme values, causing gradients to vanish</span></span> of activation functions</li>
                                <li>Zero initialization for biases</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Shapes Visualization</h2>
                    <div id="param-shapes-viz" style="width: 100%; height: 500px;"></div>
                    <div style="text-align: center; margin-top: 20px;">
                        <p style="font-size: 0.9em; color: #666;">
                            Hover over matrices to see their dimensions and role in the RNN
                        </p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Parameter Initialization</h2>
                    <div data-mcq='{
                        "question": "Why do we multiply the initial random weights by a small value (sigma = 0.01)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make training faster",
                                "correct": false,
                                "explanation": "Small weights don not necessarily speed up training; they prevent other issues."
                            },
                            {
                                "text": "To prevent activation saturation and gradient problems",
                                "correct": true,
                                "explanation": "Correct! Small initial weights help prevent tanh saturation and vanishing gradients early in training."
                            },
                            {
                                "text": "To reduce memory usage",
                                "correct": false,
                                "explanation": "The scale of weights does not affect memory usage; they are still 32-bit floats."
                            },
                            {
                                "text": "Because PyTorch requires it",
                                "correct": false,
                                "explanation": "This is a best practice, not a framework requirement."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.4: Forward Pass Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Forward Pass", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html"}]'>
                    <h2 class="truncate-title">9.5.4: Forward Pass Implementation</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>Processing Sequences Step by Step</h3>

                        <pre><code class="python" data-trim data-noescape>
def forward(self, inputs, state=None):
    # inputs shape: (num_steps, batch_size, vocab_size)
    if state is None:
        # Initialize hidden state with zeros
        state = torch.zeros((inputs.shape[1], self.num_hiddens))

    outputs = []
    for X in inputs:  # Iterate through time steps
        # Update hidden state
        state = torch.tanh(
            torch.matmul(X, self.W_xh) +      # Input contribution
            torch.matmul(state, self.W_hh) +  # Previous hidden state
            self.b_h                           # Bias
        )
        outputs.append(state)

    return outputs, state
                        </code></pre>

                        <div class="fragment emphasis-box mt-lg">
                            <p style="font-size: 0.85em;">The same parameters are used at every time step - this is parameter sharing across time</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Forward Pass</h2>
                    <div data-mcq='{
                        "question": "During the forward pass, how many times are the weight matrices W_xh and W_hh used for a sequence of length 10?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Once each",
                                "correct": false,
                                "explanation": "The weights are reused at every time step, not just once."
                            },
                            {
                                "text": "10 times each",
                                "correct": true,
                                "explanation": "Correct! The same weight matrices are used at each of the 10 time steps - this is parameter sharing."
                            },
                            {
                                "text": "W_xh 10 times, W_hh 9 times",
                                "correct": false,
                                "explanation": "Both matrices are used at every time step, including W_hh at the first step with initial hidden state."
                            },
                            {
                                "text": "Depends on the batch size",
                                "correct": false,
                                "explanation": "The number of uses equals the sequence length, regardless of batch size."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.5: Gradient Clipping -->
                <section data-sources='[{"text": "Dive into Deep Learning - Gradient Clipping", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#gradient-clipping"}]'>
                    <h2 class="truncate-title">9.5.5: Gradient Clipping</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>Preventing Exploding Gradients</h3>

                        <p>RNNs suffer from exploding gradients due to repeated multiplication</p>

                        <div class="fragment">
                            <h4>Gradient Clipping Formula</h4>
                            <p>$$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{||\mathbf{g}||}\right) \mathbf{g}$$</p>
                            <p style="font-size: 0.85em;">where $\theta$ is the clipping threshold</p>
                        </div>

                        <div class="fragment">
                            <h4>Implementation</h4>
                            <pre><code class="python" data-trim data-noescape>
def clip_gradients(self, grad_clip_val, model):
    params = [p for p in model.parameters() if p.requires_grad]
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > grad_clip_val:
        for param in params:
            param.grad[:] *= grad_clip_val / norm
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient Clipping Visualization</h2>
                    <div id="gradient-clipping-viz" style="width: 100%; height: 400px;"></div>
                    <div class="demo-controls" style="display: flex; justify-content: center; gap: 20px; margin-top: 20px; align-items: center;">
                        <label style="display: flex; align-items: center; gap: 8px;">
                            Gradient Norm:
                            <input type="range" id="grad-norm-slider" min="0" max="20" value="5" step="0.5" style="width: 150px;">
                            <span id="grad-norm-value" style="font-family: monospace; width: 50px;">5.0</span>
                        </label>
                        <label style="display: flex; align-items: center; gap: 8px;">
                            Threshold θ:
                            <input type="range" id="clip-threshold-slider" min="1" max="10" value="5" step="0.5" style="width: 150px;">
                            <span id="clip-threshold-value" style="font-family: monospace; width: 50px;">5.0</span>
                        </label>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Gradient Clipping</h2>
                    <div data-mcq='{
                        "question": "If the gradient norm is 15 and the clipping threshold is 5, what happens to the gradients?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They are set to zero",
                                "correct": false,
                                "explanation": "Clipping scales gradients down, it does not zero them out."
                            },
                            {
                                "text": "They are scaled by a factor of 5/15 = 1/3",
                                "correct": true,
                                "explanation": "Correct! The gradients are scaled by threshold/norm = 5/15 to bring the norm down to the threshold."
                            },
                            {
                                "text": "Only gradients above 5 are clipped",
                                "correct": false,
                                "explanation": "Clipping considers the total norm of all gradients, not individual values."
                            },
                            {
                                "text": "They remain unchanged",
                                "correct": false,
                                "explanation": "Since the norm (15) exceeds the threshold (5), clipping will occur."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.6: Training the Model -->
                <section data-sources='[{"text": "Dive into Deep Learning - Training", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#training"}]'>
                    <h2 class="truncate-title">9.5.6: Training the Model</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>RNN Language Model Training</h3>

                        <div class="code-section">
                            <h4>Training Step Implementation</h4>
                            <pre><code class="python" data-trim data-noescape>
def training_step(self, batch):
    # Forward pass
    l = self.loss(self(*batch[:-1]), batch[-1])

    # Track perplexity
    self.plot('ppl', torch.exp(l), train=True)

    return l

# Loss function: Cross-entropy
# Metric: Perplexity = exp(loss)
                            </code></pre>
                        </div>

                        <div class="fragment">
                            <p><span class="tooltip">Perplexity<span class="tooltiptext">A measure of how well a probability model predicts a sample. Lower perplexity means better prediction.</span></span> measures prediction quality:</p>
                            <p>$$\text{Perplexity} = \exp\left(\frac{1}{n}\sum_{t=1}^{n} -\log P(x_t | x_{&lt;t})\right)$$</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Performance</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>Training Progress on "The Time Machine" Dataset</h3>

                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div>
                                <img src="images/training_plot1.svg" alt="Training perplexity over epochs" style="width: 70%;">
                                <p style="font-size: 0.8em; text-align: center;">Initial training run</p>
                            </div>
                            <div>
                                <img src="images/training_plot2.svg" alt="Training perplexity with improvements" style="width: 70%;">
                                <p style="font-size: 0.8em; text-align: center;">With gradient clipping</p>
                            </div>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p style="font-size: 0.85em;">Gradient clipping significantly stabilizes training and improves convergence</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Training</h2>
                    <div data-mcq='{
                        "question": "What does a perplexity of 1.0 indicate about a language model?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The model is overfitting",
                                "correct": false,
                                "explanation": "Perplexity of 1.0 means perfect prediction, which might indicate overfitting but is not the definition."
                            },
                            {
                                "text": "The model perfectly predicts every next token",
                                "correct": true,
                                "explanation": "Correct! Perplexity of 1.0 means the model assigns probability 1 to the correct next token every time."
                            },
                            {
                                "text": "The model is not learning",
                                "correct": false,
                                "explanation": "A non-learning model would have high perplexity, not 1.0."
                            },
                            {
                                "text": "The gradient norm equals 1.0",
                                "correct": false,
                                "explanation": "Perplexity is a measure of prediction quality, not related to gradient norms."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Section 9.5.7: Text Generation -->
                <section data-sources='[{"text": "Dive into Deep Learning - Prediction", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#prediction"}]'>
                    <h2 class="truncate-title">9.5.7: Text Generation</h2>
                    <div class="content-container" style="font-size: 0.75em;">
                        <h3>Generating Text with the Trained RNN</h3>

                        <pre><code class="python" data-trim data-noescape>
def predict(self, prefix, num_preds, vocab, device=None):
    state, outputs = None, [vocab[prefix[0]]]

    for i in range(len(prefix) + num_preds - 1):
        X = torch.tensor([[outputs[-1]]], device=device)
        embs = self.one_hot(X)
        rnn_outputs, state = self.rnn(embs, state)

        if i < len(prefix) - 1:  # Warm-up period
            outputs.append(vocab[prefix[i + 1]])
        else:  # Generate new characters
            Y = self.output_layer(rnn_outputs[-1])
            outputs.append(int(Y.argmax(axis=1)))

    return ''.join([vocab.idx_to_token[i] for i in outputs])
                        </code></pre>

                        <div class="fragment">
                            <p style="font-size: 0.9em;"><strong>Generation process:</strong></p>
                            <ol style="font-size: 0.85em;">
                                <li>Start with a prefix (seed text)</li>
                                <li>Warm up the hidden state with the prefix</li>
                                <li>Generate one character at a time</li>
                                <li>Feed generated character back as input</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Text Generation</h2>
                    <div class="demo-container">
                        <div style="margin-bottom: 20px;">
                            <label style="font-size: 1.1em;">Prefix text:
                                <input type="text" id="gen-prefix" value="time traveller" style="padding: 8px; font-size: 1em; width: 250px;">
                            </label>
                            <label style="font-size: 1.1em; margin-left: 20px;">Generate:
                                <input type="number" id="gen-length" value="50" min="10" max="200" style="padding: 8px; font-size: 1em; width: 80px;">
                                characters
                            </label>
                            <button id="generate-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; margin-left: 20px; cursor: pointer;">
                                Generate
                            </button>
                        </div>
                        <div id="generation-output" style="background: #f5f5f5; padding: 20px; border-radius: 5px; min-height: 150px; font-family: monospace; line-height: 1.6;">
                            <span style="color: #666;">Generated text will appear here...</span>
                        </div>
                        <div id="generation-process" style="margin-top: 20px; height: 250px;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Text Generation</h2>
                    <div data-mcq='{
                        "question": "During text generation, why do we use a warm-up period with the prefix?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To train the model on the prefix",
                                "correct": false,
                                "explanation": "Generation happens after training; we are not training during prediction."
                            },
                            {
                                "text": "To initialize the hidden state with context from the prefix",
                                "correct": true,
                                "explanation": "Correct! The warm-up period builds up the hidden state with information from the prefix before generating new text."
                            },
                            {
                                "text": "To validate the model accuracy",
                                "correct": false,
                                "explanation": "The warm-up is not for validation but for context building."
                            },
                            {
                                "text": "To reset the model parameters",
                                "correct": false,
                                "explanation": "Parameters remain fixed during generation; only the hidden state changes."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- CONCISE IMPLEMENTATION SECTIONS -->

            <!-- Section: High-Level API Implementation (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.6", "url": "https://d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html"}]'>
                    <h2 class="truncate-title">Concise Implementation with High-Level APIs</h2>
                    <div class="content-container">
                        <div class="fragment">
                            <h4>From Scratch to Framework APIs</h4>
                            <p style="font-size: 0.85em;">Our scratch implementation provided insight into RNN mechanics, but production code needs:</p>
                            <ul style="font-size: 0.8em;">
                                <li><strong>Optimized Performance:</strong> Framework implementations are highly optimized</li>
                                <li><strong>Reduced Development Time:</strong> Pre-built components and functions</li>
                                <li><strong>Better Maintainability:</strong> Standard APIs and patterns</li>
                            </ul>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Insight:</strong> High-level APIs provide the same functionality with significantly less code and better performance</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework RNN APIs Overview</h2>
                    <div class="framework-comparison" style="font-size: 0.75em;">
                        <div class="fragment">
                            <h4>PyTorch</h4>
                            <pre><code class="language-python">import torch.nn as nn
# Simple one-liner RNN creation
rnn = nn.RNN(input_size, hidden_size)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>TensorFlow/Keras</h4>
                            <pre><code class="language-python">import tensorflow.keras as keras
# Keras SimpleRNN layer
rnn = keras.layers.SimpleRNN(hidden_size,
                              return_sequences=True,
                              return_state=True)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>MXNet/Gluon</h4>
                            <pre><code class="language-python">from mxnet.gluon import rnn
# Gluon RNN cell
rnn_layer = rnn.RNN(hidden_size)</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p style="font-size: 0.9em;"><strong>Note:</strong> All frameworks provide similar abstractions with slight API differences</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Benefits of High-Level APIs</h2>
                    <div class="api-benefits-viz" style="font-size: 0.75em;">
                        <div class="fragment mt-md">
                            <div style="display: flex; justify-content: space-around; font-size: 0.85em;">
                                <div style="flex: 1; margin: 10px;">
                                    <h4 style="color: #10099F;">Scratch Implementation</h4>
                                    <ul style="text-align: left;">
                                        <li>~100 lines of code</li>
                                        <li>Manual gradient computation</li>
                                        <li>Custom initialization</li>
                                        <li>Slower execution</li>
                                    </ul>
                                </div>
                                <div style="flex: 1; margin: 10px;">
                                    <h4 style="color: #2DD2C0;">Framework APIs</h4>
                                    <ul style="text-align: left;">
                                        <li>~20 lines of code</li>
                                        <li>Automatic differentiation</li>
                                        <li>Built-in initialization</li>
                                        <li>Optimized CUDA/CPU kernels</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary advantage of using high-level RNN APIs over scratch implementations?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They require more code to write",
                                "correct": false,
                                "explanation": "High-level APIs actually require significantly less code - typically 5-10x fewer lines."
                            },
                            {
                                "text": "They provide optimized implementations with better performance",
                                "correct": true,
                                "explanation": "Correct! Framework implementations use optimized CUDA kernels and CPU operations that are much faster than naive implementations."
                            },
                            {
                                "text": "They make debugging easier",
                                "correct": false,
                                "explanation": "While frameworks provide some debugging tools, scratch implementations can actually be easier to debug since you control all the code."
                            },
                            {
                                "text": "They only work on GPUs",
                                "correct": false,
                                "explanation": "High-level APIs work on both CPUs and GPUs, with automatic device management."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Model Definition with APIs (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Defining RNN Models with High-Level APIs</h2>
                    <div class="model-definition">
                        <div class="fragment">
                            <h4>The RNN Class Structure</h4>
                            <pre><code class="language-python">class RNN(d2l.Module):
    """The RNN model implemented with high-level APIs."""
    def __init__(self, num_inputs, num_hiddens):
        super().__init__()
        self.save_hyperparameters()
        # Single line to create RNN!
        self.rnn = nn.RNN(num_inputs, num_hiddens)

    def forward(self, inputs, H=None):
        return self.rnn(inputs, H)</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Compare with scratch: ~100 lines reduced to ~10 lines!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework-Specific Implementations</h2>
                    <div class="framework-tabs">
                        <div class="tab-controls demo-controls" style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">
                            <button class="tab-btn active" data-framework="pytorch" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">PyTorch</button>
                            <button class="tab-btn" data-framework="tensorflow" style="background: #e0e0e0; color: #333; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">TensorFlow</button>
                            <button class="tab-btn" data-framework="mxnet" style="background: #e0e0e0; color: #333; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">MXNet</button>
                        </div>
                        <div class="framework-code">
                            <div id="pytorch-code" class="code-panel active">
                                <pre><code class="language-python">class RNN(d2l.Module):
    def __init__(self, num_inputs, num_hiddens):
        super().__init__()
        self.rnn = nn.RNN(num_inputs, num_hiddens)

    def forward(self, inputs, H=None):
        return self.rnn(inputs, H)</code></pre>
                            </div>
                            <div id="tensorflow-code" class="code-panel" style="display: none;">
                                <pre><code class="language-python">class RNN(d2l.Module):
    def __init__(self, num_hiddens):
        super().__init__()
        self.rnn = tf.keras.layers.SimpleRNN(
            num_hiddens, return_sequences=True,
            return_state=True, time_major=True)

    def forward(self, inputs, H=None):
        outputs, H = self.rnn(inputs, H)
        return outputs, H</code></pre>
                            </div>
                            <div id="mxnet-code" class="code-panel" style="display: none;">
                                <pre><code class="language-python">class RNN(d2l.Module):
    def __init__(self, num_hiddens):
        super().__init__()
        self.rnn = rnn.RNN(num_hiddens)

    def forward(self, inputs, H=None):
        if H is None:
            H, = self.rnn.begin_state(
                inputs.shape[1], ctx=inputs.ctx)
        outputs, (H, ) = self.rnn(inputs, (H, ))
        return outputs, H</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Language Model with RNN APIs</h2>
                    <div class="lm-implementation">
                        <div class="fragment">
                            <h4>RNNLM Class Implementation</h4>
                            <pre><code class="language-python">class RNNLM(d2l.RNNLMScratch):
    """RNN-based language model with high-level APIs."""

    def init_params(self):
        # Output layer for vocabulary
        self.linear = nn.LazyLinear(self.vocab_size)

    def output_layer(self, hiddens):
        # Transform hidden states to vocabulary predictions
        return self.linear(hiddens).swapaxes(0, 1)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <h4>Key Components</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Inheritance:</strong> Reuses training logic from <span class="tooltip">RNNLMScratch<span class="tooltiptext">Base class that implements training loop, gradient clipping, and text generation methods</span></span></li>
                                <li><strong>Output Layer:</strong> <span class="tooltip">Linear transformation<span class="tooltiptext">Transforms hidden state dimensions to vocabulary size for next-token prediction</span></span> from hidden to vocab size</li>
                                <li><strong>Axis Swap:</strong> Adjusts tensor dimensions for batch processing</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Hidden State Initialization</h2>
                    <div class="hidden-state-viz">
                        <div class="fragment">
                            <pre><code class="language-python"># Different frameworks handle initialization differently

# PyTorch - Returns tensor directly
H = torch.zeros((num_layers, batch_size, num_hiddens))

# TensorFlow - Handled internally by layer
# Pass None and layer creates initial state

# MXNet - Uses begin_state method
H, = rnn_layer.begin_state(batch_size)</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>Shape: <strong>(num_layers, batch_size, num_hiddens)</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main difference between the RNNLM class using high-level APIs and the scratch implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The high-level version doesnt need an output layer",
                                "correct": false,
                                "explanation": "Both versions need an output layer to transform hidden states to vocabulary predictions."
                            },
                            {
                                "text": "The high-level version handles RNN computations with a single framework call",
                                "correct": true,
                                "explanation": "Correct! The framework RNN layer handles all forward propagation, hidden state updates, and gradient computation internally."
                            },
                            {
                                "text": "The high-level version cannot do gradient clipping",
                                "correct": false,
                                "explanation": "Gradient clipping is still available and important when using high-level APIs."
                            },
                            {
                                "text": "The high-level version requires more parameters",
                                "correct": false,
                                "explanation": "Both versions have the same number of model parameters, just different implementations."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Training and Prediction (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Training RNNs with Framework APIs</h2>
                    <div class="training-setup">
                        <div class="fragment">
                            <h4>Complete Training Setup</h4>
                            <pre><code class="language-python"># Load the Time Machine dataset
data = d2l.TimeMachine(batch_size=1024, num_steps=32)

# Create RNN with high-level API
rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)

# Create language model
model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)

# Train with high-level trainer
trainer = d2l.Trainer(max_epochs=100,
                      gradient_clip_val=1,
                      num_gpus=1)
trainer.fit(model, data)</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>Notice: Same hyperparameters as scratch implementation, but much simpler code!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Before Training: Random Predictions</h2>
                    <div class="random-predictions">
                        <div class="fragment">
                            <h4>Untrained Model Output</h4>
                            <pre><code class="language-python"># Make prediction with untrained model
model.predict('it has', 20, data.vocab)

# Outputs nonsense:
# 'it hasxlxlxlxlxlxlxlxlxlxl'  (MXNet)
# 'it hasoadd dd dd dd dd dd '  (PyTorch)
# 'it hasretsnrnrxnrnrgczntgq'  (TensorFlow)</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <div class="demo-controls" style="margin: 20px auto; text-align: center;">
                                <button id="random-gen-btn" style="background: #10099F; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer;">
                                    Generate Random Text
                                </button>
                                <div id="random-output" style="margin-top: 20px; padding: 15px; background: #f5f5f5; border-radius: 5px; font-family: monospace; min-height: 50px;"></div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>Random initialization produces random character sequences with no linguistic structure</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">After Training: Coherent Generation</h2>
                    <div class="trained-predictions">
                        <div class="fragment">
                            <h4>Trained Model Output</h4>
                            <pre><code class="language-python"># Generate text with trained model
model.predict('it has', 20, data.vocab, device)

# Produces coherent text:
# 'it has and the time the ti'
# 'it has and the trave the t'
# 'it has and the pas an and '</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <div class="interactive-generation">
                                <div class="demo-controls" style="text-align: center;">
                                    <label style="font-size: 0.9em;">Prefix:
                                        <input type="text" id="prefix-input" value="time traveller " style="padding: 5px; margin: 0 10px;">
                                    </label>
                                    <label style="font-size: 0.9em;">Length:
                                        <input type="number" id="length-input" value="30" min="10" max="100" style="width: 60px; padding: 5px; margin: 0 10px;">
                                    </label>
                                    <button id="generate-trained-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">
                                        Generate
                                    </button>
                                </div>
                                <div id="generated-text" style="margin-top: 20px; padding: 15px; background: #f5f5f5; border-radius: 5px; font-family: monospace; min-height: 50px;"></div>
                            </div>
                        </div>
                    </div>
                </section>

                

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does the high-level API implementation train faster than the scratch implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses fewer epochs",
                                "correct": false,
                                "explanation": "Both implementations use the same number of epochs (100) for training."
                            },
                            {
                                "text": "It has fewer parameters to train",
                                "correct": false,
                                "explanation": "The model architecture and parameter count are identical in both implementations."
                            },
                            {
                                "text": "It uses optimized low-level operations and parallelization",
                                "correct": true,
                                "explanation": "Correct! Framework implementations use optimized BLAS libraries, CUDA kernels, and efficient memory management that are much faster than Python loops."
                            },
                            {
                                "text": "It skips the backward pass",
                                "correct": false,
                                "explanation": "The backward pass is still performed for gradient computation - its just handled automatically by the framework."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Performance Analysis (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Performance Analysis: APIs vs Scratch</h2>
                    <div class="performance-overview">
                        <div class="fragment emphasis-box mt-md">
                            <p>High-level APIs provide 2-3x speedup with identical model quality</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Memory Usage Comparison</h2>
                    <div class="memory-comparison" style="font-size: 0.75em;">
                        <div class="fragment">
                            <h4>Memory Efficiency</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Scratch Implementation:</strong>
                                    <ul>
                                        <li>Stores intermediate values explicitly</li>
                                        <li>Python object overhead</li>
                                        <li>Less efficient gradient storage</li>
                                    </ul>
                                </li>
                                <li class="mt-md"><strong>Framework APIs:</strong>
                                    <ul>
                                        <li>Optimized tensor operations</li>
                                        <li>Automatic memory pooling</li>
                                        <li>Efficient <span class="tooltip">gradient accumulation<span class="tooltiptext">The process of storing and combining gradients during backpropagation</span></span></li>
                                    </ul>
                                </li>
                            </ul>
                        </div>
                        <div class="fragment mt-md">
                            <div id="memory-usage-viz" style="width: 100%; height: 250px;"></div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Code Complexity Metrics</h2>
                    <div class="complexity-analysis">
                        <table style="width: 100%; font-size: 0.85em;">
                            <thead>
                                <tr style="background: #10099F; color: white;">
                                    <th>Metric</th>
                                    <th>Scratch Implementation</th>
                                    <th>High-Level API</th>
                                    <th>Reduction</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Lines of Code</td>
                                    <td>~100</td>
                                    <td>~20</td>
                                    <td style="color: #2DD2C0; font-weight: bold;">80%</td>
                                </tr>
                                <tr>
                                    <td>Functions to Implement</td>
                                    <td>8-10</td>
                                    <td>2-3</td>
                                    <td style="color: #2DD2C0; font-weight: bold;">75%</td>
                                </tr>
                                <tr>
                                    <td>Bug-Prone Operations</td>
                                    <td>Many (gradients, init)</td>
                                    <td>Few</td>
                                    <td style="color: #2DD2C0; font-weight: bold;">90%</td>
                                </tr>
                                <tr>
                                    <td>Development Time</td>
                                    <td>Hours</td>
                                    <td>Minutes</td>
                                    <td style="color: #2DD2C0; font-weight: bold;">95%</td>
                                </tr>
                            </tbody>
                        </table>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Conclusion: Use framework APIs for production; implement from scratch for learning</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">When to Use Each Approach</h2>
                    <div class="use-cases">
                        <div style="display: flex; justify-content: space-around; margin-top: 30px;">
                            <div class="fragment" style="flex: 1; margin: 10px; padding: 20px; background: #f5f5f5; border-radius: 8px;">
                                <h4 style="color: #10099F;">Use Scratch Implementation</h4>
                                <ul style="font-size: 0.8em; text-align: left;">
                                    <li>Learning RNN internals</li>
                                    <li>Research on new architectures</li>
                                    <li>Custom modifications needed</li>
                                    <li>Debugging understanding</li>
                                </ul>
                            </div>
                            <div class="fragment" style="flex: 1; margin: 10px; padding: 20px; background: #f5f5f5; border-radius: 8px;">
                                <h4 style="color: #2DD2C0;">Use Framework APIs</h4>
                                <ul style="font-size: 0.8em; text-align: left;">
                                    <li>Production deployments</li>
                                    <li>Standard architectures</li>
                                    <li>Performance critical</li>
                                    <li>Rapid prototyping</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In which scenario would you choose a scratch implementation over high-level APIs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Deploying a model to production",
                                "correct": false,
                                "explanation": "Production deployments benefit from the optimized performance and reliability of framework APIs."
                            },
                            {
                                "text": "Training a standard LSTM for text generation",
                                "correct": false,
                                "explanation": "Standard architectures like LSTM are well-supported by framework APIs and should use them for efficiency."
                            },
                            {
                                "text": "Researching a novel RNN cell architecture",
                                "correct": true,
                                "explanation": "Correct! When developing new architectures or modifications, you need the flexibility of custom implementations."
                            },
                            {
                                "text": "Building a commercial chatbot",
                                "correct": false,
                                "explanation": "Commercial applications should use framework APIs for performance, reliability, and maintainability."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            

            <!-- Section 9.7: Backpropagation Through Time (Vertical) -->
            <section>
                <section>
                    <h1 class="truncate-title">Backpropagation Through Time</h1>
                    <div class="subtitle-container">
                        <p>Understanding Gradient Flow in Sequence Models</p>
                    </div>
                    <div class="content-box fragment">
                        <h3>The Challenge</h3>
                        <ul>
                            <li>RNNs share parameters across time steps</li>
                            <li>Gradients must flow through entire sequences</li>
                            <li>Long sequences pose computational and numerical challenges</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>BPTT</strong> is the application of backpropagation to unrolled RNNs</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 9.7", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html"}]'>
                    <h2 class="truncate-title">What is Backpropagation Through Time?</h2>
                    <div class="concept-explanation">
                        <p><span class="tooltip">BPTT<span class="tooltiptext">Backpropagation Through Time: The algorithm for computing gradients in RNNs by unrolling the network and applying standard backpropagation</span></span> unrolls the RNN computational graph:</p>

                        <div class="unroll-visualization" id="bptt-unroll-viz">
                            <svg width="100%" height="300" viewBox="0 0 800 300"></svg>
                        </div>

                        <div class="fragment mt-md">
                            <div class="key-points">
                                <h4>Key Properties:</h4>
                                <ul style="font-size: 0.9em;">
                                    <li>Same parameters repeated at each time step</li>
                                    <li>Gradients summed across all occurrences</li>
                                    <li>Essentially a feedforward network with <span class="tooltip">weight tying<span class="tooltiptext">Using the same weight parameters multiple times in different parts of the network</span></span></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Dependencies</h2>
                    <div class="dependency-viz" style="font-size: 0.75em;">
                        <p>Each hidden state depends on previous states and parameters:</p>
                        <div id="dependency-graph" style="width: 100%; height: 400px;"></div>
                        <div class="fragment mt-md">
                            <div class="emphasis-box">
                                <p>Gradient computation requires tracing all paths from loss to parameters</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is backpropagation through time computationally expensive for long sequences?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It requires storing all intermediate activations",
                                "correct": true,
                                "explanation": "Correct! BPTT must cache all intermediate values across time steps to compute gradients, leading to high memory usage."
                            },
                            {
                                "text": "It uses different parameters at each time step",
                                "correct": false,
                                "explanation": "RNNs share parameters across time steps, which is actually what makes them efficient for sequence modeling."
                            },
                            {
                                "text": "It only computes gradients for the final time step",
                                "correct": false,
                                "explanation": "BPTT computes gradients for all time steps, which is why it is expensive."
                            },
                            {
                                "text": "It requires special hardware",
                                "correct": false,
                                "explanation": "BPTT can run on standard hardware, but long sequences require significant memory and computation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.7.1: Mathematical Foundations (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Analysis of Gradients in RNNs</h2>
                    <div class="mathematical-foundation">
                        <p>Let's analyze gradient flow mathematically</p>
                        <div class="fragment">
                            <h4>Simplified RNN Model:</h4>
                            <div class="equation-box">
                                $$\begin{aligned}
                                h_t &= f(x_t, h_{t-1}, w_h) \\
                                o_t &= g(h_t, w_o)
                                \end{aligned}$$
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <p>Where:</p>
                            <ul style="font-size: 0.9em;">
                                <li><span style="color: #10099F;">$h_t$</span>: Hidden state at time $t$</li>
                                <li><span style="color: #2DD2C0;">$x_t$</span>: Input at time $t$</li>
                                <li><span style="color: #FC8484;">$o_t$</span>: Output at time $t$</li>
                                <li><span style="color: #FFA05F;">$w_h, w_o$</span>: Weight parameters</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-chapter-recurrent-neural-networks-bptt-0"}]'>
                    <h2 class="truncate-title">Objective Function Over Time</h2>
                    <div class="objective-function">
                        <p>The loss is averaged over all time steps:</p>
                        <div class="equation-box large">
                            $$L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t)$$
                        </div>
                        <div class="fragment mt-lg">
                            <div class="gradient-flow-viz" id="loss-aggregation">
                                <svg width="100%" height="250" viewBox="0 0 700 250"></svg>
                            </div>
                        </div>
                        <div class="fragment">
                            <p class="insight">Each time step contributes equally to the total loss</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.3", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-l-wh"}]'>
                    <h2 class="truncate-title">Gradient Computation Challenge</h2>
                    <div class="gradient-math">
                        <p>Computing gradient w.r.t. hidden layer weights:</p>
                        <div class="equation-box">
                            $$\frac{\partial L}{\partial w_h} = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t} \frac{\partial h_t}{\partial w_h}$$
                        </div>
                        <div class="fragment mt-md">
                            <div class="challenge-box" style="background: #FC848420; border: 2px solid #FC8484; padding: 15px; border-radius: 8px;">
                                <h4>The Problem:</h4>
                                <p>$\frac{\partial h_t}{\partial w_h}$ depends on all previous hidden states!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.4", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-ht-wh-recur"}]'>
                    <h2 class="truncate-title">Recursive Gradient Dependency</h2>
                    <div class="recursive-gradient">
                        <p>The total derivative involves recursion:</p>
                        <div class="equation-box">
                            $$\frac{\partial h_t}{\partial w_h} = \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}$$
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.7", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-ht-wh-gen"}]'>
                    <h2 class="truncate-title">Full Gradient Expansion</h2>
                    <div class="full-expansion" style="font-size: 0.75em;">
                        <p>Removing recursion gives us:</p>
                        <div class="equation-box small" style="font-size: 0.85em;">
                            $$\frac{\partial h_t}{\partial w_h} = \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_j, h_{j-1}, w_h)}{\partial h_{j-1}}\right) \frac{\partial f(x_i, h_{i-1}, w_h)}{\partial w_h}$$
                        </div>
                        <div class="fragment mt-md">
                            <div class="insight-box" style="background: #10099F20; border: 2px solid #10099F; padding: 15px; border-radius: 8px;">
                                <p><strong>Key Insight:</strong> Gradient involves products of many Jacobians!</p>
                                <p style="font-size: 0.9em; margin-top: 10px;">This leads to <span class="tooltip">vanishing<span class="tooltiptext">When eigenvalues < 1, repeated multiplication causes gradients to approach zero</span></span> or <span class="tooltip">exploding<span class="tooltiptext">When eigenvalues > 1, repeated multiplication causes gradients to grow exponentially</span></span> gradients</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes gradient computation in RNNs fundamentally different from feedforward networks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Gradients depend recursively on all previous time steps",
                                "correct": true,
                                "explanation": "Correct! Each hidden state depends on all previous states, creating a recursive gradient dependency that grows with sequence length."
                            },
                            {
                                "text": "RNNs use different activation functions",
                                "correct": false,
                                "explanation": "RNNs can use the same activation functions as feedforward networks. The difference is in the recurrent connections."
                            },
                            {
                                "text": "RNNs have more parameters",
                                "correct": false,
                                "explanation": "RNNs actually share parameters across time steps, often having fewer parameters than comparable feedforward networks."
                            },
                            {
                                "text": "Gradients only flow backward in time",
                                "correct": false,
                                "explanation": "While gradients flow backward through time, they also flow through layers like in feedforward networks."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.7.2: Gradient Computation Strategies (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Strategies for Computing Gradients</h2>
                    <div class="strategies-overview">
                        <p>Three approaches to handle long sequences:</p>
                        <div class="strategy-cards" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-top: 30px;">
                            <div class="fragment">
                                <div class="card" style="background: #FC848420; border: 2px solid #FC8484; padding: 15px; border-radius: 8px;">
                                    <h4>Full Computation</h4>
                                    <p style="font-size: 0.85em;">Complete gradient calculation</p>
                                    <p style="font-size: 0.8em; color: #FC8484;">❌ Almost never used</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="card" style="background: #2DD2C020; border: 2px solid #2DD2C0; padding: 15px; border-radius: 8px;">
                                    <h4>Truncated BPTT</h4>
                                    <p style="font-size: 0.85em;">Stop after τ steps</p>
                                    <p style="font-size: 0.8em; color: #2DD2C0;">✓ Most common</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="card" style="background: #FFA05F20; border: 2px solid #FFA05F; padding: 15px; border-radius: 8px;">
                                    <h4>Randomized</h4>
                                    <p style="font-size: 0.85em;">Probabilistic truncation</p>
                                    <p style="font-size: 0.8em; color: #FFA05F;">~ Theoretical interest</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Full Computation Problems</h2>
                    <div class="full-computation-issues">
                        <h4>The <span class="tooltip">Butterfly Effect<span class="tooltiptext">Small changes in initial conditions lead to large changes in outcomes, making the model unstable and hard to train</span></span></h4>
                        <div class="butterfly-demo" id="butterfly-viz" style="margin: 20px 0;">
                            <svg width="100%" height="300" viewBox="0 0 700 300"></svg>
                        </div>
                        <div class="fragment" style="font-size: 0.7em;">
                            <ul>
                                <li>Subtle initial changes → dramatic outcome changes</li>
                                <li>Computationally expensive for long sequences</li>
                                <li>Gradients can explode exponentially</li>
                                <li>Poor generalization due to overfitting to noise</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Tallec & Ollivier 2017 - Unbiasing Truncated Backpropagation", "url": "https://arxiv.org/abs/1705.08209"}]'>
                    <h2 class="truncate-title">Randomized Truncation</h2>
                    <div class="randomized-truncation">
                        <p>Replace gradient with random variable correct in expectation:</p>
                        <div class="equation-box">
                            $$z_t = \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \xi_t \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}$$
                        </div>
                        <div class="fragment mt-md">
                            <p>Where $\xi_t$ is random with $E[\xi_t] = 1$:</p>
                            <ul style="font-size: 0.9em;">
                                <li>$P(\xi_t = 0) = 1 - \pi_t$ (terminate)</li>
                                <li>$P(\xi_t = \pi_t^{-1}) = \pi_t$ (continue with reweighting)</li>
                            </ul>
                        </div>
                        <div class="fragment">
                            <div class="note-box" style="background: #FFA05F20; border: 1px solid #FFA05F; padding: 10px; border-radius: 5px; margin-top: 15px;">
                                <p style="font-size: 0.9em;">In practice, performs similar to regular truncation</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Comparing Strategies</h2>
                    <div class="strategy-comparison" style="font-size: 0.75em;">
                        <img src="images/bptt-strategies.jpg" alt="BPTT Strategies Comparison" style="width: 40%; max-width: 700px;">
                        <div class="fragment mt-md">
                            <div class="comparison-insights">
                                <h4>Key Observations:</h4>
                                <ul style="font-size: 0.9em;">
                                    <li><strong>Top:</strong> Randomized - varying segment lengths</li>
                                    <li><strong>Middle:</strong> Regular truncation - fixed segments (most used)</li>
                                    <li><strong>Bottom:</strong> Full BPTT - computationally infeasible</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is truncated BPTT preferred over full computation in practice?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "It reduces computational cost",
                                "correct": true,
                                "explanation": "Correct! Truncation limits the number of steps, making computation feasible for long sequences."
                            },
                            {
                                "text": "It provides more stable gradients",
                                "correct": true,
                                "explanation": "Correct! By limiting the chain of multiplications, truncation helps prevent gradient explosion/vanishing."
                            },
                            {
                                "text": "It acts as a form of regularization",
                                "correct": true,
                                "explanation": "Correct! Focusing on short-term dependencies can prevent overfitting to spurious long-range patterns."
                            },
                            {
                                "text": "It gives more accurate gradients",
                                "correct": false,
                                "explanation": "Truncation actually provides an approximation of the true gradient, trading accuracy for stability and efficiency."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.7.3: Detailed BPTT Implementation (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 9.7.2", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#backpropagation-through-time-in-detail"}]'>
                    <h2 class="truncate-title">Backpropagation Through Time in Detail</h2>
                    <div class="detailed-implementation">
                        <p>Let's work through the complete gradient computation</p>
                        <div class="fragment">
                            <h4>RNN without bias (identity activation):</h4>
                            <div class="equation-box">
                                $$\begin{aligned}
                                \mathbf{h}_t &= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1} \\
                                \mathbf{o}_t &= \mathbf{W}_{qh} \mathbf{h}_t
                                \end{aligned}$$
                            </div>
                        </div>
                        <div class="fragment mt-md">
                            <p>Parameters to learn:</p>
                            <ul style="font-size: 0.9em;">
                                <li>$\mathbf{W}_{hx} \in \mathbb{R}^{h \times d}$ - input to hidden</li>
                                <li>$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$ - hidden to hidden</li>
                                <li>$\mathbf{W}_{qh} \in \mathbb{R}^{q \times h}$ - hidden to output</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Graph Visualization</h2>
                    <div class="comp-graph" style="font-size: 0.5em;">
                        <p>Dependencies for 3 time steps:</p>
                        <div id="detailed-comp-graph" style="width: 100%; height: 400px;">
                            <svg width="100%" height="400" viewBox="0 0 800 400"></svg>
                        </div>
                        <div class="fragment">
                            <p class="note">Shaded boxes = parameters, Clear boxes = variables, Circles = operators</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.11", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-l-ot"}]'>
                    <h2 class="truncate-title">Step 1: Output Gradient</h2>
                    <div class="output-gradient">
                        <p>Gradient w.r.t. output at time $t$:</p>
                        <div class="equation-box">
                            $$\frac{\partial L}{\partial \mathbf{o}_t} = \frac{\partial l(\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q$$
                        </div>
                        <div class="fragment mt-md">
                            <p>Gradient w.r.t. output weights:</p>
                            <div class="equation-box">
                                $$\frac{\partial L}{\partial \mathbf{W}_{qh}} = \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top$$
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="insight-box" style="background: #2DD2C020; border: 1px solid #2DD2C0; padding: 10px; border-radius: 5px; margin-top: 15px;">
                                <p>This is straightforward - just accumulate over time steps!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equations 9.7.13-9.7.14", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-l-ht-recur"}]'>
                    <h2 class="truncate-title">Step 2: Hidden State Gradients</h2>
                    <div class="hidden-gradients">
                        <p>At final time step $T$:</p>
                        <div class="equation-box">
                            $$\frac{\partial L}{\partial \mathbf{h}_T} = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}$$
                        </div>
                        <div class="fragment mt-md">
                            <p>For earlier time steps $t < T$ (recursive):</p>
                            <div class="equation-box">
                                $$\frac{\partial L}{\partial \mathbf{h}_t} = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}$$
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.15", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-l-ht"}]'>
                    <h2 class="truncate-title">The Power Problem</h2>
                    <div class="power-problem">
                        <p>Expanding the recursion reveals:</p>
                        <div class="equation-box">
                            $$\frac{\partial L}{\partial \mathbf{h}_t} = \sum_{i=t}^T \left(\mathbf{W}_{hh}^\top\right)^{T-i} \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}$$
                        </div>
                        <div class="fragment mt-md">
                            <div class="eigenvalue-demo">
                                <h4>Effect of Eigenvalues:</h4>
                                <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin: 15px 0;">
                                    <label>λ max:
                                        <input type="range" id="eigenvalue-slider" min="0.1" max="2.0" step="0.1" value="1.0" style="width: 100px;">
                                        <span id="eigen-value">1.0</span>
                                    </label>
                                    <button id="show-eigen-effect" style="background: #FC8484; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Show Effect</button>
                                </div>
                                <div id="eigenvalue-viz" style="height: 200px;"></div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Equation 9.7.16", "url": "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-chapter-recurrent-neural-networks-bptt-6"}]'>
                    <h2 class="truncate-title">Step 3: Weight Gradients</h2>
                    <div class="weight-gradients">
                        <p>Finally, gradients w.r.t. recurrent weights:</p>
                        <div class="equation-box">
                            $$\begin{aligned}
                            \frac{\partial L}{\partial \mathbf{W}_{hx}} &= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{x}_t^\top \\
                            \frac{\partial L}{\partial \mathbf{W}_{hh}} &= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{h}_{t-1}^\top
                            \end{aligned}$$
                        </div>
                        <div class="fragment mt-lg">
                            <div class="implementation-note" style="background: #10099F20; border: 2px solid #10099F; padding: 15px; border-radius: 8px;">
                                <h4>Implementation Strategy:</h4>
                                <ol style="font-size: 0.9em;">
                                    <li>Compute $\partial L/\partial \mathbf{h}_t$ recursively (backward in time)</li>
                                    <li>Cache these values to avoid recomputation</li>
                                    <li>Accumulate weight gradients using cached values</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In BPTT implementation, why are intermediate gradients cached?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To avoid duplicate calculations when computing weight gradients",
                                "correct": true,
                                "explanation": "Correct! The gradient ∂L/∂h_t is used multiple times: for computing both ∂L/∂W_hx and ∂L/∂W_hh, so caching prevents redundant computation."
                            },
                            {
                                "text": "To make forward propagation faster",
                                "correct": false,
                                "explanation": "Caching is for backward propagation efficiency. Forward propagation computes activations, not gradients."
                            },
                            {
                                "text": "To reduce memory usage",
                                "correct": false,
                                "explanation": "Caching actually increases memory usage but reduces computation time."
                            },
                            {
                                "text": "To prevent gradient explosion",
                                "correct": false,
                                "explanation": "Caching helps efficiency but doesn not prevent gradient explosion. That requires techniques like clipping or truncation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9.7.4: Challenges and Solutions (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Numerical Stability in BPTT</h2>
                    <div class="stability-overview" style="font-size: 0.75em;">
                        <h3>The Core Challenge</h3>
                        <p>Long sequences involve many matrix multiplications:</p>
                        <div class="matrix-power-viz" id="matrix-power-demo">
                            <svg width="100%" height="250" viewBox="0 0 700 250"></svg>
                        </div>
                        <div class="fragment mt-md">
                            <div class="two-column" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div class="vanishing-box" style="background: #10099F20; border: 2px solid #10099F; padding: 15px; border-radius: 8px;">
                                    <h4>Vanishing Gradients</h4>
                                    <p style="font-size: 0.9em;">Eigenvalues < 1 → 0</p>
                                    <p style="font-size: 0.85em;">Network stops learning long-term dependencies</p>
                                </div>
                                <div class="exploding-box" style="background: #FC848420; border: 2px solid #FC8484; padding: 15px; border-radius: 8px;">
                                    <h4>Exploding Gradients</h4>
                                    <p style="font-size: 0.9em;">Eigenvalues > 1 → ∞</p>
                                    <p style="font-size: 0.85em;">Training becomes unstable</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Vanishing and Exploding Gradients</h2>
                    <div class="gradient-dynamics">
                        <div class="interactive-eigenvalue">
                            <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 20px; margin-bottom: 20px; padding: 15px; background: #f9f9f9; border-radius: 8px;">
                                <label>Eigenvalue:
                                    <input type="range" id="eigen-demo" min="0.5" max="1.5" step="0.01" value="1.0" style="width: 150px;">
                                    <span id="eigen-demo-value" style="font-family: monospace;">1.00</span>
                                </label>
                                <label>Time Steps:
                                    <input type="range" id="steps-demo" min="5" max="50" step="5" value="20" style="width: 150px;">
                                    <span id="steps-demo-value" style="font-family: monospace;">20</span>
                                </label>
                            </div>
                            <div id="gradient-magnitude-plot" style="height: 300px;"></div>
                        </div>
                        <div class="fragment">
                            <p class="insight">Even small deviations from 1.0 cause exponential effects!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient Clipping</h2>
                    <div class="gradient-clipping">
                        <p>Prevent gradient explosion by limiting magnitude:</p>
                        <div class="code-example">
                            <pre><code class="language-python">def clip_gradients(gradients, max_norm):
    """Clip gradients to maximum L2 norm"""
    total_norm = torch.sqrt(
        sum(torch.sum(g**2) for g in gradients)
    )
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for g in gradients:
            g.mul_(clip_coef)
    return gradients</code></pre>
                        </div>
                        <div class="fragment mt-md">
                            <div class="clipping-viz" id="clipping-demo">
                                <div class="demo-controls" style="margin-bottom: 15px;">
                                    <label>Max Norm:
                                        <input type="range" id="clip-threshold" min="0.5" max="5" step="0.5" value="2" style="width: 150px;">
                                        <span id="clip-value">2.0</span>
                                    </label>
                                </div>
                                <svg width="100%" height="200" viewBox="0 0 600 200"></svg>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mitigation Strategies</h2>
                    <div class="mitigation-strategies">
                        <div class="strategy-grid" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
                            <div class="fragment">
                                <div class="strategy-card" style="background: #2DD2C020; border: 2px solid #2DD2C0; padding: 15px; border-radius: 8px;">
                                    <h4>Gradient Clipping</h4>
                                    <p style="font-size: 0.85em;">Limit gradient magnitude</p>
                                    <code style="font-size: 0.8em;">torch.nn.utils.clip_grad_norm_</code>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="strategy-card" style="background: #10099F20; border: 2px solid #10099F; padding: 15px; border-radius: 8px;">
                                    <h4>Truncated BPTT</h4>
                                    <p style="font-size: 0.85em;">Limit backprop steps</p>
                                    <code style="font-size: 0.8em;">detach() after τ steps</code>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="strategy-card" style="background: #FFA05F20; border: 2px solid #FFA05F; padding: 15px; border-radius: 8px;">
                                    <h4>Better Architectures</h4>
                                    <p style="font-size: 0.85em;">LSTM, GRU designs</p>
                                    <code style="font-size: 0.8em;">Gating mechanisms</code>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="strategy-card" style="background: #FAC55B20; border: 2px solid #FAC55B; padding: 15px; border-radius: 8px;">
                                    <h4>Careful Initialization</h4>
                                    <p style="font-size: 0.85em;">Control eigenvalues</p>
                                    <code style="font-size: 0.8em;">Orthogonal init</code>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: BPTT Key Takeaways</h2>
                    <div class="summary">
                        <div class="takeaway-list">
                            <div class="fragment">
                                <div class="takeaway" style="background: #f9f9f9; padding: 15px; margin: 10px 0; border-left: 4px solid #10099F;">
                                    <h4>📊 BPTT = Backprop on Unrolled RNN</h4>
                                    <p style="font-size: 0.9em;">Apply standard backprop to time-unrolled computational graph</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="takeaway" style="background: #f9f9f9; padding: 15px; margin: 10px 0; border-left: 4px solid #2DD2C0;">
                                    <h4>⚡ Truncation for Stability</h4>
                                    <p style="font-size: 0.9em;">Limit gradient propagation to τ steps for numerical stability</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="takeaway" style="background: #f9f9f9; padding: 15px; margin: 10px 0; border-left: 4px solid #FC8484;">
                                    <h4>🔢 Matrix Powers Cause Issues</h4>
                                    <p style="font-size: 0.9em;">Eigenvalues ≠ 1 lead to vanishing/exploding gradients</p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="takeaway" style="background: #f9f9f9; padding: 15px; margin: 10px 0; border-left: 4px solid #FFA05F;">
                                    <h4>💾 Cache for Efficiency</h4>
                                    <p style="font-size: 0.9em;">Store intermediate gradients to avoid redundant computation</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which combination of techniques is most effective for training RNNs on long sequences?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Gradient clipping + Truncated BPTT",
                                "correct": true,
                                "explanation": "Correct! This combination prevents gradient explosion while keeping computation tractable."
                            },
                            {
                                "text": "Using LSTM/GRU architectures",
                                "correct": true,
                                "explanation": "Correct! These architectures are specifically designed to mitigate gradient problems through gating mechanisms."
                            },
                            {
                                "text": "Orthogonal weight initialization",
                                "correct": true,
                                "explanation": "Correct! This helps maintain gradient flow by keeping eigenvalues close to 1."
                            },
                            {
                                "text": "Using larger learning rates",
                                "correct": false,
                                "explanation": "Larger learning rates can actually worsen gradient explosion problems, making training more unstable."
                            }
                        ]
                    }'></div>
                </section>
            </section>

        </div>
    </div>

    <!-- Reveal.js Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>

    <!-- D3.js for visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/d3@7/dist/d3.min.js"></script>

    <!-- Shared JavaScript -->
    <script src="../shared/js/multiple-choice.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/title-handler.js"></script>

    <!-- Reveal.js initialization -->
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes, RevealMarkdown ]
        });
    </script>

    <!-- Interactive Visualizations for Sequence Modeling -->
    <script>
        // Wait for Reveal to be ready
        Reveal.on('ready', function() {
            // Synthetic Data Visualization
            const syntheticContainer = document.getElementById('synthetic-data-viz');
            if (syntheticContainer) {
                const T = 1000;
                const time = Array.from({length: T}, (_, i) => i + 1);
                const data = time.map(t => Math.sin(0.01 * t) + (Math.random() - 0.5) * 0.4);

                // Create simple line chart
                const width = syntheticContainer.clientWidth;
                const height = 300;
                const margin = {top: 20, right: 30, bottom: 40, left: 50};
                const innerWidth = width - margin.left - margin.right;
                const innerHeight = height - margin.top - margin.bottom;

                const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                svg.setAttribute('width', width);
                svg.setAttribute('height', height);

                // Create scales
                const xScale = (x) => margin.left + (x - 1) * innerWidth / (T - 1);
                const yMin = Math.min(...data) - 0.1;
                const yMax = Math.max(...data) + 0.1;
                const yScale = (y) => margin.top + innerHeight * (1 - (y - yMin) / (yMax - yMin));

                // Draw axes
                const axesG = document.createElementNS('http://www.w3.org/2000/svg', 'g');

                // X-axis
                const xAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                xAxis.setAttribute('x1', margin.left);
                xAxis.setAttribute('y1', height - margin.bottom);
                xAxis.setAttribute('x2', width - margin.right);
                xAxis.setAttribute('y2', height - margin.bottom);
                xAxis.setAttribute('stroke', '#666');
                axesG.appendChild(xAxis);

                // Y-axis
                const yAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                yAxis.setAttribute('x1', margin.left);
                yAxis.setAttribute('y1', margin.top);
                yAxis.setAttribute('x2', margin.left);
                yAxis.setAttribute('y2', height - margin.bottom);
                yAxis.setAttribute('stroke', '#666');
                axesG.appendChild(yAxis);

                svg.appendChild(axesG);

                // Draw data line
                const path = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                const pathData = data.map((y, i) => {
                    const x = xScale(time[i]);
                    const yPos = yScale(y);
                    return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                }).join(' ');

                path.setAttribute('d', pathData);
                path.setAttribute('fill', 'none');
                path.setAttribute('stroke', '#10099F');
                path.setAttribute('stroke-width', '1.5');

                svg.appendChild(path);

                // Add labels
                const xLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                xLabel.setAttribute('x', width / 2);
                xLabel.setAttribute('y', height - 5);
                xLabel.setAttribute('text-anchor', 'middle');
                xLabel.setAttribute('font-size', '12');
                xLabel.textContent = 'Time';
                svg.appendChild(xLabel);

                const yLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                yLabel.setAttribute('x', 15);
                yLabel.setAttribute('y', height / 2);
                yLabel.setAttribute('text-anchor', 'middle');
                yLabel.setAttribute('font-size', '12');
                yLabel.setAttribute('transform', `rotate(-90, 15, ${height / 2})`);
                yLabel.textContent = 'x';
                svg.appendChild(yLabel);

                syntheticContainer.appendChild(svg);
            }

            // One-Step Prediction Visualization
            const onestepContainer = document.getElementById('onestep-viz');
            if (onestepContainer) {
                const T = 1000;
                const tau = 4;
                const time = Array.from({length: T - tau}, (_, i) => i + tau + 1);
                const trueData = time.map(t => Math.sin(0.01 * t) + (Math.random() - 0.5) * 0.4);
                const predData = time.map(t => Math.sin(0.01 * t) + (Math.random() - 0.5) * 0.1); // Less noise for predictions

                const width = onestepContainer.clientWidth;
                const height = 350;
                const margin = {top: 20, right: 100, bottom: 40, left: 50};
                const innerWidth = width - margin.left - margin.right;
                const innerHeight = height - margin.top - margin.bottom;

                const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                svg.setAttribute('width', width);
                svg.setAttribute('height', height);

                // Create scales
                const xScale = (x) => margin.left + (x - tau - 1) * innerWidth / (T - tau - 1);
                const yMin = Math.min(...trueData, ...predData) - 0.1;
                const yMax = Math.max(...trueData, ...predData) + 0.1;
                const yScale = (y) => margin.top + innerHeight * (1 - (y - yMin) / (yMax - yMin));

                // Draw axes
                const axesG = document.createElementNS('http://www.w3.org/2000/svg', 'g');

                // X-axis
                const xAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                xAxis.setAttribute('x1', margin.left);
                xAxis.setAttribute('y1', height - margin.bottom);
                xAxis.setAttribute('x2', width - margin.right);
                xAxis.setAttribute('y2', height - margin.bottom);
                xAxis.setAttribute('stroke', '#666');
                axesG.appendChild(xAxis);

                // Y-axis
                const yAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                yAxis.setAttribute('x1', margin.left);
                yAxis.setAttribute('y1', margin.top);
                yAxis.setAttribute('x2', margin.left);
                yAxis.setAttribute('y2', height - margin.bottom);
                yAxis.setAttribute('stroke', '#666');
                axesG.appendChild(yAxis);

                svg.appendChild(axesG);

                // Draw true data line
                const truePath = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                const truePathData = trueData.map((y, i) => {
                    const x = xScale(time[i]);
                    const yPos = yScale(y);
                    return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                }).join(' ');

                truePath.setAttribute('d', truePathData);
                truePath.setAttribute('fill', 'none');
                truePath.setAttribute('stroke', '#10099F');
                truePath.setAttribute('stroke-width', '1.5');
                truePath.setAttribute('opacity', '0.7');

                svg.appendChild(truePath);

                // Draw prediction line
                const predPath = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                const predPathData = predData.map((y, i) => {
                    const x = xScale(time[i]);
                    const yPos = yScale(y);
                    return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                }).join(' ');

                predPath.setAttribute('d', predPathData);
                predPath.setAttribute('fill', 'none');
                predPath.setAttribute('stroke', '#FC8484');
                predPath.setAttribute('stroke-width', '1.5');
                predPath.setAttribute('stroke-dasharray', '5,5');

                svg.appendChild(predPath);

                // Add legend
                const legendG = document.createElementNS('http://www.w3.org/2000/svg', 'g');
                legendG.setAttribute('transform', `translate(${width - margin.right + 10}, ${margin.top})`);

                // True data legend
                const trueLegendLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                trueLegendLine.setAttribute('x1', 0);
                trueLegendLine.setAttribute('y1', 0);
                trueLegendLine.setAttribute('x2', 20);
                trueLegendLine.setAttribute('y2', 0);
                trueLegendLine.setAttribute('stroke', '#10099F');
                trueLegendLine.setAttribute('stroke-width', '1.5');
                legendG.appendChild(trueLegendLine);

                const trueLegendText = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                trueLegendText.setAttribute('x', 25);
                trueLegendText.setAttribute('y', 4);
                trueLegendText.setAttribute('font-size', '12');
                trueLegendText.textContent = 'Labels';
                legendG.appendChild(trueLegendText);

                // Pred data legend
                const predLegendLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                predLegendLine.setAttribute('x1', 0);
                predLegendLine.setAttribute('y1', 20);
                predLegendLine.setAttribute('x2', 20);
                predLegendLine.setAttribute('y2', 20);
                predLegendLine.setAttribute('stroke', '#FC8484');
                predLegendLine.setAttribute('stroke-width', '1.5');
                predLegendLine.setAttribute('stroke-dasharray', '5,5');
                legendG.appendChild(predLegendLine);

                const predLegendText = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                predLegendText.setAttribute('x', 25);
                predLegendText.setAttribute('y', 24);
                predLegendText.setAttribute('font-size', '12');
                predLegendText.textContent = '1-step';
                legendG.appendChild(predLegendText);

                svg.appendChild(legendG);

                // Add labels
                const xLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                xLabel.setAttribute('x', width / 2);
                xLabel.setAttribute('y', height - 5);
                xLabel.setAttribute('text-anchor', 'middle');
                xLabel.setAttribute('font-size', '12');
                xLabel.textContent = 'Time';
                svg.appendChild(xLabel);

                onestepContainer.appendChild(svg);
            }

            // Multi-Step Prediction Visualization
            const multistepContainer = document.getElementById('multistep-viz');
            if (multistepContainer) {
                const T = 1000;
                const tau = 4;
                const numTrain = 600;
                const time = Array.from({length: T - numTrain - tau}, (_, i) => i + numTrain + tau + 1);
                const onestepData = time.map(t => Math.sin(0.01 * t) + (Math.random() - 0.5) * 0.1);
                // Multi-step predictions decay
                const multistepData = time.map((t, i) => {
                    const decay = Math.exp(-i * 0.05);
                    return Math.sin(0.01 * time[0]) * decay + 0.5 * (1 - decay);
                });

                const width = multistepContainer.clientWidth;
                const height = 350;
                const margin = {top: 20, right: 100, bottom: 40, left: 50};
                const innerWidth = width - margin.left - margin.right;
                const innerHeight = height - margin.top - margin.bottom;

                const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                svg.setAttribute('width', width);
                svg.setAttribute('height', height);

                // Create scales
                const xScale = (x) => margin.left + (x - time[0]) * innerWidth / (time[time.length - 1] - time[0]);
                const yMin = Math.min(...onestepData, ...multistepData) - 0.1;
                const yMax = Math.max(...onestepData, ...multistepData) + 0.1;
                const yScale = (y) => margin.top + innerHeight * (1 - (y - yMin) / (yMax - yMin));

                // Draw axes
                const axesG = document.createElementNS('http://www.w3.org/2000/svg', 'g');

                // X-axis
                const xAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                xAxis.setAttribute('x1', margin.left);
                xAxis.setAttribute('y1', height - margin.bottom);
                xAxis.setAttribute('x2', width - margin.right);
                xAxis.setAttribute('y2', height - margin.bottom);
                xAxis.setAttribute('stroke', '#666');
                axesG.appendChild(xAxis);

                // Y-axis
                const yAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                yAxis.setAttribute('x1', margin.left);
                yAxis.setAttribute('y1', margin.top);
                yAxis.setAttribute('x2', margin.left);
                yAxis.setAttribute('y2', height - margin.bottom);
                yAxis.setAttribute('stroke', '#666');
                axesG.appendChild(yAxis);

                svg.appendChild(axesG);

                // Draw one-step line
                const onestepPath = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                const onestepPathData = onestepData.map((y, i) => {
                    const x = xScale(time[i]);
                    const yPos = yScale(y);
                    return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                }).join(' ');

                onestepPath.setAttribute('d', onestepPathData);
                onestepPath.setAttribute('fill', 'none');
                onestepPath.setAttribute('stroke', '#10099F');
                onestepPath.setAttribute('stroke-width', '1.5');

                svg.appendChild(onestepPath);

                // Draw multi-step line
                const multistepPath = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                const multistepPathData = multistepData.map((y, i) => {
                    const x = xScale(time[i]);
                    const yPos = yScale(y);
                    return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                }).join(' ');

                multistepPath.setAttribute('d', multistepPathData);
                multistepPath.setAttribute('fill', 'none');
                multistepPath.setAttribute('stroke', '#FC8484');
                multistepPath.setAttribute('stroke-width', '2');

                svg.appendChild(multistepPath);

                // Add legend
                const legendG = document.createElementNS('http://www.w3.org/2000/svg', 'g');
                legendG.setAttribute('transform', `translate(${width - margin.right + 10}, ${margin.top})`);

                // One-step legend
                const onestepLegendLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                onestepLegendLine.setAttribute('x1', 0);
                onestepLegendLine.setAttribute('y1', 0);
                onestepLegendLine.setAttribute('x2', 20);
                onestepLegendLine.setAttribute('y2', 0);
                onestepLegendLine.setAttribute('stroke', '#10099F');
                onestepLegendLine.setAttribute('stroke-width', '1.5');
                legendG.appendChild(onestepLegendLine);

                const onestepLegendText = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                onestepLegendText.setAttribute('x', 25);
                onestepLegendText.setAttribute('y', 4);
                onestepLegendText.setAttribute('font-size', '12');
                onestepLegendText.textContent = '1-step';
                legendG.appendChild(onestepLegendText);

                // Multi-step legend
                const multistepLegendLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                multistepLegendLine.setAttribute('x1', 0);
                multistepLegendLine.setAttribute('y1', 20);
                multistepLegendLine.setAttribute('x2', 20);
                multistepLegendLine.setAttribute('y2', 20);
                multistepLegendLine.setAttribute('stroke', '#FC8484');
                multistepLegendLine.setAttribute('stroke-width', '2');
                legendG.appendChild(multistepLegendLine);

                const multistepLegendText = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                multistepLegendText.setAttribute('x', 25);
                multistepLegendText.setAttribute('y', 24);
                multistepLegendText.setAttribute('font-size', '12');
                multistepLegendText.textContent = 'Multi-step';
                legendG.appendChild(multistepLegendText);

                svg.appendChild(legendG);

                multistepContainer.appendChild(svg);
            }

            // k-Step Comparison Visualization
            const kstepContainer = document.getElementById('kstep-comparison-viz');
            if (kstepContainer) {
                const T = 1000;
                const tau = 4;
                const steps = [1, 4, 16, 64];
                const colors = ['#10099F', '#2DD2C0', '#FFA05F', '#FC8484'];
                const time = Array.from({length: T - tau - 64}, (_, i) => i + tau + 65);

                // Generate data for different k values
                const kstepData = steps.map(k => {
                    return time.map((t, i) => {
                        const decay = Math.exp(-i * 0.001 * k);
                        const base = Math.sin(0.01 * t);
                        const noise = (Math.random() - 0.5) * 0.2 * Math.sqrt(k / 64);
                        return base * decay + 0.5 * (1 - decay) + noise;
                    });
                });

                const width = kstepContainer.clientWidth;
                const height = 400;
                const margin = {top: 20, right: 100, bottom: 40, left: 50};
                const innerWidth = width - margin.left - margin.right;
                const innerHeight = height - margin.top - margin.bottom;

                const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                svg.setAttribute('width', width);
                svg.setAttribute('height', height);

                // Create scales
                const xScale = (x) => margin.left + (x - time[0]) * innerWidth / (time[time.length - 1] - time[0]);
                const yMin = Math.min(...kstepData.flat()) - 0.1;
                const yMax = Math.max(...kstepData.flat()) + 0.1;
                const yScale = (y) => margin.top + innerHeight * (1 - (y - yMin) / (yMax - yMin));

                // Draw axes
                const axesG = document.createElementNS('http://www.w3.org/2000/svg', 'g');

                // X-axis
                const xAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                xAxis.setAttribute('x1', margin.left);
                xAxis.setAttribute('y1', height - margin.bottom);
                xAxis.setAttribute('x2', width - margin.right);
                xAxis.setAttribute('y2', height - margin.bottom);
                xAxis.setAttribute('stroke', '#666');
                axesG.appendChild(xAxis);

                // Y-axis
                const yAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                yAxis.setAttribute('x1', margin.left);
                yAxis.setAttribute('y1', margin.top);
                yAxis.setAttribute('x2', margin.left);
                yAxis.setAttribute('y2', height - margin.bottom);
                yAxis.setAttribute('stroke', '#666');
                axesG.appendChild(yAxis);

                svg.appendChild(axesG);

                // Draw lines for each k
                kstepData.forEach((data, kIdx) => {
                    const path = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                    const pathData = data.map((y, i) => {
                        const x = xScale(time[i]);
                        const yPos = yScale(y);
                        return `${i === 0 ? 'M' : 'L'} ${x},${yPos}`;
                    }).join(' ');

                    path.setAttribute('d', pathData);
                    path.setAttribute('fill', 'none');
                    path.setAttribute('stroke', colors[kIdx]);
                    path.setAttribute('stroke-width', '1.5');
                    path.setAttribute('opacity', 1 - kIdx * 0.15);

                    svg.appendChild(path);
                });

                // Add legend
                const legendG = document.createElementNS('http://www.w3.org/2000/svg', 'g');
                legendG.setAttribute('transform', `translate(${width - margin.right + 10}, ${margin.top})`);

                steps.forEach((k, idx) => {
                    // Legend line
                    const legendLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                    legendLine.setAttribute('x1', 0);
                    legendLine.setAttribute('y1', idx * 20);
                    legendLine.setAttribute('x2', 20);
                    legendLine.setAttribute('y2', idx * 20);
                    legendLine.setAttribute('stroke', colors[idx]);
                    legendLine.setAttribute('stroke-width', '1.5');
                    legendG.appendChild(legendLine);

                    // Legend text
                    const legendText = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                    legendText.setAttribute('x', 25);
                    legendText.setAttribute('y', idx * 20 + 4);
                    legendText.setAttribute('font-size', '12');
                    legendText.textContent = `${k}-step`;
                    legendG.appendChild(legendText);
                });

                svg.appendChild(legendG);

                // Add labels
                const xLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                xLabel.setAttribute('x', width / 2);
                xLabel.setAttribute('y', height - 5);
                xLabel.setAttribute('text-anchor', 'middle');
                xLabel.setAttribute('font-size', '12');
                xLabel.textContent = 'Time';
                svg.appendChild(xLabel);

                kstepContainer.appendChild(svg);
            }
        });

        // ===============================================
        // Text Processing Interactive Visualizations
        // ===============================================

        // Section 14: Pipeline Visualization
        document.addEventListener('DOMContentLoaded', function() {
            const pipelineSvg = document.getElementById('pipeline-svg');
            const pipelineStart = document.getElementById('pipeline-start');
            const pipelineReset = document.getElementById('pipeline-reset');

            if (pipelineSvg && pipelineStart && pipelineReset) {
                let animationInProgress = false;

                function initPipeline() {
                    pipelineSvg.innerHTML = '';
                    const width = pipelineSvg.clientWidth || 800;
                    const height = 400;

                    // Create pipeline stages
                    const stages = [
                        { id: 'load', text: 'Load Text', color: '#10099F' },
                        { id: 'tokenize', text: 'Tokenize', color: '#2DD2C0' },
                        { id: 'vocab', text: 'Build Vocab', color: '#FAC55B' },
                        { id: 'indices', text: 'Convert', color: '#FC8484' }
                    ];

                    const stageWidth = width / 5;
                    const stageHeight = 60;
                    const y = height / 2 - stageHeight / 2;

                    stages.forEach((stage, i) => {
                        const x = (i + 0.5) * stageWidth;

                        // Stage rectangle
                        const rect = document.createElementNS('http://www.w3.org/2000/svg', 'rect');
                        rect.setAttribute('x', x);
                        rect.setAttribute('y', y);
                        rect.setAttribute('width', stageWidth - 20);
                        rect.setAttribute('height', stageHeight);
                        rect.setAttribute('fill', stage.color);
                        rect.setAttribute('opacity', '0.3');
                        rect.setAttribute('rx', '8');
                        rect.setAttribute('id', `stage-${stage.id}`);
                        pipelineSvg.appendChild(rect);

                        // Stage text
                        const text = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                        text.setAttribute('x', x + stageWidth/2 - 10);
                        text.setAttribute('y', y + stageHeight/2 + 5);
                        text.setAttribute('text-anchor', 'middle');
                        text.setAttribute('fill', 'white');
                        text.setAttribute('font-weight', 'bold');
                        text.textContent = stage.text;
                        pipelineSvg.appendChild(text);

                        // Arrow
                        if (i < stages.length - 1) {
                            const arrow = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                            const arrowPath = `M ${x + stageWidth - 20} ${y + stageHeight/2} L ${x + stageWidth} ${y + stageHeight/2}`;
                            arrow.setAttribute('d', arrowPath);
                            arrow.setAttribute('stroke', '#666');
                            arrow.setAttribute('stroke-width', '2');
                            arrow.setAttribute('marker-end', 'url(#arrowhead)');
                            pipelineSvg.appendChild(arrow);
                        }
                    });

                    // Add arrowhead marker
                    const defs = document.createElementNS('http://www.w3.org/2000/svg', 'defs');
                    const marker = document.createElementNS('http://www.w3.org/2000/svg', 'marker');
                    marker.setAttribute('id', 'arrowhead');
                    marker.setAttribute('markerWidth', '10');
                    marker.setAttribute('markerHeight', '7');
                    marker.setAttribute('refX', '9');
                    marker.setAttribute('refY', '3.5');
                    marker.setAttribute('orient', 'auto');

                    const polygon = document.createElementNS('http://www.w3.org/2000/svg', 'polygon');
                    polygon.setAttribute('points', '0 0, 10 3.5, 0 7');
                    polygon.setAttribute('fill', '#666');
                    marker.appendChild(polygon);
                    defs.appendChild(marker);
                    pipelineSvg.appendChild(defs);
                }

                function animatePipeline() {
                    if (animationInProgress) return;
                    animationInProgress = true;

                    const stages = ['load', 'tokenize', 'vocab', 'indices'];
                    let currentStage = 0;

                    function highlightStage() {
                        if (currentStage < stages.length) {
                            const rect = document.getElementById(`stage-${stages[currentStage]}`);
                            if (rect) {
                                rect.setAttribute('opacity', '1');
                                currentStage++;
                                setTimeout(highlightStage, 800);
                            }
                        } else {
                            animationInProgress = false;
                        }
                    }

                    highlightStage();
                }

                initPipeline();

                pipelineStart.addEventListener('click', animatePipeline);
                pipelineReset.addEventListener('click', () => {
                    animationInProgress = false;
                    initPipeline();
                });
            }
        });

        // Section 15: Text Preprocessing Demo
        document.addEventListener('DOMContentLoaded', function() {
            const preprocessInput = document.getElementById('preprocess-input');
            const preprocessBtn = document.getElementById('preprocess-btn');
            const originalText = document.getElementById('original-text');
            const processedText = document.getElementById('processed-text');

            if (preprocessInput && preprocessBtn && originalText && processedText) {
                function preprocessText() {
                    const text = preprocessInput.value;
                    originalText.textContent = text;
                    // Simulate preprocessing: remove non-alphabetic and lowercase
                    const processed = text.replace(/[^A-Za-z]+/g, ' ').toLowerCase().trim();
                    processedText.textContent = processed;
                }

                preprocessBtn.addEventListener('click', preprocessText);
                preprocessText(); // Initial run
            }
        });

        // Section 16: Tokenization Comparison
        document.addEventListener('DOMContentLoaded', function() {
            const tokenInput = document.getElementById('token-input');
            const tokenizeBtn = document.getElementById('tokenize-btn');
            const charTokens = document.getElementById('char-tokens');
            const charCount = document.getElementById('char-count');
            const wordTokens = document.getElementById('word-tokens');
            const wordCount = document.getElementById('word-count');

            if (tokenInput && tokenizeBtn) {
                function tokenizeText() {
                    const text = tokenInput.value;

                    // Character tokenization
                    const chars = text.split('');
                    if (charTokens && charCount) {
                        charTokens.textContent = JSON.stringify(chars);
                        charCount.textContent = `${chars.length} tokens`;
                    }

                    // Word tokenization
                    const words = text.split(/\s+/).filter(w => w.length > 0);
                    if (wordTokens && wordCount) {
                        wordTokens.textContent = JSON.stringify(words);
                        wordCount.textContent = `${words.length} tokens`;
                    }
                }

                tokenizeBtn.addEventListener('click', tokenizeText);
                tokenizeText(); // Initial run
            }
        });

        // Section 17: Interactive Vocabulary Builder
        document.addEventListener('DOMContentLoaded', function() {
            const vocabInput = document.getElementById('vocab-input');
            const minFreqInput = document.getElementById('min-freq');
            const buildVocabBtn = document.getElementById('build-vocab-btn');
            const tokenFreqs = document.getElementById('token-freqs');
            const tokenMapping = document.getElementById('token-mapping');

            if (vocabInput && buildVocabBtn && tokenFreqs && tokenMapping) {
                function buildVocabulary() {
                    const text = vocabInput.value.toLowerCase();
                    const minFreq = parseInt(minFreqInput.value) || 1;
                    const tokens = text.split(/\s+/);

                    // Count frequencies
                    const freqMap = {};
                    tokens.forEach(token => {
                        freqMap[token] = (freqMap[token] || 0) + 1;
                    });

                    // Sort by frequency
                    const sortedFreqs = Object.entries(freqMap)
                        .filter(([_, freq]) => freq >= minFreq)
                        .sort((a, b) => b[1] - a[1]);

                    // Display frequencies
                    tokenFreqs.innerHTML = sortedFreqs
                        .map(([token, freq]) => `'${token}': ${freq}`)
                        .join('<br>');

                    // Create index mapping
                    const vocab = ['<unk>', ...sortedFreqs.map(([token]) => token)];
                    const mapping = {};
                    vocab.forEach((token, idx) => {
                        mapping[token] = idx;
                    });

                    // Display mapping
                    tokenMapping.innerHTML = Object.entries(mapping)
                        .map(([token, idx]) => `'${token}' → ${idx}`)
                        .join('<br>');
                }

                buildVocabBtn.addEventListener('click', buildVocabulary);
                buildVocabulary(); // Initial run
            }
        });

        // Section 18: Zipf's Law Visualization
        document.addEventListener('DOMContentLoaded', function() {
            const zipfAlpha = document.getElementById('zipf-alpha');
            const alphaValue = document.getElementById('alpha-value');
            const zipfScale = document.getElementById('zipf-scale');
            const zipfPlot = document.getElementById('zipf-plot');

            if (zipfAlpha && zipfPlot) {
                function updateZipfPlot() {
                    const alpha = parseFloat(zipfAlpha.value);
                    const isLogScale = zipfScale.value === 'log';
                    if (alphaValue) alphaValue.textContent = alpha.toFixed(1);

                    // Generate Zipf distribution
                    const n = 100;
                    const data = [];
                    for (let i = 1; i <= n; i++) {
                        const freq = 1000 / Math.pow(i, alpha);
                        data.push({ rank: i, freq: freq });
                    }

                    // Clear and create SVG
                    zipfPlot.innerHTML = '';
                    const width = zipfPlot.clientWidth || 800;
                    const height = 400;
                    const margin = { top: 20, right: 20, bottom: 40, left: 60 };
                    const innerWidth = width - margin.left - margin.right;
                    const innerHeight = height - margin.top - margin.bottom;

                    const svg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                    svg.setAttribute('width', width);
                    svg.setAttribute('height', height);

                    // Scales
                    const xScale = (x) => {
                        const val = isLogScale ? Math.log10(x) : x;
                        const max = isLogScale ? Math.log10(n) : n;
                        return margin.left + (val / max) * innerWidth;
                    };

                    const yScale = (y) => {
                        const val = isLogScale ? Math.log10(y + 1) : y;
                        const max = isLogScale ? Math.log10(1001) : 1000;
                        return margin.top + innerHeight * (1 - val / max);
                    };

                    // Draw axes
                    const xAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                    xAxis.setAttribute('x1', margin.left);
                    xAxis.setAttribute('y1', height - margin.bottom);
                    xAxis.setAttribute('x2', width - margin.right);
                    xAxis.setAttribute('y2', height - margin.bottom);
                    xAxis.setAttribute('stroke', '#666');
                    svg.appendChild(xAxis);

                    const yAxis = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                    yAxis.setAttribute('x1', margin.left);
                    yAxis.setAttribute('y1', margin.top);
                    yAxis.setAttribute('x2', margin.left);
                    yAxis.setAttribute('y2', height - margin.bottom);
                    yAxis.setAttribute('stroke', '#666');
                    svg.appendChild(yAxis);

                    // Draw data points
                    const pathData = data.map((d, i) => {
                        const x = xScale(d.rank);
                        const y = yScale(d.freq);
                        return `${i === 0 ? 'M' : 'L'} ${x},${y}`;
                    }).join(' ');

                    const path = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                    path.setAttribute('d', pathData);
                    path.setAttribute('fill', 'none');
                    path.setAttribute('stroke', '#10099F');
                    path.setAttribute('stroke-width', '2');
                    svg.appendChild(path);

                    // Labels
                    const xLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                    xLabel.setAttribute('x', width / 2);
                    xLabel.setAttribute('y', height - 5);
                    xLabel.setAttribute('text-anchor', 'middle');
                    xLabel.textContent = isLogScale ? 'log(rank)' : 'Word Rank';
                    svg.appendChild(xLabel);

                    const yLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                    yLabel.setAttribute('x', 15);
                    yLabel.setAttribute('y', height / 2);
                    yLabel.setAttribute('text-anchor', 'middle');
                    yLabel.setAttribute('transform', `rotate(-90 15 ${height/2})`);
                    yLabel.textContent = isLogScale ? 'log(frequency)' : 'Frequency';
                    svg.appendChild(yLabel);

                    zipfPlot.appendChild(svg);
                }

                zipfAlpha.addEventListener('input', updateZipfPlot);
                zipfScale.addEventListener('change', updateZipfPlot);
                updateZipfPlot(); // Initial plot
            }
        });

        // Section 19: N-gram Generator
        document.addEventListener('DOMContentLoaded', function() {
            const ngramInput = document.getElementById('ngram-input');
            const ngramN = document.getElementById('ngram-n');
            const generateBtn = document.getElementById('generate-ngrams');
            const ngramList = document.getElementById('ngram-list');
            const ngramStats = document.getElementById('ngram-stats');

            if (ngramInput && generateBtn && ngramList) {
                function generateNgrams() {
                    const text = ngramInput.value.toLowerCase();
                    const n = parseInt(ngramN.value);
                    const words = text.split(/\s+/).filter(w => w.length > 0);

                    const ngrams = [];
                    for (let i = 0; i <= words.length - n; i++) {
                        const ngram = words.slice(i, i + n).join(' ');
                        ngrams.push(ngram);
                    }

                    ngramList.textContent = JSON.stringify(ngrams, null, 2);
                    if (ngramStats) {
                        const label = n === 1 ? 'unigrams' : n === 2 ? 'bigrams' : 'trigrams';
                        ngramStats.textContent = `Generated ${ngrams.length} ${label}`;
                    }
                }

                generateBtn.addEventListener('click', generateNgrams);
                generateNgrams(); // Initial run
            }
        });

        // Section 20: Complete Pipeline Demo
        document.addEventListener('DOMContentLoaded', function() {
            const pipelineText = document.getElementById('pipeline-text');
            const runPipelineBtn = document.getElementById('run-pipeline');
            const step1Result = document.getElementById('step1-result');
            const step2Result = document.getElementById('step2-result');
            const step3Result = document.getElementById('step3-result');
            const step4Result = document.getElementById('step4-result');

            if (pipelineText && runPipelineBtn) {
                function runCompletePipeline() {
                    const rawText = pipelineText.value;

                    // Step 1: Preprocess
                    const preprocessed = rawText.replace(/[^A-Za-z]+/g, ' ').toLowerCase().trim();
                    if (step1Result) step1Result.textContent = preprocessed;

                    // Step 2: Tokenize (character-level)
                    const tokens = preprocessed.split('');
                    if (step2Result) step2Result.textContent = JSON.stringify(tokens.slice(0, 20)) + '...';

                    // Step 3: Build vocabulary
                    const uniqueTokens = [...new Set(tokens)].sort();
                    const vocab = ['<unk>', ...uniqueTokens];
                    if (step3Result) step3Result.textContent = JSON.stringify(vocab);

                    // Step 4: Convert to indices
                    const tokenToIdx = {};
                    vocab.forEach((token, idx) => {
                        tokenToIdx[token] = idx;
                    });
                    const indices = tokens.slice(0, 20).map(t => tokenToIdx[t] || 0);
                    if (step4Result) step4Result.textContent = JSON.stringify(indices) + '...';
                }

                runPipelineBtn.addEventListener('click', runCompletePipeline);
                runCompletePipeline(); // Initial run
            }
        });

        // Section 22-28: Language Models Interactive Visualizations

        // N-gram Model Generator (Section 23)
        document.addEventListener('DOMContentLoaded', function() {
            const ngramModelInput = document.getElementById('ngram-model-input');
            const ngramModelSize = document.getElementById('ngram-model-size');
            const generateNgramModel = document.getElementById('generate-ngram-model');
            const ngramModelList = document.getElementById('ngram-model-list');
            const ngramModelFreq = document.getElementById('ngram-model-freq');

            if (generateNgramModel) {
                function generateNgrams() {
                    const text = ngramModelInput.value.toLowerCase();
                    const n = parseInt(ngramModelSize.value);
                    const words = text.split(/\s+/).filter(w => w.length > 0);

                    const ngrams = [];
                    const freqMap = {};

                    if (n === 1) {
                        // Unigram
                        words.forEach(word => {
                            ngrams.push(word);
                            freqMap[word] = (freqMap[word] || 0) + 1;
                        });
                    } else {
                        // Bigram, Trigram
                        for (let i = 0; i <= words.length - n; i++) {
                            const ngram = words.slice(i, i + n).join(' ');
                            ngrams.push(ngram);
                            freqMap[ngram] = (freqMap[ngram] || 0) + 1;
                        }
                    }

                    // Display n-grams
                    ngramModelList.innerHTML = ngrams.map(ng => `'${ng}'`).join('<br>');

                    // Display frequencies
                    const sortedFreq = Object.entries(freqMap)
                        .sort((a, b) => b[1] - a[1])
                        .map(([ngram, count]) => `'${ngram}': ${count}`)
                        .join('<br>');
                    ngramModelFreq.innerHTML = sortedFreq;
                }

                generateNgramModel.addEventListener('click', generateNgrams);
                generateNgrams(); // Initial generation
            }
        });

        // Word Frequency Counter (Section 24)
        document.addEventListener('DOMContentLoaded', function() {
            const freqCounterInput = document.getElementById('freq-counter-input');
            const computeFrequencies = document.getElementById('compute-frequencies');
            const unigramCounts = document.getElementById('unigram-counts');
            const bigramCounts = document.getElementById('bigram-counts');
            const totalWords = document.getElementById('total-words');
            const uniqueUnigrams = document.getElementById('unique-unigrams');
            const uniqueBigrams = document.getElementById('unique-bigrams');
            const sparsityRatio = document.getElementById('sparsity-ratio');

            if (computeFrequencies) {
                function compute() {
                    const text = freqCounterInput.value.toLowerCase();
                    const words = text.match(/\b\w+\b/g) || [];

                    // Unigram counts
                    const unigramFreq = {};
                    words.forEach(word => {
                        unigramFreq[word] = (unigramFreq[word] || 0) + 1;
                    });

                    // Bigram counts
                    const bigramFreq = {};
                    for (let i = 0; i < words.length - 1; i++) {
                        const bigram = words[i] + ' ' + words[i + 1];
                        bigramFreq[bigram] = (bigramFreq[bigram] || 0) + 1;
                    }

                    // Display unigrams
                    const sortedUnigrams = Object.entries(unigramFreq)
                        .sort((a, b) => b[1] - a[1])
                        .slice(0, 10)
                        .map(([word, count]) => `${word}: ${count}`)
                        .join('\n');
                    unigramCounts.textContent = sortedUnigrams;

                    // Display bigrams
                    const sortedBigrams = Object.entries(bigramFreq)
                        .sort((a, b) => b[1] - a[1])
                        .slice(0, 10)
                        .map(([bigram, count]) => `${bigram}: ${count}`)
                        .join('\n');
                    bigramCounts.textContent = sortedBigrams;

                    // Statistics
                    totalWords.textContent = words.length;
                    uniqueUnigrams.textContent = Object.keys(unigramFreq).length;
                    uniqueBigrams.textContent = Object.keys(bigramFreq).length;

                    const possibleBigrams = Object.keys(unigramFreq).length * Object.keys(unigramFreq).length;
                    const sparsity = ((1 - Object.keys(bigramFreq).length / possibleBigrams) * 100).toFixed(1);
                    sparsityRatio.textContent = sparsity + '%';
                }

                computeFrequencies.addEventListener('click', compute);
                compute(); // Initial computation
            }
        });

        // Laplace Smoothing Demo (Section 25)
        document.addEventListener('DOMContentLoaded', function() {
            const wordCountSlider = document.getElementById('word-count-slider');
            const wordCountValue = document.getElementById('word-count-value');
            const epsilonSlider = document.getElementById('epsilon-slider');
            const epsilonValue = document.getElementById('epsilon-value');
            const vocabSizeSlider = document.getElementById('vocab-size-slider');
            const vocabSizeValue = document.getElementById('vocab-size-value');
            const unsmoothedSeen = document.getElementById('unsmoothed-seen');
            const unsmoothedUnseen = document.getElementById('unsmoothed-unseen');
            const smoothedSeen = document.getElementById('smoothed-seen');
            const smoothedUnseen = document.getElementById('smoothed-unseen');

            function updateSmoothing() {
                if (!wordCountSlider) return;

                const n = parseFloat(wordCountSlider.value);
                const epsilon = parseFloat(epsilonSlider.value);
                const m = parseFloat(vocabSizeSlider.value);
                const seenCount = 10; // Example count for a seen word

                // Update display values
                wordCountValue.textContent = n;
                epsilonValue.textContent = epsilon.toFixed(1);
                vocabSizeValue.textContent = m;

                // Calculate unsmoothed probabilities
                const unsmoothedPSeen = seenCount / n;
                const unsmoothedPUnseen = 0;

                // Calculate smoothed probabilities
                const smoothedPSeen = (seenCount + epsilon/m) / (n + epsilon);
                const smoothedPUnseen = (0 + epsilon/m) / (n + epsilon);

                // Update displays
                unsmoothedSeen.textContent = unsmoothedPSeen.toFixed(3);
                unsmoothedUnseen.textContent = unsmoothedPUnseen.toFixed(3);
                smoothedSeen.textContent = smoothedPSeen.toFixed(3);
                smoothedUnseen.textContent = smoothedPUnseen.toFixed(3);
            }

            if (wordCountSlider) {
                wordCountSlider.addEventListener('input', updateSmoothing);
                epsilonSlider.addEventListener('input', updateSmoothing);
                vocabSizeSlider.addEventListener('input', updateSmoothing);
                updateSmoothing(); // Initial update
            }
        });

        // Perplexity Calculator (Section 26)
        document.addEventListener('DOMContentLoaded', function() {
            const prob1 = document.getElementById('prob1');
            const prob2 = document.getElementById('prob2');
            const prob3 = document.getElementById('prob3');
            const prob4 = document.getElementById('prob4');
            const prob5 = document.getElementById('prob5');
            const prob1Value = document.getElementById('prob1-value');
            const prob2Value = document.getElementById('prob2-value');
            const prob3Value = document.getElementById('prob3-value');
            const prob4Value = document.getElementById('prob4-value');
            const prob5Value = document.getElementById('prob5-value');
            const individualLosses = document.getElementById('individual-losses');
            const avgLoss = document.getElementById('avg-loss');
            const perplexityValue = document.getElementById('perplexity-value');

            function updatePerplexity() {
                if (!prob1) return;

                const probs = [
                    parseFloat(prob1.value),
                    parseFloat(prob2.value),
                    parseFloat(prob3.value),
                    parseFloat(prob4.value),
                    parseFloat(prob5.value)
                ];

                // Update displayed values
                prob1Value.textContent = probs[0].toFixed(2);
                prob2Value.textContent = probs[1].toFixed(2);
                prob3Value.textContent = probs[2].toFixed(2);
                prob4Value.textContent = probs[3].toFixed(2);
                prob5Value.textContent = probs[4].toFixed(2);

                // Calculate individual losses
                const losses = probs.map(p => -Math.log(p));
                const lossesText = losses.map((l, i) => `L${i+1} = ${l.toFixed(2)}`).join('\n');
                individualLosses.textContent = lossesText;

                // Calculate average loss
                const averageLoss = losses.reduce((a, b) => a + b, 0) / losses.length;
                avgLoss.textContent = averageLoss.toFixed(2);

                // Calculate perplexity
                const perplexity = Math.exp(averageLoss);
                perplexityValue.textContent = perplexity.toFixed(1);
            }

            if (prob1) {
                [prob1, prob2, prob3, prob4, prob5].forEach(slider => {
                    slider.addEventListener('input', updatePerplexity);
                });
                updatePerplexity(); // Initial calculation
            }
        });

        // Perplexity Comparison Chart (Section 26)
        document.addEventListener('DOMContentLoaded', function() {
            const perplexityComparison = document.getElementById('perplexity-comparison');

            if (perplexityComparison) {
                // Create a simple bar chart
                const models = [
                    { name: 'Random (|V|=10000)', ppl: 10000, color: '#FC8484' },
                    { name: 'Unigram', ppl: 962, color: '#FFA05F' },
                    { name: 'Bigram', ppl: 137, color: '#FAC55B' },
                    { name: 'Trigram', ppl: 73, color: '#2DD2C0' },
                    { name: 'Neural LM', ppl: 32, color: '#10099F' }
                ];

                const maxPPL = 1000; // Cap for visualization
                const chartHTML = models.map(model => {
                    const barWidth = Math.min(model.ppl, maxPPL) / maxPPL * 100;
                    const displayPPL = model.ppl > maxPPL ? `>${maxPPL}` : model.ppl;
                    return `
                        <div style="margin: 10px 0;">
                            <div style="display: flex; align-items: center;">
                                <div style="width: 150px; text-align: right; padding-right: 10px;">
                                    ${model.name}
                                </div>
                                <div style="flex: 1; background: #f5f5f5; height: 30px; position: relative;">
                                    <div style="width: ${barWidth}%; background: ${model.color}; height: 100%; display: flex; align-items: center; padding-left: 5px;">
                                        <span style="color: white; font-weight: bold;">${displayPPL}</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }).join('');

                perplexityComparison.innerHTML = chartHTML;
            }
        });

        // RNN Computation Visualization (Section 9.4.2)
        document.addEventListener('DOMContentLoaded', function() {
            const vizContainer = document.getElementById('rnn-computation-viz');

            if (vizContainer) {
                const width = 700;
                const height = 400;

                // Create SVG
                const svg = d3.select(vizContainer)
                    .append('svg')
                    .attr('width', width)
                    .attr('height', height);

                // Define time steps
                const timeSteps = 3;
                const stepWidth = width / (timeSteps + 1);

                // Node positions
                const nodes = [];
                const connections = [];

                // Create nodes for each time step
                for (let t = 0; t < timeSteps; t++) {
                    const x = (t + 1) * stepWidth;

                    // Input node
                    nodes.push({
                        id: `x${t}`,
                        label: `x_${t}`,
                        x: x,
                        y: 320,
                        type: 'input'
                    });

                    // Hidden state node
                    nodes.push({
                        id: `h${t}`,
                        label: `h_${t}`,
                        x: x,
                        y: 200,
                        type: 'hidden'
                    });

                    // Output node
                    nodes.push({
                        id: `o${t}`,
                        label: `o_${t}`,
                        x: x,
                        y: 80,
                        type: 'output'
                    });

                    // Connections
                    connections.push({from: `x${t}`, to: `h${t}`}); // Input to hidden
                    connections.push({from: `h${t}`, to: `o${t}`}); // Hidden to output

                    // Recurrent connection
                    if (t > 0) {
                        connections.push({from: `h${t-1}`, to: `h${t}`, recurrent: true});
                    }
                }

                // Draw connections
                const linkGroup = svg.append('g').attr('class', 'links');

                connections.forEach(conn => {
                    const fromNode = nodes.find(n => n.id === conn.from);
                    const toNode = nodes.find(n => n.id === conn.to);

                    const link = linkGroup.append('line')
                        .attr('x1', fromNode.x)
                        .attr('y1', fromNode.y)
                        .attr('x2', toNode.x)
                        .attr('y2', toNode.y)
                        .attr('stroke', conn.recurrent ? '#2DD2C0' : '#ccc')
                        .attr('stroke-width', conn.recurrent ? 3 : 2)
                        .attr('stroke-dasharray', conn.recurrent ? '5,5' : 'none')
                        .attr('opacity', 0.5)
                        .attr('class', `link-${conn.from}-${conn.to}`);

                    // Add arrow for recurrent connections
                    if (conn.recurrent) {
                        const midX = (fromNode.x + toNode.x) / 2;
                        const midY = (fromNode.y + toNode.y) / 2;

                        svg.append('defs').append('marker')
                            .attr('id', `arrow-${conn.from}-${conn.to}`)
                            .attr('viewBox', '0 -5 10 10')
                            .attr('refX', 5)
                            .attr('refY', 0)
                            .attr('markerWidth', 5)
                            .attr('markerHeight', 5)
                            .attr('orient', 'auto')
                            .append('path')
                            .attr('d', 'M0,-5L10,0L0,5')
                            .attr('fill', '#2DD2C0');

                        link.attr('marker-mid', `url(#arrow-${conn.from}-${conn.to})`);
                    }
                });

                // Draw nodes
                const nodeGroup = svg.append('g').attr('class', 'nodes');

                nodes.forEach(node => {
                    const g = nodeGroup.append('g')
                        .attr('class', `node-${node.id}`)
                        .attr('transform', `translate(${node.x}, ${node.y})`);

                    // Node circle
                    g.append('circle')
                        .attr('r', 25)
                        .attr('fill', node.type === 'input' ? '#10099F' :
                                     node.type === 'hidden' ? '#2DD2C0' : '#FC8484')
                        .attr('stroke', '#fff')
                        .attr('stroke-width', 2);

                    // Node label
                    g.append('text')
                        .attr('text-anchor', 'middle')
                        .attr('dy', 5)
                        .attr('fill', 'white')
                        .attr('font-size', '14px')
                        .attr('font-weight', 'bold')
                        .text(node.label);
                });

                // Animation functions
                function animateForwardPass() {
                    // Disable button during animation
                    const animateBtn = document.getElementById('rnn-animate-btn');
                    animateBtn.disabled = true;

                    // Reset all opacities
                    svg.selectAll('.links line').attr('opacity', 0.5);
                    svg.selectAll('.nodes circle').attr('opacity', 0.3);

                    let delay = 0;
                    const stepDelay = 800;

                    for (let t = 0; t < timeSteps; t++) {
                        // Highlight input
                        setTimeout(() => {
                            svg.select(`.node-x${t} circle`)
                                .transition()
                                .duration(300)
                                .attr('opacity', 1)
                                .attr('r', 30)
                                .transition()
                                .duration(300)
                                .attr('r', 25);
                        }, delay);

                        delay += 300;

                        // Highlight input to hidden connection
                        setTimeout(() => {
                            svg.select(`.link-x${t}-h${t}`)
                                .transition()
                                .duration(300)
                                .attr('opacity', 1)
                                .attr('stroke-width', 4);
                        }, delay);

                        // Highlight recurrent connection if exists
                        if (t > 0) {
                            setTimeout(() => {
                                svg.select(`.link-h${t-1}-h${t}`)
                                    .transition()
                                    .duration(300)
                                    .attr('opacity', 1)
                                    .attr('stroke-width', 4);
                            }, delay);
                        }

                        delay += 300;

                        // Highlight hidden state
                        setTimeout(() => {
                            svg.select(`.node-h${t} circle`)
                                .transition()
                                .duration(300)
                                .attr('opacity', 1)
                                .attr('r', 30)
                                .transition()
                                .duration(300)
                                .attr('r', 25);
                        }, delay);

                        delay += 300;

                        // Highlight hidden to output connection
                        setTimeout(() => {
                            svg.select(`.link-h${t}-o${t}`)
                                .transition()
                                .duration(300)
                                .attr('opacity', 1)
                                .attr('stroke-width', 4);
                        }, delay);

                        delay += 300;

                        // Highlight output
                        setTimeout(() => {
                            svg.select(`.node-o${t} circle`)
                                .transition()
                                .duration(300)
                                .attr('opacity', 1)
                                .attr('r', 30)
                                .transition()
                                .duration(300)
                                .attr('r', 25);
                        }, delay);

                        delay += stepDelay;
                    }

                    // Re-enable button after animation
                    setTimeout(() => {
                        animateBtn.disabled = false;
                    }, delay);
                }

                function resetVisualization() {
                    svg.selectAll('.links line')
                        .transition()
                        .duration(300)
                        .attr('opacity', 0.5)
                        .attr('stroke-width', d => d.recurrent ? 3 : 2);

                    svg.selectAll('.nodes circle')
                        .transition()
                        .duration(300)
                        .attr('opacity', 1)
                        .attr('r', 25);
                }

                // Attach event handlers
                document.getElementById('rnn-animate-btn').addEventListener('click', animateForwardPass);
                document.getElementById('rnn-reset-btn').addEventListener('click', resetVisualization);
            }
        });

        // Character Prediction Demo (Section 9.4.3)
        document.addEventListener('DOMContentLoaded', function() {
            const charInput = document.getElementById('char-input');
            const predictBtn = document.getElementById('predict-btn');
            const predictionOutput = document.getElementById('prediction-output');

            if (predictBtn && charInput && predictionOutput) {
                // Simple character prediction simulation
                const commonNextChars = {
                    'machin': 'e',
                    'learnin': 'g',
                    'networ': 'k',
                    'neura': 'l',
                    'deepl': 'e',
                    'trainin': 'g',
                    'model': 's',
                    'pytho': 'n'
                };

                predictBtn.addEventListener('click', () => {
                    const input = charInput.value.toLowerCase();

                    // Simulate prediction with probabilities
                    const vocab = 'abcdefghijklmnopqrstuvwxyz ';
                    const probs = {};

                    // Generate pseudo-probabilities
                    vocab.split('').forEach(char => {
                        probs[char] = Math.random() * 0.05;
                    });

                    // Check if we have a known pattern
                    let predictedChar = ' ';
                    if (commonNextChars[input]) {
                        predictedChar = commonNextChars[input];
                        probs[predictedChar] = 0.85;
                    } else if (input.length > 0) {
                        // Simple heuristic: predict based on last character
                        const lastChar = input[input.length - 1];
                        const charIndex = vocab.indexOf(lastChar);
                        if (charIndex >= 0 && charIndex < vocab.length - 1) {
                            predictedChar = vocab[(charIndex + 1) % vocab.length];
                            probs[predictedChar] = 0.65;
                        }
                    }

                    // Normalize probabilities
                    const total = Object.values(probs).reduce((a, b) => a + b, 0);
                    Object.keys(probs).forEach(key => {
                        probs[key] = probs[key] / total;
                    });

                    // Sort by probability
                    const sortedChars = Object.entries(probs)
                        .sort((a, b) => b[1] - a[1])
                        .slice(0, 5);

                    // Display results
                    let outputHTML = `
                        <h5>Prediction Results for "${input}":</h5>
                        <div style="margin-top: 15px;">
                            <p><strong>Top Predictions:</strong></p>
                            <div style="display: flex; flex-direction: column; gap: 8px;">
                    `;

                    sortedChars.forEach(([char, prob], idx) => {
                        const barWidth = prob * 300;
                        const displayChar = char === ' ' ? '[space]' : char;
                        outputHTML += `
                            <div style="display: flex; align-items: center; gap: 10px;">
                                <span style="width: 60px; font-family: monospace; font-weight: ${idx === 0 ? 'bold' : 'normal'};">
                                    '${displayChar}'
                                </span>
                                <div style="flex: 1; background: #e0e0e0; height: 20px; border-radius: 3px; position: relative;">
                                    <div style="background: ${idx === 0 ? '#10099F' : '#2DD2C0'};
                                               width: ${barWidth}px;
                                               height: 100%;
                                               border-radius: 3px;">
                                    </div>
                                </div>
                                <span style="width: 60px; text-align: right; font-size: 0.9em;">
                                    ${(prob * 100).toFixed(1)}%
                                </span>
                            </div>
                        `;
                    });

                    outputHTML += `
                            </div>
                            <p style="margin-top: 15px; color: #666; font-size: 0.9em;">
                                <strong>Predicted next character:</strong>
                                <span style="font-size: 1.2em; color: #10099F; font-family: monospace;">
                                    '${sortedChars[0][0] === ' ' ? '[space]' : sortedChars[0][0]}'
                                </span>
                            </p>
                        </div>
                    `;

                    predictionOutput.innerHTML = outputHTML;
                });
            }
        });

        // Sequence Partitioner (Section 27)
        document.addEventListener('DOMContentLoaded', function() {
            const partitionText = document.getElementById('partition-text');
            const seqLength = document.getElementById('seq-length');
            const offset = document.getElementById('offset');
            const seqLengthValue = document.getElementById('seq-length-value');
            const offsetValue = document.getElementById('offset-value');
            const partitionBtn = document.getElementById('partition-btn');
            const inputSequences = document.getElementById('input-sequences');
            const targetSequences = document.getElementById('target-sequences');
            const numSequences = document.getElementById('num-sequences');

            function updatePartition() {
                if (!partitionText) return;

                const text = partitionText.value;
                const n = parseInt(seqLength.value);
                const d = parseInt(offset.value);

                // Update display values
                seqLengthValue.textContent = n;
                offsetValue.textContent = d;

                // Update max offset based on sequence length
                offset.max = n - 1;
                if (parseInt(offset.value) >= n) {
                    offset.value = n - 1;
                    offsetValue.textContent = n - 1;
                }

                // Create subsequences
                const corpus = text.split('');
                const subsequences = [];

                // Start from offset d and create sequences of length n
                for (let i = d; i <= corpus.length - n - 1; i += n) {
                    const inputSeq = corpus.slice(i, i + n).join('');
                    const targetSeq = corpus.slice(i + 1, i + n + 1).join('');
                    subsequences.push({ input: inputSeq, target: targetSeq });
                }

                // Display sequences
                inputSequences.textContent = subsequences.map(s => `[${s.input}]`).join('\n');
                targetSequences.textContent = subsequences.map(s => `[${s.target}]`).join('\n');
                numSequences.textContent = subsequences.length;
            }

            if (partitionBtn) {
                seqLength.addEventListener('input', updatePartition);
                offset.addEventListener('input', updatePartition);
                partitionBtn.addEventListener('click', updatePartition);
                updatePartition(); // Initial partitioning
            }
        });

        // Section 9.5: RNN Implementation from Scratch Visualizations

        // RNN Forward Pass Visualization (Section 9.5.1)
        document.addEventListener('DOMContentLoaded', function() {
            const container = document.getElementById('rnn-forward-viz');
            if (!container) return;

            const width = container.offsetWidth;
            const height = 500;

            const svg = d3.select(container)
                .append('svg')
                .attr('width', width)
                .attr('height', height);

            // RNN structure
            const layers = [
                { name: 'Input', x: 100, neurons: 3 },
                { name: 'Hidden', x: 400, neurons: 4 },
                { name: 'Output', x: 700, neurons: 3 }
            ];

            const timeSteps = 5;
            let currentStep = 0;

            // Draw RNN unrolled through time
            function drawRNN() {
                svg.selectAll('*').remove();

                for (let t = 0; t < timeSteps; t++) {
                    const yOffset = 100 * t + 50;

                    // Draw neurons
                    layers.forEach((layer, layerIdx) => {
                        const neuronGroup = svg.append('g')
                            .attr('class', `layer-${layerIdx}-time-${t}`);

                        for (let i = 0; i < layer.neurons; i++) {
                            const y = yOffset - (layer.neurons - 1) * 15 + i * 30;

                            neuronGroup.append('circle')
                                .attr('cx', layer.x)
                                .attr('cy', y)
                                .attr('r', 12)
                                .attr('fill', layerIdx === 0 ? '#10099F' :
                                            layerIdx === 1 ? '#2DD2C0' : '#FC8484')
                                .attr('stroke', '#333')
                                .attr('stroke-width', 2)
                                .attr('opacity', t <= currentStep ? 1 : 0.3);
                        }
                    });

                    // Draw connections
                    if (t <= currentStep) {
                        // Input to hidden
                        for (let i = 0; i < layers[0].neurons; i++) {
                            for (let j = 0; j < layers[1].neurons; j++) {
                                const y1 = yOffset - (layers[0].neurons - 1) * 15 + i * 30;
                                const y2 = yOffset - (layers[1].neurons - 1) * 15 + j * 30;

                                svg.append('line')
                                    .attr('x1', layers[0].x + 12)
                                    .attr('y1', y1)
                                    .attr('x2', layers[1].x - 12)
                                    .attr('y2', y2)
                                    .attr('stroke', '#999')
                                    .attr('stroke-width', 0.5)
                                    .attr('opacity', 0.5);
                            }
                        }

                        // Hidden to output
                        for (let i = 0; i < layers[1].neurons; i++) {
                            for (let j = 0; j < layers[2].neurons; j++) {
                                const y1 = yOffset - (layers[1].neurons - 1) * 15 + i * 30;
                                const y2 = yOffset - (layers[2].neurons - 1) * 15 + j * 30;

                                svg.append('line')
                                    .attr('x1', layers[1].x + 12)
                                    .attr('y1', y1)
                                    .attr('x2', layers[2].x - 12)
                                    .attr('y2', y2)
                                    .attr('stroke', '#999')
                                    .attr('stroke-width', 0.5)
                                    .attr('opacity', 0.5);
                            }
                        }

                        // Recurrent connections
                        if (t > 0 && t <= currentStep) {
                            for (let i = 0; i < layers[1].neurons; i++) {
                                const y1 = yOffset - 100 - (layers[1].neurons - 1) * 15 + i * 30;
                                const y2 = yOffset - (layers[1].neurons - 1) * 15 + i * 30;

                                svg.append('path')
                                    .attr('d', `M ${layers[1].x} ${y1 + 12} Q ${layers[1].x + 50} ${(y1 + y2) / 2} ${layers[1].x} ${y2 - 12}`)
                                    .attr('fill', 'none')
                                    .attr('stroke', '#FFA05F')
                                    .attr('stroke-width', 2)
                                    .attr('opacity', 0.7)
                                    .attr('marker-end', 'url(#arrowhead)');
                            }
                        }
                    }

                    // Add time step labels
                    svg.append('text')
                        .attr('x', 50)
                        .attr('y', yOffset)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '14px')
                        .attr('fill', t <= currentStep ? '#333' : '#999')
                        .text(`t=${t}`);
                }

                // Add arrow marker
                svg.append('defs').append('marker')
                    .attr('id', 'arrowhead')
                    .attr('markerWidth', 10)
                    .attr('markerHeight', 7)
                    .attr('refX', 9)
                    .attr('refY', 3.5)
                    .attr('orient', 'auto')
                    .append('polygon')
                    .attr('points', '0 0, 10 3.5, 0 7')
                    .attr('fill', '#FFA05F');
            }

            drawRNN();

            // Button handlers
            document.getElementById('rnn-step-btn').addEventListener('click', () => {
                currentStep = Math.min(currentStep + 1, timeSteps - 1);
                drawRNN();
            });

            document.getElementById('rnn-reset-btn').addEventListener('click', () => {
                currentStep = 0;
                drawRNN();
            });
        });

        // One-Hot Encoding Visualization (Section 9.5.2)
        document.addEventListener('DOMContentLoaded', function() {
            const container = document.getElementById('onehot-visualization');
            if (!container) return;

            const encodeBtn = document.getElementById('encode-btn');
            const inputField = document.getElementById('onehot-input');

            function visualizeOneHot() {
                container.innerHTML = '';

                const text = inputField.value.toLowerCase();
                const vocab = 'abcdefghijklmnopqrstuvwxyz ';
                const vocabSize = vocab.length;

                const width = container.offsetWidth;
                const height = 400;

                // Ensure container is properly sized before rendering
                if (!width || width < 100) {
                    return; // Wait for container to be properly sized
                }

                const svg = d3.select(container)
                    .append('svg')
                    .attr('width', width)
                    .attr('height', height);

                const cellSize = Math.max(4, Math.min(30, width / Math.max(text.length, vocabSize)));

                // Draw the encoding matrix
                for (let i = 0; i < text.length; i++) {
                    const char = text[i];
                    const charIndex = vocab.indexOf(char);

                    for (let j = 0; j < vocabSize; j++) {
                        const isOne = j === charIndex;

                        svg.append('rect')
                            .attr('x', i * cellSize + 50)
                            .attr('y', j * 10 + 50)
                            .attr('width', Math.max(1, cellSize - 2))
                            .attr('height', 8)
                            .attr('fill', isOne ? '#10099F' : '#f0f0f0')
                            .attr('stroke', '#ccc')
                            .attr('stroke-width', 0.5);

                        if (isOne) {
                            svg.append('text')
                                .attr('x', i * cellSize + 50 + cellSize/2)
                                .attr('y', j * 10 + 50 + 6)
                                .attr('text-anchor', 'middle')
                                .attr('font-size', '8px')
                                .attr('fill', 'white')
                                .text('1');
                        }
                    }

                    // Character label
                    svg.append('text')
                        .attr('x', i * cellSize + 50 + cellSize/2)
                        .attr('y', 35)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .attr('font-weight', 'bold')
                        .text(char === ' ' ? '_' : char);
                }

                // Vocabulary labels
                for (let j = 0; j < Math.min(vocabSize, 10); j++) {
                    svg.append('text')
                        .attr('x', 40)
                        .attr('y', j * 10 + 55)
                        .attr('text-anchor', 'end')
                        .attr('font-size', '8px')
                        .text(vocab[j] === ' ' ? '_' : vocab[j]);
                }

                if (vocabSize > 10) {
                    svg.append('text')
                        .attr('x', 40)
                        .attr('y', 10 * 10 + 55)
                        .attr('text-anchor', 'end')
                        .attr('font-size', '8px')
                        .text('...');
                }

                // Title
                svg.append('text')
                    .attr('x', width / 2)
                    .attr('y', 20)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text('One-Hot Encoding Matrix');
            }

            encodeBtn.addEventListener('click', visualizeOneHot);
            visualizeOneHot(); // Initial visualization
        });

        // Parameter Shapes Visualization (Section 9.5.3)
        document.addEventListener('DOMContentLoaded', function() {
            const container = document.getElementById('param-shapes-viz');
            if (!container) return;

            const width = container.offsetWidth;
            const height = 500;

            const svg = d3.select(container)
                .append('svg')
                .attr('width', width)
                .attr('height', height);

            // Define parameter matrices
            const params = [
                { name: 'W_xh', rows: 27, cols: 128, x: 100, y: 100, desc: 'Input to Hidden' },
                { name: 'W_hh', rows: 128, cols: 128, x: 300, y: 100, desc: 'Hidden to Hidden' },
                { name: 'W_hq', rows: 128, cols: 27, x: 500, y: 100, desc: 'Hidden to Output' },
                { name: 'b_h', rows: 128, cols: 1, x: 200, y: 300, desc: 'Hidden Bias' },
                { name: 'b_q', rows: 27, cols: 1, x: 400, y: 300, desc: 'Output Bias' }
            ];

            // Draw matrices
            params.forEach(param => {
                const group = svg.append('g')
                    .attr('class', `param-${param.name}`);

                const maxDim = Math.max(param.rows, param.cols);
                const scale = 100 / maxDim;
                const rectWidth = param.cols * scale;
                const rectHeight = param.rows * scale;

                // Matrix rectangle
                group.append('rect')
                    .attr('x', param.x)
                    .attr('y', param.y)
                    .attr('width', rectWidth)
                    .attr('height', rectHeight)
                    .attr('fill', '#10099F')
                    .attr('fill-opacity', 0.3)
                    .attr('stroke', '#10099F')
                    .attr('stroke-width', 2);

                // Matrix name
                group.append('text')
                    .attr('x', param.x + rectWidth / 2)
                    .attr('y', param.y - 10)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text(param.name);

                // Dimensions
                group.append('text')
                    .attr('x', param.x + rectWidth / 2)
                    .attr('y', param.y + rectHeight + 20)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .text(`${param.rows} × ${param.cols}`);

                // Tooltip on hover
                group.on('mouseover', function() {
                    d3.select(this).select('rect')
                        .attr('fill-opacity', 0.6);

                    svg.append('text')
                        .attr('id', 'param-tooltip')
                        .attr('x', width / 2)
                        .attr('y', height - 20)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '14px')
                        .attr('fill', '#666')
                        .text(`${param.name}: ${param.desc} (${param.rows * param.cols} parameters)`);
                })
                .on('mouseout', function() {
                    d3.select(this).select('rect')
                        .attr('fill-opacity', 0.3);
                    svg.select('#param-tooltip').remove();
                });
            });

            // Add title
            svg.append('text')
                .attr('x', width / 2)
                .attr('y', 30)
                .attr('text-anchor', 'middle')
                .attr('font-size', '16px')
                .attr('font-weight', 'bold')
                .text('RNN Parameter Shapes');
        });

        // Forward Pass Animation (Section 9.5.4)
        document.addEventListener('DOMContentLoaded', function() {
            const container = document.getElementById('forward-pass-animation');
            if (!container) return;

            const width = container.offsetWidth;
            const height = 450;

            const svg = d3.select(container)
                .append('svg')
                .attr('width', width)
                .attr('height', height);

            let animationInterval;
            let currentStep = 0;
            const sequence = ['h', 'e', 'l', 'l', 'o'];

            function animateForward() {
                svg.selectAll('*').remove();

                // Draw sequence
                sequence.forEach((char, idx) => {
                    const x = 100 + idx * 120;
                    const y = 100;

                    // Input node
                    svg.append('circle')
                        .attr('cx', x)
                        .attr('cy', y)
                        .attr('r', 20)
                        .attr('fill', idx <= currentStep ? '#10099F' : '#e0e0e0')
                        .attr('stroke', '#333')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', y + 5)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '14px')
                        .attr('fill', 'white')
                        .text(char);

                    // Hidden state
                    svg.append('rect')
                        .attr('x', x - 30)
                        .attr('y', 200)
                        .attr('width', 60)
                        .attr('height', 60)
                        .attr('fill', idx <= currentStep ? '#2DD2C0' : '#e0e0e0')
                        .attr('stroke', '#333')
                        .attr('stroke-width', 2)
                        .attr('rx', 5);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', 235)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .attr('fill', '#333')
                        .text(`h_${idx}`);

                    // Output
                    svg.append('circle')
                        .attr('cx', x)
                        .attr('cy', 350)
                        .attr('r', 20)
                        .attr('fill', idx <= currentStep ? '#FC8484' : '#e0e0e0')
                        .attr('stroke', '#333')
                        .attr('stroke-width', 2);

                    // Connections
                    if (idx <= currentStep) {
                        // Input to hidden
                        svg.append('line')
                            .attr('x1', x)
                            .attr('y1', y + 20)
                            .attr('x2', x)
                            .attr('y2', 200)
                            .attr('stroke', '#666')
                            .attr('stroke-width', 2)
                            .attr('marker-end', 'url(#arrow)');

                        // Hidden to output
                        svg.append('line')
                            .attr('x1', x)
                            .attr('y1', 260)
                            .attr('x2', x)
                            .attr('y2', 330)
                            .attr('stroke', '#666')
                            .attr('stroke-width', 2)
                            .attr('marker-end', 'url(#arrow)');

                        // Recurrent connection
                        if (idx > 0) {
                            svg.append('path')
                                .attr('d', `M ${x - 60} 230 Q ${x - 30} 180 ${x - 30} 230`)
                                .attr('fill', 'none')
                                .attr('stroke', '#FFA05F')
                                .attr('stroke-width', 3)
                                .attr('marker-end', 'url(#arrow-orange)');
                        }
                    }
                });

                // Arrow markers
                const defs = svg.append('defs');

                defs.append('marker')
                    .attr('id', 'arrow')
                    .attr('markerWidth', 10)
                    .attr('markerHeight', 10)
                    .attr('refX', 9)
                    .attr('refY', 5)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M 0 0 L 10 5 L 0 10')
                    .attr('fill', '#666');

                defs.append('marker')
                    .attr('id', 'arrow-orange')
                    .attr('markerWidth', 10)
                    .attr('markerHeight', 10)
                    .attr('refX', 9)
                    .attr('refY', 5)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M 0 0 L 10 5 L 0 10')
                    .attr('fill', '#FFA05F');

                // Labels
                svg.append('text')
                    .attr('x', 50)
                    .attr('y', 105)
                    .attr('font-size', '12px')
                    .text('Input');

                svg.append('text')
                    .attr('x', 50)
                    .attr('y', 235)
                    .attr('font-size', '12px')
                    .text('Hidden');

                svg.append('text')
                    .attr('x', 50)
                    .attr('y', 355)
                    .attr('font-size', '12px')
                    .text('Output');
            }

            animateForward();

            const playBtn = document.getElementById('forward-play-btn');
            const resetBtn = document.getElementById('forward-reset-btn');
            const speedSlider = document.getElementById('forward-speed');

            playBtn.addEventListener('click', () => {
                clearInterval(animationInterval);
                animationInterval = setInterval(() => {
                    currentStep = (currentStep + 1) % (sequence.length);
                    animateForward();
                    if (currentStep === sequence.length - 1) {
                        clearInterval(animationInterval);
                    }
                }, parseInt(speedSlider.value));
            });

            resetBtn.addEventListener('click', () => {
                clearInterval(animationInterval);
                currentStep = 0;
                animateForward();
            });
        });

        // Gradient Clipping Visualization (Section 9.5.5)
        document.addEventListener('DOMContentLoaded', function() {
            const container = document.getElementById('gradient-clipping-viz');
            if (!container) return;

            const width = container.offsetWidth;
            const height = 400;

            const svg = d3.select(container)
                .append('svg')
                .attr('width', width)
                .attr('height', height);

            const centerX = width / 2;
            const centerY = height / 2;

            function updateClipping() {
                svg.selectAll('*').remove();

                const gradNorm = parseFloat(document.getElementById('grad-norm-slider').value);
                const threshold = parseFloat(document.getElementById('clip-threshold-slider').value);

                document.getElementById('grad-norm-value').textContent = gradNorm.toFixed(1);
                document.getElementById('clip-threshold-value').textContent = threshold.toFixed(1);

                const scale = 10;
                const clipped = Math.min(gradNorm, threshold);
                const clipFactor = clipped / gradNorm;

                // Draw threshold circle
                svg.append('circle')
                    .attr('cx', centerX)
                    .attr('cy', centerY)
                    .attr('r', threshold * scale)
                    .attr('fill', 'none')
                    .attr('stroke', '#10099F')
                    .attr('stroke-width', 2)
                    .attr('stroke-dasharray', '5,5');

                // Draw original gradient
                svg.append('line')
                    .attr('x1', centerX)
                    .attr('y1', centerY)
                    .attr('x2', centerX + gradNorm * scale * Math.cos(-Math.PI/4))
                    .attr('y2', centerY + gradNorm * scale * Math.sin(-Math.PI/4))
                    .attr('stroke', '#FF6B6B')
                    .attr('stroke-width', 3)
                    .attr('opacity', 0.5)
                    .attr('marker-end', 'url(#arrow-red)');

                // Draw clipped gradient
                svg.append('line')
                    .attr('x1', centerX)
                    .attr('y1', centerY)
                    .attr('x2', centerX + clipped * scale * Math.cos(-Math.PI/4))
                    .attr('y2', centerY + clipped * scale * Math.sin(-Math.PI/4))
                    .attr('stroke', '#2DD2C0')
                    .attr('stroke-width', 3)
                    .attr('marker-end', 'url(#arrow-green)');

                // Arrow markers
                const defs = svg.append('defs');

                ['red', 'green'].forEach(color => {
                    defs.append('marker')
                        .attr('id', `arrow-${color}`)
                        .attr('markerWidth', 10)
                        .attr('markerHeight', 10)
                        .attr('refX', 9)
                        .attr('refY', 5)
                        .attr('orient', 'auto')
                        .append('path')
                        .attr('d', 'M 0 0 L 10 5 L 0 10')
                        .attr('fill', color === 'red' ? '#FF6B6B' : '#2DD2C0');
                });

                // Labels
                svg.append('text')
                    .attr('x', centerX)
                    .attr('y', 30)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text('Gradient Clipping Visualization');

                svg.append('text')
                    .attr('x', centerX)
                    .attr('y', centerY - threshold * scale - 10)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .attr('fill', '#10099F')
                    .text(`Threshold: ${threshold}`);

                // Legend
                svg.append('line')
                    .attr('x1', 20)
                    .attr('y1', height - 40)
                    .attr('x2', 50)
                    .attr('y2', height - 40)
                    .attr('stroke', '#FF6B6B')
                    .attr('stroke-width', 3)
                    .attr('opacity', 0.5);

                svg.append('text')
                    .attr('x', 55)
                    .attr('y', height - 36)
                    .attr('font-size', '12px')
                    .text(`Original (norm: ${gradNorm.toFixed(1)})`);

                svg.append('line')
                    .attr('x1', 20)
                    .attr('y1', height - 20)
                    .attr('x2', 50)
                    .attr('y2', height - 20)
                    .attr('stroke', '#2DD2C0')
                    .attr('stroke-width', 3);

                svg.append('text')
                    .attr('x', 55)
                    .attr('y', height - 16)
                    .attr('font-size', '12px')
                    .text(`Clipped (norm: ${clipped.toFixed(1)}, factor: ${clipFactor.toFixed(2)})`);
            }

            document.getElementById('grad-norm-slider').addEventListener('input', updateClipping);
            document.getElementById('clip-threshold-slider').addEventListener('input', updateClipping);

            updateClipping();
        });

        // Text Generation Demo (Section 9.5.7)
        document.addEventListener('DOMContentLoaded', function() {
            const generateBtn = document.getElementById('generate-btn');
            const outputDiv = document.getElementById('generation-output');
            const processDiv = document.getElementById('generation-process');

            if (!generateBtn) return;

            generateBtn.addEventListener('click', () => {
                const prefix = document.getElementById('gen-prefix').value;
                const length = parseInt(document.getElementById('gen-length').value);

                // Simple character generation simulation
                const vocab = 'abcdefghijklmnopqrstuvwxyz .,!?';
                let generated = prefix;

                // Generate pseudo-random text based on patterns
                const patterns = {
                    'time ': 'traveller',
                    'the ': 'machine',
                    'traveller ': 'went',
                    'machine ': 'was',
                    'went ': 'through',
                    'was ': 'incredible'
                };

                for (let i = prefix.length; i < prefix.length + length; i++) {
                    const lastWord = generated.split(' ').slice(-2).join(' ');

                    if (patterns[lastWord + ' ']) {
                        generated += patterns[lastWord + ' '][0];
                    } else {
                        // Generate based on last character
                        const lastChar = generated[generated.length - 1];
                        const charIndex = vocab.indexOf(lastChar);
                        const nextIndex = (charIndex + Math.floor(Math.random() * 5) + 1) % vocab.length;
                        generated += vocab[nextIndex];
                    }
                }

                // Display the generated text with highlighting
                outputDiv.innerHTML = `
                    <span style="color: #10099F; font-weight: bold;">${prefix}</span><span style="color: #2DD2C0;">${generated.slice(prefix.length)}</span>
                `;

                // Visualize generation process
                visualizeGeneration(prefix, generated.slice(prefix.length));
            });

            function visualizeGeneration(prefix, generated) {
                processDiv.innerHTML = '';

                const width = processDiv.offsetWidth;
                const height = 250;

                const svg = d3.select(processDiv)
                    .append('svg')
                    .attr('width', width)
                    .attr('height', height);

                // Show probability distribution for next character
                const vocab = 'abcdefghijklmnopqrstuvwxyz .,!?';
                const probs = [];

                for (let i = 0; i < Math.min(vocab.length, 15); i++) {
                    probs.push({
                        char: vocab[i],
                        prob: Math.random() * 0.3
                    });
                }

                // Normalize
                const total = probs.reduce((sum, p) => sum + p.prob, 0);
                probs.forEach(p => p.prob = p.prob / total);

                // Sort by probability
                probs.sort((a, b) => b.prob - a.prob);

                // Draw bars
                const barWidth = width / probs.length - 10;
                const maxProb = Math.max(...probs.map(p => p.prob));

                probs.forEach((p, idx) => {
                    const barHeight = p.prob / maxProb * 150;
                    const x = idx * (barWidth + 10) + 20;

                    svg.append('rect')
                        .attr('x', x)
                        .attr('y', 200 - barHeight)
                        .attr('width', barWidth)
                        .attr('height', barHeight)
                        .attr('fill', idx === 0 ? '#10099F' : '#2DD2C0');

                    svg.append('text')
                        .attr('x', x + barWidth / 2)
                        .attr('y', 215)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .text(p.char === ' ' ? '_' : p.char);

                    svg.append('text')
                        .attr('x', x + barWidth / 2)
                        .attr('y', 195 - barHeight)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '10px')
                        .text((p.prob * 100).toFixed(1) + '%');
                });

                svg.append('text')
                    .attr('x', width / 2)
                    .attr('y', 20)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text('Character Probability Distribution');
            }
        });

        // CONCISE IMPLEMENTATION VISUALIZATIONS

        // Framework tab switching
        document.addEventListener('DOMContentLoaded', function() {
            const tabBtns = document.querySelectorAll('.tab-btn');
            const codePanels = document.querySelectorAll('.code-panel');

            tabBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    const framework = this.dataset.framework;

                    // Update button styles
                    tabBtns.forEach(b => {
                        b.style.background = '#e0e0e0';
                        b.style.color = '#333';
                    });
                    this.style.background = '#10099F';
                    this.style.color = 'white';

                    // Show corresponding code panel
                    codePanels.forEach(panel => {
                        panel.style.display = 'none';
                    });
                    const targetPanel = document.getElementById(`${framework}-code`);
                    if (targetPanel) {
                        targetPanel.style.display = 'block';
                    }
                });
            });

            // API comparison chart
            if (document.getElementById('api-comparison-chart')) {
                const chartDiv = document.getElementById('api-comparison-chart');
                const svg = d3.select(chartDiv)
                    .append('svg')
                    .attr('width', '100%')
                    .attr('height', '100%')
                    .attr('viewBox', '0 0 800 400');

                const data = [
                    {category: 'Lines of Code', scratch: 100, api: 20},
                    {category: 'Training Speed', scratch: 100, api: 300},
                    {category: 'Memory Efficiency', scratch: 100, api: 150},
                    {category: 'Development Time', scratch: 100, api: 10}
                ];

                const barWidth = 60;
                const groupWidth = 150;

                data.forEach((d, i) => {
                    const x = 100 + i * groupWidth;

                    // Scratch bar
                    svg.append('rect')
                        .attr('x', x)
                        .attr('y', 300 - d.scratch)
                        .attr('width', barWidth)
                        .attr('height', d.scratch)
                        .attr('fill', '#FC8484')
                        .attr('opacity', 0.8);

                    // API bar
                    svg.append('rect')
                        .attr('x', x + barWidth + 10)
                        .attr('y', 300 - d.api)
                        .attr('width', barWidth)
                        .attr('height', d.api)
                        .attr('fill', '#2DD2C0')
                        .attr('opacity', 0.8);

                    // Labels
                    svg.append('text')
                        .attr('x', x + barWidth + 5)
                        .attr('y', 320)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .text(d.category);
                });

                // Legend
                svg.append('rect')
                    .attr('x', 650)
                    .attr('y', 50)
                    .attr('width', 20)
                    .attr('height', 20)
                    .attr('fill', '#FC8484');

                svg.append('text')
                    .attr('x', 675)
                    .attr('y', 65)
                    .attr('font-size', '14px')
                    .text('Scratch');

                svg.append('rect')
                    .attr('x', 650)
                    .attr('y', 80)
                    .attr('width', 20)
                    .attr('height', 20)
                    .attr('fill', '#2DD2C0');

                svg.append('text')
                    .attr('x', 675)
                    .attr('y', 95)
                    .attr('font-size', '14px')
                    .text('API');
            }

            // Hidden state diagram
            if (document.getElementById('hidden-state-diagram')) {
                const hiddenDiv = document.getElementById('hidden-state-diagram');
                const svg = d3.select(hiddenDiv)
                    .append('svg')
                    .attr('width', '100%')
                    .attr('height', '100%')
                    .attr('viewBox', '0 0 800 300');

                // Draw tensor representation
                const layers = 2;
                const batchSize = 32;
                const hiddenSize = 128;

                // Draw 3D tensor
                const startX = 250;
                const startY = 100;
                const depth = 30;

                for (let l = 0; l < layers; l++) {
                    const offsetX = l * depth;
                    const offsetY = l * depth;

                    // Front face
                    svg.append('rect')
                        .attr('x', startX + offsetX)
                        .attr('y', startY - offsetY)
                        .attr('width', 200)
                        .attr('height', 100)
                        .attr('fill', '#10099F')
                        .attr('opacity', 0.3 + l * 0.2)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2);
                }

                // Labels
                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 230)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .text('Batch Size: 32');

                svg.append('text')
                    .attr('x', 150)
                    .attr('y', 150)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .text('Layers: 2');

                svg.append('text')
                    .attr('x', 550)
                    .attr('y', 150)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .text('Hidden: 128');
            }

            // Performance chart
            if (document.getElementById('performance-chart')) {
                const perfDiv = document.getElementById('performance-chart');
                const svg = d3.select(perfDiv)
                    .append('svg')
                    .attr('width', '100%')
                    .attr('height', '100%')
                    .attr('viewBox', '0 0 800 350');

                // Training time comparison
                const timeData = [
                    {name: 'Data Loading', scratch: 10, api: 10},
                    {name: 'Forward Pass', scratch: 40, api: 15},
                    {name: 'Backward Pass', scratch: 45, api: 18},
                    {name: 'Parameter Update', scratch: 5, api: 2}
                ];

                let yPos = 50;
                timeData.forEach(d => {
                    // Scratch bar
                    svg.append('rect')
                        .attr('x', 200)
                        .attr('y', yPos)
                        .attr('width', d.scratch * 5)
                        .attr('height', 30)
                        .attr('fill', '#FC8484')
                        .attr('opacity', 0.8);

                    // API bar
                    svg.append('rect')
                        .attr('x', 200)
                        .attr('y', yPos + 35)
                        .attr('width', d.api * 5)
                        .attr('height', 30)
                        .attr('fill', '#2DD2C0')
                        .attr('opacity', 0.8);

                    // Label
                    svg.append('text')
                        .attr('x', 190)
                        .attr('y', yPos + 20)
                        .attr('text-anchor', 'end')
                        .attr('font-size', '14px')
                        .text(d.name);

                    // Time values
                    svg.append('text')
                        .attr('x', 210 + d.scratch * 5)
                        .attr('y', yPos + 20)
                        .attr('font-size', '12px')
                        .text(`${d.scratch}ms`);

                    svg.append('text')
                        .attr('x', 210 + d.api * 5)
                        .attr('y', yPos + 55)
                        .attr('font-size', '12px')
                        .text(`${d.api}ms`);

                    yPos += 80;
                });
            }

            // Memory usage visualization
            if (document.getElementById('memory-usage-viz')) {
                const memDiv = document.getElementById('memory-usage-viz');
                const svg = d3.select(memDiv)
                    .append('svg')
                    .attr('width', '100%')
                    .attr('height', '100%')
                    .attr('viewBox', '0 0 800 250');

                // Memory blocks
                const scratchMem = [
                    {name: 'Python Objects', size: 30},
                    {name: 'Intermediate Values', size: 40},
                    {name: 'Gradients', size: 35},
                    {name: 'Overhead', size: 20}
                ];

                const apiMem = [
                    {name: 'Tensors', size: 40},
                    {name: 'Gradients', size: 30},
                    {name: 'Buffer Pool', size: 15}
                ];

                // Draw scratch memory
                let xPos = 50;
                scratchMem.forEach(m => {
                    svg.append('rect')
                        .attr('x', xPos)
                        .attr('y', 50)
                        .attr('width', m.size * 4)
                        .attr('height', 60)
                        .attr('fill', '#FC8484')
                        .attr('opacity', 0.7)
                        .attr('stroke', 'white')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xPos + m.size * 2)
                        .attr('y', 80)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '10px')
                        .attr('fill', 'white')
                        .text(m.name);

                    xPos += m.size * 4;
                });

                svg.append('text')
                    .attr('x', 50)
                    .attr('y', 40)
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text('Scratch: ~500MB');

                // Draw API memory
                xPos = 50;
                apiMem.forEach(m => {
                    svg.append('rect')
                        .attr('x', xPos)
                        .attr('y', 140)
                        .attr('width', m.size * 4)
                        .attr('height', 60)
                        .attr('fill', '#2DD2C0')
                        .attr('opacity', 0.7)
                        .attr('stroke', 'white')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xPos + m.size * 2)
                        .attr('y', 170)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '10px')
                        .attr('fill', 'white')
                        .text(m.name);

                    xPos += m.size * 4;
                });

                svg.append('text')
                    .attr('x', 50)
                    .attr('y', 130)
                    .attr('font-size', '14px')
                    .attr('font-weight', 'bold')
                    .text('API: ~340MB');
            }

            // Random generation demo
            const randomGenBtn = document.getElementById('random-gen-btn');
            if (randomGenBtn) {
                randomGenBtn.addEventListener('click', function() {
                    const output = document.getElementById('random-output');
                    const chars = 'abcdefghijklmnopqrstuvwxyz ';
                    let result = 'it has';

                    for (let i = 0; i < 20; i++) {
                        result += chars[Math.floor(Math.random() * chars.length)];
                    }

                    output.innerHTML = `<span style="color: #10099F; font-weight: bold;">it has</span><span style="color: #FC8484;">${result.slice(6)}</span>`;
                    output.style.border = '2px solid #FC8484';
                });
            }

            // Trained generation demo
            const generateTrainedBtn = document.getElementById('generate-trained-btn');
            if (generateTrainedBtn) {
                generateTrainedBtn.addEventListener('click', function() {
                    const prefix = document.getElementById('prefix-input').value;
                    const length = parseInt(document.getElementById('length-input').value);
                    const output = document.getElementById('generated-text');

                    // Simulate coherent generation
                    const patterns = [
                        ' and the ', ' with the ', ' in the ', ' was ', ' had ',
                        'time ', 'traveller ', 'machine ', 'dimension ', 'future '
                    ];

                    let result = prefix;
                    for (let i = 0; i < length / 5; i++) {
                        result += patterns[Math.floor(Math.random() * patterns.length)];
                    }

                    output.innerHTML = `<span style="color: #10099F; font-weight: bold;">${prefix}</span><span style="color: #2DD2C0;">${result.slice(prefix.length)}</span>`;
                    output.style.border = '2px solid #2DD2C0';
                });
            }

            // Overfitting demo
            const overfitBtn = document.getElementById('overfit-demo-btn');
            if (overfitBtn) {
                overfitBtn.addEventListener('click', function() {
                    const hiddenSize = document.getElementById('hidden-size-slider').value;
                    const results = document.getElementById('overfit-results');

                    const overfitRisk = hiddenSize > 256 ? 'High' : hiddenSize > 128 ? 'Medium' : 'Low';
                    const color = overfitRisk === 'High' ? '#FC8484' : overfitRisk === 'Medium' ? '#FFA05F' : '#2DD2C0';

                    results.innerHTML = `
                        <div style="padding: 15px; background: ${color}20; border: 2px solid ${color}; border-radius: 8px;">
                            <p><strong>Hidden Size:</strong> ${hiddenSize}</p>
                            <p><strong>Parameters:</strong> ~${Math.round(hiddenSize * hiddenSize / 1000)}K</p>
                            <p><strong>Overfitting Risk:</strong> <span style="color: ${color}; font-weight: bold;">${overfitRisk}</span></p>
                            <p><strong>Training Perplexity:</strong> ${(1.0 + Math.random() * 0.5).toFixed(2)}</p>
                            <p><strong>Validation Perplexity:</strong> ${(2.0 + (hiddenSize / 100) + Math.random()).toFixed(2)}</p>
                        </div>
                    `;
                });
            }

            // Update hidden size value display
            const hiddenSlider = document.getElementById('hidden-size-slider');
            if (hiddenSlider) {
                hiddenSlider.addEventListener('input', function() {
                    const valueDisplay = document.getElementById('hidden-size-value');
                    if (valueDisplay) {
                        valueDisplay.textContent = this.value;
                    }
                });
            }

            // ==========================================
            // BPTT Visualizations
            // ==========================================

            // 1. Basic BPTT Unroll Visualization
            const bpttUnrollViz = document.querySelector('#bptt-unroll-viz svg');
            if (bpttUnrollViz) {
                const svg = d3.select(bpttUnrollViz);
                const width = 800;
                const height = 300;

                // Draw initial RNN box
                const rnnGroup = svg.append('g').attr('transform', 'translate(100, 100)');

                rnnGroup.append('rect')
                    .attr('x', 0)
                    .attr('y', 0)
                    .attr('width', 120)
                    .attr('height', 100)
                    .attr('fill', '#10099F')
                    .attr('opacity', 0.3)
                    .attr('stroke', '#10099F')
                    .attr('stroke-width', 2)
                    .attr('rx', 5);

                rnnGroup.append('text')
                    .attr('x', 60)
                    .attr('y', 50)
                    .attr('text-anchor', 'middle')
                    .attr('fill', '#10099F')
                    .attr('font-weight', 'bold')
                    .text('RNN');

                // Add recurrent arrow
                const arc = d3.arc()
                    .innerRadius(30)
                    .outerRadius(32)
                    .startAngle(0)
                    .endAngle(Math.PI * 1.5);

                rnnGroup.append('path')
                    .attr('d', arc)
                    .attr('transform', 'translate(60, -10) rotate(45)')
                    .attr('fill', '#2DD2C0');

                // Arrow to unrolled version
                svg.append('path')
                    .attr('d', 'M 250 150 L 350 150')
                    .attr('stroke', '#FFA05F')
                    .attr('stroke-width', 2)
                    .attr('marker-end', 'url(#arrow-bptt)');

                // Unrolled network
                for (let i = 0; i < 3; i++) {
                    const xPos = 400 + i * 130;

                    svg.append('rect')
                        .attr('x', xPos)
                        .attr('y', 100)
                        .attr('width', 100)
                        .attr('height', 100)
                        .attr('fill', '#10099F')
                        .attr('opacity', 0.2)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2)
                        .attr('rx', 5)
                        .attr('class', 'unrolled-cell');

                    svg.append('text')
                        .attr('x', xPos + 50)
                        .attr('y', 150)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#10099F')
                        .text('t=' + i);

                    if (i < 2) {
                        svg.append('path')
                            .attr('d', `M ${xPos + 100} 150 L ${xPos + 130} 150`)
                            .attr('stroke', '#2DD2C0')
                            .attr('stroke-width', 2)
                            .attr('marker-end', 'url(#arrow-bptt-green)');
                    }
                }

                // Add arrow markers
                const defs = svg.append('defs');

                const markers = [
                    { id: 'arrow-bptt', color: '#FFA05F' },
                    { id: 'arrow-bptt-green', color: '#2DD2C0' },
                    { id: 'arrow-bptt-red', color: '#FC8484' }
                ];

                markers.forEach(marker => {
                    defs.append('marker')
                        .attr('id', marker.id)
                        .attr('markerWidth', 10)
                        .attr('markerHeight', 10)
                        .attr('refX', 9)
                        .attr('refY', 3)
                        .attr('orient', 'auto')
                        .attr('markerUnits', 'strokeWidth')
                        .append('path')
                        .attr('d', 'M0,0 L0,6 L9,3 z')
                        .attr('fill', marker.color);
                });
            }

            // 2. Interactive Unrolling Animation
            const unrollAnimation = document.getElementById('unroll-animation');
            if (unrollAnimation) {
                const svg = d3.select(unrollAnimation).append('svg')
                    .attr('width', '100%')
                    .attr('height', '350')
                    .attr('viewBox', '0 0 800 350');

                let isUnrolled = false;
                const cells = [];

                // Create initial rolled RNN
                const rolledGroup = svg.append('g')
                    .attr('transform', 'translate(350, 125)')
                    .attr('opacity', 1);

                rolledGroup.append('rect')
                    .attr('x', -60)
                    .attr('y', -50)
                    .attr('width', 120)
                    .attr('height', 100)
                    .attr('fill', '#10099F')
                    .attr('opacity', 0.3)
                    .attr('stroke', '#10099F')
                    .attr('stroke-width', 2)
                    .attr('rx', 5);

                rolledGroup.append('text')
                    .attr('x', 0)
                    .attr('y', 5)
                    .attr('text-anchor', 'middle')
                    .attr('fill', '#10099F')
                    .attr('font-weight', 'bold')
                    .attr('font-size', '18px')
                    .text('RNN');

                // Unroll button handler
                document.getElementById('unroll-btn').addEventListener('click', function() {
                    if (!isUnrolled) {
                        const steps = parseInt(document.getElementById('unroll-steps').value);

                        // Fade out rolled version
                        rolledGroup.transition()
                            .duration(500)
                            .attr('opacity', 0);

                        // Create unrolled cells
                        for (let i = 0; i < steps; i++) {
                            const cellGroup = svg.append('g')
                                .attr('transform', `translate(${150 + i * 140}, 175)`)
                                .attr('opacity', 0);

                            cellGroup.append('rect')
                                .attr('x', -50)
                                .attr('y', -50)
                                .attr('width', 100)
                                .attr('height', 100)
                                .attr('fill', '#10099F')
                                .attr('opacity', 0.2)
                                .attr('stroke', '#10099F')
                                .attr('stroke-width', 2)
                                .attr('rx', 5);

                            cellGroup.append('text')
                                .attr('x', 0)
                                .attr('y', 0)
                                .attr('text-anchor', 'middle')
                                .attr('fill', '#10099F')
                                .attr('font-weight', 'bold')
                                .text(`t=${i}`);

                            // Add connections
                            if (i < steps - 1) {
                                cellGroup.append('path')
                                    .attr('d', 'M 50 0 L 90 0')
                                    .attr('stroke', '#2DD2C0')
                                    .attr('stroke-width', 2)
                                    .attr('marker-end', 'url(#arrow-unroll)');
                            }

                            cells.push(cellGroup);

                            // Animate appearance
                            cellGroup.transition()
                                .duration(300)
                                .delay(i * 150)
                                .attr('opacity', 1);
                        }

                        isUnrolled = true;
                    }
                });

                // Roll back button handler
                document.getElementById('roll-btn').addEventListener('click', function() {
                    if (isUnrolled) {
                        // Remove unrolled cells
                        cells.forEach((cell, i) => {
                            cell.transition()
                                .duration(300)
                                .delay(i * 100)
                                .attr('opacity', 0)
                                .remove();
                        });

                        cells.length = 0;

                        // Show rolled version
                        setTimeout(() => {
                            rolledGroup.transition()
                                .duration(500)
                                .attr('opacity', 1);
                        }, 500);

                        isUnrolled = false;
                    }
                });

                // Update steps display
                document.getElementById('unroll-steps').addEventListener('input', function() {
                    document.getElementById('steps-value').textContent = this.value;

                    // If already unrolled, re-unroll with new step count
                    if (isUnrolled) {
                        document.getElementById('roll-btn').click();
                        setTimeout(() => {
                            document.getElementById('unroll-btn').click();
                        }, 1000);
                    }
                });

                // Add arrow marker
                svg.append('defs').append('marker')
                    .attr('id', 'arrow-unroll')
                    .attr('markerWidth', 10)
                    .attr('markerHeight', 10)
                    .attr('refX', 9)
                    .attr('refY', 3)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M0,0 L0,6 L9,3 z')
                    .attr('fill', '#2DD2C0');
            }

            // 3. Dependency Graph
            const dependencyGraph = document.getElementById('dependency-graph');
            if (dependencyGraph) {
                const svg = d3.select(dependencyGraph).append('svg')
                    .attr('width', '100%')
                    .attr('height', '400')
                    .attr('viewBox', '0 0 800 400');

                const timesteps = 4;
                const layers = ['x', 'h', 'o', 'L'];
                const nodeRadius = 25;

                // Create nodes
                layers.forEach((layer, layerIdx) => {
                    for (let t = 0; t < timesteps; t++) {
                        const x = 150 + t * 150;
                        const y = 80 + layerIdx * 80;

                        const node = svg.append('g')
                            .attr('transform', `translate(${x}, ${y})`);

                        node.append('circle')
                            .attr('r', nodeRadius)
                            .attr('fill', layer === 'x' ? '#2DD2C0' :
                                          layer === 'h' ? '#10099F' :
                                          layer === 'o' ? '#FC8484' : '#FFA05F')
                            .attr('opacity', 0.3)
                            .attr('stroke', layer === 'x' ? '#2DD2C0' :
                                           layer === 'h' ? '#10099F' :
                                           layer === 'o' ? '#FC8484' : '#FFA05F')
                            .attr('stroke-width', 2);

                        node.append('text')
                            .attr('text-anchor', 'middle')
                            .attr('dy', '.35em')
                            .attr('fill', 'white')
                            .attr('font-weight', 'bold')
                            .text(layer === 'L' ? 'L' : `${layer}_${t}`);
                    }
                });

                // Add connections
                for (let t = 0; t < timesteps; t++) {
                    // x to h
                    svg.append('path')
                        .attr('d', `M ${150 + t * 150} ${80 + nodeRadius} L ${150 + t * 150} ${160 - nodeRadius}`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5)
                        .attr('opacity', 0.5);

                    // h to o
                    svg.append('path')
                        .attr('d', `M ${150 + t * 150} ${160 + nodeRadius} L ${150 + t * 150} ${240 - nodeRadius}`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5)
                        .attr('opacity', 0.5);

                    // o to L
                    svg.append('path')
                        .attr('d', `M ${150 + t * 150} ${240 + nodeRadius} L ${150 + t * 150} ${320 - nodeRadius}`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5)
                        .attr('opacity', 0.5);

                    // h to h (recurrent)
                    if (t < timesteps - 1) {
                        svg.append('path')
                            .attr('d', `M ${150 + t * 150 + nodeRadius} 160 L ${150 + (t+1) * 150 - nodeRadius} 160`)
                            .attr('stroke', '#10099F')
                            .attr('stroke-width', 2)
                            .attr('opacity', 0.7);
                    }
                }

                // Add parameter boxes
                const params = [
                    { name: 'W_hx', x: 50, y: 120, color: '#10099F' },
                    { name: 'W_hh', x: 50, y: 200, color: '#10099F' },
                    { name: 'W_qh', x: 50, y: 280, color: '#FC8484' }
                ];

                params.forEach(param => {
                    const g = svg.append('g')
                        .attr('transform', `translate(${param.x}, ${param.y})`);

                    g.append('rect')
                        .attr('x', -25)
                        .attr('y', -15)
                        .attr('width', 50)
                        .attr('height', 30)
                        .attr('fill', param.color)
                        .attr('opacity', 0.2)
                        .attr('stroke', param.color)
                        .attr('stroke-width', 2);

                    g.append('text')
                        .attr('text-anchor', 'middle')
                        .attr('dy', '.35em')
                        .attr('fill', param.color)
                        .attr('font-size', '12px')
                        .attr('font-weight', 'bold')
                        .text(param.name);
                });
            }

            // 4. Loss Aggregation Visualization
            const lossAggregation = document.querySelector('#loss-aggregation svg');
            if (lossAggregation) {
                const svg = d3.select(lossAggregation);
                const timesteps = 5;

                // Draw individual losses
                for (let t = 0; t < timesteps; t++) {
                    const x = 100 + t * 120;

                    svg.append('rect')
                        .attr('x', x - 30)
                        .attr('y', 50)
                        .attr('width', 60)
                        .attr('height', 40)
                        .attr('fill', '#FC8484')
                        .attr('opacity', 0.3)
                        .attr('stroke', '#FC8484')
                        .attr('stroke-width', 2)
                        .attr('rx', 5);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', 70)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#FC8484')
                        .attr('font-weight', 'bold')
                        .text(`l_${t}`);

                    // Arrow to sum
                    svg.append('path')
                        .attr('d', `M ${x} 90 L ${350} 150`)
                        .attr('stroke', '#FFA05F')
                        .attr('stroke-width', 1.5)
                        .attr('opacity', 0.5);
                }

                // Draw sum/average
                svg.append('circle')
                    .attr('cx', 350)
                    .attr('cy', 170)
                    .attr('r', 30)
                    .attr('fill', '#FFA05F')
                    .attr('opacity', 0.3)
                    .attr('stroke', '#FFA05F')
                    .attr('stroke-width', 2);

                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 175)
                    .attr('text-anchor', 'middle')
                    .attr('fill', '#FFA05F')
                    .attr('font-weight', 'bold')
                    .attr('font-size', '18px')
                    .text('L');

                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 220)
                    .attr('text-anchor', 'middle')
                    .attr('fill', '#666')
                    .attr('font-size', '14px')
                    .text('L = (1/T) Σ l_t');
            }

            // 5. Butterfly Effect Visualization
            const butterflyViz = document.querySelector('#butterfly-viz svg');
            if (butterflyViz) {
                const svg = d3.select(butterflyViz);

                // Initial state
                svg.append('circle')
                    .attr('cx', 100)
                    .attr('cy', 150)
                    .attr('r', 5)
                    .attr('fill', '#10099F');

                svg.append('text')
                    .attr('x', 100)
                    .attr('y', 190)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .text('Initial State');

                // Show diverging paths
                const paths = 8;
                for (let i = 0; i < paths; i++) {
                    const angle = (i - paths/2) * 0.15;
                    const path = d3.path();
                    path.moveTo(100, 150);

                    let x = 100;
                    let y = 150;
                    let currentAngle = angle;

                    for (let step = 0; step < 5; step++) {
                        currentAngle += (Math.random() - 0.5) * 0.3;
                        x += 100;
                        y += Math.sin(currentAngle) * 50;
                        path.lineTo(x, y);
                    }

                    svg.append('path')
                        .attr('d', path.toString())
                        .attr('fill', 'none')
                        .attr('stroke', i < paths/2 ? '#FC8484' : '#2DD2C0')
                        .attr('stroke-width', 1.5)
                        .attr('opacity', 0.5);

                    svg.append('circle')
                        .attr('cx', x)
                        .attr('cy', y)
                        .attr('r', 3)
                        .attr('fill', i < paths/2 ? '#FC8484' : '#2DD2C0');
                }

                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 280)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('fill', '#666')
                    .text('Small initial differences → Large outcome variations');
            }

            // 6. Truncation Visualization
            const truncationViz = document.getElementById('truncation-viz');
            if (truncationViz) {
                const svg = d3.select(truncationViz).append('svg')
                    .attr('width', '100%')
                    .attr('height', '250')
                    .attr('viewBox', '0 0 700 250');

                function drawTruncation(tau) {
                    svg.selectAll('*').remove();

                    const totalSteps = 10;
                    const stepWidth = 60;

                    for (let t = 0; t < totalSteps; t++) {
                        const x = 50 + t * stepWidth;
                        const isActive = t >= totalSteps - tau;

                        // Hidden state
                        svg.append('circle')
                            .attr('cx', x)
                            .attr('cy', 100)
                            .attr('r', 20)
                            .attr('fill', isActive ? '#10099F' : '#ccc')
                            .attr('opacity', isActive ? 0.8 : 0.3);

                        svg.append('text')
                            .attr('x', x)
                            .attr('y', 105)
                            .attr('text-anchor', 'middle')
                            .attr('fill', 'white')
                            .attr('font-size', '12px')
                            .text(`h_${t}`);

                        // Gradient flow
                        if (t < totalSteps - 1) {
                            svg.append('path')
                                .attr('d', `M ${x + 20} 100 L ${x + 40} 100`)
                                .attr('stroke', isActive && t >= totalSteps - tau - 1 ? '#2DD2C0' : '#ccc')
                                .attr('stroke-width', 2)
                                .attr('opacity', isActive && t >= totalSteps - tau - 1 ? 0.8 : 0.3);
                        }

                        // Loss contribution
                        svg.append('path')
                            .attr('d', `M ${x} 120 L ${x} 160`)
                            .attr('stroke', '#FC8484')
                            .attr('stroke-width', 1)
                            .attr('opacity', 0.5);

                        svg.append('rect')
                            .attr('x', x - 15)
                            .attr('y', 160)
                            .attr('width', 30)
                            .attr('height', 20)
                            .attr('fill', '#FC8484')
                            .attr('opacity', 0.3);

                        svg.append('text')
                            .attr('x', x)
                            .attr('y', 172)
                            .attr('text-anchor', 'middle')
                            .attr('fill', '#FC8484')
                            .attr('font-size', '10px')
                            .text(`L_${t}`);
                    }

                    // Truncation indicator
                    if (tau < totalSteps) {
                        const truncX = 50 + (totalSteps - tau - 1) * stepWidth + 30;
                        svg.append('line')
                            .attr('x1', truncX)
                            .attr('y1', 60)
                            .attr('x2', truncX)
                            .attr('y2', 140)
                            .attr('stroke', '#FFA05F')
                            .attr('stroke-width', 3)
                            .attr('stroke-dasharray', '5,5');

                        svg.append('text')
                            .attr('x', truncX)
                            .attr('y', 50)
                            .attr('text-anchor', 'middle')
                            .attr('fill', '#FFA05F')
                            .attr('font-weight', 'bold')
                            .text('τ = ' + tau);
                    }
                }

                // Initial draw
                drawTruncation(5);

                // Update on slider change
                document.getElementById('tau-slider').addEventListener('input', function() {
                    document.getElementById('tau-value').textContent = this.value;
                });

                document.getElementById('show-truncation').addEventListener('click', function() {
                    const tau = parseInt(document.getElementById('tau-slider').value);
                    drawTruncation(tau);
                });
            }

            // 7. Detailed Computational Graph
            const detailedCompGraph = document.querySelector('#detailed-comp-graph svg');
            if (detailedCompGraph) {
                const svg = d3.select(detailedCompGraph);

                // Parameters (shaded)
                const params = [
                    { name: 'W_hx', x: 100, y: 100, color: '#10099F' },
                    { name: 'W_hh', x: 100, y: 200, color: '#10099F' },
                    { name: 'W_qh', x: 100, y: 300, color: '#FC8484' }
                ];

                params.forEach(param => {
                    const g = svg.append('g');

                    g.append('rect')
                        .attr('x', param.x - 30)
                        .attr('y', param.y - 15)
                        .attr('width', 60)
                        .attr('height', 30)
                        .attr('fill', param.color)
                        .attr('opacity', 0.5)
                        .attr('stroke', param.color)
                        .attr('stroke-width', 2);

                    g.append('text')
                        .attr('x', param.x)
                        .attr('y', param.y + 5)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-weight', 'bold')
                        .text(param.name);
                });

                // Variables for 3 timesteps
                for (let t = 0; t < 3; t++) {
                    const xBase = 250 + t * 180;

                    // Input x_t
                    svg.append('rect')
                        .attr('x', xBase - 25)
                        .attr('y', 35)
                        .attr('width', 50)
                        .attr('height', 30)
                        .attr('fill', 'none')
                        .attr('stroke', '#2DD2C0')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 52)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#2DD2C0')
                        .text(`x_${t}`);

                    // Hidden h_t
                    svg.append('rect')
                        .attr('x', xBase - 25)
                        .attr('y', 135)
                        .attr('width', 50)
                        .attr('height', 30)
                        .attr('fill', 'none')
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 152)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#10099F')
                        .text(`h_${t}`);

                    // Output o_t
                    svg.append('rect')
                        .attr('x', xBase - 25)
                        .attr('y', 235)
                        .attr('width', 50)
                        .attr('height', 30)
                        .attr('fill', 'none')
                        .attr('stroke', '#FC8484')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 252)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#FC8484')
                        .text(`o_${t}`);

                    // Loss L_t
                    svg.append('rect')
                        .attr('x', xBase - 25)
                        .attr('y', 335)
                        .attr('width', 50)
                        .attr('height', 30)
                        .attr('fill', 'none')
                        .attr('stroke', '#FFA05F')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 352)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#FFA05F')
                        .text(`L_${t}`);

                    // Operators (circles)
                    svg.append('circle')
                        .attr('cx', xBase)
                        .attr('cy', 100)
                        .attr('r', 12)
                        .attr('fill', '#666')
                        .attr('opacity', 0.3);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 105)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-size', '16px')
                        .text('+');

                    svg.append('circle')
                        .attr('cx', xBase)
                        .attr('cy', 200)
                        .attr('r', 12)
                        .attr('fill', '#666')
                        .attr('opacity', 0.3);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 205)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-size', '16px')
                        .text('×');

                    svg.append('circle')
                        .attr('cx', xBase)
                        .attr('cy', 300)
                        .attr('r', 12)
                        .attr('fill', '#666')
                        .attr('opacity', 0.3);

                    svg.append('text')
                        .attr('x', xBase)
                        .attr('y', 305)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-size', '16px')
                        .text('l');

                    // Connections
                    // x_t to operator
                    svg.append('path')
                        .attr('d', `M ${xBase} 65 L ${xBase} 88`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // operator to h_t
                    svg.append('path')
                        .attr('d', `M ${xBase} 112 L ${xBase} 135`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // h_t to operator
                    svg.append('path')
                        .attr('d', `M ${xBase} 165 L ${xBase} 188`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // operator to o_t
                    svg.append('path')
                        .attr('d', `M ${xBase} 212 L ${xBase} 235`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // o_t to loss operator
                    svg.append('path')
                        .attr('d', `M ${xBase} 265 L ${xBase} 288`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // loss operator to L_t
                    svg.append('path')
                        .attr('d', `M ${xBase} 312 L ${xBase} 335`)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1.5);

                    // Recurrent connection
                    if (t < 2) {
                        svg.append('path')
                            .attr('d', `M ${xBase + 25} 150 L ${xBase + 155} 150`)
                            .attr('stroke', '#10099F')
                            .attr('stroke-width', 2)
                            .attr('marker-end', 'url(#arrow-comp)');
                    }

                    // Parameter connections
                    // W_hx connection
                    svg.append('path')
                        .attr('d', `M 130 100 L ${xBase - 12} 100`)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 1)
                        .attr('opacity', 0.5)
                        .attr('stroke-dasharray', '2,2');

                    // W_hh connection
                    if (t > 0) {
                        svg.append('path')
                            .attr('d', `M 130 200 C ${(130 + xBase)/2} 200, ${(130 + xBase)/2} 100, ${xBase - 12} 100`)
                            .attr('stroke', '#10099F')
                            .attr('stroke-width', 1)
                            .attr('opacity', 0.5)
                            .attr('stroke-dasharray', '2,2')
                            .attr('fill', 'none');
                    }

                    // W_qh connection
                    svg.append('path')
                        .attr('d', `M 130 300 C ${(130 + xBase)/2} 300, ${(130 + xBase)/2} 200, ${xBase - 12} 200`)
                        .attr('stroke', '#FC8484')
                        .attr('stroke-width', 1)
                        .attr('opacity', 0.5)
                        .attr('stroke-dasharray', '2,2')
                        .attr('fill', 'none');
                }

                // Arrow marker for comp graph
                svg.append('defs').append('marker')
                    .attr('id', 'arrow-comp')
                    .attr('markerWidth', 10)
                    .attr('markerHeight', 10)
                    .attr('refX', 9)
                    .attr('refY', 3)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M0,0 L0,6 L9,3 z')
                    .attr('fill', '#10099F');
            }

            // 8. Hidden Gradient Flow
            const hiddenGradientFlow = document.querySelector('#hidden-gradient-flow svg');
            if (hiddenGradientFlow) {
                const svg = d3.select(hiddenGradientFlow);

                // Draw nodes for 4 time steps
                for (let t = 0; t < 4; t++) {
                    const x = 100 + t * 150;

                    // Hidden state
                    svg.append('circle')
                        .attr('cx', x)
                        .attr('cy', 100)
                        .attr('r', 25)
                        .attr('fill', '#10099F')
                        .attr('opacity', 0.3)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', 105)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-weight', 'bold')
                        .text(`h_${t}`);

                    // Output contribution
                    svg.append('circle')
                        .attr('cx', x)
                        .attr('cy', 40)
                        .attr('r', 15)
                        .attr('fill', '#FC8484')
                        .attr('opacity', 0.3);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', 45)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-size', '12px')
                        .text(`o_${t}`);

                    // Gradient flow from output
                    svg.append('path')
                        .attr('d', `M ${x} 55 L ${x} 75`)
                        .attr('stroke', '#FC8484')
                        .attr('stroke-width', 2)
                        .attr('marker-end', 'url(#arrow-grad-red)')
                        .attr('opacity', 0.7);

                    // Backward gradient flow
                    if (t < 3) {
                        svg.append('path')
                            .attr('d', `M ${x + 125} 100 L ${x + 25} 100`)
                            .attr('stroke', '#2DD2C0')
                            .attr('stroke-width', 2)
                            .attr('marker-end', 'url(#arrow-grad-green)')
                            .attr('opacity', 0.7);
                    }
                }

                // Gradient equation
                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 170)
                    .attr('text-anchor', 'middle')
                    .attr('fill', '#666')
                    .attr('font-size', '12px')
                    .text('∂L/∂h_t = W_hh^T ∂L/∂h_{t+1} + W_qh^T ∂L/∂o_t');

                // Arrow markers
                const defs = svg.append('defs');

                defs.append('marker')
                    .attr('id', 'arrow-grad-red')
                    .attr('markerWidth', 8)
                    .attr('markerHeight', 8)
                    .attr('refX', 7)
                    .attr('refY', 3)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M0,0 L0,6 L7,3 z')
                    .attr('fill', '#FC8484');

                defs.append('marker')
                    .attr('id', 'arrow-grad-green')
                    .attr('markerWidth', 8)
                    .attr('markerHeight', 8)
                    .attr('refX', 7)
                    .attr('refY', 3)
                    .attr('orient', 'auto')
                    .append('path')
                    .attr('d', 'M0,0 L0,6 L7,3 z')
                    .attr('fill', '#2DD2C0');
            }

            // 9. Eigenvalue Effect Visualization
            const eigenvalueViz = document.getElementById('eigenvalue-viz');
            if (eigenvalueViz) {
                const svg = d3.select(eigenvalueViz).append('svg')
                    .attr('width', '100%')
                    .attr('height', '200')
                    .attr('viewBox', '0 0 700 200');

                function showEigenEffect() {
                    svg.selectAll('*').remove();

                    const eigenvalue = parseFloat(document.getElementById('eigenvalue-slider').value);
                    document.getElementById('eigen-value').textContent = eigenvalue.toFixed(1);

                    const steps = 10;
                    const values = [];

                    for (let i = 0; i < steps; i++) {
                        values.push(Math.pow(eigenvalue, i));
                    }

                    // Scale for visualization
                    const maxVal = Math.max(...values);
                    const scale = maxVal > 100 ? 100 / maxVal : 1;

                    // Draw bars
                    values.forEach((val, i) => {
                        const x = 50 + i * 60;
                        const height = Math.min(Math.abs(val * scale), 150);
                        const color = eigenvalue > 1 ? '#FC8484' : eigenvalue < 1 ? '#10099F' : '#2DD2C0';

                        svg.append('rect')
                            .attr('x', x - 20)
                            .attr('y', 160 - height)
                            .attr('width', 40)
                            .attr('height', height)
                            .attr('fill', color)
                            .attr('opacity', 0.6);

                        svg.append('text')
                            .attr('x', x)
                            .attr('y', 180)
                            .attr('text-anchor', 'middle')
                            .attr('font-size', '10px')
                            .text(`t=${i}`);

                        // Show value
                        if (i % 2 === 0) {
                            svg.append('text')
                                .attr('x', x)
                                .attr('y', 150 - height)
                                .attr('text-anchor', 'middle')
                                .attr('font-size', '10px')
                                .attr('fill', color)
                                .text(val > 1000 ? '∞' : val < 0.001 ? '≈0' : val.toFixed(2));
                        }
                    })

                    // Add label
                    svg.append('text')
                        .attr('x', 350)
                        .attr('y', 20)
                        .attr('text-anchor', 'middle')
                        .attr('font-weight', 'bold')
                        .attr('fill', eigenvalue > 1 ? '#FC8484' : eigenvalue < 1 ? '#10099F' : '#2DD2C0')
                        .text(eigenvalue > 1 ? 'Exploding Gradients!' : eigenvalue < 1 ? 'Vanishing Gradients!' : 'Stable');
                }

                document.getElementById('eigenvalue-slider').addEventListener('input', showEigenEffect);
                document.getElementById('show-eigen-effect').addEventListener('click', showEigenEffect);

                // Initial draw
                showEigenEffect();
            }

            // 10. Gradient Magnitude Plot
            const gradientMagnitudePlot = document.getElementById('gradient-magnitude-plot');
            if (gradientMagnitudePlot) {
                const svg = d3.select(gradientMagnitudePlot).append('svg')
                    .attr('width', '100%')
                    .attr('height', '300')
                    .attr('viewBox', '0 0 700 300');

                function updateGradientPlot() {
                    svg.selectAll('*').remove();

                    const eigenvalue = parseFloat(document.getElementById('eigen-demo').value);
                    const steps = parseInt(document.getElementById('steps-demo').value);

                    document.getElementById('eigen-demo-value').textContent = eigenvalue.toFixed(2);
                    document.getElementById('steps-demo-value').textContent = steps;

                    // Generate data
                    const data = [];
                    for (let i = 0; i <= steps; i++) {
                        data.push({
                            x: i,
                            y: Math.pow(eigenvalue, i)
                        });
                    }

                    // Scales
                    const xScale = d3.scaleLinear()
                        .domain([0, steps])
                        .range([60, 640]);

                    const maxY = Math.max(...data.map(d => d.y));
                    const minY = Math.min(...data.map(d => d.y));

                    const yScale = d3.scaleLog()
                        .domain([Math.max(0.001, minY), Math.max(1, maxY)])
                        .range([250, 50])
                        .clamp(true);

                    // Axes
                    svg.append('g')
                        .attr('transform', 'translate(0, 250)')
                        .call(d3.axisBottom(xScale).ticks(10));

                    svg.append('g')
                        .attr('transform', 'translate(60, 0)')
                        .call(d3.axisLeft(yScale).ticks(5, '.0e'));

                    // Line
                    const line = d3.line()
                        .x(d => xScale(d.x))
                        .y(d => yScale(Math.max(0.001, d.y)));

                    svg.append('path')
                        .datum(data)
                        .attr('fill', 'none')
                        .attr('stroke', eigenvalue > 1 ? '#FC8484' : eigenvalue < 1 ? '#10099F' : '#2DD2C0')
                        .attr('stroke-width', 2)
                        .attr('d', line);

                    // Reference line at y=1
                    svg.append('line')
                        .attr('x1', 60)
                        .attr('y1', yScale(1))
                        .attr('x2', 640)
                        .attr('y2', yScale(1))
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1)
                        .attr('stroke-dasharray', '3,3')
                        .attr('opacity', 0.5);

                    // Labels
                    svg.append('text')
                        .attr('x', 350)
                        .attr('y', 290)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .text('Time Steps');

                    svg.append('text')
                        .attr('x', 20)
                        .attr('y', 150)
                        .attr('text-anchor', 'middle')
                        .attr('font-size', '12px')
                        .attr('transform', 'rotate(-90, 20, 150)')
                        .text('Gradient Magnitude');

                    // Title
                    svg.append('text')
                        .attr('x', 350)
                        .attr('y', 30)
                        .attr('text-anchor', 'middle')
                        .attr('font-weight', 'bold')
                        .attr('fill', eigenvalue > 1 ? '#FC8484' : eigenvalue < 1 ? '#10099F' : '#2DD2C0')
                        .text(`λ^t for λ = ${eigenvalue.toFixed(2)}`);
                }

                document.getElementById('eigen-demo').addEventListener('input', updateGradientPlot);
                document.getElementById('steps-demo').addEventListener('input', updateGradientPlot);

                // Initial draw
                updateGradientPlot();
            }

            // 11. Gradient Clipping Demo
            const clippingDemo = document.querySelector('#clipping-demo svg');
            if (clippingDemo) {
                const svg = d3.select(clippingDemo);

                function updateClipping() {
                    svg.selectAll('*').remove();

                    const maxNorm = parseFloat(document.getElementById('clip-threshold').value);
                    document.getElementById('clip-value').textContent = maxNorm.toFixed(1);

                    // Draw coordinate system
                    const centerX = 300;
                    const centerY = 100;

                    // Axes
                    svg.append('line')
                        .attr('x1', 100)
                        .attr('y1', centerY)
                        .attr('x2', 500)
                        .attr('y2', centerY)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1);

                    svg.append('line')
                        .attr('x1', centerX)
                        .attr('y1', 20)
                        .attr('x2', centerX)
                        .attr('y2', 180)
                        .attr('stroke', '#666')
                        .attr('stroke-width', 1);

                    // Clipping circle
                    svg.append('circle')
                        .attr('cx', centerX)
                        .attr('cy', centerY)
                        .attr('r', maxNorm * 20)
                        .attr('fill', 'none')
                        .attr('stroke', '#2DD2C0')
                        .attr('stroke-width', 2)
                        .attr('stroke-dasharray', '5,5');

                    // Original gradient
                    const origGradX = 4;
                    const origGradY = 3;
                    const origNorm = Math.sqrt(origGradX * origGradX + origGradY * origGradY);

                    svg.append('line')
                        .attr('x1', centerX)
                        .attr('y1', centerY)
                        .attr('x2', centerX + origGradX * 30)
                        .attr('y2', centerY - origGradY * 30)
                        .attr('stroke', '#FC8484')
                        .attr('stroke-width', 3)
                        .attr('marker-end', 'url(#arrow-clip-orig)')
                        .attr('opacity', 0.5);

                    // Clipped gradient
                    let clippedX = origGradX;
                    let clippedY = origGradY;

                    if (origNorm > maxNorm) {
                        const scale = maxNorm / origNorm;
                        clippedX *= scale;
                        clippedY *= scale;
                    }

                    svg.append('line')
                        .attr('x1', centerX)
                        .attr('y1', centerY)
                        .attr('x2', centerX + clippedX * 30)
                        .attr('y2', centerY - clippedY * 30)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 3)
                        .attr('marker-end', 'url(#arrow-clip-new)');

                    // Labels
                    svg.append('text')
                        .attr('x', centerX + origGradX * 30 + 10)
                        .attr('y', centerY - origGradY * 30)
                        .attr('fill', '#FC8484')
                        .attr('font-size', '12px')
                        .text(`Original (||g|| = ${origNorm.toFixed(1)})`);

                    svg.append('text')
                        .attr('x', centerX + clippedX * 30 + 10)
                        .attr('y', centerY - clippedY * 30 + 20)
                        .attr('fill', '#10099F')
                        .attr('font-size', '12px')
                        .text(`Clipped (||g|| = ${Math.sqrt(clippedX*clippedX + clippedY*clippedY).toFixed(1)})`);

                    // Arrow markers
                    const defs = svg.append('defs');

                    defs.append('marker')
                        .attr('id', 'arrow-clip-orig')
                        .attr('markerWidth', 10)
                        .attr('markerHeight', 10)
                        .attr('refX', 9)
                        .attr('refY', 3)
                        .attr('orient', 'auto')
                        .append('path')
                        .attr('d', 'M0,0 L0,6 L9,3 z')
                        .attr('fill', '#FC8484');

                    defs.append('marker')
                        .attr('id', 'arrow-clip-new')
                        .attr('markerWidth', 10)
                        .attr('markerHeight', 10)
                        .attr('refX', 9)
                        .attr('refY', 3)
                        .attr('orient', 'auto')
                        .append('path')
                        .attr('d', 'M0,0 L0,6 L9,3 z')
                        .attr('fill', '#10099F');
                }

                document.getElementById('clip-threshold').addEventListener('input', updateClipping);

                // Initial draw
                updateClipping();
            }

            // 12. Matrix Power Visualization
            const matrixPowerDemo = document.querySelector('#matrix-power-demo svg');
            if (matrixPowerDemo) {
                const svg = d3.select(matrixPowerDemo);

                // Show matrix multiplication chain
                for (let i = 0; i < 5; i++) {
                    const x = 80 + i * 130;

                    // Matrix block
                    svg.append('rect')
                        .attr('x', x - 30)
                        .attr('y', 100)
                        .attr('width', 60)
                        .attr('height', 60)
                        .attr('fill', '#10099F')
                        .attr('opacity', 0.2 + i * 0.15)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', x)
                        .attr('y', 135)
                        .attr('text-anchor', 'middle')
                        .attr('fill', '#10099F')
                        .attr('font-weight', 'bold')
                        .text('W_hh');

                    if (i < 4) {
                        svg.append('text')
                            .attr('x', x + 50)
                            .attr('y', 135)
                            .attr('text-anchor', 'middle')
                            .attr('font-size', '20px')
                            .text('×');
                    }
                }

                // Result
                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 200)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '16px')
                    .attr('fill', '#666')
                    .text('(W_hh)^T → eigenvalues^T');

                // Eigenvalue effects
                svg.append('text')
                    .attr('x', 200)
                    .attr('y', 230)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('fill', '#10099F')
                    .text('λ < 1 → 0');

                svg.append('text')
                    .attr('x', 500)
                    .attr('y', 230)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '14px')
                    .attr('fill', '#FC8484')
                    .text('λ > 1 → ∞');
            }

            // 13. Recursive Dependencies Visualization
            const recursiveDeps = document.querySelector('#recursive-deps svg');
            if (recursiveDeps) {
                const svg = d3.select(recursiveDeps);

                // Show recursive expansion
                const levels = [
                    { t: 't', x: 350, y: 50 },
                    { t: 't-1', x: 250, y: 120 },
                    { t: 't-2', x: 150, y: 120 },
                    { t: 't-1', x: 450, y: 120 }
                ];

                levels.forEach(level => {
                    svg.append('circle')
                        .attr('cx', level.x)
                        .attr('cy', level.y)
                        .attr('r', 20)
                        .attr('fill', '#10099F')
                        .attr('opacity', 0.3)
                        .attr('stroke', '#10099F')
                        .attr('stroke-width', 2);

                    svg.append('text')
                        .attr('x', level.x)
                        .attr('y', level.y + 5)
                        .attr('text-anchor', 'middle')
                        .attr('fill', 'white')
                        .attr('font-size', '12px')
                        .text(`h_${level.t}`);
                });

                // Connections showing dependencies
                svg.append('path')
                    .attr('d', 'M 330 60 L 270 110')
                    .attr('stroke', '#2DD2C0')
                    .attr('stroke-width', 2);

                svg.append('path')
                    .attr('d', 'M 370 60 L 430 110')
                    .attr('stroke', '#FC8484')
                    .attr('stroke-width', 2);

                svg.append('path')
                    .attr('d', 'M 230 120 L 170 120')
                    .attr('stroke', '#2DD2C0')
                    .attr('stroke-width', 2);

                // Labels
                svg.append('text')
                    .attr('x', 300)
                    .attr('y', 85)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '10px')
                    .attr('fill', '#2DD2C0')
                    .text('∂h_t/∂h_{t-1}');

                svg.append('text')
                    .attr('x', 400)
                    .attr('y', 85)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '10px')
                    .attr('fill', '#FC8484')
                    .text('∂h_t/∂w_h');

                svg.append('text')
                    .attr('x', 350)
                    .attr('y', 170)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .attr('fill', '#666')
                    .text('Recursion continues back through time...');
            }
        });
    </script>
</body>
</html>