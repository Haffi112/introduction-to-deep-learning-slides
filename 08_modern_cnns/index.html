<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modern Convolutional Neural Networks - Introduction to Deep Learning</title>

    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    <link rel="stylesheet" href="css/modern-cnn-custom.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Modern Convolutional Neural Networks</h1>
                <p>Chapter 8: From AlexNet to Transformers</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>

            <!-- Section 1: Introduction to Modern CNNs (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 8", "url": "https://d2l.ai/chapter_modern-convolutional-neural-networks/index.html"}]'>
                    <h2 class="truncate-title">The Evolution of CNNs: From Basic to Modern</h2>
                    <div class="evolution-overview">
                        <div class="fragment">
                            <h4>Why Modern CNNs Matter</h4>
                            <p style="font-size: 0.8em;">CNNs have revolutionized computer vision and serve as fundamental building blocks for:</p>
                        </div>
                        <div class="application-grid fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <div class="app-card">
                                <div class="icon">üéØ</div>
                                <h5>Object Tracking</h5>
                                <p><span class="tooltip">ByteTrack<span class="tooltiptext">Multi-object tracking by associating every detection box (Zhang et al., 2021)</span></span></p>
                            </div>
                            <div class="app-card">
                                <div class="icon">üñºÔ∏è</div>
                                <h5>Segmentation</h5>
                                <p><span class="tooltip">FCN<span class="tooltiptext">Fully Convolutional Networks for semantic segmentation (Long et al., 2015)</span></span></p>
                            </div>
                            <div class="app-card">
                                <div class="icon">üì¶</div>
                                <h5>Object Detection</h5>
                                <p><span class="tooltip">YOLO<span class="tooltiptext">You Only Look Once - real-time object detection (Redmon & Farhadi, 2018)</span></span></p>
                            </div>
                            <div class="app-card">
                                <div class="icon">üé®</div>
                                <h5>Style Transfer</h5>
                                <p><span class="tooltip">Neural Style<span class="tooltiptext">Image style transfer using CNNs (Gatys et al., 2016)</span></span></p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>Modern CNNs are not just classifiers - they're <strong>universal feature extractors</strong> for vision tasks</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "ImageNet Large Scale Visual Recognition Challenge", "url": "https://www.image-net.org/challenges/LSVRC/"}]'>
                    <h2 class="truncate-title">ImageNet: The Benchmark of Progress</h2>
                    <div class="imagenet-overview">
                        <div class="fragment">
                            <p style="font-size: 0.5em;">Since 2010, ImageNet has been the barometer of supervised learning in computer vision</p>
                        </div>
                        <div class="stats-grid fragment" style="font-size: 0.5em; margin-top: 30px;">
                            <div class="stat-card">
                                <div class="number">14M+</div>
                                <div class="label">Images</div>
                            </div>
                            <div class="stat-card">
                                <div class="number">20K+</div>
                                <div class="label">Categories</div>
                            </div>
                            <div class="stat-card">
                                <div class="number">1000</div>
                                <div class="label">Competition Classes</div>
                            </div>
                        </div>
                        <div class="winners-timeline fragment" style="margin-top: 30px;">
                            <h5 style="font-size: 0.8em;">Key Winners & Breakthroughs (top 5 error rates)</h5>
                            <div class="timeline-compact">
                                <div class="entry">2012: <strong>AlexNet</strong> - Deep learning arrives (16.4% error)</div>
                                <div class="entry">2014: <strong>VGG</strong> - Deeper is better (7.3% error)</div>
                                <div class="entry">2014: <strong>GoogLeNet</strong> - Inception modules (6.7% error)</div>
                                <div class="entry">2015: <strong>ResNet</strong> - Skip connections (3.6% error)</div>
                                <div class="entry">2017: <strong>SENet</strong> - Attention in CNNs (2.25% error)</div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What has been the primary role of the ImageNet competition in deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It provides free images for training models",
                                "correct": false,
                                "explanation": "While ImageNet does provide a large dataset, its primary importance is as a benchmark for measuring progress."
                            },
                            {
                                "text": "It serves as a benchmark measuring progress in computer vision since 2010",
                                "correct": true,
                                "explanation": "Correct! ImageNet has been the barometer of progress in supervised learning for computer vision, with each years competition driving innovation."
                            },
                            {
                                "text": "It only tests how fast models can run",
                                "correct": false,
                                "explanation": "ImageNet primarily measures accuracy, not speed. The focus is on classification performance."
                            },
                            {
                                "text": "It is used exclusively for style transfer applications",
                                "correct": false,
                                "explanation": "ImageNet is primarily for image classification. Style transfer is just one of many applications that use CNNs trained on ImageNet."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 2: Tour of Modern Architectures (Vertical) -->
            <section>
                <section data-sources='[{"text": "Krizhevsky et al. (2012) - ImageNet Classification with Deep CNNs", "url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"}, {"text": "He et al. (2016) - Deep Residual Learning", "url": "https://arxiv.org/abs/1512.03385"}]'>
                    <h2 class="truncate-title">A Tour of Groundbreaking Architectures</h2>
                    <div class="architecture-timeline">
                        <h4 style="font-size: 0.9em;">The CNN Hall of Fame</h4>
                        <div id="architecture-timeline-viz" style="margin-top: 20px;">
                            <svg width="800" height="400" style="display: block; margin: 0 auto;">
                                <!-- Timeline will be drawn here by JavaScript -->
                            </svg>
                        </div>
                        <div class="timeline-legend fragment" style="font-size: 0.5em; margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                            <div class="legend-item"><span class="dot alexnet"></span> AlexNet (2012): First deep CNN to win ImageNet</div>
                            <div class="legend-item"><span class="dot vgg"></span> VGG (2014): Showed that depth matters</div>
                            <div class="legend-item"><span class="dot nin"></span> NiN (2013): Network in Network - 1x1 convolutions</div>
                            <div class="legend-item"><span class="dot googlenet"></span> GoogLeNet (2015): Inception modules</div>
                            <div class="legend-item"><span class="dot resnet"></span> ResNet (2016): Residual connections - 152 layers!</div>
                            <div class="legend-item"><span class="dot densenet"></span> DenseNet (2017): Dense connections</div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Ioffe & Szegedy (2015) - Batch Normalization", "url": "https://arxiv.org/abs/1502.03167"}, {"text": "He et al. (2016) - Deep Residual Learning", "url": "https://arxiv.org/abs/1512.03385"}]'>
                    <h2 class="truncate-title">Two Ideas That Changed Everything</h2>
                    <div class="key-innovations" style="font-size: 0.7em;">
                        <div class="innovation-card fragment" style="margin-bottom: 30px;">
                            <h4>üîÑ Batch Normalization</h4>
                            <div class="innovation-details" style="font-size: 0.7em;">
                                <p><strong>Problem:</strong> Internal covariate shift - layer inputs constantly changing</p>
                                <p><strong>Solution:</strong> Normalize inputs to each layer</p>
                                <p><strong>Impact:</strong> Faster training, higher learning rates, less careful initialization</p>
                            </div>
                        </div>
                        <div class="innovation-card fragment">
                            <h4>‚ûï Residual Connections</h4>
                            <div class="innovation-details" style="font-size: 0.7em;">
                                <p><strong>Problem:</strong> Vanishing gradients in very deep networks</p>
                                <p><strong>Solution:</strong> Skip connections that add input to output</p>
                                <p><strong>Impact:</strong> Networks with 100s of layers became trainable</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.8em;">
                            <p>These techniques are now standard in nearly all deep networks, not just CNNs!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Howard et al. (2019) - Searching for MobileNetV3", "url": "https://arxiv.org/abs/1905.02244"}, {"text": "Radosavovic et al. (2020) - Designing Network Design Spaces", "url": "https://arxiv.org/abs/2003.13678"}]'>
                    <h2 class="truncate-title">The Era of Efficient and Automated Design</h2>
                    <div class="modern-approaches" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">From Manual to Automatic Architecture Design</h4>
                        <div class="approach-grid" style="font-size: 0.65em; margin-top: 30px;">
                            <div class="approach fragment">
                                <h5>üì± Efficiency Focus</h5>
                                <ul>
                                    <li><strong>MobileNet:</strong> Depthwise separable convolutions</li>
                                    <li><strong>ShiftNet:</strong> Zero-FLOP coordinate shifts</li>
                                    <li><strong>EfficientNet:</strong> Compound scaling</li>
                                </ul>
                                <p class="highlight">Goal: Deploy on mobile/edge devices</p>
                            </div>
                            <div class="approach fragment">
                                <h5>ü§ñ Automated Search</h5>
                                <ul>
                                    <li><strong>NASNet:</strong> Neural Architecture Search</li>
                                    <li><strong>MobileNet v3:</strong> Hardware-aware NAS</li>
                                    <li><strong>RegNet:</strong> Design space exploration</li>
                                </ul>
                                <p class="highlight">Goal: Let algorithms design networks</p>
                            </div>
                        </div>
                        <div class="fragment mt-lg" style="font-size: 0.8em;">
                            <div class="insight-box">
                                <p><strong>RegNet Insight:</strong> Systematic exploration revealed that good networks share common design principles - depth, width, and groups follow simple quantized linear relationships</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which two techniques have recently become standard in training deep neural networks?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Batch Normalization",
                                "correct": true,
                                "explanation": "Correct! Batch normalization addresses internal covariate shift and enables faster training with higher learning rates."
                            },
                            {
                                "text": "Residual Connections",
                                "correct": true,
                                "explanation": "Correct! Residual connections solve the vanishing gradient problem, enabling networks with hundreds of layers."
                            },
                            {
                                "text": "Random Weight Initialization",
                                "correct": false,
                                "explanation": "While important, random initialization predates modern CNNs and isnt one of the two key modern innovations discussed."
                            },
                            {
                                "text": "Max Pooling",
                                "correct": false,
                                "explanation": "Max pooling has been used since early CNNs like LeNet and isnt one of the two modern breakthrough techniques."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 3: Recent Developments and Future (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dosovitskiy et al. (2021) - Vision Transformer", "url": "https://arxiv.org/abs/2010.11929"}, {"text": "Liu et al. (2021) - Swin Transformer", "url": "https://arxiv.org/abs/2103.14030"}]'>
                    <h2 class="truncate-title">The Transformer Revolution in Vision</h2>
                    <div class="transformer-revolution">
                        <div class="fragment">
                            <h4>Beyond Convolutions: Vision Transformers</h4>
                            <p style="font-size: 0.8em;">Starting with ViT (2021), Transformers began displacing CNNs in computer vision</p>
                        </div>
                        <div class="comparison-table fragment" style="font-size: 0.65em; margin-top: 30px;">
                            <table style="width: 100%; margin: 0 auto;">
                                <thead>
                                    <tr>
                                        <th>Aspect</th>
                                        <th>CNNs</th>
                                        <th>Vision Transformers</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Inductive Bias</strong></td>
                                        <td>Strong (locality, translation invariance)</td>
                                        <td>Weak (learned from data)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Data Requirements</strong></td>
                                        <td>Works well with less data</td>
                                        <td>Needs large-scale data</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Receptive Field</strong></td>
                                        <td>Grows with depth</td>
                                        <td>Global from the start</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Computational Cost</strong></td>
                                        <td>Linear with image size</td>
                                        <td>Quadratic with sequence length</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="fragment mt-lg">
                            <h5 style="font-size: 0.8em;">Key Innovations:</h5>
                            <ul style="font-size: 0.7em;">
                                <li><strong>ViT:</strong> "An image is worth 16x16 words" - treats image patches as tokens</li>
                                <li><strong>Swin Transformer:</strong> Hierarchical vision transformer with shifted windows</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Liu et al. (2022) - A ConvNet for the 2020s", "url": "https://arxiv.org/abs/2201.03545"}]'>
                    <h2 class="truncate-title">Modern Insights: It's Not Just Architecture</h2>
                    <div class="modern-insights" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">ConvNeXt: Modernizing CNNs (2022)</h4>
                        <div class="insights-grid fragment" style="font-size: 0.7em; margin-top: 30px;">
                            <div class="insight-card">
                                <h5>üéØ Training Matters</h5>
                                <p>Modern training recipes (optimizers, augmentation, regularization) can be more important than architecture</p>
                            </div>
                            <div class="insight-card">
                                <h5>üìê Revisiting Assumptions</h5>
                                <p>Larger kernels (7x7) can outperform the standard 3x3 with modern compute and data</p>
                            </div>
                            <div class="insight-card">
                                <h5>‚öñÔ∏è Design Principles</h5>
                                <p>Simple, uniform designs often outperform complex, heterogeneous ones</p>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <div class="results-box" style="font-size: 0.75em;">
                                <h5>ConvNeXt Results:</h5>
                                <p>By modernizing a standard ResNet with Transformer-inspired modifications and training techniques, ConvNeXt matched or exceeded Swin Transformer performance!</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-md" style="font-size: 0.8em;">
                            <p><strong>Key Takeaway:</strong> The gap between CNNs and Transformers may be smaller than we thought - training recipes and design details matter immensely</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What did the ConvNeXt paper (Liu et al., 2022) demonstrate about modern CNNs?",
                        "type": "single",
                        "options": [
                            {
                                "text": "CNNs can never match Transformer performance",
                                "correct": false,
                                "explanation": "Actually, ConvNeXt showed that modernized CNNs can match or exceed Transformer performance."
                            },
                            {
                                "text": "Training techniques and design details can be as important as architecture choice",
                                "correct": true,
                                "explanation": "Correct! ConvNeXt demonstrated that with modern training techniques and careful design modifications, CNNs can achieve competitive performance with Transformers."
                            },
                            {
                                "text": "Only 3x3 kernels should be used in CNNs",
                                "correct": false,
                                "explanation": "ConvNeXt actually showed that larger kernels (7x7) can be beneficial with modern compute and data."
                            },
                            {
                                "text": "Vision Transformers require less data than CNNs",
                                "correct": false,
                                "explanation": "Vision Transformers typically require more data than CNNs due to their weaker inductive bias."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Summary Slide -->
            <section data-sources='[{"text": "Dive into Deep Learning - Chapter 8", "url": "https://d2l.ai/chapter_modern-convolutional-neural-networks/index.html"}]'>
                <h2 class="truncate-title">What's Next in This Chapter</h2>
                <div class="chapter-preview">
                    <h4 style="font-size: 0.9em;">Deep Dives into Each Architecture</h4>
                    <div class="topics-grid" style="font-size: 0.65em; margin-top: 30px;">
                        <div class="topic-card">
                            <h5>8.1 AlexNet</h5>
                            <p>The network that started the deep learning revolution</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.2 VGG</h5>
                            <p>Networks using repeated blocks</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.3 NiN</h5>
                            <p>Network in Network - 1x1 convolutions</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.4 GoogLeNet</h5>
                            <p>Multi-branch Inception modules</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.5 Batch Norm</h5>
                            <p>Training deep networks effectively</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.6 ResNet</h5>
                            <p>Residual connections and ResNeXt</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.7 DenseNet</h5>
                            <p>Dense connections between layers</p>
                        </div>
                        <div class="topic-card">
                            <h5>8.8 Design</h5>
                            <p>Systematic architecture design</p>
                        </div>
                    </div>
                    <div class="emphasis-box mt-lg" style="font-size: 0.8em;">
                        <p>Each architecture contributed key ideas that are still used today!</p>
                    </div>
                </div>
            </section>

            <!-- Section 4: AlexNet Introduction (Vertical) -->
            <section>
                <section data-sources='[{"text": "Krizhevsky et al. (2012) - ImageNet Classification with Deep CNNs", "url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"}]'>
                    <h2 class="truncate-title">AlexNet: The 2012 Breakthrough That Changed Everything</h2>
                    <div class="alexnet-intro" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4 style="font-size: 0.9em;">The Network That Started the Deep Learning Revolution</h4>
                            <p style="font-size: 0.8em;">In 2012, AlexNet won the ImageNet competition by a stunning margin</p>
                        </div>
                        <div class="achievement-grid fragment" style="font-size: 0.7em; margin-top: 30px;">
                            <div class="achievement-card">
                                <div class="metric">16.4%</div>
                                <div class="label">Top-5 Error Rate</div>
                                <div class="comparison">Previous best: 26.2%</div>
                            </div>
                            <div class="achievement-card">
                                <div class="metric">10.8%</div>
                                <div class="label">Absolute Improvement</div>
                                <div class="comparison">Largest jump in history</div>
                            </div>
                            <div class="achievement-card">
                                <div class="metric">60M</div>
                                <div class="label">Parameters</div>
                                <div class="comparison">5x larger than LeNet</div>
                            </div>
                        </div>
                        <div class="key-people fragment" style="font-size: 0.75em; margin-top: 30px;">
                            <h5>The Architects</h5>
                            <p><strong>Alex Krizhevsky</strong> - Lead developer, created cuda-convnet</p>
                            <p><strong>Ilya Sutskever</strong> - Co-architect, former Chief Scientist at OpenAI and founder of Safe Superintelligence</p>
                            <p><strong>Geoffrey Hinton</strong> - Advisor, "Godfather of Deep Learning"</p>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.8em;">
                            <p><strong>Impact:</strong> AlexNet proved that deep learning could outperform hand-crafted features, triggering the AI revolution we see today</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Lowe (2004) - SIFT Features", "url": "https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf"}, {"text": "Bay et al. (2006) - SURF Features", "url": "https://vision.ee.ethz.ch/~surf/eccv06.pdf"}]'>
                    <h2 class="truncate-title">From Feature Engineering to Feature Learning</h2>
                    <div class="feature-evolution" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">The Paradigm Shift in Computer Vision</h4>
                        <div class="pipeline-comparison" style="font-size: 0.65em; margin-top: 30px;">
                            <div class="pipeline-old fragment">
                                <h5>Traditional Pipeline (Pre-2012)</h5>
                                <div class="pipeline-steps">
                                    <div class="step">1. Raw Images</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step">2. Hand-crafted Features (<span class="tooltip">SIFT<span class="tooltiptext">Scale-Invariant Feature Transform: Detects and describes local features in images that are invariant to scale and rotation</span></span>, <span class="tooltip">SURF<span class="tooltiptext">Speeded Up Robust Features: A faster version of SIFT that uses integral images for efficiency</span></span>, <span class="tooltip">HOG<span class="tooltiptext">Histogram of Oriented Gradients: Counts occurrences of gradient orientation in localized portions of an image</span></span>)</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step">3. Feature Engineering</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step">4. Classifier (SVM, Random Forest)</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step">5. Predictions</div>
                                </div>
                            </div>
                            <div class="pipeline-new fragment">
                                <h5>Deep Learning Pipeline (Post-2012)</h5>
                                <div class="pipeline-steps">
                                    <div class="step">1. Raw Images</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step highlight">2. Learned Features (CNN Layers)</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step highlight">3. End-to-End Training</div>
                                    <div class="arrow">‚Üì</div>
                                    <div class="step">4. Predictions</div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment mt-lg">
                            <h5 style="font-size: 0.8em;">The Key Insight</h5>
                            <p style="font-size: 0.75em;">Instead of manually designing features, let the network learn them from data!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "LeCun et al. (1998) - Gradient-Based Learning", "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"}]'>
                    <h2 class="truncate-title">AlexNet vs LeNet: Evolution of CNN Architecture</h2>
                    <div class="architecture-comparison">
                        <h4 style="font-size: 0.9em;">17 Years of Progress (1995 ‚Üí 2012)</h4>
                        <div class="comparison-table fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <table style="width: 100%; margin: 0 auto;">
                                <thead>
                                    <tr>
                                        <th>Aspect</th>
                                        <th>LeNet-5 (1998)</th>
                                        <th>AlexNet (2012)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Depth</strong></td>
                                        <td>5 layers</td>
                                        <td>8 layers</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Parameters</strong></td>
                                        <td>60K</td>
                                        <td>60M (1000x more!)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Input Size</strong></td>
                                        <td>32√ó32 grayscale</td>
                                        <td>224√ó224 RGB</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Activation</strong></td>
                                        <td>Tanh/Sigmoid</td>
                                        <td><span class="tooltip">ReLU<span class="tooltiptext">Rectified Linear Unit: f(x) = max(0, x), faster to compute and helps with vanishing gradients</span></span></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Regularization</strong></td>
                                        <td>None</td>
                                        <td><span class="tooltip">Dropout<span class="tooltiptext">Randomly drops neurons during training to prevent overfitting</span></span></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Normalization</strong></td>
                                        <td>None</td>
                                        <td><span class="tooltip">LRN<span class="tooltiptext">Local Response Normalization: Normalizes activations across neighboring channels</span></span></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Training</strong></td>
                                        <td>CPU</td>
                                        <td>GPU (2√ó GTX 580)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Dataset</strong></td>
                                        <td>MNIST (60K)</td>
                                        <td>ImageNet (1.2M)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What was the most significant paradigm shift that AlexNet represented in computer vision?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It was the first neural network ever created",
                                "correct": false,
                                "explanation": "Neural networks existed long before AlexNet. LeNet was created in the 1990s."
                            },
                            {
                                "text": "It proved that learned features could outperform hand-crafted features",
                                "correct": true,
                                "explanation": "Correct! AlexNet showed that end-to-end learning of features from raw pixels could dramatically outperform traditional hand-crafted features like SIFT and SURF."
                            },
                            {
                                "text": "It was the first network to use convolutions",
                                "correct": false,
                                "explanation": "Convolutions were used in earlier networks like LeNet. AlexNet built upon these foundations."
                            },
                            {
                                "text": "It introduced the concept of deep learning",
                                "correct": false,
                                "explanation": "Deep learning concepts existed before AlexNet, but AlexNet proved their practical effectiveness at scale."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 5: AlexNet Architecture Deep Dive (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - AlexNet", "url": "https://d2l.ai/chapter_convolutional-modern/alexnet.html"}]'>
                    <h2 class="truncate-title">AlexNet Architecture: Layer-by-Layer Breakdown</h2>
                    <div class="architecture-details">
                        <h4 style="font-size: 0.9em;">8 Layers of Learned Representations</h4>
                        <div id="alexnet-architecture-viz" style="margin: 20px 0;">
                            <!-- Interactive architecture visualization will be rendered here -->
                        </div>
                        <div class="layer-progression fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <h5>Progressive Feature Extraction</h5>
                            <div class="layer-grid">
                                <div class="layer-info">
                                    <strong>Conv1:</strong> 11√ó11 kernels, 96 channels<br>
                                    <em>Detects edges, colors, textures</em>
                                </div>
                                <div class="layer-info">
                                    <strong>Conv2:</strong> 5√ó5 kernels, 256 channels<br>
                                    <em>Combines simple features</em>
                                </div>
                                <div class="layer-info">
                                    <strong>Conv3-5:</strong> 3√ó3 kernels, 384/384/256 channels<br>
                                    <em>Builds complex patterns</em>
                                </div>
                                <div class="layer-info">
                                    <strong>FC6-8:</strong> 4096/4096/1000 units<br>
                                    <em>High-level reasoning & classification</em>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Krizhevsky et al. (2012) - ImageNet Classification", "url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"}]'>
                    <h2 class="truncate-title">Key Architectural Design Choices</h2>
                    <div class="design-choices" style="font-size: 0.6em;">
                        <h4 style="font-size: 0.9em;">Why These Specific Numbers?</h4>

                        <div class="choice-explanation fragment" style="font-size: 0.7em;">
                            <h5>Convolution Window Sizes</h5>
                            <div class="window-sizes">
                                <div class="size-card">
                                    <div class="size">11√ó11</div>
                                    <div class="reason">Large receptive field for 224√ó224 images</div>
                                    <div class="math">$\text{Output size} = \lfloor \frac{224 - 11 + 2 \times 1}{4} \rfloor + 1 = 55$</div>
                                </div>
                                <div class="size-card">
                                    <div class="size">5√ó5</div>
                                    <div class="reason">Medium features, computational efficiency</div>
                                </div>
                                <div class="size-card">
                                    <div class="size">3√ó3</div>
                                    <div class="reason">Fine details, proven effective</div>
                                </div>
                            </div>
                        </div>

                        <div class="choice-explanation fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Channel Progression: 96 ‚Üí 256 ‚Üí 384 ‚Üí 384 ‚Üí 256</h5>
                            <p>Increases capacity in middle layers where features are most complex</p>
                        </div>

                        <div class="choice-explanation fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Max Pooling Strategy</h5>
                            <ul>
                                <li>3√ó3 windows with stride 2 (overlapping pooling)</li>
                                <li>Reduces overfitting compared to non-overlapping</li>
                                <li>Applied after layers 1, 2, and 5</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does AlexNet use progressively smaller convolution kernels (11√ó11 ‚Üí 5√ó5 ‚Üí 3√ó3)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Smaller kernels are always better",
                                "correct": false,
                                "explanation": "Not always - larger kernels capture broader context which is important in early layers."
                            },
                            {
                                "text": "To progressively refine features from coarse to fine as spatial dimensions shrink",
                                "correct": true,
                                "explanation": "Correct! Early layers need large receptive fields for the 224√ó224 input, while later layers work on smaller feature maps needing only local connectivity."
                            },
                            {
                                "text": "It was a random choice",
                                "correct": false,
                                "explanation": "The kernel sizes were carefully chosen based on the input size and feature hierarchy needs."
                            },
                            {
                                "text": "To reduce the number of parameters",
                                "correct": false,
                                "explanation": "While smaller kernels have fewer parameters, the main reason was to match the receptive field to the feature map size."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 6: AlexNet Innovations (Vertical) -->
            <section>
                <section data-sources='[{"text": "Nair & Hinton (2010) - Rectified Linear Units", "url": "https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf"}]'>
                    <h2 class="truncate-title">ReLU: The Activation Function Revolution</h2>
                    <div class="relu-explanation" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">Why ReLU Changed Everything</h4>

                        <div class="relu-advantages fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>ReLU Advantages</h5>
                            <div class="advantage-grid">
                                <div class="advantage-card">
                                    <h6>Computational Efficiency</h6>
                                    <p>$$\text{ReLU}(x) = \max(0, x)$$</p>
                                    <p>No exponentials, just comparison!</p>
                                </div>
                                <div class="advantage-card">
                                    <h6>Gradient Flow</h6>
                                    <p>$$\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}$$</p>
                                    <p>No vanishing gradients for positive inputs</p>
                                </div>
                                <div class="advantage-card">
                                    <h6>Sparsity</h6>
                                    <p>~50% of neurons output zero</p>
                                    <p>Creates sparse representations</p>
                                </div>
                            </div>
                        </div>

                        <div class="sigmoid-problems fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Problems with Sigmoid/Tanh</h5>
                            <ul>
                                <li><strong>Vanishing Gradients:</strong> $$\frac{d}{dx}\sigma(x) = \sigma(x)(1-\sigma(x))$$ approaches 0 for large |x|</li>
                                <li><strong>Computational Cost:</strong> <span class="tooltip">Exponential operations are expensive<span class="tooltiptext">Exponential functions require multiple CPU cycles because they involve complex mathematical operations like Taylor series approximation or lookup tables with interpolation. Modern CPUs need ~10-20 cycles for exp() vs 1 cycle for ReLU's simple comparison (max(0,x)). With millions of activations per forward pass, this difference becomes significant.</span></span></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Srivastava et al. (2014) - Dropout", "url": "https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"}]'>
                    <h2 class="truncate-title">Dropout: Regularization Through Randomness</h2>
                    <div class="dropout-explanation" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">Preventing Overfitting with 60M Parameters</h4>

                        <div class="dropout-math fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Mathematical Formulation</h5>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                <p><strong>Training:</strong></p>
                                <p>$$r_j \sim \text{Bernoulli}(p)$$</p>
                                <p>$$\tilde{y} = r \odot y$$</p>
                                <p><strong>Inference:</strong></p>
                                <p>$$y_{\text{test}} = p \cdot y$$</p>
                                <p>Where p is the keep probability (1 - dropout rate)</p>
                            </div>
                        </div>

                        <div class="dropout-intuition fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Why Dropout Works</h5>
                            <ul>
                                <li><strong>Ensemble Effect:</strong> Training multiple sub-networks simultaneously</li>
                                <li><strong>Co-adaptation Prevention:</strong> Neurons can't rely on specific other neurons</li>
                                <li><strong>Robustness:</strong> Network learns redundant representations</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Krizhevsky et al. (2012) - Data Augmentation", "url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"}]'>
                    <h2 class="truncate-title">Data Augmentation: Creating More Training Data</h2>
                    <div class="augmentation-explanation">
                        <h4 style="font-size: 0.9em;">Expanding the Dataset Without New Images</h4>

                        <div class="augmentation-demo" style="margin: 20px 0;">
                            <div class="demo-controls" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 10px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px;">
                                <button class="aug-btn" data-aug="original" style="padding: 6px 12px; border: 1px solid #10099F; background: #10099F; color: white; border-radius: 5px; cursor: pointer;">Original</button>
                                <button class="aug-btn" data-aug="flip" style="padding: 6px 12px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">Vertical Flip</button>
                                <button class="aug-btn" data-aug="crop" style="padding: 6px 12px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">Random Crop</button>
                                <button class="aug-btn" data-aug="color" style="padding: 6px 12px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">Color Jitter</button>
                                <button class="aug-btn" data-aug="pca" style="padding: 6px 12px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">PCA Color</button>
                            </div>
                            <div id="augmentation-viz" style="background: white; padding: 20px; border-radius: 8px;">
                                <!-- Interactive augmentation visualization -->
                            </div>
                        </div>

                        <div class="augmentation-techniques fragment" style="font-size: 0.5em; margin-top: 20px;">
                            <h5>AlexNet's Augmentation Pipeline</h5>
                            <div class="technique-grid">
                                <div class="technique-card">
                                    <h6>Random Crops</h6>
                                    <p>Extract 224√ó224 patches from 256√ó256 images</p>
                                    <p>Increases dataset by 2048√ó</p>
                                </div>
                                <div class="technique-card">
                                    <h6>Horizontal Flips</h6>
                                    <p>Mirror images horizontally</p>
                                    <p>Doubles the dataset</p>
                                </div>
                                <div class="technique-card">
                                    <h6>PCA Color Augmentation</h6>
                                    <p>Add multiples of principal components</p>
                                    <p>Captures lighting variations</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Jarrett et al. (2009) - Local Contrast Normalization", "url": "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf"}]'>
                    <h2 class="truncate-title">Local Response Normalization (LRN)</h2>
                    <div class="lrn-explanation" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">Lateral Inhibition in Neural Networks</h4>

                        <div class="lrn-formula fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Mathematical Definition</h5>
                            <div style="background: #f9f9f9; padding: 20px; border-radius: 5px;">
                                <p>$$b_{x,y}^i = \frac{a_{x,y}^i}{\left(k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{x,y}^j)^2\right)^\beta}$$</p>
                                <p style="margin-top: 15px;">Where:</p>
                                <ul style="text-align: left; display: inline-block;">
                                    <li>$a_{x,y}^i$ = activity of neuron at position (x,y) in channel i</li>
                                    <li>$b_{x,y}^i$ = normalized activity</li>
                                    <li>n = number of adjacent channels to normalize over</li>
                                    <li>k = 2, Œ± = 10‚Åª‚Å¥, Œ≤ = 0.75 (hyperparameters)</li>
                                </ul>
                            </div>
                        </div>

                        <div class="lrn-intuition fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Biological Inspiration</h5>
                            <p>Mimics <span class="tooltip">lateral inhibition<span class="tooltiptext">A neural mechanism where activated neurons suppress the activity of neighboring neurons, creating competition</span></span> in the visual cortex</p>
                            <p>Creates competition between feature maps</p>
                        </div>

                        <div class="lrn-note fragment emphasis-box" style="font-size: 0.7em; margin-top: 20px;">
                            <p><strong>Note:</strong> LRN is rarely used today - Batch Normalization proved more effective</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which AlexNet innovations are still widely used in modern deep learning?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "ReLU activation function",
                                "correct": true,
                                "explanation": "Correct! ReLU remains the most popular activation function due to its simplicity and effectiveness."
                            },
                            {
                                "text": "Dropout regularization",
                                "correct": true,
                                "explanation": "Correct! Dropout is still widely used for regularization, though sometimes replaced by other techniques like DropBlock."
                            },
                            {
                                "text": "Local Response Normalization",
                                "correct": false,
                                "explanation": "LRN has been largely replaced by Batch Normalization which is more effective."
                            },
                            {
                                "text": "Data augmentation",
                                "correct": true,
                                "explanation": "Correct! Data augmentation is essential in modern deep learning and has become even more sophisticated."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 7: The Missing Ingredients (Vertical) -->
            <section>
                <section data-sources='[{"text": "Deng et al. (2009) - ImageNet", "url": "http://www.image-net.org/papers/imagenet_cvpr09.pdf"}, {"text": "Russakovsky et al. (2015) - ImageNet Challenge", "url": "https://arxiv.org/abs/1409.0575"}]'>
                    <h2 class="truncate-title">The Data Revolution: ImageNet's Game-Changing Scale</h2>
                    <div class="imagenet-impact" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">From Toy Datasets to Real-World Scale</h4>

                        <div class="dataset-comparison fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <h5>Dataset Evolution</h5>
                            <div class="dataset-timeline">
                                <div class="dataset-card">
                                    <h6>MNIST (1998)</h6>
                                    <div class="stats">
                                        <div>60K images</div>
                                        <div>28√ó28 pixels</div>
                                        <div>10 classes</div>
                                        <div>Grayscale</div>
                                    </div>
                                </div>
                                <div class="dataset-card">
                                    <h6>CIFAR-10 (2009)</h6>
                                    <div class="stats">
                                        <div>60K images</div>
                                        <div>32√ó32 pixels</div>
                                        <div>10 classes</div>
                                        <div>Color</div>
                                    </div>
                                </div>
                                <div class="dataset-card highlight">
                                    <h6>ImageNet (2009)</h6>
                                    <div class="stats">
                                        <div><strong>1.2M images</strong></div>
                                        <div><strong>224√ó224+ pixels</strong></div>
                                        <div><strong>1000 classes</strong></div>
                                        <div><strong>High-res color</strong></div>
                                    </div>
                                </div>
                                <div class="dataset-card">
                                    <h6>LAION-5B (2022)</h6>
                                    <div class="stats">
                                        <div>5.85B images</div>
                                        <div>Variable size</div>
                                        <div>Text pairs</div>
                                        <div>Web-scale</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="imagenet-details fragment" style="font-size: 0.7em; margin-top: 30px;">
                            <h5>Why ImageNet Was Different</h5>
                            <div class="detail-grid">
                                <div class="detail-card">
                                    <h6>üìä Scale</h6>
                                    <p>20√ó larger than previous benchmarks</p>
                                    <p>1000 images per class</p>
                                </div>
                                <div class="detail-card">
                                    <h6>üéØ Diversity</h6>
                                    <p>Real-world images from the web</p>
                                    <p>Natural backgrounds and variations</p>
                                </div>
                                <div class="detail-card">
                                    <h6>üè∑Ô∏è Quality</h6>
                                    <p>Human-verified labels</p>
                                    <p>WordNet hierarchy organization</p>
                                </div>
                                <div class="detail-card">
                                    <h6>üî¨ Resolution</h6>
                                    <p>High-resolution images</p>
                                    <p>Enables learning fine details</p>
                                </div>
                            </div>
                        </div>

                        <div class="data-insight fragment emphasis-box" style="font-size: 0.75em; margin-top: 20px;">
                            <p><strong>Key Insight:</strong> Deep networks need large datasets to reach their potential - with small data, simpler models often work better</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Fernando (2004) - GPU Gems", "url": "https://developer.nvidia.com/gpugems/gpugems/foreword"}]'>
                    <h2 class="truncate-title">The Hardware Revolution: GPUs Enable Deep Learning</h2>
                    <div class="gpu-revolution" style="font-size: 0.7em;">
                        <div class="hardware-timeline fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <h5>GPU Evolution for Deep Learning</h5>
                            <div class="timeline-cards">
                                <div class="hw-card">
                                    <h6>1999: GeForce 256</h6>
                                    <p>480 MFLOPS</p>
                                    <p>Graphics only</p>
                                </div>
                                <div class="hw-card">
                                    <h6>2006: CUDA Launch</h6>
                                    <p>General computing on GPUs</p>
                                    <p>C-like programming</p>
                                </div>
                                <div class="hw-card highlight">
                                    <h6>2012: GTX 580 (AlexNet)</h6>
                                    <p>1.5 TFLOPS</p>
                                    <p>3GB memory</p>
                                    <p>cuda-convnet library</p>
                                </div>
                                <div class="hw-card">
                                    <h6>2020: A100</h6>
                                    <p>312 TFLOPS (TF32)</p>
                                    <p>40GB memory</p>
                                    <p>Tensor cores</p>
                                </div>
                            </div>
                        </div>

                        <div class="gpu-advantages fragment" style="font-size: 0.7em; margin-top: 30px;">
                            <h5>Why GPUs Excel at Deep Learning</h5>
                            <div class="advantage-comparison">
                                <div class="cpu-col">
                                    <h6>CPU Design</h6>
                                    <ul>
                                        <li>4-64 powerful cores</li>
                                        <li>Complex control logic</li>
                                        <li>Large caches</li>
                                        <li><span class="tooltip">Branch prediction<span class="tooltiptext">Hardware feature that predicts which way conditional branches will go to avoid pipeline stalls. Example: In code like "if (x > 0) { ... } else { ... }", the CPU guesses which branch will be taken based on past behavior to keep the instruction pipeline full</span></span></li>
                                        <li><span class="tooltip">Sequential optimization<span class="tooltiptext">CPUs are designed to execute instructions one after another very efficiently, with features like out-of-order execution and instruction pipelining to maximize single-threaded performance</span></span></li>
                                    </ul>
                                </div>
                                <div class="gpu-col">
                                    <h6>GPU Design</h6>
                                    <ul>
                                        <li>1000s of simple cores</li>
                                        <li>Massive parallelism</li>
                                        <li>High memory bandwidth</li>
                                        <li>Matrix operations</li>
                                        <li>Throughput optimization</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="alexnet-gpu fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>AlexNet's GPU Strategy</h5>
                            <p>Used 2√ó GTX 580 GPUs with model parallelism</p>
                            <p>Split network across GPUs due to 3GB memory limit</p>
                            <p>Training took 5-6 days (would take weeks on CPU)</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Power Comparison</h2>
                    <div class="computation-comparison">
                        <h4 style="font-size: 0.9em;">CPUs vs GPUs: A Visual Comparison (assuming $2n^3$ operations for matrix multiplication)</h4>

                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Matrix Size:
                                <input type="range" id="matrix-size" min="100" max="5000" value="1000" step="100" style="width: 150px;">
                                <span id="matrix-size-value" style="font-family: monospace; min-width: 50px;">1000</span>
                            </label>
                            <button id="run-computation" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Simulate Computation</button>
                        </div>

                        <div id="computation-viz" style="background: white; padding: 20px; border-radius: 8px; min-height: 300px;">
                            <!-- Interactive computation comparison -->
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What combination of factors enabled AlexNets breakthrough performance?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Large-scale dataset (ImageNet)",
                                "correct": true,
                                "explanation": "Correct! The 1.2M image ImageNet dataset provided enough data for deep networks to generalize well."
                            },
                            {
                                "text": "GPU acceleration",
                                "correct": true,
                                "explanation": "Correct! GPUs made it computationally feasible to train deep networks in reasonable time."
                            },
                            {
                                "text": "Better optimization algorithms",
                                "correct": false,
                                "explanation": "While SGD with momentum was used, the optimization algorithms were not fundamentally different from earlier work."
                            },
                            {
                                "text": "Architectural innovations (ReLU, Dropout)",
                                "correct": true,
                                "explanation": "Correct! ReLU enabled training deeper networks, and dropout prevented overfitting."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 8: Implementation and Training (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - AlexNet Implementation", "url": "https://d2l.ai/chapter_convolutional-modern/alexnet.html"}]'>
                    <h2 class="truncate-title">Complete AlexNet Implementation in PyTorch</h2>
                    <div class="implementation-section" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">Building AlexNet from Scratch (without LRN)</h4>

                        <pre style="font-size: 0.55em; background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto;"><code class="language-python">import torch
from torch import nn
from d2l import torch as d2l

class AlexNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            # First convolutional layer
            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Second convolutional layer
            nn.LazyConv2d(256, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Third, fourth, and fifth convolutional layers
            nn.LazyConv2d(384, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LazyConv2d(384, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.LazyConv2d(256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Flatten for fully connected layers
            nn.Flatten(),

            # First fully connected layer
            nn.LazyLinear(4096),
            nn.ReLU(),
            nn.Dropout(p=0.5),

            # Second fully connected layer
            nn.LazyLinear(4096),
            nn.ReLU(),
            nn.Dropout(p=0.5),

            # Output layer
            nn.LazyLinear(num_classes)
        )
        self.net.apply(d2l.init_cnn)</code></pre>

                        <div class="implementation-notes fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Key Implementation Details</h5>
                            <ul>
                                <li><strong>LazyConv2d:</strong> Automatically infers input channels</li>
                                <li><strong>Padding strategy:</strong> Maintains spatial dimensions where needed</li>
                                <li><strong>Dropout placement:</strong> Only in fully connected layers</li>
                                <li><strong>Initialization:</strong> Uses proper CNN initialization</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Configuration and Hyperparameters</h2>
                    <div class="training-config">
                        <h4 style="font-size: 0.9em;">AlexNet Training Recipe</h4>

                        <div class="hyperparameter-table fragment" style="font-size: 0.45em; margin-top: 20px;">
                            <table style="width: 100%; margin: 0 auto;">
                                <thead>
                                    <tr>
                                        <th>Hyperparameter</th>
                                        <th>Original (2012)</th>
                                        <th>Modern Default</th>
                                        <th>Purpose</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Learning Rate</strong></td>
                                        <td>0.01 (with decay)</td>
                                        <td>0.001-0.01</td>
                                        <td>Controls step size</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Batch Size</strong></td>
                                        <td>128</td>
                                        <td>256-512</td>
                                        <td>Samples per update</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Momentum</strong></td>
                                        <td>0.9</td>
                                        <td>0.9</td>
                                        <td>Accelerates SGD</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Weight Decay</strong></td>
                                        <td>0.0005</td>
                                        <td>0.0001-0.0005</td>
                                        <td>L2 regularization</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Dropout Rate</strong></td>
                                        <td>0.5</td>
                                        <td>0.2-0.5</td>
                                        <td>Regularization</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Epochs</strong></td>
                                        <td>90</td>
                                        <td>100-200</td>
                                        <td>Training iterations</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="training-code fragment" style="font-size: 0.6em; margin-top: 20px;">
                            <h5>Training Loop</h5>
                            <pre style="background: #f5f5f5; padding: 15px; border-radius: 5px;"><code class="language-python"># Initialize model and data
model = AlexNet(lr=0.01)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)

# Train the model
trainer.fit(model, data)</code></pre>
                        </div>

                        <div class="learning-schedule fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>Learning Rate Schedule</h5>
                            <p>Original paper: Divide by 10 when validation error plateaus</p>
                            <p>Modern approach: Cosine annealing or one-cycle policy</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does AlexNet use dropout only in the fully connected layers and not in convolutional layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Convolutional layers dont need regularization",
                                "correct": false,
                                "explanation": "Convolutional layers can benefit from regularization, but they have other properties that reduce overfitting."
                            },
                            {
                                "text": "Fully connected layers have most parameters and are more prone to overfitting",
                                "correct": true,
                                "explanation": "Correct! The two FC layers contain ~90% of AlexNets parameters (over 50M), making them the primary source of overfitting."
                            },
                            {
                                "text": "Dropout doesnt work with convolutions",
                                "correct": false,
                                "explanation": "Dropout can work with convolutions, though spatial dropout variants are often preferred."
                            },
                            {
                                "text": "It was a random design choice",
                                "correct": false,
                                "explanation": "This was a deliberate choice based on where overfitting was most likely to occur."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9: Learned Features Analysis (Vertical) -->
            <section>
                <section data-sources='[{"text": "Zeiler & Fergus (2013) - Visualizing CNNs", "url": "https://arxiv.org/abs/1311.2901"}]'>
                    <h2 class="truncate-title">What Does AlexNet Learn? First Layer Filters</h2>
                    <div class="learned-features">
                        <h4 style="font-size: 0.9em;">Visualizing the 96 Learned Kernels</h4>

                        <div class="filter-visualization" style="margin: 20px 0;">
                            <img src="./images/alexnet_filters.jpg" alt="AlexNet First Layer Filters" style="max-width: 300px; margin: 0 auto; display: block; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                            <p style="font-size: 0.7em; text-align: center; margin-top: 10px;">First layer filters from AlexNet (Krizhevsky et al., 2012)</p>
                        </div>

                        <div class="filter-analysis fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>What Do We See?</h5>
                            <div class="filter-types">
                                <div class="filter-type">
                                    <h6>Edge Detectors</h6>
                                    <p>Gabor-like filters at various orientations</p>
                                </div>
                                <div class="filter-type">
                                    <h6>Color Blobs</h6>
                                    <p>Red-green and blue-yellow opponents</p>
                                </div>
                                <div class="filter-type">
                                    <h6>Frequency Patterns</h6>
                                    <p>High and low frequency detectors</p>
                                </div>
                            </div>
                        </div>

                        <div class="gpu-split fragment" style="font-size: 0.5em; margin-top: 20px;">
                            <h5>GPU Specialization Pattern</h5>
                            <p>Top 48 filters (GPU 1): More color-agnostic, edge-focused</p>
                            <p>Bottom 48 filters (GPU 2): More color-specific patterns</p>
                            <p>This emerged naturally from the split architecture!</p>
                        </div>
                    </div>
                </section>


                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What pattern emerged in AlexNets first layer filters due to the two-GPU architecture?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Both GPUs learned identical filters",
                                "correct": false,
                                "explanation": "The GPUs actually learned different specialized filters despite having the same architecture."
                            },
                            {
                                "text": "One GPU focused on color-agnostic features, the other on color-specific features",
                                "correct": true,
                                "explanation": "Correct! This specialization emerged naturally - one GPU learned more grayscale edge detectors while the other learned color-sensitive patterns."
                            },
                            {
                                "text": "The filters were randomly distributed",
                                "correct": false,
                                "explanation": "The distribution showed clear patterns of specialization, not random assignment."
                            },
                            {
                                "text": "Only one GPU actually learned useful features",
                                "correct": false,
                                "explanation": "Both GPUs learned useful but complementary features."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 10: Computational Analysis (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Computational Performance", "url": "https://d2l.ai/chapter_computational-performance/index.html"}]'>
                    <h2 class="truncate-title">Memory Footprint: Where Do 60M Parameters Live?</h2>
                    <div class="memory-analysis">
                        <h4 style="font-size: 0.9em;">Layer-by-Layer Memory Requirements</h4>
                        <div class="memory-table fragment" style="font-size: 0.6em; margin-top: 20px;">
                            <table style="width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Layer</th>
                                        <th>Parameters</th>
                                        <th>Memory (MB)</th>
                                        <th>% of Total</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Conv1 (11√ó11√ó3√ó96)</td>
                                        <td>35K</td>
                                        <td>0.13</td>
                                        <td>0.06%</td>
                                    </tr>
                                    <tr>
                                        <td>Conv2 (5√ó5√ó96√ó256)</td>
                                        <td>614K</td>
                                        <td>2.34</td>
                                        <td>1.0%</td>
                                    </tr>
                                    <tr>
                                        <td>Conv3 (3√ó3√ó256√ó384)</td>
                                        <td>885K</td>
                                        <td>3.37</td>
                                        <td>1.5%</td>
                                    </tr>
                                    <tr>
                                        <td>Conv4 (3√ó3√ó384√ó384)</td>
                                        <td>1.3M</td>
                                        <td>5.06</td>
                                        <td>2.2%</td>
                                    </tr>
                                    <tr>
                                        <td>Conv5 (3√ó3√ó384√ó256)</td>
                                        <td>885K</td>
                                        <td>3.37</td>
                                        <td>1.5%</td>
                                    </tr>
                                    <tr class="highlight">
                                        <td><strong>FC6 (6400√ó4096)</strong></td>
                                        <td><strong>26.2M</strong></td>
                                        <td><strong>100</strong></td>
                                        <td><strong>43.7%</strong></td>
                                    </tr>
                                    <tr class="highlight">
                                        <td><strong>FC7 (4096√ó4096)</strong></td>
                                        <td><strong>16.8M</strong></td>
                                        <td><strong>64</strong></td>
                                        <td><strong>28%</strong></td>
                                    </tr>
                                    <tr>
                                        <td>FC8 (4096√ó1000)</td>
                                        <td>4.1M</td>
                                        <td>15.6</td>
                                        <td>6.8%</td>
                                    </tr>
                                    <tr style="border-top: 2px solid #10099F;">
                                        <td><strong>Total</strong></td>
                                        <td><strong>60M</strong></td>
                                        <td><strong>~230</strong></td>
                                        <td><strong>100%</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="memory-insight fragment emphasis-box" style="font-size: 0.7em; margin-top: 20px;">
                            <p><strong>Key Insight:</strong> Fully connected layers (FC6 & FC7) contain ~90% of parameters but process <1% of computations!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">FLOP Analysis: Where Is Computation Spent?</h2>
                    <div class="flop-analysis">
                        <div class="flop-calculations fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <h5>FLOP Calculation Formula</h5>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                <p><strong>Convolutional Layer:</strong></p>
                                <p>$$\text{FLOPs} = 2 \times H_{out} \times W_{out} \times C_{in} \times C_{out} \times K_h \times K_w$$</p>
                                <p><strong>Fully Connected Layer:</strong></p>
                                <p>$$\text{FLOPs} = 2 \times \text{Input} \times \text{Output}$$</p>
                            </div>
                        </div>

                        <div class="flop-distribution fragment" style="font-size: 0.65em; margin-top: 20px;">
                            <h5>Computation Distribution</h5>
                            <div class="computation-bars">
                                <div class="comp-bar">
                                    <div class="label">Conv Layers</div>
                                    <div class="bar" style="width: 95%; background: #10099F;">95%</div>
                                </div>
                                <div class="comp-bar">
                                    <div class="label">FC Layers</div>
                                    <div class="bar" style="width: 5%; background: #2DD2C0;">5%</div>
                                </div>
                            </div>
                        </div>

                        <div class="computation-paradox fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <h5>The AlexNet Paradox</h5>
                            <ul>
                                <li>Conv layers: 5% parameters, 95% computation</li>
                                <li>FC layers: 95% parameters, 5% computation</li>
                                <li>This motivated architectures like GoogLeNet and ResNet to reduce FC layers</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Bottleneck Analysis</h2>
                    <div class="bottleneck-analysis">
                        <h4 style="font-size: 0.9em;">Where Does Training Time Go?</h4>

                        <div class="bottleneck-viz" style="margin: 20px 0;">
                            <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px;">
                                <button class="analysis-btn" data-analysis="forward" style="padding: 8px 16px; border: 1px solid #10099F; background: #10099F; color: white; border-radius: 5px; cursor: pointer;">Forward Pass</button>
                                <button class="analysis-btn" data-analysis="backward" style="padding: 8px 16px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">Backward Pass</button>
                                <button class="analysis-btn" data-analysis="update" style="padding: 8px 16px; border: 1px solid #10099F; background: white; border-radius: 5px; cursor: pointer;">Weight Update</button>
                            </div>
                            <div id="bottleneck-chart" style="background: white;">
                                <!-- Bottleneck visualization -->
                            </div>
                        </div>

                        <div class="optimization-tips fragment" style="font-size: 0.5em; margin-top: 20px;">
                            <h5>Performance Optimization Strategies</h5>
                            <div class="tip-grid">
                                <div class="tip-card">
                                    <h6>Memory Bandwidth</h6>
                                    <p>Conv1 is memory-bound due to <span class="tooltip">low arithmetic intensity<span class="tooltiptext">Ratio of computation to memory access. Conv1 has large 11√ó11 kernels on 224√ó224 images, requiring ~150MB memory transfers but only ~700M operations. This 4.7 ops/byte ratio means the GPU spends more time waiting for data than computing</span></span></p>
                                    <p>Solution: <span class="tooltip">Larger batch sizes<span class="tooltiptext">With batch size 1, we load 150MB for 700M ops. With batch size 32, we load the same 150MB of weights but get 32√ó700M = 22.4B ops, improving arithmetic intensity from 4.7 to 149 ops/byte - now compute-bound instead of memory-bound</span></span></p>
                                </div>
                                <div class="tip-card">
                                    <h6>Compute Bound</h6>
                                    <p>Conv3-5 are compute-bound</p>
                                    <p>Solution: <span class="tooltip">Tensor cores, mixed precision<span class="tooltiptext">Tensor cores are specialized hardware units that accelerate matrix operations using lower precision (FP16/BF16) while maintaining accuracy. Mixed precision training uses FP16 for forward/backward passes but FP32 for weight updates, providing 1.5-2x speedup on modern GPUs</span></span></p>
                                </div>
                                <div class="tip-card">
                                    <h6>Communication</h6>
                                    <p>Multi-GPU requires synchronization</p>
                                    <p>Solution: <span class="tooltip">Gradient accumulation, data parallelism<span class="tooltiptext">Instead of synchronizing gradients after every mini-batch, accumulate gradients locally across multiple mini-batches before synchronizing. This reduces communication frequency from every step to every N steps, improving bandwidth utilization and reducing synchronization overhead. Data parallelism replicates the entire model on each GPU and processes different batches of data, only synchronizing gradients during backpropagation - more efficient than model parallelism which splits the model itself across GPUs</span></span></p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is AlexNets design considered inefficient by modern standards?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "The fully connected layers contain 90% of parameters but only 5% of computation",
                                "correct": true,
                                "explanation": "Correct! This massive parameter overhead in FC layers is inefficient for both memory and regularization."
                            },
                            {
                                "text": "Large 11√ó11 kernels in the first layer are computationally expensive",
                                "correct": true,
                                "explanation": "Correct! Modern networks use smaller kernels (3√ó3) stacked for the same receptive field with fewer parameters."
                            },
                            {
                                "text": "It uses too many convolutional layers",
                                "correct": false,
                                "explanation": "Actually, AlexNet has relatively few conv layers (5) compared to modern networks like ResNet (50+)."
                            },
                            {
                                "text": "The model parallelism across GPUs adds communication overhead",
                                "correct": true,
                                "explanation": "Correct! Splitting the model across GPUs requires synchronization, which modern networks avoid with data parallelism."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- VGG Networks Section -->
            <section>
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - VGG", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html"}, {"text": "Simonyan & Zisserman (2014) - Very Deep Convolutional Networks", "url": "https://arxiv.org/abs/1409.1556"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">VGG: Networks Using Blocks</h1>
                    <p>The Dawn of Modular Deep Learning Architecture</p>
                    <p class="mt-lg">
                        <small>Visual Geometry Group, Oxford University (2014)</small>
                    </p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - VGG", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html"}]'>
                    <h2 class="truncate-title">From Individual Layers to Blocks</h2>
                    <div class="two-column-layout" style="font-size: 0.7em;">
                        <div class="left-column">
                            <h4>The Evolution of Network Design</h4>
                            <ul style="font-size: 0.85em;">
                                <li>AlexNet: <span class="tooltip">Ad-hoc<span class="tooltiptext">Designed for specific purposes without a general architectural principle</span></span> layer design</li>
                                <li>VGG: Systematic use of <strong>blocks</strong></li>
                            </ul>
                            <div class="emphasis-box" style="margin-top: 20px;">
                                <p><strong>Key Innovation:</strong> Repeating patterns of layers as fundamental building blocks</p>
                            </div>
                        </div>
                        <div class="right-column">
                            <div id="vgg-evolution-viz"></div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">VGG's Core Question</h2>
                    <div style="display: flex; flex-direction: column; align-items: center;font-size: 0.7em;">
                        <div style="background: linear-gradient(135deg, #10099F, #2DD2C0); padding: 30px; border-radius: 10px; margin: 20px; color: white;">
                            <h3 style="color: white; margin: 0;">Deep vs. Wide Networks</h3>
                            <p style="font-size: 1.1em; margin-top: 15px;">Which performs better?</p>
                        </div>

                        <div style="display: flex; gap: 40px; margin-top: 30px;">
                            <div class="network-comparison">
                                <h4>Deep & Narrow</h4>
                                <ul>
                                    <li>Multiple 3√ó3 convolutions</li>
                                    <li>Fewer parameters per layer</li>
                                    <li>More non-linearities</li>
                                </ul>
                            </div>
                            <div class="network-comparison">
                                <h4>Shallow & Wide</h4>
                                <ul>
                                    <li>Larger kernels (5√ó5, 7√ó7)</li>
                                    <li>More parameters per layer</li>
                                    <li>Fewer non-linearities</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <div class="emphasis-box">
                                <p><strong>VGG's Answer:</strong> Deep and narrow networks significantly outperform shallow ones!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What was the key architectural innovation introduced by VGG?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using ReLU activation functions",
                                "correct": false,
                                "explanation": "ReLU was already popularized by AlexNet. VGG continued using it but this was not its innovation."
                            },
                            {
                                "text": "Introducing the concept of reusable blocks in network design",
                                "correct": true,
                                "explanation": "Correct! VGG introduced the systematic use of blocks - repeating patterns of layers that could be stacked to create deeper networks."
                            },
                            {
                                "text": "Using dropout for regularization",
                                "correct": false,
                                "explanation": "Dropout was introduced before VGG, notably used in AlexNet."
                            },
                            {
                                "text": "Training on ImageNet dataset",
                                "correct": false,
                                "explanation": "AlexNet had already demonstrated success on ImageNet. VGG also used it but this was not its innovation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- VGG Block Architecture -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - VGG", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html"}]'>
                    <h2 class="truncate-title">The VGG Block</h2>
                    <div style="display: flex; flex-direction: column; align-items: center;font-size: 0.7em;">
                        <div class="vgg-block-structure" style="background: #f9f9f9; padding: 20px; border-radius: 10px; width: 80%;">
                            <h4>Components of a VGG Block:</h4>
                            <ol style="font-size: 0.9em;">
                                <li><strong>Sequence of 3√ó3 Convolutions</strong>
                                    <ul>
                                        <li>Padding = 1 (preserves spatial dimensions)</li>
                                        <li>Same number of output channels</li>
                                    </ul>
                                </li>
                                <li><strong>ReLU after each convolution</strong></li>
                                <li><strong>2√ó2 Max-Pooling at the end</strong>
                                    <ul>
                                        <li>Stride = 2 (halves spatial dimensions)</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>

                        <div id="vgg-block-animation" style="margin-top: 30px;"></div>

                        <div class="demo-controls" style="margin-top: 20px; z-index: 100;">
                            <button id="animate-vgg-block" style="background: #10099F; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer;">
                                Animate Forward Pass
                            </button>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why 3√ó3 Convolutions?</h2>
                    <div class="receptive-field-analysis" style="font-size: 0.7em;">
                        <h4>Receptive Field Analysis</h4>
                        <div style="display: flex; justify-content: space-around; margin: 20px 0;">
                            <div class="comparison-box" style="padding: 20px; background: #f5f5f5; border-radius: 8px;">
                                <h5>Two 3√ó3 Convolutions</h5>
                                <ul style="font-size: 0.85em;">
                                    <li><span class="tooltip">Receptive field<span class="tooltiptext">The region of the input that affects a particular output neuron</span></span>: 5√ó5</li>
                                    <li>Parameters: $$2 \times (3 \times 3 \times C^2) = 18C^2$$</li>
                                    <li>Non-linearities: 2</li>
                                </ul>
                            </div>
                            <div class="comparison-box" style="padding: 20px; background: #f5f5f5; border-radius: 8px;">
                                <h5>One 5√ó5 Convolution</h5>
                                <ul style="font-size: 0.85em;">
                                    <li>Receptive field: 5√ó5</li>
                                    <li>Parameters: $$5 \times 5 \times C^2 = 25C^2$$</li>
                                    <li>Non-linearities: 1</li>
                                </ul>
                            </div>
                        </div>

                        <div class="emphasis-box">
                            <p><strong>Key Insight:</strong> Multiple small convolutions achieve the same receptive field with fewer parameters and more non-linearities!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">VGG Block Implementation</h2>
                    <pre><code class="python" data-trim data-noescape>
def vgg_block(num_convs, out_channels):
    """
    Create a VGG block with specified number of convolutions

    Args:
        num_convs: Number of 3√ó3 convolutional layers
        out_channels: Number of output channels for all convolutions

    Returns:
        A sequential container of layers forming a VGG block
    """
    layers = []
    for _ in range(num_convs):
        # 3√ó3 convolution with padding=1 to preserve dimensions
        layers.append(nn.LazyConv2d(out_channels,
                                    kernel_size=3,
                                    padding=1))
        # ReLU activation after each convolution
        layers.append(nn.ReLU())

    # Max pooling to halve spatial dimensions
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))

    return nn.Sequential(*layers)
                    </code></pre>

                    <div class="code-explanation" style="margin-top: 20px; font-size: 0.65em;">
                        <p><strong>Design Choices:</strong></p>
                        <ul>
                            <li><code>LazyConv2d</code>: Automatically infers input channels</li>
                            <li>Padding=1: Ensures width and height are preserved after convolution</li>
                            <li>Stride=2 in pooling: Reduces dimensions by half</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Spatial Resolution Reduction</h2>
                    <div style="text-align: center;font-size: 0.5em;">
                        <h4>Problem with Traditional CNN Design</h4>
                        <p style="font-size: 0.9em;">Each pooling layer halves spatial dimensions</p>

                        <div class="math-derivation" style="margin: 30px auto; background: #f5f5f5; padding: 20px; border-radius: 10px; width: 70%;">
                            <p>Starting with input size: $d \times d$</p>
                            <p>After $n$ pooling layers: $\frac{d}{2^n} \times \frac{d}{2^n}$</p>
                            <p>Maximum layers before 1√ó1: $n_{max} = \log_2 d$</p>

                            <div style="margin-top: 20px; padding: 15px; background: white; border-left: 4px solid #10099F;">
                                <p><strong>For ImageNet (224√ó224):</strong></p>
                                <p>$n_{max} = \log_2 224 \approx 7.8$ ‚ö†Ô∏è Maximum of ~8 conv+pool pairs!</p>
                            </div>
                        </div>

                        <div class="fragment">
                            <h4 style="margin-top: 30px;">VGG's Solution</h4>
                            <div class="emphasis-box">
                                <p>Use multiple convolutions <strong>between</strong> pooling layers!</p>
                                <p>This allows deeper networks while managing spatial dimensions carefully.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does VGG prefer multiple 3√ó3 convolutions over single larger kernels?",
                        "type": "single",
                        "options": [
                            {
                                "text": "3√ó3 convolutions are faster to compute on all hardware",
                                "correct": false,
                                "explanation": "While 3√ó3 convolutions have been optimized on modern GPUs, this was a consequence, not the original motivation."
                            },
                            {
                                "text": "They achieve the same receptive field with fewer parameters and more non-linearities",
                                "correct": true,
                                "explanation": "Correct! Two 3√ó3 convolutions cover the same 5√ó5 receptive field but use only 18C¬≤ parameters vs 25C¬≤, and add an extra non-linearity for better representation power."
                            },
                            {
                                "text": "Larger kernels cannot learn complex patterns",
                                "correct": false,
                                "explanation": "Larger kernels can learn complex patterns, but they are less parameter-efficient."
                            },
                            {
                                "text": "3√ó3 is the maximum kernel size supported by GPUs",
                                "correct": false,
                                "explanation": "GPUs support various kernel sizes. The choice of 3√ó3 was based on efficiency and effectiveness, not hardware limitations."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- VGG Network Architecture -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - VGG", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html"}]'>
                    <h2 class="truncate-title">VGG Network Architecture</h2>
                    <div style="display: flex; flex-direction: column; align-items: center;">
                        <img src="./images/vgg-architecture.svg" alt="VGG Architecture" style="width: 100%; max-width: 300px;">

                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px; width: 100%;">
                            <div class="architecture-part">
                                <h4>Convolutional Part</h4>
                                <ul style="font-size: 0.85em;">
                                    <li>Sequential VGG blocks</li>
                                    <li>Progressive channel increase</li>
                                    <li>Spatial dimension reduction</li>
                                </ul>
                            </div>
                            <div class="architecture-part">
                                <h4>Fully Connected Part</h4>
                                <ul style="font-size: 0.85em;">
                                    <li>Similar to AlexNet</li>
                                    <li>Three FC layers</li>
                                    <li>4096 ‚Üí 4096 ‚Üí num_classes</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The VGG Family</h2>
                    <div style="overflow-x: auto;">
                        <table class="vgg-variants" style="font-size: 0.5em; margin: 0 auto;">
                            <thead>
                                <tr>
                                    <th>Network</th>
                                    <th>Configuration</th>
                                    <th>Conv Layers</th>
                                    <th>Parameters</th>
                                    <th>Key Feature</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>VGG-11</strong></td>
                                    <td>1, 1, 2, 2, 2</td>
                                    <td>8</td>
                                    <td>133M</td>
                                    <td>Original, simplest</td>
                                </tr>
                                <tr>
                                    <td><strong>VGG-13</strong></td>
                                    <td>2, 2, 2, 2, 2</td>
                                    <td>10</td>
                                    <td>133M</td>
                                    <td>All blocks have 2 convs</td>
                                </tr>
                                <tr>
                                    <td><strong>VGG-16</strong></td>
                                    <td>2, 2, 3, 3, 3</td>
                                    <td>13</td>
                                    <td>138M</td>
                                    <td>Most popular variant</td>
                                </tr>
                                <tr>
                                    <td><strong>VGG-19</strong></td>
                                    <td>2, 2, 4, 4, 4</td>
                                    <td>16</td>
                                    <td>144M</td>
                                    <td>Deepest VGG network</td>
                                </tr>
                            </tbody>
                        </table>

                        <div class="configuration-explanation" style="margin-top: 20px; font-size: 0.8em;">
                            <p><strong>Configuration notation:</strong> Number of convolutions in each of the 5 blocks</p>
                            <p><strong>Channels:</strong> 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 512</p>
                        </div>
                    </div>

                    <div id="vgg-comparison-viz" style="margin-top: 30px;"></div>
                </section>

                <section>
                    <h2 class="truncate-title">VGG-11 Detailed Architecture</h2>
                    <pre><code class="python" data-trim data-noescape>
class VGG(d2l.Classifier):
    def __init__(self, arch, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()

        # Build convolutional blocks
        conv_blks = []
        for (num_convs, out_channels) in arch:
            conv_blks.append(vgg_block(num_convs, out_channels))

        # Complete network: conv blocks + FC layers
        self.net = nn.Sequential(
            *conv_blks,
            nn.Flatten(),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(num_classes)
        )
        self.net.apply(d2l.init_cnn)

# VGG-11 configuration
vgg11_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
model = VGG(arch=vgg11_arch)
                    </code></pre>

                    <div class="architecture-flow" style="margin-top: 20px;">
                        <p style="font-size: 0.9em;"><strong>Data flow through VGG-11:</strong></p>
                        <div id="vgg11-flow-viz"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Layer-by-Layer Dimension Analysis</h2>
                    <pre><code class="plaintext" data-trim data-noescape>
Input:                      torch.Size([1, 3, 224, 224])

Block 1 (1 conv, 64 ch):    torch.Size([1, 64, 112, 112])
Block 2 (1 conv, 128 ch):   torch.Size([1, 128, 56, 56])
Block 3 (2 conv, 256 ch):   torch.Size([1, 256, 28, 28])
Block 4 (2 conv, 512 ch):   torch.Size([1, 512, 14, 14])
Block 5 (2 conv, 512 ch):   torch.Size([1, 512, 7, 7])

Flatten:                    torch.Size([1, 25088])
FC1 (4096):                 torch.Size([1, 4096])
FC2 (4096):                 torch.Size([1, 4096])
Output (10 classes):        torch.Size([1, 10])
                    </code></pre>

                    <div class="dimension-insights" style="margin-top: 30px;font-size: 0.5em;">
                        <h4>Key Observations</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div class="insight-box" style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Spatial Dimensions:</strong></p>
                                <p style="font-size: 0.85em;">Halved at each block: 224 ‚Üí 112 ‚Üí 56 ‚Üí 28 ‚Üí 14 ‚Üí 7</p>
                            </div>
                            <div class="insight-box" style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Channel Progression:</strong></p>
                                <p style="font-size: 0.85em;">Doubles until 512: 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 512</p>
                            </div>
                        </div>

                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p>Final feature maps (7√ó7√ó512) contain high-level representations ready for classification</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In VGG-11, why are there 8 convolutional layers but the network is called VGG-11?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The name is incorrect; it should be VGG-8",
                                "correct": false,
                                "explanation": "The name VGG-11 is correct. It counts layers with learnable parameters."
                            },
                            {
                                "text": "VGG-11 has 8 convolutional layers and 3 fully connected layers, totaling 11 layers with parameters",
                                "correct": true,
                                "explanation": "Correct! VGG-11 has 8 convolutional layers + 3 fully connected layers = 11 layers with learnable parameters. Pooling and activation layers are not counted."
                            },
                            {
                                "text": "There are 11 convolutional layers, but only 8 are shown",
                                "correct": false,
                                "explanation": "VGG-11 has exactly 8 convolutional layers. The architecture description is complete."
                            },
                            {
                                "text": "It includes 3 pooling layers in the count",
                                "correct": false,
                                "explanation": "Pooling layers don not have learnable parameters and are not included in the layer count."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Training and Performance -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - VGG Training", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html#training"}]'>
                    <h2 class="truncate-title">Training VGG Networks</h2>
                    <div class="training-setup">
                        <h4>Training Configuration</h4>
                        <pre><code class="python" data-trim data-noescape>
# Smaller VGG for Fashion-MNIST (reduced channels for efficiency)
model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)),
           lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))

# Initialize with proper weight initialization
model.apply_init([next(iter(data.get_dataloader(True)))[0]],
                 d2l.init_cnn)

# Train the model
trainer.fit(model, data)
                        </code></pre>

                        <div class="training-notes" style="margin-top: 20px; font-size: 0.85em;">
                            <p><strong>Key Points:</strong></p>
                            <ul>
                                <li>Reduced channels for Fashion-MNIST (computational efficiency)</li>
                                <li>Input resized to 224√ó224 (VGG standard)</li>
                                <li>Lower learning rate (0.01) for stable training</li>
                                <li>Proper weight initialization crucial for deep networks</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Considerations</h2>
                    <div class="computation-analysis">
                        <h4>VGG vs AlexNet</h4>
                        <table style="font-size: 0.8em; margin: 20px auto;">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>AlexNet</th>
                                    <th>VGG-16</th>
                                    <th>Impact</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Parameters</td>
                                    <td>~60M</td>
                                    <td>~138M</td>
                                    <td>2.3√ó more memory</td>
                                </tr>
                                <tr>
                                    <td>Conv Layers</td>
                                    <td>5</td>
                                    <td>13</td>
                                    <td>2.6√ó deeper</td>
                                </tr>
                                <tr>
                                    <td>FLOPs</td>
                                    <td>~1G</td>
                                    <td>~15G</td>
                                    <td>15√ó more computation</td>
                                </tr>
                                <tr>
                                    <td>FC Parameters</td>
                                    <td>~58M</td>
                                    <td>~124M</td>
                                    <td>Most params in FC</td>
                                </tr>
                            </tbody>
                        </table>

                        <div class="optimization-tips" style="margin-top: 30px;">
                            <h5>Modern Optimizations</h5>
                            <ul style="font-size: 0.85em;">
                                <li>Replace FC layers with <span class="tooltip">Global Average Pooling<span class="tooltiptext">Averages each feature map to a single value, drastically reducing parameters</span></span></li>
                                <li>Use <span class="tooltip">depthwise separable convolutions<span class="tooltiptext">Factorizes standard convolution into depthwise and pointwise operations</span></span></li>
                                <li>Apply <span class="tooltip">pruning<span class="tooltiptext">Removing less important connections to reduce model size</span></span> and <span class="tooltip">quantization<span class="tooltiptext">Using lower precision (e.g., int8 instead of float32) for weights</span></span></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Fast 3√ó3 Convolution Implementations</h2>
                    <div style="text-align: center;font-size: 0.75em;">
                        <h4>Why 3√ó3 Became the Gold Standard</h4>

                        <div class="implementation-benefits" style="margin: 30px auto; width: 80%;">
                            <ol style="text-align: left; font-size: 0.9em;">
                                <li><strong>Hardware Optimization:</strong>
                                    <ul>
                                        <li><span class="tooltip">Winograd algorithm<span class="tooltiptext">Reduces multiplication count for small convolutions by 2.25√ó for 3√ó3 kernels</span></span> particularly efficient for 3√ó3</li>
                                        <li>GPU kernels highly optimized (cuDNN)</li>
                                    </ul>
                                </li>
                                <li><strong>Memory Efficiency:</strong>
                                    <ul>
                                        <li>Small kernels fit in cache</li>
                                        <li>Better memory access patterns</li>
                                    </ul>
                                </li>
                                <li><strong>Parallelization:</strong>
                                    <ul>
                                        <li>More opportunities for parallel computation</li>
                                        <li>Better GPU utilization</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>

                        <div class="emphasis-box">
                            <p>VGG's choice of 3√ó3 convolutions influenced GPU optimization for the next decade!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main computational bottleneck in VGG networks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The convolutional layers due to many 3√ó3 operations",
                                "correct": false,
                                "explanation": "While conv layers have many operations, they are not the parameter bottleneck."
                            },
                            {
                                "text": "The pooling layers due to downsampling",
                                "correct": false,
                                "explanation": "Pooling layers have no learnable parameters and are computationally cheap."
                            },
                            {
                                "text": "The fully connected layers which contain most parameters",
                                "correct": true,
                                "explanation": "Correct! The three FC layers (4096‚Üí4096‚Üí1000) contain ~124M of VGG-16s 138M parameters. This is why modern networks often replace them with global pooling."
                            },
                            {
                                "text": "The batch normalization layers",
                                "correct": false,
                                "explanation": "VGG does not use batch normalization; it was invented after VGG."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Impact and Legacy -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - VGG", "url": "https://d2l.ai/chapter_convolutional-modern/vgg.html"}]'>
                    <h2 class="truncate-title">VGG's Impact on Deep Learning</h2>
                    <div class="impact-timeline" style="font-size: 0.7em;">
                        <h4>Key Contributions</h4>
                        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 30px 0;">
                            <div class="contribution-card" style="background: linear-gradient(135deg, #10099F, #2DD2C0); color: white; padding: 20px; border-radius: 10px;">
                                <h5 style="color: white;">1. Block-Based Design</h5>
                                <p>First network family with systematic architecture</p>
                            </div>
                            <div class="contribution-card" style="background: linear-gradient(135deg, #2DD2C0, #00FFBA); color: #262626; padding: 20px; border-radius: 10px;">
                                <h5>2. Deep & Narrow Principle</h5>
                                <p>Proved deeper networks with smaller kernels work better</p>
                            </div>
                            <div class="contribution-card" style="background: linear-gradient(135deg, #FAC55B, #FFA05F); color: #262626; padding: 20px; border-radius: 10px;">
                                <h5>3. Transfer Learning</h5>
                                <p>VGG features became standard for many vision tasks</p>
                            </div>
                            <div class="contribution-card" style="background: linear-gradient(135deg, #FC8484, #FFA05F); color: white; padding: 20px; border-radius: 10px;">
                                <h5 style="color: white;">4. Architecture Search</h5>
                                <p>Inspired systematic exploration of network families</p>
                            </div>
                        </div>
                    </div>

                    <div class="modern-perspective" style="margin-top: 30px;font-size: 0.7em;">
                        <h4>Modern Developments</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>ResNet (2015):</strong> Built on VGG blocks with skip connections</li>
                            <li><strong>DenseNet (2016):</strong> Extended block concept with dense connections</li>
                            <li><strong>EfficientNet (2019):</strong> Optimized depth/width/resolution based on VGG principles</li>
                            <li><strong>ConvNeXt (2022):</strong> Modernized pure ConvNet inspired by VGG simplicity</li>
                        </ul>
                    </div>
                </section>

                <section data-sources='[{"text": "ParNet: Non-deep Networks", "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/2d52879ef2ba487445ca2e143b104c3b-Abstract-Conference.html"}]'>
                    <h2 class="truncate-title">Recent Alternative: ParNet</h2>
                    <div style="text-align: center;font-size: 0.7em;">
                        <h4>Non-Deep Networks (2021)</h4>

                        <div class="parnet-comparison" style="margin: 30px auto;">
                            <p style="font-size: 0.9em;">
                                <strong>ParNet Challenge to VGG Philosophy:</strong><br>
                                Can we achieve competitive performance without depth?
                            </p>

                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                                <div class="approach-box" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                    <h5>VGG Approach</h5>
                                    <ul style="font-size: 0.85em; text-align: left;">
                                        <li>Sequential deep processing</li>
                                        <li>Hierarchical features</li>
                                        <li>Many layers (11-19)</li>
                                    </ul>
                                </div>
                                <div class="approach-box" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                    <h5>ParNet Approach</h5>
                                    <ul style="font-size: 0.85em; text-align: left;">
                                        <li>Parallel shallow branches</li>
                                        <li>Multi-scale processing</li>
                                        <li>Fewer layers (~12)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="emphasis-box" style="margin-top: 30px;">
                            <p>ParNet shows that the depth imperative established by VGG may not be absolute - exciting possibilities for efficient architectures!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: VGG's Legacy</h2>
                    <div class="summary-container" style="font-size: 0.7em;">
                        <div class="key-lessons" style="background: #f9f9f9; padding: 30px; border-radius: 10px;">
                            <h4>Key Lessons from VGG</h4>
                            <ol style="font-size: 0.9em; text-align: left;">
                                <li><strong>Simplicity Scales:</strong> Uniform architecture is easier to understand and optimize</li>
                                <li><strong>Blocks as Building Units:</strong> Modular design enables systematic exploration</li>
                                <li><strong>Deep & Narrow Wins:</strong> Multiple small operations better than few large ones</li>
                                <li><strong>Framework Importance:</strong> Modern frameworks make complex architectures accessible</li>
                            </ol>
                        </div>

                        <div style="margin-top: 30px;">
                            <h4>VGG in Practice Today</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Still used for feature extraction and style transfer</li>
                                <li>Principles applied in modern architectures</li>
                                <li>3√ó3 convolution remains standard</li>
                                <li>Block concept evolved into residual blocks, dense blocks, etc.</li>
                            </ul>
                        </div>

                        <div class="closing-thought" style="margin-top: 30px; padding: 20px; background: linear-gradient(135deg, #10099F, #2DD2C0); color: white; border-radius: 10px;">
                            <p style="margin: 0; font-size: 1.1em;">
                                "VGG proved that systematic architecture design beats ad-hoc engineering"
                            </p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which aspect of VGG has had the most lasting impact on modern deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using exactly 5 convolutional blocks",
                                "correct": false,
                                "explanation": "Modern networks use varying numbers of blocks. The specific number 5 was not the key contribution."
                            },
                            {
                                "text": "The concept of using repeating blocks as modular building units",
                                "correct": true,
                                "explanation": "Correct! The block-based design philosophy introduced by VGG became fundamental to modern architectures like ResNet, DenseNet, and beyond."
                            },
                            {
                                "text": "Having three fully connected layers at the end",
                                "correct": false,
                                "explanation": "Modern networks often avoid large FC layers, using global pooling instead to reduce parameters."
                            },
                            {
                                "text": "Training on the ImageNet dataset",
                                "correct": false,
                                "explanation": "AlexNet had already used ImageNet. This was standard practice, not VGGs innovation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Network in Network (NiN) Section -->
            <section>
                <section class="title-slide" data-sources='[{"text": "Dive into Deep Learning - Network in Network", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html"}, {"text": "Lin et al. (2013) - Network In Network", "url": "https://arxiv.org/abs/1312.4400"}]'>
                    <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                    <h1 class="truncate-title">Network in Network (NiN)</h1>
                    <p>Rethinking the Linear Structure of CNNs</p>
                    <p class="mt-lg">
                        <small>Lin, Chen, and Yan (2013)</small><br>
                        <small>National University of Singapore</small>
                    </p>
                </section>

                <!-- Vertical Section: Motivation and Problem -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html"}]'>
                    <h2 class="truncate-title">The Problem with Fully Connected Layers</h2>
                    <div class="problem-analysis" style="font-size: 0.7em;">
                        <h4 style="font-size: 0.9em;">Two Major Challenges in CNN Design</h4>

                        <div class="challenge-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                            <div class="challenge-box" style="background: #f9f9f9; padding: 20px; border-radius: 10px;">
                                <h5 style="color: #10099F;">Challenge 1: Parameter Explosion</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>VGG-11 FC layers: ~400MB in FP32</li>
                                    <li>Mobile devices (2013): 512MB total RAM</li>
                                    <li>FC layers dominate memory usage</li>
                                </ul>
                                <div class="fragment" style="margin-top: 15px; padding: 10px; background: #FC8484; color: white; border-radius: 5px;">
                                    <strong>Impact:</strong> Models unusable on mobile/embedded devices
                                </div>
                            </div>

                            <div class="challenge-box" style="background: #f9f9f9; padding: 20px; border-radius: 10px;">
                                <h5 style="color: #10099F;">Challenge 2: Spatial Structure Loss</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>FC layers destroy spatial information</li>
                                    <li>Can't add FC layers early in network</li>
                                    <li>Limited nonlinearity in conv layers</li>
                                </ul>
                                <div class="fragment" style="margin-top: 15px; padding: 10px; background: #FFA05F; color: white; border-radius: 5px;">
                                    <strong>Impact:</strong> Networks can't learn complex channel interactions
                                </div>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <div class="emphasis-box">
                                <h5 style="color: #262626;">The NiN Solution</h5>
                                <p style="font-size: 0.9em;">Use <span class="tooltip">1√ó1 convolutions<span class="tooltiptext">Convolutions with kernel size 1√ó1 that operate on channel dimension only, preserving spatial structure</span></span> to add local nonlinearities and <span class="tooltip">global average pooling<span class="tooltiptext">Averaging all spatial locations for each channel to produce a single value per channel</span></span> to eliminate FC layers</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Memory Footprint Comparison</h2>
                    <div class="memory-comparison">
                        <h4 style="font-size: 0.9em;">Where Do Parameters Live?</h4>

                        <div style="font-size: 0.7em;">
                            <table style="width: 100%; margin-top: 20px;">
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>Conv Layers</th>
                                        <th>FC Layers</th>
                                        <th>Total Parameters</th>
                                        <th>Memory (MB)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>AlexNet</td>
                                        <td>3.7M (6%)</td>
                                        <td>56.3M (94%)</td>
                                        <td>60M</td>
                                        <td>~230</td>
                                    </tr>
                                    <tr>
                                        <td>VGG-11</td>
                                        <td>9.2M (7%)</td>
                                        <td>123.6M (93%)</td>
                                        <td>132.9M</td>
                                        <td>~507</td>
                                    </tr>
                                    <tr class="fragment" style="background: #2DD2C0; color: white;">
                                        <td><strong>NiN</strong></td>
                                        <td><strong>1.95M (100%)</strong></td>
                                        <td><strong>0 (0%)</strong></td>
                                        <td><strong>1.95M</strong></td>
                                        <td><strong>~7.5</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="insight fragment" style="margin-top: 30px; font-size: 0.85em;">
                            <div style="background: linear-gradient(135deg, #10099F, #2DD2C0); padding: 20px; border-radius: 10px; color: white;">
                                <p style="margin: 0;"><strong>Key Insight:</strong> NiN reduces parameters by 30-68√ó while maintaining competitive accuracy!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What motivated the development of Network in Network?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Fully connected layers consumed too many parameters",
                                "correct": true,
                                "explanation": "Correct! FC layers in AlexNet/VGG used 90%+ of parameters, making models impractical for mobile devices."
                            },
                            {
                                "text": "Need for more nonlinearity without destroying spatial structure",
                                "correct": true,
                                "explanation": "Correct! Traditional CNNs could not add complex nonlinearities early without using FC layers that destroy spatial information."
                            },
                            {
                                "text": "Desire to make training faster",
                                "correct": false,
                                "explanation": "Actually, NiN could potentially increase training time due to additional 1√ó1 convolutions."
                            },
                            {
                                "text": "Memory constraints on mobile devices",
                                "correct": true,
                                "explanation": "Correct! With mobile devices having only 512MB RAM in 2013, VGGs 400MB+ models were impractical."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Core Innovation - NiN Blocks -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN Blocks", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html#nin-blocks"}]'>
                    <h2 class="truncate-title">The NiN Block: MLPConv Layer</h2>
                    <div class="nin-block-explanation">
                        <h4 style="font-size: 0.9em;">Network in Network Block Structure</h4>

                        <div id="nin-block-viz" style="margin: 20px 0;">
                            <!-- NiN block visualization will be inserted here by JS -->
                        </div>

                        <div class="mlpconv-explanation fragment" style="margin-top: 20px; font-size: 0.5em;">
                            <h5>Why "Network in Network"?</h5>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                    <h6 style="color: #10099F;">Traditional Conv Layer</h6>
                                    <ul>
                                        <li>Single linear transformation</li>
                                        <li>Limited expressive power</li>
                                        <li>ReLU only between layers</li>
                                    </ul>
                                </div>
                                <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                    <h6 style="color: #2DD2C0;">NiN Block (MLPConv)</h6>
                                    <ul>
                                        <li>Mini neural network at each position</li>
                                        <li>Multiple nonlinear transformations</li>
                                        <li>ReLU after each 1√ó1 conv</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding 1√ó1 Convolutions</h2>
                    <div class="one-by-one-explanation">
                        <h4 style="font-size: 0.9em;">The Power of Pointwise Convolutions</h4>

                        <div class="mathematical-view" style="margin: 20px 0; font-size: 0.5em;">
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                <p><strong>Mathematical Interpretation:</strong></p>
                                <p>For input tensor $X \in \mathbb{R}^{H \times W \times C_{in}}$ and weights $W \in \mathbb{R}^{1 \times 1 \times C_{in} \times C_{out}}$:</p>
                                <p>$$Y_{h,w,c_{out}} = \sum_{c_{in}=1}^{C_{in}} X_{h,w,c_{in}} \cdot W_{1,1,c_{in},c_{out}} + b_{c_{out}}$$</p>
                                <p class="fragment">This is equivalent to a fully connected layer applied independently at each spatial location!</p>
                            </div>
                        </div>

                        <div class="benefits fragment" style="margin-top: 20px; font-size: 0.75em;">
                            <h5>Benefits of 1√ó1 Convolutions</h5>
                            <ul>
                                <li>Increase network depth without losing resolution</li>
                                <li>Add nonlinearity across channels</li>
                                <li>Dimensionality reduction/expansion</li>
                                <li>Preserve spatial structure unlike FC layers</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key insight behind using 1√ó1 convolutions in NiN?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They reduce the spatial dimensions of feature maps",
                                "correct": false,
                                "explanation": "1√ó1 convolutions preserve spatial dimensions; they only change the channel dimension."
                            },
                            {
                                "text": "They act as fully connected layers applied independently at each spatial location",
                                "correct": true,
                                "explanation": "Correct! 1√ó1 convolutions are mathematically equivalent to FC layers applied pointwise, preserving spatial structure while adding channel-wise nonlinearity."
                            },
                            {
                                "text": "They increase the receptive field of the network",
                                "correct": false,
                                "explanation": "1√ó1 convolutions have a receptive field of 1 and do not increase it."
                            },
                            {
                                "text": "They replace pooling operations",
                                "correct": false,
                                "explanation": "NiN still uses max pooling between blocks; 1√ó1 convolutions serve a different purpose."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Complete Architecture -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN Model", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html#nin-model"}]'>
                    <h2 class="truncate-title">Complete NiN Architecture</h2>
                    <div class="architecture-details">
                        <div class="layer-details fragment" style="margin-top: 20px; font-size: 0.65em;">
                            <h5>Layer-by-Layer Breakdown</h5>
                            <table style="width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Block</th>
                                        <th>Layers</th>
                                        <th>Output Shape</th>
                                        <th>Parameters</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>NiN Block 1</td>
                                        <td>Conv 11√ó11√ó96 + 2√ó Conv 1√ó1√ó96</td>
                                        <td>54√ó54√ó96</td>
                                        <td>~35K</td>
                                    </tr>
                                    <tr>
                                        <td>MaxPool</td>
                                        <td>3√ó3, stride 2</td>
                                        <td>26√ó26√ó96</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td>NiN Block 2</td>
                                        <td>Conv 5√ó5√ó256 + 2√ó Conv 1√ó1√ó256</td>
                                        <td>26√ó26√ó256</td>
                                        <td>~750K</td>
                                    </tr>
                                    <tr>
                                        <td>MaxPool</td>
                                        <td>3√ó3, stride 2</td>
                                        <td>12√ó12√ó256</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td>NiN Block 3</td>
                                        <td>Conv 3√ó3√ó384 + 2√ó Conv 1√ó1√ó384</td>
                                        <td>12√ó12√ó384</td>
                                        <td>~1.1M</td>
                                    </tr>
                                    <tr>
                                        <td>MaxPool</td>
                                        <td>3√ó3, stride 2</td>
                                        <td>5√ó5√ó384</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td>Dropout</td>
                                        <td>p=0.5</td>
                                        <td>5√ó5√ó384</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td>NiN Block 4</td>
                                        <td>Conv 3√ó3√ó10 + 2√ó Conv 1√ó1√ó10</td>
                                        <td>5√ó5√ó10</td>
                                        <td>~35K</td>
                                    </tr>
                                    <tr style="background: #10099F; color: white;">
                                        <td><strong>Global Avg Pool</strong></td>
                                        <td><strong>Average across spatial dims</strong></td>
                                        <td><strong>1√ó1√ó10</strong></td>
                                        <td><strong>0</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Feature Map Evolution</h2>
                    <div class="feature-evolution">
                        <h4 style="font-size: 0.9em;">How Features Transform Through NiN</h4>

                        <div id="nin-feature-evolution" style="margin: 20px 0;">
                            <!-- Feature evolution visualization -->
                        </div>

                        <div class="evolution-insights fragment" style="margin-top: 20px; font-size: 0.75em;">
                            <h5>Key Observations</h5>
                            <ul>
                                <li>Spatial dimensions reduce gradually: 224 ‚Üí 54 ‚Üí 26 ‚Üí 12 ‚Üí 5 ‚Üí 1</li>
                                <li>Channel dimensions increase then decrease: 3 ‚Üí 96 ‚Üí 256 ‚Üí 384 ‚Üí 10</li>
                                <li>Final block has exactly num_classes channels</li>
                                <li>Global average pooling naturally produces class scores</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does the last NiN block have exactly 10 channels (for 10 classes)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It was a random design choice",
                                "correct": false,
                                "explanation": "This is a deliberate design decision crucial to NiNs architecture."
                            },
                            {
                                "text": "To match the number of classes so global average pooling directly produces class scores",
                                "correct": true,
                                "explanation": "Correct! By having one channel per class, global average pooling produces one value per class, eliminating the need for FC layers."
                            },
                            {
                                "text": "To reduce computation in the final layer",
                                "correct": false,
                                "explanation": "While it does reduce computation, this is not the primary reason."
                            },
                            {
                                "text": "Because 10 is the optimal number for feature extraction",
                                "correct": false,
                                "explanation": "The number matches the classes in the dataset; it would be different for other datasets."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Global Average Pooling -->
                <section data-sources='[{"text": "Dive into Deep Learning - Global Average Pooling", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html"}]'>
                    <h2 class="truncate-title">Global Average Pooling: The FC Killer</h2>
                    <div class="gap-explanation">
                        <h4 style="font-size: 0.9em;">Replacing Fully Connected Layers</h4>

                        <div id="gap-vs-fc-viz" style="margin: 20px 0;">
                            <!-- GAP vs FC comparison visualization -->
                        </div>

                        <div class="gap-advantages fragment" style="margin-top: 20px; font-size: 0.5em;">
                            <h5>Advantages of Global Average Pooling</h5>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div>
                                    <h6 style="color: #10099F;">Parameter Reduction</h6>
                                    <ul>
                                        <li>Zero additional parameters</li>
                                        <li>No overfitting from FC layers</li>
                                        <li>Dramatic model size reduction</li>
                                    </ul>
                                </div>
                                <div>
                                    <h6 style="color: #2DD2C0;">Translation Invariance</h6>
                                    <ul>
                                        <li>Averaging enforces spatial invariance</li>
                                        <li>More robust to input shifts</li>
                                        <li>Natural regularization effect</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mathematical Formulation</h2>
                    <div class="math-formulation">
                        <h4 style="font-size: 0.9em;">Global Average Pooling Operation</h4>

                        <div style="background: #f9f9f9; padding: 20px; border-radius: 10px; margin: 20px 0;">
                            <p style="font-size: 0.85em;">For feature map $F \in \mathbb{R}^{H \times W \times C}$:</p>

                            <p style="font-size: 0.9em;">$$\text{GAP}(F)_c = \frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} F_{h,w,c}$$</p>

                            <p style="font-size: 0.85em;">Output: Vector $y \in \mathbb{R}^C$ where $C$ = number of classes</p>
                        </div>

                        <div class="comparison-table fragment" style="font-size: 0.7em; margin-top: 20px;">
                            <table style="width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Aspect</th>
                                        <th>Fully Connected</th>
                                        <th>Global Average Pooling</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Input</td>
                                        <td>Flattened vector</td>
                                        <td>Spatial feature map</td>
                                    </tr>
                                    <tr>
                                        <td>Parameters</td>
                                        <td>$H \times W \times C_{in} \times C_{out}$</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td>Spatial Info</td>
                                        <td>Destroyed</td>
                                        <td>Aggregated</td>
                                    </tr>
                                    <tr>
                                        <td>Overfitting Risk</td>
                                        <td>High</td>
                                        <td>Low</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are the main benefits of global average pooling over fully connected layers?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Zero additional parameters to learn",
                                "correct": true,
                                "explanation": "Correct! GAP is parameter-free, dramatically reducing model size."
                            },
                            {
                                "text": "Enforces correspondence between feature maps and categories",
                                "correct": true,
                                "explanation": "Correct! Each feature map corresponds to one category, making the network more interpretable."
                            },
                            {
                                "text": "Faster forward propagation",
                                "correct": false,
                                "explanation": "While it may be faster due to fewer operations, this is not a primary benefit."
                            },
                            {
                                "text": "Acts as structural regularizer preventing overfitting",
                                "correct": true,
                                "explanation": "Correct! By summing out spatial information, GAP acts as a natural regularizer."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Implementation -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN Implementation", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html#nin-model"}]'>
                    <h2 class="truncate-title">Implementation in PyTorch</h2>
                    <div class="implementation-details">
                        <div style="font-size: 0.75em; margin-top: 20px;">
                            <h5>NiN Block Implementation</h5>
                            <pre style="background: #f9f9f9; padding: 15px; border-radius: 5px;"><code class="language-python">def nin_block(out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.LazyConv2d(out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1),
        nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1),
        nn.ReLU()
    )</code></pre>
                        </div>

                        <div class="fragment" style="font-size: 0.75em; margin-top: 20px;">
                            <h5>Complete NiN Model</h5>
                            <pre style="background: #f9f9f9; padding: 15px; border-radius: 5px; max-height: 300px; overflow-y: auto;"><code class="language-python">class NiN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.net = nn.Sequential(
            nin_block(96, kernel_size=11, strides=4, padding=0),
            nn.MaxPool2d(3, stride=2),
            nin_block(256, kernel_size=5, strides=1, padding=2),
            nn.MaxPool2d(3, stride=2),
            nin_block(384, kernel_size=3, strides=1, padding=1),
            nn.MaxPool2d(3, stride=2),
            nn.Dropout(0.5),
            nin_block(num_classes, kernel_size=3, strides=1, padding=1),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Layer Output Shapes</h2>
                    <div class="output-shapes">
                        <h4 style="font-size: 0.9em;">Tracing Through the Network</h4>

                        <div style="font-size: 0.65em; margin-top: 20px;">
                            <pre style="background: #262626; color: #2DD2C0; padding: 15px; border-radius: 5px;">
Input:                          torch.Size([1, 3, 224, 224])
NiN Block 1 output:             torch.Size([1, 96, 54, 54])
MaxPool2d output:               torch.Size([1, 96, 26, 26])
NiN Block 2 output:             torch.Size([1, 256, 26, 26])
MaxPool2d output:               torch.Size([1, 256, 12, 12])
NiN Block 3 output:             torch.Size([1, 384, 12, 12])
MaxPool2d output:               torch.Size([1, 384, 5, 5])
Dropout output:                 torch.Size([1, 384, 5, 5])
NiN Block 4 output:             torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output:       torch.Size([1, 10, 1, 1])
Flatten output:                 torch.Size([1, 10])
                            </pre>
                        </div>

                        <div class="implementation-notes fragment" style="margin-top: 20px; font-size: 0.75em;">
                            <h5>Implementation Notes</h5>
                            <ul>
                                <li><code>LazyConv2d</code> automatically infers input channels</li>
                                <li><code>AdaptiveAvgPool2d((1,1))</code> performs global average pooling</li>
                                <li>Final <code>Flatten()</code> removes spatial dimensions (1√ó1)</li>
                                <li>Output directly gives class scores (logits)</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the NiN implementation, why do we use AdaptiveAvgPool2d((1, 1)) instead of regular AvgPool2d?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It is computationally more efficient",
                                "correct": false,
                                "explanation": "Both have similar computational complexity for this use case."
                            },
                            {
                                "text": "It automatically adapts the pooling size to produce 1√ó1 output regardless of input size",
                                "correct": true,
                                "explanation": "Correct! AdaptiveAvgPool2d ensures the output is always 1√ó1, making the network flexible to different input sizes."
                            },
                            {
                                "text": "It provides better gradient flow",
                                "correct": false,
                                "explanation": "Both pooling methods have similar gradient properties."
                            },
                            {
                                "text": "Regular AvgPool2d cannot handle 5√ó5 inputs",
                                "correct": false,
                                "explanation": "Regular AvgPool2d can handle 5√ó5 inputs, but you would need to specify kernel_size=5."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Comparative Analysis -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN Summary", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html#summary"}]'>
                    <h2 class="truncate-title">Architecture Comparison: AlexNet vs VGG vs NiN</h2>
                    <div class="comparative-analysis">
                        <div id="nin-param-comparison" style="margin: 20px 0;">
                            <!-- Parameter comparison chart -->
                        </div>

                        <div class="comparison-insights fragment" style="margin-top: 20px; font-size: 0.5em;">
                            <h5>Key Differences</h5>
                            <table style="width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Aspect</th>
                                        <th>AlexNet</th>
                                        <th>VGG</th>
                                        <th>NiN</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Design Philosophy</td>
                                        <td>Deep & Wide</td>
                                        <td>Very Deep, Uniform</td>
                                        <td>Network in Network</td>
                                    </tr>
                                    <tr>
                                        <td>Key Innovation</td>
                                        <td>GPU training, Dropout</td>
                                        <td>3√ó3 conv blocks</td>
                                        <td>1√ó1 conv, GAP</td>
                                    </tr>
                                    <tr>
                                        <td>FC Layer Strategy</td>
                                        <td>3 large FC layers</td>
                                        <td>3 large FC layers</td>
                                        <td>No FC layers</td>
                                    </tr>
                                    <tr>
                                        <td>Mobile Friendly</td>
                                        <td>‚ùå</td>
                                        <td>‚ùå</td>
                                        <td>‚úÖ</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Efficiency Analysis</h2>
                    <div class="efficiency-analysis">
                        <h4 style="font-size: 0.9em;">FLOPs and Memory Comparison</h4>

                        <div style="font-size: 0.7em; margin-top: 20px;">
                            <table style="width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>Parameters</th>
                                        <th>FLOPs</th>
                                        <th>Memory (Training)</th>
                                        <th>Memory (Inference)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>AlexNet</td>
                                        <td>60M</td>
                                        <td>~720M</td>
                                        <td>~250MB</td>
                                        <td>~230MB</td>
                                    </tr>
                                    <tr>
                                        <td>VGG-11</td>
                                        <td>133M</td>
                                        <td>~7.6B</td>
                                        <td>~550MB</td>
                                        <td>~507MB</td>
                                    </tr>
                                    <tr style="background: #2DD2C0;">
                                        <td><strong>NiN</strong></td>
                                        <td><strong>1.95M</strong></td>
                                        <td><strong>~350M</strong></td>
                                        <td><strong>~15MB</strong></td>
                                        <td><strong>~7.5MB</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="efficiency-impact fragment" style="margin-top: 20px;">
                            <div class="emphasis-box">
                                <h5>Impact on Deep Learning</h5>
                                <p style="font-size: 0.85em;">NiN's innovations directly influenced:</p>
                                <ul style="font-size: 0.8em;">
                                    <li>GoogLeNet (Inception) - adopted 1√ó1 convolutions</li>
                                    <li>ResNet - uses 1√ó1 convs for dimension matching</li>
                                    <li>MobileNet - built entirely on depthwise separable convs</li>
                                    <li>Most modern architectures use GAP instead of FC</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which NiN innovation has had the most lasting impact on modern architectures?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using dropout with p=0.5",
                                "correct": false,
                                "explanation": "Dropout was already introduced by AlexNet and is not NiNs innovation."
                            },
                            {
                                "text": "The combination of 1√ó1 convolutions and global average pooling",
                                "correct": true,
                                "explanation": "Correct! Both techniques are now standard in modern CNNs, from ResNet to EfficientNet."
                            },
                            {
                                "text": "Using 11√ó11 kernels in the first layer",
                                "correct": false,
                                "explanation": "This was inherited from AlexNet; modern networks typically use smaller kernels."
                            },
                            {
                                "text": "Having exactly 4 convolutional blocks",
                                "correct": false,
                                "explanation": "The specific number of blocks is not the key innovation."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Vertical Section: Training and Results -->
                <section data-sources='[{"text": "Dive into Deep Learning - NiN Training", "url": "https://d2l.ai/chapter_convolutional-modern/nin.html#training"}]'>
                    <h2 class="truncate-title">Training NiN on Fashion-MNIST</h2>
                    <div class="training-results">
                        <h4 style="font-size: 0.9em;">Training Configuration and Results</h4>

                        <div style="font-size: 0.8em; margin-top: 20px;">
                            <h5>Training Setup</h5>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                <ul>
                                    <li>Dataset: Fashion-MNIST (resized to 224√ó224)</li>
                                    <li>Batch size: 128</li>
                                    <li>Learning rate: 0.05</li>
                                    <li>Optimizer: SGD</li>
                                    <li>Epochs: 10</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="training-code fragment" style="font-size: 0.75em; margin-top: 20px;">
                        <h5>Training Code</h5>
                        <pre style="background: #f9f9f9; padding: 15px; border-radius: 5px;"><code class="language-python">model = NiN(lr=0.05)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)</code></pre>
                    </div>
                </section>

                <section>
                   

                    <div class="results fragment" style="margin-top: 20px;">
                        <h5>Results</h5>
                        <img src="images/nin-training-results.png"
                             alt="NiN Training Results"
                             style="max-width: 300px; margin: 0 auto; display: block;">
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Performance Analysis</h2>
                    <div class="performance-analysis">
                        <h4 style="font-size: 0.9em;">Training Dynamics</h4>

                        <div style="font-size: 0.75em; margin-top: 20px;">
                            <h5>Observations</h5>
                            <ul>
                                <li>Convergence: NiN converges similarly to AlexNet despite fewer parameters</li>
                                <li>Training time: May be slower due to additional 1√ó1 convolutions</li>
                                <li>Memory usage: Dramatically lower than AlexNet/VGG</li>
                                <li>Final accuracy: Comparable to much larger models</li>
                            </ul>
                        </div>

                        <div class="training-tips fragment" style="margin-top: 20px; font-size: 0.75em;">
                            <h5>Training Tips for NiN</h5>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                <ul>
                                    <li><strong>Learning rate:</strong> Can use higher LR due to fewer parameters</li>
                                    <li><strong>Regularization:</strong> Less dropout needed due to GAP</li>
                                    <li><strong>Batch size:</strong> Can use larger batches due to lower memory</li>
                                    <li><strong>Data augmentation:</strong> Still beneficial for small datasets</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why might NiN training be slower per epoch than AlexNet despite having fewer parameters?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Global average pooling is computationally expensive",
                                "correct": false,
                                "explanation": "GAP is actually very efficient - just averaging values."
                            },
                            {
                                "text": "Multiple 1√ó1 convolutions add computational overhead",
                                "correct": true,
                                "explanation": "Correct! Each NiN block has 3 convolution operations (one main + two 1√ó1), increasing computation despite fewer parameters."
                            },
                            {
                                "text": "The network is deeper than AlexNet",
                                "correct": false,
                                "explanation": "NiN is not necessarily deeper; it has a similar number of major blocks."
                            },
                            {
                                "text": "Dropout slows down training",
                                "correct": false,
                                "explanation": "Both AlexNet and NiN use dropout, so this is not the differentiator."
                            }
                        ]
                    }'></div>
                </section>


                <section>
                    <h2 class="truncate-title">Modern Applications of NiN Ideas</h2>
                    <div class="modern-applications">
                        <h4 style="font-size: 0.9em;">NiN's Legacy in Current Architectures</h4>

                        <div style="font-size: 0.75em; margin-top: 20px;">
                            <h5>Where NiN Ideas Live On</h5>

                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                    <h5 style="color: #10099F;">1√ó1 Convolutions</h5>
                                    <ul style="font-size: 0.65em;">
                                        <li><strong>Inception:</strong> Dimension reduction</li>
                                        <li><strong>ResNet:</strong> Bottleneck blocks</li>
                                        <li><strong>MobileNet:</strong> Pointwise convolutions</li>
                                        <li><strong>EfficientNet:</strong> Squeeze-excitation</li>
                                    </ul>
                                </div>

                                <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
                                    <h5 style="color: #2DD2C0;">Global Average Pooling</h5>
                                    <ul style="font-size: 0.65em;">
                                        <li><strong>ResNet:</strong> Final classification</li>
                                        <li><strong>DenseNet:</strong> Replaces FC layers</li>
                                        <li><strong>Vision Transformer:</strong> Token pooling</li>
                                        <li><strong>ConvNeXt:</strong> Standard practice</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="implementation-guidance fragment" style="margin-top: 20px; font-size: 0.75em;">
                            <h5>When to Use NiN Concepts</h5>
                            <ul>
                                <li><strong>Mobile deployment:</strong> When model size is critical</li>
                                <li><strong>Interpretability:</strong> GAP enables <span class="tooltip">class activation maps<span class="tooltiptext">By having one feature map per class before GAP, we can visualize which spatial regions contribute most to each class score. The pre-GAP feature maps act as heatmaps showing class-specific activations across the image</span></span></li>
                                <li><strong>Transfer learning:</strong> 1√ó1 convs for adaptation layers</li>
                                <li><strong>Multi-scale processing:</strong> Inception-style architectures</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: NiN's Revolutionary Impact</h2>
                    <div class="nin-summary" style="font-size: 0.7em;">
                        <div style="background: linear-gradient(135deg, #10099F, #2DD2C0); padding: 30px; border-radius: 10px; color: white;">
                            <h4 style="color: white;">Key Contributions</h4>
                            <ol style="font-size: 0.9em; text-align: left;">
                                <li><strong>1√ó1 Convolutions:</strong> Channel-wise feature learning</li>
                                <li><strong>MLPConv:</strong> Mini networks within networks</li>
                                <li><strong>Global Average Pooling:</strong> Eliminated FC layers</li>
                                <li><strong>Parameter Efficiency:</strong> 30-68√ó reduction vs AlexNet/VGG</li>
                            </ol>
                        </div>

                        <div style="margin-top: 30px; font-size: 0.85em;">
                            <h5>Lasting Impact</h5>
                            <p>NiN proved that clever architecture design could achieve more with less. Its innovations are now fundamental building blocks in nearly every modern CNN architecture.</p>
                        </div>

                        <div class="closing-thought" style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-left: 5px solid #10099F;">
                            <p style="margin: 0; font-size: 0.9em; font-style: italic;">
                                "Sometimes the best solution is not to add more, but to rethink the problem entirely."
                            </p>
                            <p style="margin-top: 10px; font-size: 0.8em;">- The lesson from Network in Network</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the most important lesson from NiN for modern deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Always use 1√ó1 convolutions",
                                "correct": false,
                                "explanation": "1√ó1 convolutions are useful but not always necessary; context matters."
                            },
                            {
                                "text": "Architectural innovations can overcome parameter and computational constraints",
                                "correct": true,
                                "explanation": "Correct! NiN showed that smart design (1√ó1 conv + GAP) could achieve similar performance with 30-68√ó fewer parameters."
                            },
                            {
                                "text": "Fully connected layers should never be used",
                                "correct": false,
                                "explanation": "FC layers still have uses; the lesson is to consider alternatives when appropriate."
                            },
                            {
                                "text": "Global average pooling is always better than max pooling",
                                "correct": false,
                                "explanation": "GAP is for replacing FC layers at the end; max pooling still useful for spatial downsampling."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: GoogLeNet Introduction (Vertical) -->
            <section>
                <section data-sources='[{"text": "Going Deeper with Convolutions - Szegedy et al. (2015)", "url": "https://arxiv.org/abs/1409.4842"}, {"text": "Dive into Deep Learning - GoogLeNet", "url": "https://d2l.ai/chapter_convolutional-modern/googlenet.html"}]'>
                    <h2 class="truncate-title">GoogLeNet: Going Deeper with Convolutions</h2>
                    <div class="googlenet-intro" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>The 2014 ImageNet Champion</h4>
                            <p style="font-size: 0.8em;">GoogLeNet reduced error rate from 11.7% (VGG) to 6.67% while being 12√ó more computationally efficient</p>
                        </div>
                        <div class="key-innovations fragment" style="margin-top: 30px;">
                            <h5 style="font-size: 0.9em;">Revolutionary Ideas</h5>
                            <div class="innovation-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; font-size: 0.7em;">
                                <div class="innovation-card" style="padding: 15px; background: #f9f9f9; border-radius: 8px;">
                                    <div style="font-weight: bold; color: #10099F;">üß© Inception Blocks</div>
                                    <p>Multi-branch convolutions processing at multiple scales simultaneously</p>
                                </div>
                                <div class="innovation-card" style="padding: 15px; background: #f9f9f9; border-radius: 8px;">
                                    <div style="font-weight: bold; color: #10099F;">üèóÔ∏è Stem-Body-Head Design</div>
                                    <p>First network with clear separation of data ingest, processing, and prediction</p>
                                </div>
                                <div class="innovation-card" style="padding: 15px; background: #f9f9f9; border-radius: 8px;">
                                    <div style="font-weight: bold; color: #10099F;">üìê Dimension Reduction</div>
                                    <p>Strategic use of 1√ó1 convolutions to reduce computational cost</p>
                                </div>
                                <div class="innovation-card" style="padding: 15px; background: #f9f9f9; border-radius: 8px;">
                                    <div style="font-weight: bold; color: #10099F;">üéØ 9 Inception Blocks</div>
                                    <p>Carefully designed progression of channel dimensions</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 30px;">
                            <p>"We need to go deeper" - The <span class="tooltip">Inception<span class="tooltiptext">Named after the movie "Inception" where the characters go deeper into dream levels, similar to how the network goes deeper into feature extraction</span></span> philosophy</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Multi-Scale Processing Challenge</h2>
                    <div class="processing-challenge" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Why Different Filter Sizes Matter</h4>
                            <p style="font-size: 0.8em;">Objects in images appear at different scales and require different receptive fields</p>
                        </div>
                        <div class="filter-comparison fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; text-align: center;">
                                <div>
                                    <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                        <strong>1√ó1 Convolution</strong>
                                        <p style="font-size: 0.7em; margin-top: 10px;">Captures channel-wise patterns<br/>Cross-channel correlations</p>
                                    </div>
                                </div>
                                <div>
                                    <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                        <strong>3√ó3 Convolution</strong>
                                        <p style="font-size: 0.7em; margin-top: 10px;">Captures local patterns<br/>Small object features</p>
                                    </div>
                                </div>
                                <div>
                                    <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                        <strong>5√ó5 Convolution</strong>
                                        <p style="font-size: 0.7em; margin-top: 10px;">Captures regional patterns<br/>Larger context features</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <h5>The Naive Approach Problem</h5>
                            <p style="font-size: 0.8em;">Simply stacking different filter sizes would explode the computational cost</p>
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px; margin-top: 15px;">
                                <p style="font-size: 0.7em; margin: 0;">üí° <strong>Solution:</strong> Use parallel branches with dimension reduction</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: GoogLeNet Basics</h2>
                    <div data-mcq='{
                        "question": "What was GoogLeNets most significant innovation for handling multi-scale features?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using only 5√ó5 convolutions for larger receptive fields",
                                "correct": false,
                                "explanation": "GoogLeNet actually used multiple filter sizes in parallel, not just large filters."
                            },
                            {
                                "text": "Inception blocks with parallel branches of different filter sizes",
                                "correct": true,
                                "explanation": "Correct! Inception blocks process input through 1√ó1, 3√ó3, 5√ó5 convolutions and pooling in parallel, capturing features at multiple scales simultaneously."
                            },
                            {
                                "text": "Increasing network depth to 152 layers",
                                "correct": false,
                                "explanation": "GoogLeNet had 22 layers. The 152-layer depth was achieved later by ResNet."
                            },
                            {
                                "text": "Using dilated convolutions for multi-scale processing",
                                "correct": false,
                                "explanation": "Dilated convolutions came later. GoogLeNet used parallel branches with different kernel sizes."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Inception Block Deep Dive (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Inception Blocks", "url": "https://d2l.ai/chapter_convolutional-modern/googlenet.html#inception-blocks"}]'>
                    <h2 class="truncate-title">The Inception Block: Architecture Details</h2>
                    <div class="inception-architecture">
                        <div class="fragment">
                            <h4>Four Parallel Branches</h4>
                            <p style="font-size: 0.8em;">Each branch extracts different types of features</p>
                        </div>
                        <div class="branch-details fragment" style="margin-top: 30px; font-size: 0.7em;">
                            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Branch 1:</strong> 1√ó1 conv ‚Üí <span class="tooltip">c1<span class="tooltiptext">Number of output channels for the 1√ó1 convolution branch</span></span> channels
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Branch 2:</strong> 1√ó1 conv ‚Üí c2[0] ‚Üí 3√ó3 conv ‚Üí c2[1] channels
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Branch 3:</strong> 1√ó1 conv ‚Üí c3[0] ‚Üí 5√ó5 conv ‚Üí c3[1] channels
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Branch 4:</strong> 3√ó3 max pool ‚Üí 1√ó1 conv ‚Üí c4 channels
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Inception Block: Mathematical Formulation</h2>
                    <div class="inception-math" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4>Branch Computations</h4>
                            <p style="font-size: 0.8em;">Let input be $X \in \mathbb{R}^{C_{in} \times H \times W}$</p>
                        </div>
                        <div class="math-branches fragment" style="margin-top: 20px; font-size: 0.8em; display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Branch 1:</strong> $$B_1 = \text{ReLU}(\text{Conv}_{1 \times 1}^{c_1}(X))$$</p>
                            </div>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Branch 2:</strong> $$B_2 = \text{ReLU}(\text{Conv}_{3 \times 3}^{c_2[1]}(\text{ReLU}(\text{Conv}_{1 \times 1}^{c_2[0]}(X))))$$</p>
                            </div>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Branch 3:</strong> $$B_3 = \text{ReLU}(\text{Conv}_{5 \times 5}^{c_3[1]}(\text{ReLU}(\text{Conv}_{1 \times 1}^{c_3[0]}(X))))$$</p>
                            </div>
                            <div style="background: #f5f5f5; padding: 15px; border-radius: 8px;">
                                <p><strong>Branch 4:</strong> $$B_4 = \text{ReLU}(\text{Conv}_{1 \times 1}^{c_4}(\text{MaxPool}_{3 \times 3}(X)))$$</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <p style="font-size: 0.9em; margin: 0;"><strong>Output:</strong> $$Y = \text{Concat}([B_1, B_2, B_3, B_4])$$</p>
                                <p style="font-size: 0.7em; margin-top: 10px;">Output channels: $C_{out} = c_1 + c_2[1] + c_3[1] + c_4$</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">PyTorch Implementation: Inception Block</h2>
                    <div class="inception-implementation">
                        <div class="fragment">
                            <h4>Complete Inception Block Code</h4>
                        </div>
                        <pre class="fragment"><code class="language-python" data-line-numbers="1-20">class Inception(nn.Module):
    # c1--c4 are the number of output channels for each branch
    def __init__(self, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # Branch 1: 1√ó1 convolution
        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)
        # Branch 2: 1√ó1 followed by 3√ó3
        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)
        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)
        # Branch 3: 1√ó1 followed by 5√ó5
        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)
        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)
        # Branch 4: 3√ó3 max pooling followed by 1√ó1
        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)

    def forward(self, x):
        b1 = F.relu(self.b1_1(x))
        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))
        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))
        b4 = F.relu(self.b4_2(self.b4_1(x)))
        return torch.cat((b1, b2, b3, b4), dim=1)</code></pre>
                        <div class="fragment" style="margin-top: 20px; font-size: 0.6em;">
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <p style="margin: 0;"><strong>üí° Key Design Choices:</strong></p>
                                <ul style="margin: 10px 0 0 20px;">
                                    <li><span class="tooltip">LazyConv2d<span class="tooltiptext">Automatically infers input channels from the first forward pass</span></span> for automatic dimension inference</li>
                                    <li>Padding ensures all branches output the same H√óW</li>
                                    <li>1√ó1 convolutions reduce dimensions before expensive 3√ó3 and 5√ó5 operations</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Dimension Reduction: The Efficiency Secret</h2>
                    <div class="dimension-reduction">
                        <div class="fragment">
                            <h4>Computational Cost Analysis</h4>
                            <p style="font-size: 0.8em;">Why 1√ó1 convolutions before larger kernels?</p>
                        </div>
                        <div class="cost-comparison fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #ffebee; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #d32f2f;">Without Reduction</h5>
                                    <p style="font-size: 0.7em;">Input: 192 channels ‚Üí 5√ó5 conv ‚Üí 32 channels</p>
                                    <p style="font-size: 0.8em; font-weight: bold;">Operations: 192 √ó 5 √ó 5 √ó 32 = <span style="color: #d32f2f;">153,600</span></p>
                                </div>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #388e3c;">With Reduction</h5>
                                    <p style="font-size: 0.7em;">192 channels ‚Üí 1√ó1 conv ‚Üí 16 channels ‚Üí 5√ó5 conv ‚Üí 32 channels</p>
                                    <p style="font-size: 0.8em; font-weight: bold;">Operations: 192 √ó 16 + 16 √ó 5 √ó 5 √ó 32 = <span style="color: #388e3c;">15,872</span></p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 30px;">
                            <p><strong>9.7√ó reduction</strong> in computational cost while maintaining representational power!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Inception Blocks</h2>
                    <div data-mcq='{
                        "question": "Why does the Inception block use 1√ó1 convolutions before the 3√ó3 and 5√ó5 convolutions?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To increase the receptive field",
                                "correct": false,
                                "explanation": "1√ó1 convolutions dont change the receptive field; they operate on single pixels."
                            },
                            {
                                "text": "To reduce computational cost through dimension reduction",
                                "correct": true,
                                "explanation": "Correct! 1√ó1 convolutions reduce the channel dimension before expensive spatial convolutions, dramatically reducing computational cost."
                            },
                            {
                                "text": "To add more non-linearity to the network",
                                "correct": false,
                                "explanation": "While they do add non-linearity, the primary purpose is dimension reduction for efficiency."
                            },
                            {
                                "text": "To ensure all branches have the same output size",
                                "correct": false,
                                "explanation": "Padding ensures same spatial dimensions; 1√ó1 convs are for channel reduction."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: GoogLeNet Full Architecture (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - GoogLeNet Model", "url": "https://d2l.ai/chapter_convolutional-modern/googlenet.html#googlenet-model"}]'>
                    <h2 class="truncate-title">GoogLeNet: Complete Architecture</h2>
                    <div class="googlenet-architecture">
                        <div class="fragment">
                            <h4>Three-Part Design Pattern</h4>
                            <p style="font-size: 0.8em;">A clear separation of concerns that became standard in deep learning</p>
                        </div>
                        <div class="architecture-parts fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px;">
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="margin: 0; color: #1976d2;">üå± Stem</h5>
                                    <p style="font-size: 0.7em; margin-top: 10px;">Data ingestion<br/>Low-level feature extraction</p>
                                </div>
                                <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="margin: 0; color: #7b1fa2;">üèóÔ∏è Body</h5>
                                    <p style="font-size: 0.7em; margin-top: 10px;">9 Inception blocks<br/>Multi-scale processing</p>
                                </div>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="margin: 0; color: #388e3c;">üéØ Head</h5>
                                    <p style="font-size: 0.7em; margin-top: 10px;">Global average pooling<br/>Final classification</p>
                                </div>
                            </div>
                        </div>
                        <div id="googlenet-full-arch" style="margin: 30px auto;"></div>
                        <div class="demo-controls" style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
                            <button id="explore-stages" style="background: #10099F; color: white; padding: 8px 16px; border: none; border-radius: 5px;">Explore Stages</button>
                            <button id="show-dimensions" style="background: #2DD2C0; color: white; padding: 8px 16px; border: none; border-radius: 5px;">Show Dimensions</button>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Stem: Initial Feature Extraction</h2>
                    <div class="stem-details">
                        <div class="fragment">
                            <h4>Block 1: Entry Point</h4>
                            <pre><code class="language-python">def b1(self):
    return nn.Sequential(
        nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    )</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4>Block 2: Dimension Expansion</h4>
                            <pre><code class="language-python">def b2(self):
    return nn.Sequential(
        nn.LazyConv2d(64, kernel_size=1),
        nn.ReLU(),
        nn.LazyConv2d(192, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    )</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px; background: #f9f9f9; padding: 15px; border-radius: 8px;">
                            <p style="font-size: 0.8em; margin: 0;"><strong>Output after stem:</strong> 192 channels, spatial dimensions reduced by 8√ó</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Body: 9 Inception Blocks in 3 Groups</h2>
                    <div class="body-details" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Block 3: First Inception Group</h4>
                            <pre><code class="language-python" data-line-numbers="1-5">def b3(self):
    return nn.Sequential(
        Inception(64, (96, 128), (16, 32), 32),   # 256 output channels
        Inception(128, (128, 192), (32, 96), 64), # 480 output channels
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    )</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4>Block 4: Second Inception Group (5 blocks)</h4>
                            <pre><code class="language-python" data-line-numbers="1-8">def b4(self):
    return nn.Sequential(
        Inception(192, (96, 208), (16, 48), 64),   # 512 channels
        Inception(160, (112, 224), (24, 64), 64),  # 512 channels
        Inception(128, (128, 256), (24, 64), 64),  # 512 channels
        Inception(112, (144, 288), (32, 64), 64),  # 528 channels
        Inception(256, (160, 320), (32, 128), 128),# 832 channels
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    )</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h4>Block 5: Final Inception Group + Head</h4>
                            <pre><code class="language-python" data-line-numbers="1-6">def b5(self):
    return nn.Sequential(
        Inception(256, (160, 320), (32, 128), 128),  # 832 channels
        Inception(384, (192, 384), (48, 128), 128),  # 1024 channels
        nn.AdaptiveAvgPool2d((1,1)),  # Global Average Pooling
        nn.Flatten()
    )</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Channel Evolution Through the Network</h2>
                    <div class="channel-evolution">
                        <div class="fragment">
                            <h4>Progressive Channel Growth</h4>
                            <p style="font-size: 0.8em;">Tracking output channels through each Inception block</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <table style="font-size: 0.7em; margin: 0 auto; border-collapse: collapse;">
                                <thead>
                                    <tr style="background: #10099F; color: white;">
                                        <th style="padding: 10px;">Stage</th>
                                        <th style="padding: 10px;">Blocks</th>
                                        <th style="padding: 10px;">Output Channels</th>
                                        <th style="padding: 10px;">Spatial Size (96√ó96 input)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="background: #f9f9f9;">
                                        <td style="padding: 8px;">Stem</td>
                                        <td style="padding: 8px;">b1 + b2</td>
                                        <td style="padding: 8px;">192</td>
                                        <td style="padding: 8px;">12√ó12</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;">Group 1</td>
                                        <td style="padding: 8px;">2 Inception</td>
                                        <td style="padding: 8px;">256 ‚Üí 480</td>
                                        <td style="padding: 8px;">6√ó6</td>
                                    </tr>
                                    <tr style="background: #f9f9f9;">
                                        <td style="padding: 8px;">Group 2</td>
                                        <td style="padding: 8px;">5 Inception</td>
                                        <td style="padding: 8px;">512 ‚Üí 832</td>
                                        <td style="padding: 8px;">3√ó3</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;">Group 3</td>
                                        <td style="padding: 8px;">2 Inception</td>
                                        <td style="padding: 8px;">832 ‚Üí 1024</td>
                                        <td style="padding: 8px;">1√ó1 (after GAP)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: GoogLeNet Architecture</h2>
                    <div data-mcq='{
                        "question": "What is the purpose of Global Average Pooling in GoogLeNets head?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To increase the receptive field",
                                "correct": false,
                                "explanation": "GAP doesnt increase receptive field; it aggregates spatial information."
                            },
                            {
                                "text": "To reduce overfitting by replacing fully connected layers",
                                "correct": true,
                                "explanation": "Correct! GAP reduces each feature map to a single value, dramatically reducing parameters compared to FC layers and helping prevent overfitting."
                            },
                            {
                                "text": "To make the network deeper",
                                "correct": false,
                                "explanation": "GAP actually reduces spatial dimensions to 1√ó1, not adding depth."
                            },
                            {
                                "text": "To speed up training",
                                "correct": false,
                                "explanation": "While GAP is computationally efficient, its primary purpose is regularization, not speed."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Implementation and Training (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - GoogLeNet Training", "url": "https://d2l.ai/chapter_convolutional-modern/googlenet.html#training"}]'>
                    <h2 class="truncate-title">Complete GoogLeNet Implementation</h2>
                    <div class="full-implementation">
                        <div class="fragment">
                            <h4>Putting It All Together</h4>
                        </div>
                        <pre class="fragment"><code class="language-python" data-line-numbers="1-15">class GoogleNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super(GoogleNet, self).__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),
            nn.LazyLinear(num_classes)
        )
        self.net.apply(d2l.init_cnn)

    # b1-b5 methods as defined earlier...</code></pre>
                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Shape Evolution (96√ó96 input)</h5>
                            <pre><code class="language-python">model = GoogleNet().layer_summary((1, 1, 96, 96))

# Output:
# Sequential output shape:     torch.Size([1, 64, 24, 24])   # b1
# Sequential output shape:     torch.Size([1, 192, 12, 12])  # b2
# Sequential output shape:     torch.Size([1, 480, 6, 6])    # b3
# Sequential output shape:     torch.Size([1, 832, 3, 3])    # b4
# Sequential output shape:     torch.Size([1, 1024])         # b5
# Linear output shape:         torch.Size([1, 10])           # classifier</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Configuration and Results</h2>
                    <div class="training-details">
                        <div class="fragment">
                            <h4>Training Setup</h4>
                            <pre><code class="language-python">model = GoogleNet(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Key Training Parameters</h5>
                            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; font-size: 0.8em;">
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Learning Rate:</strong> 0.01
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Batch Size:</strong> 128
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Input Size:</strong> 96√ó96 (reduced for Fashion-MNIST)
                                </div>
                                <div style="background: #f9f9f9; padding: 10px; border-radius: 5px;">
                                    <strong>Epochs:</strong> 10
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                <p style="font-size: 0.8em; margin: 0;"><strong>üí° Note:</strong> Original GoogLeNet used 224√ó224 images on ImageNet. We use 96√ó96 for faster training on Fashion-MNIST.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Efficiency Analysis</h2>
                    <div class="efficiency-analysis">
                        <div class="fragment">
                            <h4>GoogLeNet vs Previous Architectures</h4>
                        </div>
                        <div class="comparison-table fragment" style="margin-top: 20px;">
                            <table style="font-size: 0.75em; margin: 0 auto; border-collapse: collapse;">
                                <thead>
                                    <tr style="background: #10099F; color: white;">
                                        <th style="padding: 10px;">Architecture</th>
                                        <th style="padding: 10px;">Parameters</th>
                                        <th style="padding: 10px;">Operations</th>
                                        <th style="padding: 10px;">Top-5 Error</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="background: #f9f9f9;">
                                        <td style="padding: 8px;">AlexNet (2012)</td>
                                        <td style="padding: 8px;">60M</td>
                                        <td style="padding: 8px;">1.5B</td>
                                        <td style="padding: 8px;">16.4%</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;">VGG-16 (2014)</td>
                                        <td style="padding: 8px;">138M</td>
                                        <td style="padding: 8px;">15.3B</td>
                                        <td style="padding: 8px;">7.3%</td>
                                    </tr>
                                    <tr style="background: #e8f5e9;">
                                        <td style="padding: 8px;"><strong>GoogLeNet (2014)</strong></td>
                                        <td style="padding: 8px;"><strong>6.8M</strong></td>
                                        <td style="padding: 8px;"><strong>1.5B</strong></td>
                                        <td style="padding: 8px;"><strong>6.67%</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 30px;">
                            <p><strong>20√ó fewer parameters</strong> than VGG while achieving <strong>better accuracy</strong>!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation Tips and Tricks</h2>
                    <div class="implementation-tips">
                        <div class="fragment">
                            <h4>Practical Considerations</h4>
                        </div>
                        <div class="tips-grid fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr; gap: 15px; font-size: 0.6em;">
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <strong>üéØ Channel Ratio Guidelines:</strong>
                                <p style="margin-top: 5px;">Typically follow 2:4:1:1 or similar ratios across branches to balance computational load</p>
                            </div>
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <strong>‚ö° Memory Optimization:</strong>
                                <p style="margin-top: 5px;">Use <span class="tooltip">gradient checkpointing<span class="tooltiptext">Trade compute for memory by recomputing activations during backprop instead of storing them</span></span> for deeper variants</p>
                            </div>
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <strong>üîß Initialization:</strong>
                                <p style="margin-top: 5px;">Careful initialization is crucial due to the concatenation of multiple branches</p>
                            </div>
                            <div style="background: #fce4ec; padding: 15px; border-radius: 8px;">
                                <strong>üìä Batch Normalization:</strong>
                                <p style="margin-top: 5px;">Later versions (Inception-v2/v3) added <span class="tooltip">BN<span class="tooltiptext">Batch Normalization: normalizes inputs to each layer, improving training stability</span></span> after each convolution</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Implementation</h2>
                    <div data-mcq='{
                        "question": "What makes GoogLeNet computationally efficient compared to VGG?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Strategic use of 1√ó1 convolutions for dimension reduction",
                                "correct": true,
                                "explanation": "Correct! 1√ó1 convolutions reduce channels before expensive spatial operations."
                            },
                            {
                                "text": "Global average pooling instead of fully connected layers",
                                "correct": true,
                                "explanation": "Correct! GAP dramatically reduces parameters in the classifier head."
                            },
                            {
                                "text": "Using only 3√ó3 convolutions throughout",
                                "correct": false,
                                "explanation": "GoogLeNet actually uses multiple kernel sizes (1√ó1, 3√ó3, 5√ó5), not just 3√ó3."
                            },
                            {
                                "text": "Parallel processing of different filter sizes",
                                "correct": false,
                                "explanation": "While parallel branches exist, they dont inherently reduce computation; dimension reduction does."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Evolution and Impact (Vertical) -->
            <section>
                <section data-sources='[{"text": "Rethinking the Inception Architecture - Szegedy et al. (2016)", "url": "https://arxiv.org/abs/1512.00567"}, {"text": "Inception-v4, Inception-ResNet - Szegedy et al. (2017)", "url": "https://arxiv.org/abs/1602.07261"}]'>
                    <h2 class="truncate-title">The Inception Family Evolution</h2>
                    <div class="inception-evolution">
                        <div class="fragment">
                            <h4>From GoogLeNet to Modern Variants</h4>
                            <p style="font-size: 0.8em;">Continuous refinement of the Inception principle</p>
                        </div>
                        <div class="evolution-timeline fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr; gap: 15px; font-size: 0.75em;">
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: start;">
                                    <strong>GoogLeNet/Inception-v1 (2014)</strong>
                                    <ul style="margin: 0 0 0 20px;">
                                        <li>Original Inception blocks</li>
                                        <li>Auxiliary classifiers for training</li>
                                    </ul>
                                </div>
                                <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: start;">
                                    <strong>Inception-v2 (2015)</strong>
                                    <ul style="margin: 0 0 0 20px;">
                                        <li>Batch normalization added</li>
                                        <li>Factorized 5√ó5 into two 3√ó3 convolutions</li>
                                    </ul>
                                </div>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: start;">
                                    <strong>Inception-v3 (2015)</strong>
                                    <ul style="margin: 0 0 0 20px;">
                                        <li>Factorized n√ón into 1√ón and n√ó1</li>
                                        <li>Label smoothing regularization</li>
                                    </ul>
                                </div>
                                <div style="background: #fff3e0; padding: 15px; border-radius: 8px; display: grid; grid-template-columns: 1fr 2fr; gap: 15px; align-items: start;">
                                    <strong>Inception-v4 & Inception-ResNet (2016)</strong>
                                    <ul style="margin: 0 0 0 20px;">
                                        <li>Residual connections added</li>
                                        <li>Simplified architecture</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Key Innovations and Their Impact</h2>
                    <div class="innovations-impact">
                        <div class="fragment">
                            <h4>GoogLeNet's Lasting Contributions</h4>
                        </div>
                        <div class="contributions-grid fragment" style="margin-top: 20px; display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; font-size: 0.8em;">
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin: 0;">Multi-Scale Processing</h5>
                                <p style="margin-top: 10px;">Parallel branches with different receptive fields became standard in many architectures</p>
                            </div>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin: 0;">Computational Efficiency</h5>
                                <p style="margin-top: 10px;">Showed that clever design could achieve more with fewer parameters</p>
                            </div>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin: 0;">Modular Design</h5>
                                <p style="margin-top: 10px;">Inception blocks as reusable components influenced later architectures</p>
                            </div>
                            <div style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin: 0;">Network Architecture Search</h5>
                                <p style="margin-top: 10px;">Inspired automated architecture design and <span class="tooltip">NAS<span class="tooltiptext">Neural Architecture Search: automated methods for designing neural network architectures</span></span></p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Practical Applications and Use Cases</h2>
                    <div class="applications">
                        <div class="fragment">
                            <h4>Where Inception Shines</h4>
                        </div>
                        <div class="use-cases fragment" style="margin-top: 20px; font-size: 0.8em;">
                            <div style="display: grid; grid-template-columns: 1fr; gap: 15px;">
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <strong>‚úÖ Ideal For:</strong>
                                    <ul style="margin: 10px 0 0 20px;">
                                        <li>Mobile and embedded deployment (low parameter count)</li>
                                        <li>Multi-scale object detection tasks</li>
                                        <li>Applications requiring good accuracy-efficiency trade-off</li>
                                        <li>Transfer learning with limited computational resources</li>
                                    </ul>
                                </div>
                                <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                    <strong>‚ö†Ô∏è Consider Alternatives When:</strong>
                                    <ul style="margin: 10px 0 0 20px;">
                                        <li>Simplicity is preferred (ResNet may be easier)</li>
                                        <li>Training from scratch with limited data</li>
                                        <li>Very deep networks needed (ResNet scales better)</li>
                                        <li>Real-time inference on CPU (MobileNet may be faster)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">GoogLeNet's Place in History</h2>
                    <div class="historical-context">
                        <div class="fragment">
                            <h4>A Pivotal Moment in Deep Learning</h4>
                            <p style="font-size: 0.8em;">2014: The year that changed computer vision</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.85em; margin: 0;">
                                    <strong>Key Achievement:</strong> GoogLeNet proved that network architecture innovation could overcome the "bigger is better" paradigm, paving the way for efficient deep learning.
                                </p>
                            </div>
                        </div>
                        <div class="fragment closing-thought" style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-left: 5px solid #10099F;">
                            <p style="margin: 0; font-size: 0.9em; font-style: italic;">
                                "Going deeper with convolutions showed us that the path forward wasn't just depth, but intelligent design."
                            </p>
                            <p style="margin-top: 10px; font-size: 0.8em;">- The legacy of GoogLeNet</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Impact and Evolution</h2>
                    <div data-mcq='{
                        "question": "Which of the following are direct contributions of GoogLeNet to modern deep learning?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "The concept of processing features at multiple scales in parallel",
                                "correct": true,
                                "explanation": "Correct! Inception blocks parallel multi-scale processing is now widely used."
                            },
                            {
                                "text": "The first use of ReLU activation functions",
                                "correct": false,
                                "explanation": "ReLU was popularized by AlexNet in 2012, before GoogLeNet."
                            },
                            {
                                "text": "Demonstrating that efficiency and accuracy can be achieved together",
                                "correct": true,
                                "explanation": "Correct! GoogLeNet achieved better accuracy than VGG with 20√ó fewer parameters."
                            },
                            {
                                "text": "The stem-body-head architectural pattern",
                                "correct": true,
                                "explanation": "Correct! GoogLeNet clearly separated these three components, influencing future designs."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 5: Batch Normalization (Vertical) -->
            <section>
                <section data-sources='[{"text": "Batch Normalization - Dive into Deep Learning", "url": "https://d2l.ai/chapter_convolutional-modern/batch-norm.html"}, {"text": "Batch Normalization: Accelerating Deep Network Training", "url": "https://arxiv.org/abs/1502.03167"}]'>
                    <h2 class="truncate-title">Batch Normalization: Accelerating Deep Network Training</h2>
                    <div class="batch-norm-intro" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>The Training Acceleration Breakthrough</h4>
                            <p style="font-size: 0.85em;">Introduced in 2015, <span class="tooltip">Batch Normalization<span class="tooltiptext">A technique that normalizes inputs of each layer to have zero mean and unit variance, dramatically improving training speed and stability</span></span> revolutionized deep learning</p>
                        </div>
                        <div class="benefits-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <div style="font-size: 1.5em; color: #10099F;">üöÄ</div>
                                <h5 style="color: #10099F; margin: 10px 0;">Faster Convergence</h5>
                                <p style="font-size: 0.8em;">Allows more aggressive learning rates, reducing training time by orders of magnitude</p>
                            </div>
                            <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                <div style="font-size: 1.5em; color: #10099F;">üìä</div>
                                <h5 style="color: #10099F; margin: 10px 0;">Numerical Stability</h5>
                                <p style="font-size: 0.8em;">Prevents <span class="tooltip">gradient explosion/vanishing<span class="tooltiptext">Problems where gradients become too large (explosion) or too small (vanishing) during backpropagation</span></span></p>
                            </div>
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <div style="font-size: 1.5em; color: #10099F;">üéØ</div>
                                <h5 style="color: #10099F; margin: 10px 0;">Built-in Regularization</h5>
                                <p style="font-size: 0.8em;">Provides regularization through noise injection from batch statistics</p>
                            </div>
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <div style="font-size: 1.5em; color: #10099F;">üèóÔ∏è</div>
                                <h5 style="color: #10099F; margin: 10px 0;">Deeper Networks</h5>
                                <p style="font-size: 0.8em;">Makes training networks with 100+ layers practical and stable</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Innovation:</strong> Normalize activations <em>during</em> training, not just inputs</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Deep Networks: The Challenges</h2>
                    <div class="training-challenges" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Why Deep Networks Are Hard to Train</h4>
                        </div>
                        <div class="challenge-list fragment" style="margin-top: 30px;">
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px; margin-bottom: 15px;">
                                <h5 style="color: #d32f2f;">1. Internal Variable Distribution Shift</h5>
                                <p style="font-size: 0.85em;">As we train, the distribution of activations in intermediate layers changes constantly, requiring continuous adaptation of later layers</p>
                            </div>
                            <div style="background: #fce4ec; padding: 20px; border-radius: 8px; margin-bottom: 15px;">
                                <h5 style="color: #c2185b;">2. Varying Activation Magnitudes</h5>
                                <p style="font-size: 0.85em;">Different layers may have activations that vary by orders of magnitude, necessitating careful learning rate tuning</p>
                            </div>
                            <div style="background: #f3e5f5; padding: 20px; border-radius: 8px; margin-bottom: 15px;">
                                <h5 style="color: #7b1fa2;">3. Optimization Landscape Complexity</h5>
                                <p style="font-size: 0.85em;">Deeper networks have more complex loss surfaces with many local minima and saddle points</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Solution:</strong> Normalize each layer's inputs to maintain stable distributions</p>
                        </div>
                    </div>
                </section>

                <section style="font-size: 0.5em;">
                    <h2 class="truncate-title">Mathematical Formulation</h2>
                    <div class="math-formulation">
                        <div class="fragment">
                            <h4>The Batch Normalization Transform</h4>
                            <p style="font-size: 0.85em;">For a minibatch $\mathcal{B}$ and input $\mathbf{x} \in \mathcal{B}$:</p>
                        </div>
                        <div class="fragment main-equation" style="background: #e3f2fd; padding: 25px; border-radius: 8px; margin: 20px 0;">
                            <p style="font-size: 1.1em; margin: 0;">
                                $$\textrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}$$
                            </p>
                        </div>
                        <div class="fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                            <div>
                                <h5>Where:</h5>
                                <ul style="font-size: 0.85em; line-height: 1.8;">
                                    <li><span style="color: #10099F;">$\hat{\boldsymbol{\mu}}_\mathcal{B}$</span> = <span class="tooltip">Batch mean<span class="tooltiptext">The mean of the current minibatch, computed across all examples in the batch</span></span></li>
                                    <li><span style="color: #10099F;">$\hat{\boldsymbol{\sigma}}_\mathcal{B}$</span> = <span class="tooltip">Batch standard deviation<span class="tooltiptext">The standard deviation of the current minibatch, computed across all examples</span></span></li>
                                    <li><span style="color: #2DD2C0;">$\boldsymbol{\gamma}$</span> = <span class="tooltip">Scale parameter<span class="tooltiptext">Learnable parameter that scales the normalized values, recovering representation power</span></span> (learned)</li>
                                    <li><span style="color: #2DD2C0;">$\boldsymbol{\beta}$</span> = <span class="tooltip">Shift parameter<span class="tooltiptext">Learnable parameter that shifts the normalized values, recovering representation power</span></span> (learned)</li>
                                </ul>
                            </div>
                            <div>
                                <h5>Batch Statistics Computation:</h5>
                                <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                    <p style="margin: 10px 0;">
                                        $$\hat{\boldsymbol{\mu}}_\mathcal{B} = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x}$$
                                    </p>
                                    <p style="margin: 10px 0;">
                                        $$\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon$$
                                    </p>
                                </div>
                                <p style="font-size: 0.8em; margin-top: 10px;">Note: $\epsilon > 0$ prevents division by zero</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Batch Normalization for Fully Connected Layers</h2>
                    <div class="bn-fc-layers" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Application in Dense Networks</h4>
                            <p style="font-size: 0.85em;">Applied <strong>after</strong> affine transformation, <strong>before</strong> activation</p>
                        </div>
                        <div class="fragment computation-flow" style="margin-top: 30px;">
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.9em; margin: 10px 0;">
                                    <strong>Standard Layer:</strong><br>
                                    $\mathbf{h} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$
                                </p>
                                <p style="font-size: 0.9em; margin: 20px 0 10px 0;">
                                    <strong>With Batch Normalization:</strong><br>
                                    $\mathbf{h} = \phi(\textrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}))$
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Key Points for Fully Connected Layers:</h5>
                            <ul style="font-size: 0.85em; line-height: 1.8;">
                                <li>Normalization is performed <strong>per feature</strong> across the batch</li>
                                <li>Each feature has its own $\gamma$ and $\beta$ parameters</li>
                                <li>For a layer with $d$ features, we learn $2d$ parameters</li>
                                <li>The bias $\mathbf{b}$ can often be omitted as $\beta$ serves a similar role</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Batch Normalization for Convolutional Layers</h2>
                    <div class="bn-conv-layers" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Channel-wise Normalization</h4>
                            <p style="font-size: 0.85em;">Preserves <span class="tooltip">translation invariance<span class="tooltiptext">The property that a pattern can be detected regardless of its position in the input</span></span> by normalizing per channel</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Convolutional Batch Norm Statistics</h5>
                                <p style="font-size: 0.85em;">For minibatch size $m$, height $p$, width $q$:</p>
                                <ul style="font-size: 0.85em; margin-top: 10px;">
                                    <li>Normalize over $m \cdot p \cdot q$ elements per channel</li>
                                    <li>Same mean/variance applied to all spatial locations in a channel</li>
                                    <li>Each channel gets its own $\gamma$ and $\beta$</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.75em;">
                            <p><strong>Important:</strong> This maintains the CNN's ability to detect features regardless of position</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Layer Normalization: An Alternative Approach</h2>
                    <div class="layer-norm" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Normalizing Within Each Sample</h4>
                            <p style="font-size: 0.85em;"><span class="tooltip">Layer Normalization<span class="tooltiptext">Normalizes across features within each sample, rather than across the batch</span></span> works even with batch size 1</p>
                        </div>
                        <div class="comparison-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">Batch Normalization</h5>
                                <ul style="font-size: 0.8em; line-height: 1.6;">
                                    <li>Normalizes across batch dimension</li>
                                    <li>Different behavior train/test</li>
                                    <li>Requires batch size > 1</li>
                                    <li>Better for CNNs</li>
                                </ul>
                            </div>
                            <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #7b1fa2;">Layer Normalization</h5>
                                <ul style="font-size: 0.8em; line-height: 1.6;">
                                    <li>Normalizes across feature dimension</li>
                                    <li>Same behavior train/test</li>
                                    <li>Works with batch size = 1</li>
                                    <li>Better for RNNs/Transformers</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Layer Norm Formula:</h5>
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <p style="margin: 10px 0;">
                                    $$\textrm{LN}(\mathbf{x}) = \frac{\mathbf{x} - \hat{\mu}}{\hat{\sigma}}$$
                                </p>
                                <p style="margin: 10px 0; font-size: 0.85em;">
                                    where $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i$ and $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2 + \epsilon$
                                </p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training vs Inference Mode</h2>
                    <div class="train-vs-inference" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Different Behavior for Different Phases</h4>
                        </div>
                        <div class="mode-comparison fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">üèãÔ∏è Training Mode</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Uses current batch statistics</li>
                                    <li>Different normalization per batch</li>
                                    <li>Provides regularization through noise</li>
                                    <li>Updates moving averages</li>
                                </ul>
                                <div style="background: white; padding: 10px; border-radius: 5px; margin-top: 15px;">
                                    <code style="font-size: 0.75em;">
                                        mean = batch.mean()<br>
                                        var = batch.var()<br>
                                        x_norm = (x - mean) / sqrt(var)
                                    </code>
                                </div>
                            </div>
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">üéØ Inference Mode</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Uses population statistics</li>
                                    <li>Deterministic normalization</li>
                                    <li>No noise injection</li>
                                    <li>Uses stored moving averages</li>
                                </ul>
                                <div style="background: white; padding: 10px; border-radius: 5px; margin-top: 15px;">
                                    <code style="font-size: 0.75em;">
                                        mean = moving_mean<br>
                                        var = moving_var<br>
                                        x_norm = (x - mean) / sqrt(var)
                                    </code>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Moving Average Update (Training):</h5>
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <p style="font-size: 0.85em; margin: 5px 0;">
                                    <code>moving_mean = (1 - momentum) * moving_mean + momentum * batch_mean</code>
                                </p>
                                <p style="font-size: 0.85em; margin: 5px 0;">
                                    <code>moving_var = (1 - momentum) * moving_var + momentum * batch_var</code>
                                </p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation from Scratch</h2>
                    <div class="implementation-scratch">
                        <div class="fragment">
                            <h4>PyTorch-style Batch Normalization</h4>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <pre><code class="language-python" data-trim data-noescape>
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # Use is_grad_enabled to determine whether we are in training mode
    if not torch.is_grad_enabled():
        # In prediction mode, use mean and variance obtained by moving average
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # When using a fully connected layer, calculate the mean and
            # variance on the feature dimension
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # When using a two-dimensional convolutional layer, calculate the
            # mean and variance on the channel dimension (axis=1). Here we
            # need to maintain the shape of X, so that the broadcasting
            # operation can be carried out later
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # In training mode, the current mean and variance are used
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # Update the mean and variance using moving average
        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean
        moving_var = (1.0 - momentum) * moving_var + momentum * var
    Y = gamma * X_hat + beta  # Scale and shift
    return Y, moving_mean.data, moving_var.data
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">BatchNorm Layer Implementation</h2>
                    <div class="bn-layer-impl">
                        <div class="fragment">
                            <h4>Creating a Reusable BatchNorm Layer</h4>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <pre><code class="language-python" data-trim data-noescape>
class BatchNorm(nn.Module):
    # num_features: the number of outputs for a fully connected layer or the
    # number of output channels for a convolutional layer. num_dims: 2 for a
    # fully connected layer and 4 for a convolutional layer
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # The scale parameter and the shift parameter (model parameters) are
        # initialized to 1 and 0, respectively
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # The variables that are not model parameters are initialized to 0 and 1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # If X is not on the main memory, copy moving_mean and moving_var to
        # the device where X is located
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # Save the updated moving_mean and moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.1)
        return Y
                            </code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Note:</strong> Modern frameworks provide optimized implementations - use those in practice!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">LeNet with Batch Normalization</h2>
                    <div class="lenet-bn">
                        <div class="fragment">
                            <h4>Enhancing Classic Architectures</h4>
                            <p style="font-size: 0.85em;">Adding Batch Norm to LeNet dramatically improves training</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <pre><code class="language-python" data-trim data-noescape>
class BNLeNetScratch(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Flatten(), nn.LazyLinear(120),
            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),
            BatchNorm(84, num_dims=2), nn.Sigmoid(),
            nn.LazyLinear(num_classes))
                            </code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Placement Pattern</h5>
                                <p style="font-size: 0.85em;">
                                    Conv/Linear ‚Üí <strong style="color: #10099F;">BatchNorm</strong> ‚Üí Activation ‚Üí Pooling
                                </p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Using High-Level APIs</h2>
                    <div class="high-level-api" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Framework Implementations</h4>
                            <p style="font-size: 0.85em;">Production code should use optimized built-in layers</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <pre><code class="language-python" data-trim data-noescape>
class BNLeNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),
            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),
            nn.Sigmoid(), nn.LazyLinear(num_classes))
                            </code></pre>
                        </div>
                        <div class="api-comparison fragment" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <h6 style="color: #10099F; margin: 5px 0;">PyTorch</h6>
                                <code style="font-size: 0.75em;">nn.BatchNorm2d()</code><br>
                                <code style="font-size: 0.75em;">nn.BatchNorm1d()</code>
                            </div>
                            <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
                                <h6 style="color: #7b1fa2; margin: 5px 0;">TensorFlow</h6>
                                <code style="font-size: 0.75em;">tf.keras.layers.BatchNormalization()</code>
                            </div>
                            <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                <h6 style="color: #2e7d32; margin: 5px 0;">JAX/Flax</h6>
                                <code style="font-size: 0.75em;">nn.BatchNorm()</code>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Tip:</strong> These implementations are heavily optimized and handle all edge cases automatically</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Performance Impact Visualization</h2>
                    <div class="performance-impact">
                        <div class="fragment">
                            <h4>Training with vs without Batch Normalization</h4>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div id="performance-comparison-viz" style="height: 400px; background: #f5f5f5; border-radius: 8px;"></div>
                        </div>
                        <div class="metrics-grid fragment" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 30px; font-size: 0.65em;">
                            <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;">
                                <div style="font-size: 2em; color: #10099F; font-weight: bold;">10x</div>
                                <p style="font-size: 0.8em; margin: 5px 0;">Faster Convergence</p>
                            </div>
                            <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; text-align: center;">
                                <div style="font-size: 2em; color: #7b1fa2; font-weight: bold;">100x</div>
                                <p style="font-size: 0.8em; margin: 5px 0;">Larger Learning Rates</p>
                            </div>
                            <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;">
                                <div style="font-size: 2em; color: #2e7d32; font-weight: bold;">2-3%</div>
                                <p style="font-size: 0.8em; margin: 5px 0;">Better Accuracy</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "How Does Batch Normalization Help Optimization?", "url": "https://arxiv.org/abs/1805.11604"}, {"text": "Understanding Batch Normalization", "url": "https://arxiv.org/abs/1806.02375"}]'>
                    <h2 class="truncate-title">Discussion: Internal Covariate Shift</h2>
                    <div class="discussion" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>The Original Hypothesis and Debates</h4>
                        </div>
                        <div class="fragment" style="margin-top: 30px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #d32f2f;">Original Claim (2015)</h5>
                                <p style="font-size: 0.85em;">"Batch Normalization reduces <span class="tooltip">internal covariate shift<span class="tooltiptext">The change in distribution of network activations due to parameter updates during training</span></span>"</p>
                            </div>
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #f57c00;">Later Research (2018)</h5>
                                <p style="font-size: 0.85em;">Found that BN actually works by:</p>
                                <ul style="font-size: 0.8em; margin-top: 10px;">
                                    <li>Smoothing the optimization landscape</li>
                                    <li>Reducing sensitivity to hyperparameters</li>
                                    <li>Providing implicit regularization through noise</li>
                                    <li>Making gradients more predictable</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Current Understanding</h5>
                                <p style="font-size: 0.85em;">Batch Normalization's success comes from multiple factors:</p>
                                <ul style="font-size: 0.8em; margin-top: 10px;">
                                    <li><strong>Regularization:</strong> Noise from batch statistics</li>
                                    <li><strong>Optimization:</strong> Smoother loss landscape</li>
                                    <li><strong>Stability:</strong> Prevents activation explosion/vanishing</li>
                                </ul>
                            </div>
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">Key Insight</h5>
                                <p style="font-size: 0.85em;">The exact mechanism matters less than the empirical success. BN works through multiple complementary effects that together make training more stable and effective.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Practical Considerations</h2>
                    <div class="practical-tips">
                        <div class="fragment">
                            <h4>Best Practices and Common Pitfalls</h4>
                        </div>
                        <div class="tips-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px; font-size: 0.65em;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">‚úÖ Do's</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Use larger batch sizes (32-256)</li>
                                    <li>Increase learning rates (10-100x)</li>
                                    <li>Place BN before activation</li>
                                    <li>Use framework implementations</li>
                                    <li>Fine-tune momentum parameter</li>
                                </ul>
                            </div>
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #d32f2f;">‚ùå Don'ts</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Use batch size = 1</li>
                                    <li>Forget to switch to eval mode</li>
                                    <li>Apply after dropout</li>
                                    <li>Use with very small batches</li>
                                    <li>Ignore moving averages</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px; font-size: 0.65em; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">Batch Size Sensitivity</h5>
                                <p style="font-size: 0.85em;">Optimal batch sizes for Batch Normalization:</p>
                                <ul style="font-size: 0.8em; margin-top: 10px;">
                                    <li><strong>Too Small (&lt;16):</strong> Noisy statistics, poor performance</li>
                                    <li><strong>Optimal (32-128):</strong> Good regularization and stability</li>
                                    <li><strong>Large (&gt;256):</strong> Less regularization, may need other techniques</li>
                                </ul>
                            </div>
                            <div class="fragment emphasis-box">
                                <p><strong>Remember:</strong> Batch Normalization changed deep learning forever - use it wisely!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Batch Normalization</h2>
                    <div data-mcq='{
                        "question": "Which statements about Batch Normalization are correct?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "It normalizes activations to have zero mean and unit variance",
                                "correct": true,
                                "explanation": "Correct! BN standardizes activations then applies learnable scale and shift."
                            },
                            {
                                "text": "It behaves identically during training and inference",
                                "correct": false,
                                "explanation": "Incorrect. BN uses batch statistics during training but population statistics during inference."
                            },
                            {
                                "text": "It allows the use of higher learning rates",
                                "correct": true,
                                "explanation": "Correct! BN stabilizes training, allowing 10-100x larger learning rates."
                            },
                            {
                                "text": "It can be applied to any batch size including 1",
                                "correct": false,
                                "explanation": "Incorrect. BN requires batch size > 1 to compute meaningful statistics."
                            },
                            {
                                "text": "It provides regularization through noise injection",
                                "correct": true,
                                "explanation": "Correct! The stochastic batch statistics act as a form of regularization."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- ResNet Section -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - ResNet", "url": "https://d2l.ai/chapter_convolutional-modern/resnet.html"}]'>
                    <h1 class="truncate-title">Residual Networks (ResNet)</h1>
                    <div class="resnet-intro">
                        <div class="fragment">
                            <h3>The Deep Learning Revolution Continues</h3>
                            <p>From 8 layers (AlexNet) to <strong>152 layers</strong> (ResNet-152)</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                                <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                    <h4 style="color: #2e7d32;">Key Innovation</h4>
                                    <p style="font-size: 0.9em;"><span class="tooltip">Skip connections<span class="tooltiptext">Direct connections that bypass one or more layers, allowing gradients to flow directly through the network</span></span> that enable training of very deep networks</p>
                                </div>
                                <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h4 style="color: #10099F;">Impact</h4>
                                    <p style="font-size: 0.9em;">Won ImageNet 2015, influenced all future architectures</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Core Insight:</strong> Make it easy for networks to learn <span class="tooltip">identity mappings<span class="tooltiptext">A function that returns its input unchanged: f(x) = x</span></span></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Problem: Why Can't We Just Stack More Layers?</h2>
                    <div class="problem-analysis">
                        <div class="fragment">
                            <h4>Degradation Problem</h4>
                            <p>Adding more layers can actually <em>hurt</em> performance!</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #d32f2f;">Counterintuitive Observation</h5>
                                <ul style="font-size: 0.85em; line-height: 1.8;">
                                    <li>56-layer network has <strong>higher</strong> training error than 20-layer</li>
                                    <li>Not overfitting (training error is worse!)</li>
                                    <li>Optimization problem: harder to optimize deeper networks</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Question:</strong> If shallow networks work well, why can't deep networks at least learn to copy shallow ones?</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Function Classes: A Theoretical Perspective</h2>
                    <div class="function-classes" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Nested Function Classes</h4>
                            <p>Consider function classes of increasing complexity:</p>
                            <p style="text-align: center; font-size: 1.2em;">$$\mathcal{F}_1 \subseteq \mathcal{F}_2 \subseteq \ldots \subseteq \mathcal{F}_n$$</p>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">Optimization Goal</h5>
                                <p style="text-align: center; font-size: 1.1em;">$$f^*_{\mathcal{F}} = \arg\min_f L(\mathbf{X}, \mathbf{y}, f) \text{ subject to } f \in \mathcal{F}$$</p>
                                <p style="font-size: 0.85em; margin-top: 10px;">We want larger classes to contain all functions from smaller classes</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Problem:</strong> In practice, deeper networks don't always contain shallower ones!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Solution: Residual Blocks</h2>
                    <div class="residual-solution" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Core Idea</h4>
                            <p>Instead of learning $f(\mathbf{x})$, learn the <strong>residual</strong> $f(\mathbf{x}) - \mathbf{x}$</p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Residual Block Formula</h5>
                                <p style="text-align: center; font-size: 1.3em;">$$f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x})$$</p>
                                <p style="font-size: 0.9em; margin-top: 10px;">where $g(\mathbf{x})$ is the residual function to be learned</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Why it works:</strong> If identity is optimal, easier to learn $g(\mathbf{x}) = 0$ than $f(\mathbf{x}) = \mathbf{x}$</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Residual Block Implementation</h2>
                    <div class="implementation-details">
                        <div class="fragment">
                            <h4>PyTorch Implementation</h4>
                            <pre><code class="language-python">class Residual(nn.Module):
    def __init__(self, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3,
                                   padding=1, stride=strides)
        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3,
                                   padding=1)
        if use_1x1conv:
            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,
                                       stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)  # Adjust channels/size if needed
        Y += X  # Skip connection
        return F.relu(Y)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px; font-size: 0.65em;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #2e7d32; font-size: 1em;">When <code>use_1x1conv=False</code></h5>
                                    <p style="font-size: 0.8em;">Direct skip connection<br>Requires same dimensions</p>
                                </div>
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                    <h5 style="color: #10099F; font-size: 1em;">When <code>use_1x1conv=True</code></h5>
                                    <p style="font-size: 0.8em;">1√ó1 conv adjusts channels<br>Enables dimension changes</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Two Types of Residual Blocks</h2>
                    <div class="block-types">
                        <div class="fragment">
                            <img src="images/resnet-block.svg" style="width: 50%; margin: auto; display: block;" alt="ResNet Block Types">
                        </div>
                        <div class="fragment" style="margin-top: 30px; font-size: 0.65em;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #7b1fa2;">Regular Block</h5>
                                    <ul style="font-size: 0.8em; line-height: 1.6;">
                                        <li>Two 3√ó3 convolutions</li>
                                        <li>Maintains dimensions</li>
                                        <li>Used in ResNet-18/34</li>
                                    </ul>
                                </div>
                                <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #e65100;"><span class="tooltip">Bottleneck Block<span class="tooltiptext">Uses 1√ó1 convolutions to reduce and then restore dimensions, reducing computational cost</span></span></h5>
                                    <ul style="font-size: 0.8em; line-height: 1.6;">
                                        <li>1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1 convolutions</li>
                                        <li>Reduces parameters</li>
                                        <li>Used in ResNet-50/101/152</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.65em;">
                            <p><strong>Bottleneck Design:</strong> Reduces computation while maintaining representational power</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Building ResNet Models</h2>
                    <div class="model-building">
                        <div class="fragment">
                            <h4>Modular Construction</h4>
                            <pre><code class="language-python">def resnet_block(num_channels, num_residuals, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(num_channels, use_1x1conv=True,
                               strides=2))
        else:
            blk.append(Residual(num_channels))
    return nn.Sequential(*blk)

# ResNet-18 architecture
class ResNet18(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.b1 = nn.Sequential(
            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.b2 = resnet_block(64, 2, first_block=True)
        self.b3 = resnet_block(128, 2)
        self.b4 = resnet_block(256, 2)
        self.b5 = resnet_block(512, 2)
        self.net = nn.Sequential(
            self.b1, self.b2, self.b3, self.b4, self.b5,
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.LazyLinear(num_classes)
        )</code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-sm" style="font-size: 0.65em;">
                            <p><strong>Design Pattern:</strong> Progressive downsampling with doubling channels</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: ResNet Basics</h2>
                    <div data-mcq='{
                        "question": "What is the main innovation that allows ResNet to train very deep networks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using batch normalization after every layer",
                                "correct": false,
                                "explanation": "While ResNet uses batch normalization, it is not the main innovation. The key is skip connections."
                            },
                            {
                                "text": "Skip connections that create shortcuts for gradient flow",
                                "correct": true,
                                "explanation": "Correct! Skip connections allow gradients to flow directly through shortcuts, preventing vanishing gradients."
                            },
                            {
                                "text": "Using 1√ó1 convolutions to reduce parameters",
                                "correct": false,
                                "explanation": "1√ó1 convolutions are used in bottleneck blocks, but skip connections are the main innovation."
                            },
                            {
                                "text": "Replacing all activations with ReLU",
                                "correct": false,
                                "explanation": "ReLU was already common before ResNet. The innovation is the residual connection structure."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">ResNeXt: Aggregated Residual Transformations</h2>
                    <div class="resnext-intro">
                        <div class="fragment">
                            <h4>The Next Evolution</h4>
                            <p>Introducing a new dimension: <strong>cardinality</strong></p>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Key Innovation: <span class="tooltip">Grouped Convolutions<span class="tooltiptext">Dividing channels into groups and performing convolutions separately on each group</span></span></h5>
                                <p style="font-size: 0.9em;">Split transformation into multiple parallel paths</p>
                                <p style="text-align: center; margin-top: 15px; font-size: 1.1em;">$$\mathcal{O}(c_i \cdot c_o) \rightarrow \mathcal{O}(c_i \cdot c_o / g)$$</p>
                                <p style="font-size: 0.8em; text-align: center; color: #666;">Computational complexity reduced by factor of $g$ (groups)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">ResNeXt Block Architecture</h2>
                    <div class="resnext-block-diagram">
                        <img src="images/resnext-block.svg" alt="ResNeXt Block Architecture" style="max-width: 90%; height: auto;">
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">ResNeXt Block Implementation</h2>
                    <div class="resnext-implementation">
                        <div class="fragment">
                            <h4>PyTorch Implementation with Grouped Convolutions</h4>
                            <pre><code class="language-python">class ResNeXtBlock(nn.Module):
    """ResNeXt block with grouped convolutions"""
    def __init__(self, num_channels, groups, bot_mul,
                 use_1x1conv=False, strides=1):
        super().__init__()
        bot_channels = int(round(num_channels * bot_mul))

        # 1√ó1 conv to reduce dimensions
        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1,
                                   stride=1)
        # 3√ó3 grouped convolution
        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,
                                   stride=strides, padding=1,
                                   groups=bot_channels//groups)
        # 1√ó1 conv to restore dimensions
        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,
                                   stride=1)

        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()
        self.bn3 = nn.LazyBatchNorm2d()

        if use_1x1conv:
            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,
                                       stride=strides)
            self.bn4 = nn.LazyBatchNorm2d()
        else:
            self.conv4 = None

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = F.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))

        if self.conv4:
            X = self.bn4(self.conv4(X))

        return F.relu(Y + X)</code></pre>
                        </div>
                        <div class="fragment" style="margin-top: 20px; font-size: 0.65em;">
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <ul style="font-size: 0.85em; line-height: 1.8;">
                                    <li><strong>Cardinality:</strong> Number of parallel paths (groups)</li>
                                    <li><strong>Homogeneous design:</strong> All paths have same topology</li>
                                    <li><strong>Increasing cardinality</strong> more effective than deeper/wider</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">ResNeXt vs ResNet: Architectural Comparison</h2>
                    <div class="comparison">
                        <div class="fragment">
                            <table style="margin: auto; font-size: 0.85em;">
                                <thead>
                                    <tr>
                                        <th>Aspect</th>
                                        <th>ResNet</th>
                                        <th>ResNeXt</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Basic Unit</td>
                                        <td>Residual block</td>
                                        <td>Grouped residual block</td>
                                    </tr>
                                    <tr>
                                        <td>Dimensions</td>
                                        <td>Depth, Width</td>
                                        <td>Depth, Width, <strong>Cardinality</strong></td>
                                    </tr>
                                    <tr>
                                        <td>3√ó3 Conv</td>
                                        <td>Standard convolution</td>
                                        <td>Grouped convolution</td>
                                    </tr>
                                    <tr>
                                        <td>Parameters</td>
                                        <td>Higher</td>
                                        <td>Lower (with same accuracy)</td>
                                    </tr>
                                    <tr>
                                        <td>Typical Config</td>
                                        <td>ResNet-50</td>
                                        <td>ResNeXt-50 (32√ó4d)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">Performance Impact</h5>
                                <p style="font-size: 0.85em;">ResNeXt-101 (32√ó4d) achieves similar accuracy to ResNet-200 with only half the parameters!</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.65em;">
                            <p><strong>Key Insight:</strong> Cardinality (number of groups) is as important as depth and width</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Residual Networks Work</h2>
                    <div class="why-it-works">
                        <div class="fragment">
                            <h4>Multiple Perspectives</h4>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #2e7d32;">Optimization View</h5>
                                    <ul style="font-size: 0.8em; line-height: 1.6;">
                                        <li>Easier to optimize</li>
                                        <li>Smoother loss landscape</li>
                                        <li>Better gradient flow</li>
                                    </ul>
                                </div>
                                <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #10099F;">Representation View</h5>
                                    <ul style="font-size: 0.8em; line-height: 1.6;">
                                        <li>Ensemble of shallow networks</li>
                                        <li>Adaptive depth</li>
                                        <li>Feature reuse</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px; font-size: 0.65em;">
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #e65100;">Gradient Flow Analysis</h5>
                                <p style="font-size: 0.9em;">With skip connections, gradient can flow through multiple paths:</p>
                                <p style="text-align: center; margin-top: 10px;">$$\frac{\partial L}{\partial x_l} = \frac{\partial L}{\partial x_L} + \sum_{i=l}^{L-1} \frac{\partial L}{\partial x_L} \prod_{j=i}^{L-1} \frac{\partial f_j}{\partial x_j}$$</p>
                                <p style="font-size: 0.85em; margin-top: 10px;">Direct path prevents vanishing gradients!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: ResNeXt</h2>
                    <div data-mcq='{
                        "question": "What are the main advantages of ResNeXt over ResNet?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Introduces cardinality as a new dimension for network design",
                                "correct": true,
                                "explanation": "Correct! Cardinality (number of parallel paths) is a key innovation in ResNeXt."
                            },
                            {
                                "text": "Uses grouped convolutions to reduce computational complexity",
                                "correct": true,
                                "explanation": "Correct! Grouped convolutions reduce complexity from O(c_i √ó c_o) to O(c_i √ó c_o / g)."
                            },
                            {
                                "text": "Eliminates the need for skip connections",
                                "correct": false,
                                "explanation": "Incorrect. ResNeXt still uses skip connections like ResNet."
                            },
                            {
                                "text": "Achieves better accuracy with fewer parameters",
                                "correct": true,
                                "explanation": "Correct! ResNeXt can match ResNet performance with significantly fewer parameters."
                            },
                            {
                                "text": "Requires special hardware for grouped convolutions",
                                "correct": false,
                                "explanation": "Incorrect. Grouped convolutions are supported by standard deep learning frameworks."
                            }
                        ]
                    }'></div>
                </section>

                <section data-sources='[{"text": "Deep Residual Learning for Image Recognition", "url": "https://arxiv.org/abs/1512.03385"}, {"text": "Aggregated Residual Transformations for Deep Neural Networks", "url": "https://arxiv.org/abs/1611.05431"}]'>
                    <h2 class="truncate-title">Impact and Legacy</h2>
                    <div class="impact-summary" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>ResNet's Influence on Deep Learning</h4>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="color: #2e7d32; font-size: 1em;">Computer Vision</h5>
                                    <p style="font-size: 0.8em;">Backbone for detection, segmentation</p>
                                </div>
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="color: #10099F; font-size: 1em;">NLP</h5>
                                    <p style="font-size: 0.8em;">Inspired Transformer residuals</p>
                                </div>
                                <div style="background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5 style="color: #e65100; font-size: 1em;">RL</h5>
                                    <p style="font-size: 0.8em;">Deep RL architectures</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <table style="margin: auto; font-size: 0.85em;">
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>Year</th>
                                        <th>Top-1 Error</th>
                                        <th>Parameters</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>AlexNet</td>
                                        <td>2012</td>
                                        <td>36.7%</td>
                                        <td>62M</td>
                                    </tr>
                                    <tr>
                                        <td>VGG-19</td>
                                        <td>2014</td>
                                        <td>25.5%</td>
                                        <td>143M</td>
                                    </tr>
                                    <tr>
                                        <td>ResNet-152</td>
                                        <td>2015</td>
                                        <td>21.4%</td>
                                        <td>60M</td>
                                    </tr>
                                    <tr>
                                        <td>ResNeXt-101</td>
                                        <td>2017</td>
                                        <td>20.4%</td>
                                        <td>84M</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Legacy:</strong> Skip connections are now fundamental to all modern deep architectures</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: Key Takeaways</h2>
                    <div class="summary-points" style="font-size: 0.6em;">
                        <div class="fragment">
                            <h4>What We Learned</h4>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #2e7d32;">Core Concepts</h5>
                                    <ul style="font-size: 0.85em; line-height: 1.8;">
                                        <li>Function classes and expressiveness</li>
                                        <li>Residual learning: $f(x) = x + g(x)$</li>
                                        <li>Skip connections for gradient flow</li>
                                        <li>Identity mappings as default</li>
                                    </ul>
                                </div>
                                <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #10099F;">Architectural Innovations</h5>
                                    <ul style="font-size: 0.85em; line-height: 1.8;">
                                        <li>Bottleneck blocks for efficiency</li>
                                        <li>Progressive downsampling</li>
                                        <li>Grouped convolutions (ResNeXt)</li>
                                        <li>Cardinality as design dimension</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #7b1fa2;">Practical Impact</h5>
                                <p style="font-size: 0.9em;">‚Ä¢ Enabled training of 100+ layer networks<br>
                                ‚Ä¢ Became backbone for countless applications<br>
                                ‚Ä¢ Influenced all future architecture designs<br>
                                ‚Ä¢ Still widely used in production systems</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Remember:</strong> "The depth of representations is of central importance for many visual recognition tasks." - He et al., 2015</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Final Test: Comprehensive Understanding</h2>
                    <div data-mcq='{
                        "question": "A colleague suggests removing skip connections from ResNet to simplify the architecture. What would be the most likely consequence?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The model would train faster due to simpler architecture",
                                "correct": false,
                                "explanation": "Actually, training would likely fail or be much slower due to vanishing gradients."
                            },
                            {
                                "text": "Training deep networks would become difficult or impossible due to vanishing gradients",
                                "correct": true,
                                "explanation": "Correct! Skip connections are essential for gradient flow in very deep networks. Without them, gradients vanish and training fails."
                            },
                            {
                                "text": "The model would use less memory and be more efficient",
                                "correct": false,
                                "explanation": "While skip connections do use some memory, the main issue would be training failure, not efficiency."
                            },
                            {
                                "text": "Accuracy would improve because the model would be forced to learn better features",
                                "correct": false,
                                "explanation": "Without skip connections, deep networks often perform worse than shallow ones due to optimization difficulties."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 8: DenseNet (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - DenseNet", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}, {"text": "Densely Connected Convolutional Networks", "url": "https://arxiv.org/abs/1608.06993"}]'>
                    <h2 class="truncate-title">DenseNet: Densely Connected Networks</h2>
                    <div class="densenet-intro" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Beyond Skip Connections</h4>
                            <p style="font-size: 0.85em;">What if we connect <em>every</em> layer to <em>every</em> other layer?</p>
                        </div>

                        <div class="comparison-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F;">ResNet Approach</h5>
                                <p style="font-size: 0.8em;"><strong>Addition:</strong></p>
                                <p style="font-family: monospace; font-size: 0.75em;">$$\mathbf{x} \rightarrow \mathbf{x} + f(\mathbf{x})$$</p>
                                <ul style="font-size: 0.75em; margin-top: 10px;">
                                    <li>Preserves dimensions</li>
                                    <li>Identity mapping</li>
                                    <li>Gradient highway</li>
                                </ul>
                            </div>
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2DD2C0;">DenseNet Innovation</h5>
                                <p style="font-size: 0.8em;"><strong>Concatenation:</strong></p>
                                <p style="font-family: monospace; font-size: 0.75em;">$$\mathbf{x} \rightarrow [\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})]), ...]$$</p>
                                <ul style="font-size: 0.75em; margin-top: 10px;">
                                    <li>Feature reuse</li>
                                    <li>Stronger gradients</li>
                                    <li>More diverse features</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Insight:</strong> <span class="tooltip">Dense connections<span class="tooltiptext">Each layer receives direct inputs from all preceding layers and passes its own feature maps to all subsequent layers</span></span> maximize information flow through the network</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "DenseNet Architecture Visualization", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}]'>
                    <h2 class="truncate-title">Connection Patterns: ResNet vs DenseNet</h2>
                    <div class="architecture-comparison" style="font-size: 0.65em;">
                        <img src="images/densenet-block.svg" alt="ResNet vs DenseNet connections" style="width: 40%; margin: 20px auto; display: block;">

                        <div class="fragment" style="margin-top: 30px;">
                            <h4>Mathematical Formulation</h4>
                            <div style="background: #f9f9f9; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.85em;"><strong>Traditional:</strong> $$\mathbf{x}_{\ell} = H_{\ell}(\mathbf{x}_{\ell-1})$$</p>
                                <p style="font-size: 0.85em;"><strong>ResNet:</strong> $$\mathbf{x}_{\ell} = H_{\ell}(\mathbf{x}_{\ell-1}) + \mathbf{x}_{\ell-1}$$</p>
                                <p style="font-size: 0.85em;"><strong>DenseNet:</strong> $$\mathbf{x}_{\ell} = H_{\ell}([\mathbf{x}_0, \mathbf{x}_1, ..., \mathbf{x}_{\ell-1}])$$</p>
                            </div>
                        </div>

                    </div>
                </section>

                <section data-sources='[{"text": "DenseNet Implementation", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}]'>
                    <h2 class="truncate-title">Dense Blocks and Growth Rate</h2>
                    <div class="dense-block-explanation">
                        <div class="fragment">
                            <h4>Dense Block Structure</h4>
                            <pre><code class="python">def conv_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))

class DenseBlock(nn.Module):
    def __init__(self, num_convs, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # Concatenate input and output of each block
            X = torch.cat((X, Y), dim=1)
        return X</code></pre>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Transition Layers in DenseNet", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}]'>
                    <h2 class="truncate-title">Transition Layers: Controlling Complexity</h2>
                    <div class="transition-layers">
                        <div class="fragment">
                            <h4>Why Transition Layers?</h4>
                            <p style="font-size: 0.85em;">Dense connections increase channels rapidly - we need to control this growth!</p>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <pre><code class="python">def transition_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(),
        nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=1),  # 1√ó1 conv reduces channels
        nn.AvgPool2d(kernel_size=2, stride=2))        # Halve spatial dimensions</code></pre>
                        </div>

                        <div class="fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #ff6f00; font-size: 0.9em;">Channel Reduction</h5>
                                <p style="font-size: 0.75em;">‚Ä¢ <span class="tooltip">1√ó1 convolution<span class="tooltiptext">Pointwise convolution that combines features across channels without spatial mixing</span></span><br>
                                ‚Ä¢ Typically reduce by 50%<br>
                                ‚Ä¢ Compress feature maps</p>
                            </div>
                            <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #7b1fa2; font-size: 0.9em;">Spatial Reduction</h5>
                                <p style="font-size: 0.75em;">‚Ä¢ Average pooling 2√ó2<br>
                                ‚Ä¢ Stride 2<br>
                                ‚Ä¢ Halve H and W</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Complete DenseNet Architecture", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}]'>
                    <h2 class="truncate-title">Complete DenseNet Architecture</h2>
                    <div class="complete-architecture">
                        <div class="fragment" style="max-height: 400px; overflow-y: auto;">
                            <pre><code class="python">class DenseNet(d2l.Classifier):
    def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),
                 lr=0.1, num_classes=10):
        super(DenseNet, self).__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(self.b1())

        for i, num_convs in enumerate(arch):
            # Add dense block
            self.net.add_module(f'dense_blk{i+1}',
                DenseBlock(num_convs, growth_rate))

            # Update number of channels
            num_channels += num_convs * growth_rate

            # Add transition layer (except after last block)
            if i != len(arch) - 1:
                num_channels //= 2
                self.net.add_module(f'tran_blk{i+1}',
                    transition_block(num_channels))

        # Final layers
        self.net.add_module('last', nn.Sequential(
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))

        self.net.apply(d2l.init_cnn)

    def b1(self):  # Initial convolution block
        return nn.Sequential(
            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))</code></pre>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <img src="images/densenet.svg" alt="DenseNet architecture visualization" style="width: 60%; margin: 0 auto; display: block;">
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "DenseNet Training and Performance", "url": "https://d2l.ai/chapter_convolutional-modern/densenet.html"}]'>
                    <h2 class="truncate-title">Training DenseNet on Fashion-MNIST</h2>
                    <div class="training-details" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Training Configuration</h4>
                            <pre><code class="python"># Model instantiation
model = DenseNet(lr=0.01)

# Training setup
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=256, resize=(96, 96))

# Train the model
trainer.fit(model, data)</code></pre>
                        </div>

                        <div class="fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">Advantages</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>Strong gradient flow</li>
                                    <li>Feature reuse</li>
                                    <li>Parameter efficiency</li>
                                    <li>Implicit deep supervision</li>
                                </ul>
                            </div>
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #c62828;">Trade-offs</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>Higher memory usage</li>
                                    <li>More computation</li>
                                    <li>Complex implementation</li>
                                    <li>Slower inference</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Key Achievement:</strong> DenseNet-121 matched ResNet-1001 accuracy with 10√ó fewer parameters!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "DenseNet Paper - Huang et al., 2017", "url": "https://arxiv.org/abs/1608.06993"}]'>
                    <h2 class="truncate-title">DenseNet: Key Innovations and Impact</h2>
                    <div class="densenet-summary" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Architectural Innovations</h4>
                            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 0px; margin-top: 20px;">
                                <div class="innovation-card" style="background: #e3f2fd; padding: 5px; border-radius: 8px;">
                                    <h5 style="color: #1565c0; font-size: 0.9em;">Feature Reuse</h5>
                                    <p style="font-size: 0.75em;">Every layer can access all preceding features directly</p>
                                </div>
                                <div class="innovation-card" style="background: #f3e5f5; padding: 5px; border-radius: 8px;">
                                    <h5 style="color: #7b1fa2; font-size: 0.9em;">Gradient Highway</h5>
                                    <p style="font-size: 0.75em;">Direct paths to all layers improve gradient flow</p>
                                </div>
                                <div class="innovation-card" style="background: #e8f5e9; padding: 5px; border-radius: 8px;">
                                    <h5 style="color: #2e7d32; font-size: 0.9em;">Parameter Efficiency</h5>
                                    <p style="font-size: 0.75em;">Narrow layers (small growth rate) suffice</p>
                                </div>
                                <div class="innovation-card" style="background: #fff3e0; padding: 5px; border-radius: 8px;">
                                    <h5 style="color: #ff6f00; font-size: 0.9em;">Implicit Supervision</h5>
                                    <p style="font-size: 0.75em;">Loss gradient reaches all layers directly</p>
                                </div>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px; font-size: 0.65em;">
                            <h4>Comparison with ResNet</h4>
                            <table style="width: 100%; font-size: 0.8em;">
                                <thead>
                                    <tr style="background: #10099F; color: white;">
                                        <th style="padding: 10px;">Aspect</th>
                                        <th style="padding: 10px;">ResNet</th>
                                        <th style="padding: 10px;">DenseNet</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Connection Type</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Addition</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Concatenation</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Feature Access</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Previous block</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">All previous layers</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Parameter Count</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Higher</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Lower</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Memory Usage</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Lower</td>
                                        <td style="padding: 8px; border: 1px solid #ddd;">Higher</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="fragment emphasis-box mt-lg" style="font-size: 0.65em;">
                            <p>"DenseNets exploit the potential of the network through <span class="tooltip">feature reuse<span class="tooltiptext">Using features computed in earlier layers directly in later layers without recomputation</span></span>, yielding condensed models that are easy to train and highly parameter-efficient." - Huang et al.</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: DenseNet</h2>
                    <div data-mcq='{
                        "question": "What is the fundamental difference between how ResNet and DenseNet combine features from different layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "ResNet uses multiplication while DenseNet uses addition",
                                "correct": false,
                                "explanation": "Incorrect. ResNet uses addition (x + F(x)), not multiplication."
                            },
                            {
                                "text": "ResNet uses addition while DenseNet uses concatenation",
                                "correct": true,
                                "explanation": "Correct! ResNet adds features (x + F(x)) which preserves dimensions, while DenseNet concatenates features [x, F(x)] which increases dimensions."
                            },
                            {
                                "text": "Both use the same method but DenseNet has more connections",
                                "correct": false,
                                "explanation": "They use fundamentally different operations. ResNet adds while DenseNet concatenates."
                            },
                            {
                                "text": "ResNet uses concatenation while DenseNet uses addition",
                                "correct": false,
                                "explanation": "This is backwards. ResNet adds, DenseNet concatenates."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Advanced Understanding: Growth Rate Impact</h2>
                    <div data-mcq='{
                        "question": "Your team is implementing DenseNet but running into GPU memory issues. Which modification would help most while maintaining model effectiveness?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Increase the growth rate to reduce the number of layers needed",
                                "correct": false,
                                "explanation": "Increasing growth rate would actually increase memory usage as each layer would produce more feature maps."
                            },
                            {
                                "text": "Remove all transition layers to simplify the architecture",
                                "correct": false,
                                "explanation": "Transition layers are crucial for controlling memory growth. Removing them would make the problem worse."
                            },
                            {
                                "text": "Reduce the growth rate and potentially add more layers",
                                "correct": true,
                                "explanation": "Correct! A smaller growth rate reduces memory consumption per layer. DenseNets work well with small growth rates (e.g., 12-32) due to feature reuse."
                            },
                            {
                                "text": "Replace concatenation with addition like in ResNet",
                                "correct": false,
                                "explanation": "This would fundamentally change the architecture to ResNet, losing the unique benefits of feature reuse in DenseNet."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Introduction to CNN Design Spaces (Vertical) -->
            <section>
                <section data-sources='[{"text": "Designing Network Design Spaces - Radosavovic et al., 2020", "url": "https://arxiv.org/abs/2003.13678"}, {"text": "Dive into Deep Learning - Chapter 8.8", "url": "https://d2l.ai/chapter_convolutional-modern/cnn-design.html"}]'>
                    <h2 class="truncate-title">From Manual Design to Design Spaces</h2>
                    <div class="design-space-intro" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>The Evolution of Network Design</h4>
                            <p style="font-size: 0.8em;">Moving beyond finding the <em>single best network</em> to optimizing <em>distributions of networks</em></p>
                        </div>

                        <div class="fragment" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-top: 30px;">
                            <div class="evolution-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F; font-size: 0.9em;">Manual Engineering</h5>
                                <p style="font-size: 0.7em;">AlexNet, VGG, ResNet</p>
                                <p style="font-size: 0.65em;">Human intuition and creativity</p>
                            </div>
                            <div class="evolution-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2DD2C0; font-size: 0.9em;"><span class="tooltip">NAS<span class="tooltiptext">Neural Architecture Search: Automated methods to find optimal network architectures using reinforcement learning, evolutionary algorithms, or gradient-based optimization</span></span></h5>
                                <p style="font-size: 0.7em;">EfficientNet, NASNet</p>
                                <p style="font-size: 0.65em;">Enormous computational cost</p>
                            </div>
                            <div class="evolution-card" style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F; font-size: 0.9em;"><span class="tooltip">Design Spaces<span class="tooltiptext">A parameterized family of network architectures that share common design principles, allowing systematic exploration of architectural choices</span></span></h5>
                                <p style="font-size: 0.7em;"><span class="tooltip">RegNet<span class="tooltiptext">Regular Networks: A family of networks designed through systematic exploration of design spaces, resulting in simple, regular architectures</span></span>, AnyNet</p>
                                <p style="font-size: 0.65em;">Systematic exploration with insights</p>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Key Innovation: Design Space Strategy</h5>
                            <ul style="font-size: 0.8em;">
                                <li>Operate on <strong>distributions</strong> of networks, not single instances</li>
                                <li>Find design principles that work across many networks</li>
                                <li>Balance manual insight with systematic exploration</li>
                                <li>Computationally efficient compared to NAS</li>
                            </ul>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p>Instead of asking "What's the best network?", we ask "What design principles lead to good networks?"</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - AnyNet Design", "url": "https://d2l.ai/chapter_convolutional-modern/cnn-design.html#the-anynet-design-space"}]'>
                    <h2 class="truncate-title">The AnyNet Architecture Template</h2>
                    <div class="anynet-structure">
                        <div class="fragment">
                            <h4>Universal Structure: Stem ‚Üí Body ‚Üí Head</h4>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Component Breakdown</h5>
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px;">
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #1565c0;">Stem</h6>
                                    <p style="font-size: 0.7em;">Initial processing<br>3√ó3 conv, stride 2<br>Halves resolution</p>
                                </div>
                                <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #7b1fa2;">Body (4 stages)</h6>
                                    <p style="font-size: 0.7em;">Main transformations<br>ResNeXt blocks<br>Progressive downsampling</p>
                                </div>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #2e7d32;">Head</h6>
                                    <p style="font-size: 0.7em;">Global avg pooling<br>Fully connected<br>Classification output</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">AnyNet Architecture Visualization</h2>
                    <div class="anynet-visualization">
                        <div class="fragment">
                            <img src="./images/anynet.svg" alt="AnyNet Architecture Diagram" style="width: 100%; max-width: 900px; height: auto; display: block; margin: 0 auto;">
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">AnyNet Design Space Parameters</h2>
                    <div class="design-parameters" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4>17 Parameters Define the Entire Space</h4>
                            <p style="font-size: 0.8em;">Each stage $i$ has four key parameters:</p>
                        </div>

                        <div class="fragment" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 20px;">
                            <div class="param-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #10099F; font-size: 0.9em;">$d_i$ - Depth</h5>
                                <p style="font-size: 0.75em;">Number of blocks in stage $i$</p>
                                <p style="font-size: 0.7em; color: #666;">Range: typically 2-16 blocks</p>
                            </div>
                            <div class="param-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2DD2C0; font-size: 0.9em;">$c_i$ - Width</h5>
                                <p style="font-size: 0.75em;">Number of output channels</p>
                                <p style="font-size: 0.7em; color: #666;">Range: 32-1024 channels</p>
                            </div>
                            <div class="param-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #FFA05F; font-size: 0.9em;">$g_i$ - Groups</h5>
                                <p style="font-size: 0.75em;">Number of groups for grouped convolution</p>
                                <p style="font-size: 0.7em; color: #666;">Range: 1-32 groups</p>
                            </div>
                            <div class="param-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #FC8484; font-size: 0.9em;">$k_i$ - Bottleneck</h5>
                                <p style="font-size: 0.75em;"><span class="tooltip">Bottleneck ratio<span class="tooltiptext">Ratio between input/output channels and the bottleneck channels within a block. k=1 means no bottleneck</span></span></p>
                                <p style="font-size: 0.7em; color: #666;">Range: 0.25-4 (typically 1)</p>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px; display: grid; grid-template-columns: 1fr 1fr; gap: 30px; align-items: start;">
                            <div>
                                <h5>Total Configuration Space</h5>
                                <p style="font-size: 0.8em;">With 4 stages and stem channel count $c_0$:</p>
                                <p style="font-size: 0.9em; font-family: 'Courier New', monospace; background: #f5f5f5; padding: 15px; border-radius: 5px;">
                                    17 parameters = $c_0$ + 4√ó($d_i$ + $c_i$ + $g_i$ + $k_i$)
                                </p>
                                <p style="font-size: 0.8em; margin-top: 15px;">Even with just 2 choices per parameter: $2^{17} = 131,072$ possible networks!</p>
                            </div>

                            <div class="emphasis-box" style="height: fit-content;">
                                <p>The challenge: How to efficiently explore this massive space and find general design principles?</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Design Spaces</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of the design space approach over traditional NAS?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It always finds the single best network architecture",
                                "correct": false,
                                "explanation": "Design spaces do not aim for a single best network, but rather find principles that work across many networks."
                            },
                            {
                                "text": "It requires more computational resources than NAS",
                                "correct": false,
                                "explanation": "Design space exploration is actually more computationally efficient than traditional NAS methods."
                            },
                            {
                                "text": "It discovers generalizable design principles while being computationally efficient",
                                "correct": true,
                                "explanation": "Correct! Design spaces find principles that work across network families, providing insights and efficiency."
                            },
                            {
                                "text": "It eliminates the need for any human design choices",
                                "correct": false,
                                "explanation": "Design spaces combine human insight with systematic exploration, not eliminate human input entirely."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: AnyNet Implementation (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - AnyNet Implementation", "url": "https://d2l.ai/chapter_convolutional-modern/cnn-design.html#the-anynet-design-space"}]'>
                    <h2 class="truncate-title">Building AnyNet: The Stem</h2>
                    <div class="anynet-stem-implementation" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Stem: Initial Image Processing</h4>
                            <p style="font-size: 0.8em;">Transforms RGB images from $(3, r, r)$ to $(c_0, r/2, r/2)$</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>PyTorch Implementation</h5>
                            <pre><code class="language-python">class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        return nn.Sequential(
            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),
            nn.LazyBatchNorm2d(),
            nn.ReLU()
        )</code></pre>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Mathematical Transformation</h5>
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.9em;">$$\text{Input: } \mathbb{R}^{3 \times r \times r} \xrightarrow{\text{Conv3√ó3, stride 2}} \mathbb{R}^{c_0 \times \frac{r}{2} \times \frac{r}{2}}$$</p>
                                <p style="font-size: 0.8em; margin-top: 10px;">For ImageNet (224√ó224): Output is 112√ó112√ó$c_0$</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "ResNeXt Block - Xie et al., 2017", "url": "https://arxiv.org/abs/1611.05431"}]'>
                    <h2 class="truncate-title">Building AnyNet: Stage Implementation</h2>
                    <div class="anynet-stage-implementation">
                        <div class="fragment">
                            <h4>Each Stage Uses ResNeXt Blocks</h4>
                            <p style="font-size: 0.8em;">First block reduces resolution, others maintain it</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Stage Implementation</h5>
                            <pre><code class="language-python">@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    blk = []
    for i in range(depth):
        if i == 0:
            # First block: halves resolution
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                use_1x1conv=True, strides=2))
        else:
            # Other blocks: maintain resolution
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))
    return nn.Sequential(*blk)</code></pre>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #1565c0;">Spatial Resolution</h6>
                                    <p style="font-size: 0.7em;">First block: $r/2$ ‚Üí $r/4$<br>Other blocks: maintain $r/4$</p>
                                </div>
                                <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #7b1fa2;">Channel Progression</h6>
                                    <p style="font-size: 0.7em;">All blocks output $c_i$ channels<br>Typically: $c_{i+1} > c_i$</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Complete AnyNet Architecture</h2>
                    <div class="complete-anynet">
                        <div class="fragment">
                            <h4>Putting It All Together</h4>
                            <pre><code class="language-python">@d2l.add_to_class(AnyNet)
def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):
    super(AnyNet, self).__init__()
    self.save_hyperparameters()
    self.net = nn.Sequential(self.stem(stem_channels))

    # Add stages based on architecture specification
    for i, s in enumerate(arch):
        self.net.add_module(f'stage{i+1}', self.stage(*s))

    # Add head for classification
    self.net.add_module('head', nn.Sequential(
        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten(),
        nn.LazyLinear(num_classes)
    ))
    self.net.apply(d2l.init_cnn)</code></pre>
                        </div>
                    </div>

                </section>
                <section>
                    <div class="fragment" style="margin-top: 20px;">
                        <h5>Architecture Specification Format</h5>
                        <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                            <pre style="font-size: 0.8em;"><code class="language-python"># Each tuple specifies (depth, channels, groups, bottleneck_ratio)
arch = (
(4, 32, 16, 1),  # Stage 1: 4 blocks, 32 channels, 16 groups, no bottleneck
(6, 64, 16, 1),  # Stage 2: 6 blocks, 64 channels, 16 groups, no bottleneck
(8, 128, 16, 1), # Stage 3: 8 blocks, 128 channels, 16 groups, no bottleneck
(2, 256, 16, 1)  # Stage 4: 2 blocks, 256 channels, 16 groups, no bottleneck
)</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: AnyNet Structure</h2>
                    <div data-mcq='{
                        "question": "In AnyNet, why does only the first block of each stage use stride=2?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To increase the number of parameters in the network",
                                "correct": false,
                                "explanation": "Stride affects spatial resolution, not the number of parameters directly."
                            },
                            {
                                "text": "To progressively reduce spatial resolution while other blocks extract features at the same scale",
                                "correct": true,
                                "explanation": "Correct! The first block downsamples to a new resolution, then subsequent blocks operate at that resolution to extract hierarchical features."
                            },
                            {
                                "text": "To make the network train faster",
                                "correct": false,
                                "explanation": "While downsampling can reduce computation, this is not the primary design reason."
                            },
                            {
                                "text": "It is a requirement of the ResNeXt block architecture",
                                "correct": false,
                                "explanation": "ResNeXt blocks can use any stride value; this is an AnyNet design choice."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Design Space Optimization Strategy (Vertical) -->
            <section>

                <section data-sources='[{"text": "Radosavovic et al. (2019) - On Network Design Spaces for Visual Recognition", "url": "https://arxiv.org/abs/1905.11946"}]'>
                    <h2 class="truncate-title">The Challenge: Exploring Design Spaces Efficiently</h2>
                    <div class="design-space-challenge" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>The Brute Force Problem</h4>
                            <p style="font-size: 0.9em;">Finding the single best parameter choice seems straightforward...</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #d32f2f;">‚ùå Brute Force Approach</h5>
                                <ul style="font-size: 0.8em; line-height: 1.6;">
                                    <li>Even with 2 choices per parameter: $2^{17} = 131,072$ networks</li>
                                    <li>Training each to convergence is computationally prohibitive</li>
                                    <li>Results don't generalize to new operations or stages</li>
                                    <li>Stochasticity makes exact comparisons difficult</li>
                                </ul>
                            </div>
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">‚úÖ Better Strategy</h5>
                                <p style="font-size: 0.85em;">Instead of finding one best network, discover <strong>general design principles</strong> that work across many networks.</p>
                                <p style="font-size: 0.8em; margin-top: 10px; font-style: italic;">"Find the rules, not just the solution"</p>
                            </div>
                        </div>

                        <div class="fragment emphasis-box" style="margin-top: 30px;">
                            <p>The key insight: There are many good "needles in the haystack" - we need to find the patterns that make them good.</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Four Key Assumptions for Efficient Exploration</h2>
                    <div class="design-assumptions" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>The RegNet Strategy (Radosavovic et al., 2019)</h4>
                        </div>

                        <div class="assumptions-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                            <div class="assumption-card" style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #1976d2;">üéØ Assumption 1: Many Good Networks</h5>
                                <p style="font-size: 0.85em;">General design principles exist, so many networks following these rules should perform well.</p>
                                <p style="font-size: 0.8em; font-style: italic; margin-top: 10px;">Strategy: Identify distributions over good networks</p>
                            </div>
                            <div class="assumption-card" style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #7b1fa2;">‚ö° Assumption 2: Early Stopping Works</h5>
                                <p style="font-size: 0.85em;">Don't need full convergence - intermediate results reliably predict final accuracy.</p>
                                <p style="font-size: 0.8em; font-style: italic; margin-top: 10px;">Strategy: Multi-fidelity optimization</p>
                            </div>
                        </div>

                        <div class="assumptions-grid fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                            <div class="assumption-card" style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #388e3c;">üìè Assumption 3: Scale Invariance</h5>
                                <p style="font-size: 0.85em;">Results from smaller networks generalize to larger ones with similar structure.</p>
                                <p style="font-size: 0.8em; font-style: italic; margin-top: 10px;">Strategy: Optimize on small networks, verify at scale</p>
                            </div>
                            <div class="assumption-card" style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #f57c00;">üîß Assumption 4: Factorizable Design</h5>
                                <p style="font-size: 0.85em;">Design aspects can be optimized somewhat independently.</p>
                                <p style="font-size: 0.8em; font-style: italic; margin-top: 10px;">Strategy: Decompose the optimization problem</p>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px; background: #f5f5f5; padding: 20px; border-radius: 8px;">
                            <h5>The Result: RegNet Design Space</h5>
                            <p style="font-size: 0.85em;">These assumptions led to discovering simple rules that govern channel widths, depths, and other parameters across network families - dramatically reducing the search space while maintaining performance.</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Multi-Fidelity Optimization in Practice</h2>
                    <div class="multi-fidelity-viz" style="font-size: 0.7em;">
                        <div class="fragment">
                            <h4>Training Time vs. Accuracy Correlation</h4>
                            <p style="font-size: 0.9em;">Key insight: Early training results predict final performance</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                            <div>
                                <h5>Traditional Approach</h5>
                                <div style="background: #ffebee; padding: 15px; border-radius: 8px;">
                                    <p style="font-size: 0.8em;">‚Ä¢ Train each network for 100+ epochs</p>
                                    <p style="font-size: 0.8em;">‚Ä¢ Wait for full convergence</p>
                                    <p style="font-size: 0.8em;">‚Ä¢ Compare final accuracies</p>
                                    <p style="font-size: 0.8em; margin-top: 10px;"><strong>Cost:</strong> Weeks to months</p>
                                </div>
                            </div>
                            <div>
                                <h5>Multi-Fidelity Approach</h5>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <p style="font-size: 0.8em;">‚Ä¢ Train for just 10-20 epochs</p>
                                    <p style="font-size: 0.8em;">‚Ä¢ Use intermediate accuracy as proxy</p>
                                    <p style="font-size: 0.8em;">‚Ä¢ Rank networks early</p>
                                    <p style="font-size: 0.8em; margin-top: 10px;"><strong>Cost:</strong> Hours to days</p>
                                </div>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #1976d2;">Validation of the Approach</h5>
                                <p style="font-size: 0.85em;">Networks that perform well after 10 epochs consistently perform well after 100 epochs. This correlation allows us to explore thousands of architectures efficiently.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Design Space Strategy</h2>
                    <div data-mcq='{
                        "question": "Why is the multi-fidelity optimization approach effective for neural architecture search?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It always finds the globally optimal architecture",
                                "correct": false,
                                "explanation": "Multi-fidelity optimization does not guarantee global optimality, but rather efficient exploration of the design space."
                            },
                            {
                                "text": "It eliminates the need for any actual training",
                                "correct": false,
                                "explanation": "Multi-fidelity still requires training, just for fewer epochs to get reliable performance estimates."
                            },
                            {
                                "text": "Early training results reliably predict final performance, allowing faster evaluation",
                                "correct": true,
                                "explanation": "Correct! Networks that perform well after few epochs tend to perform well after full training, enabling efficient architecture comparison."
                            },
                            {
                                "text": "It only works for very small networks",
                                "correct": false,
                                "explanation": "Multi-fidelity optimization works across network sizes, and results from smaller networks often generalize to larger ones."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">The Challenge: Evaluating Design Spaces</h2>
                    <div class="design-space-challenge" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>From Individual Networks to Distributions</h4>
                            <p style="font-size: 0.8em;">We need to evaluate entire design spaces, not just individual architectures</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #d32f2f;">‚ùå Traditional Approach</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Design one "best" network</li>
                                    <li>Compare individual architectures</li>
                                    <li>Cherry-pick successful examples</li>
                                    <li>No systematic evaluation</li>
                                </ul>
                            </div>
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">‚úÖ Design Space Approach</h5>
                                <ul style="font-size: 0.8em; line-height: 1.8;">
                                    <li>Sample many networks uniformly</li>
                                    <li>Evaluate performance distributions</li>
                                    <li>Compare entire design spaces</li>
                                    <li>Statistical significance</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #1976d2;">Key Question</h5>
                                <p style="font-size: 0.85em;">How do we compare design spaces systematically? We need a way to evaluate the <em>distribution</em> of network performance, not just individual examples.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cumulative Distribution Functions (CDFs)</h2>
                    <div class="cdf-introduction" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Mathematical Foundation</h4>
                            <p style="font-size: 0.8em;">CDFs let us compare entire distributions of network performance</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Theoretical CDF</h5>
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.9em;">For networks drawn from distribution $p$:</p>
                                <p style="font-size: 1em; margin: 15px 0;">$$F(e, p) \stackrel{\text{def}}{=} P_{\text{net} \sim p} \{e(\text{net}) \leq e\}$$</p>
                                <p style="font-size: 0.8em; margin-top: 10px;"><strong>Interpretation:</strong> Probability that a random network has error ‚â§ $e$</p>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Practical Reality: Empirical CDF</h5>
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.8em;">We can't evaluate all networks, so we sample:</p>
                                <p style="font-size: 0.9em; margin: 10px 0;">Sample: $\mathcal{Z} = \{\text{net}_1, \text{net}_2, \ldots, \text{net}_n\}$</p>
                                <p style="font-size: 1em; margin: 15px 0;">$$\hat{F}(e, \mathcal{Z}) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(e_i \leq e)$$</p>
                                <p style="font-size: 0.8em; margin-top: 10px;"><strong>Interpretation:</strong> Fraction of sampled networks with error ‚â§ $e$</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">CDF Dominance: Comparing Design Spaces</h2>
                    <div class="cdf-dominance" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>The Key Insight</h4>
                            <p style="font-size: 0.8em;">When one CDF dominates another, we have a clear winner</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #2e7d32;">CDF Dominance</h5>
                                <p style="font-size: 0.8em;">Design space A dominates B if:</p>
                                <p style="font-size: 0.9em; margin: 10px 0;">$F_A(e) \geq F_B(e)$ for all $e$</p>
                                <p style="font-size: 0.8em;"><strong>Meaning:</strong> A has more networks below any error threshold</p>
                            </div>
                            <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #1976d2;">Practical Benefit</h5>
                                <ul style="font-size: 0.8em; line-height: 1.6;">
                                    <li>Objective comparison method</li>
                                    <li>No cherry-picking</li>
                                    <li>Statistical significance</li>
                                    <li>Guides design decisions</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <h5 style="color: #f57c00;">Our Goal</h5>
                                <p style="font-size: 0.85em;">Find a distribution $p$ over networks where:</p>
                                <ul style="font-size: 0.8em; margin-top: 10px;">
                                    <li>Most networks have low error rates</li>
                                    <li>The design space is concise (few parameters)</li>
                                    <li>The CDF dominates other design spaces</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>


                <section>
                    <h2 class="truncate-title">Test Your Understanding: Design Space Optimization</h2>
                    <div data-mcq='{
                        "question": "What does it mean when the CDF of design space A dominates design space B?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Networks from A always outperform networks from B",
                                "correct": false,
                                "explanation": "Not always - CDFs show probability distributions. Some B networks might still outperform some A networks."
                            },
                            {
                                "text": "For any error threshold, A has a higher proportion of networks below that threshold",
                                "correct": true,
                                "explanation": "Correct! CDF dominance means A consistently produces a higher fraction of better-performing networks at all error levels."
                            },
                            {
                                "text": "Design space A has fewer total networks than B",
                                "correct": false,
                                "explanation": "CDF dominance is about performance distribution, not the size of the design space."
                            },
                            {
                                "text": "The best network in A is better than the best in B",
                                "correct": false,
                                "explanation": "CDF dominance is about the overall distribution, not just the best performers."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: From AnyNet to RegNet (Vertical) -->
            <section>
                <section data-sources='[{"text": "Designing Network Design Spaces - RegNet", "url": "https://arxiv.org/abs/2003.13678"}]'>
                    <h2 class="truncate-title">Progressive Constraints: From AnyNet to RegNet</h2>
                    <div class="anynet-to-regnet" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Systematic Simplification Through Constraints</h4>
                            <p style="font-size: 0.8em;">Each constraint reduces parameters while maintaining performance</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div class="constraint-progression">
                                <div style="display: flex; align-items: center; justify-content: space-between; margin: 5px 0;">
                                    <div style="flex: 1; background: #f5f5f5; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #10099F;">AnyNetX<sub>A</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">Original: 17 params</p>
                                    </div>
                                    <div style="font-size: 1.5em;">‚Üí</div>
                                    <div style="flex: 1; background: #e3f2fd; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #1565c0;">AnyNetX<sub>B</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">$k_i = k$ (shared bottleneck)</p>
                                        <p style="font-size: 0.65em; color: #666; margin: 0;">14 params (-3)</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: center; justify-content: space-between; margin: 5px 0;">
                                    <div style="flex: 1; background: #e3f2fd; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #1565c0;">AnyNetX<sub>B</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">14 params</p>
                                    </div>
                                    <div style="font-size: 1.5em;">‚Üí</div>
                                    <div style="flex: 1; background: #f3e5f5; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #7b1fa2;">AnyNetX<sub>C</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">$g_i = g$ (shared groups)</p>
                                        <p style="font-size: 0.65em; color: #666; margin: 0;">11 params (-3)</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: center; justify-content: space-between; margin: 5px 0;">
                                    <div style="flex: 1; background: #f3e5f5; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #7b1fa2;">AnyNetX<sub>C</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">11 params</p>
                                    </div>
                                    <div style="font-size: 1.5em;">‚Üí</div>
                                    <div style="flex: 1; background: #e8f5e9; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #2e7d32;">AnyNetX<sub>D</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">$c_i \leq c_{i+1}$ (increasing width)</p>
                                        <p style="font-size: 0.65em; color: #666; margin: 0;">Better performance!</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: center; justify-content: space-between; margin: 5px 0;">
                                    <div style="flex: 1; background: #e8f5e9; padding: 5px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #2e7d32;">AnyNetX<sub>D</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">With constraints</p>
                                    </div>
                                    <div style="font-size: 1.5em;">‚Üí</div>
                                    <div style="flex: 1; background: #fff3e0; padding: 15px; border-radius: 8px; margin: 0 5px;">
                                        <h6 style="color: #ff6f00;">AnyNetX<sub>E</sub></h6>
                                        <p style="font-size: 0.7em; margin: 0;">$d_i \leq d_{i+1}$ (increasing depth)</p>
                                        <p style="font-size: 0.65em; color: #666; margin: 0;">Best performance!</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p>Result: Simple, interpretable design rules with <strong>no performance loss!</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">RegNet Design Principles</h2>
                    <div class="regnet-principles" style="font-size: 0.65em;">
                        <div class="fragment">
                            <h4>Discovered Optimal Design Rules</h4>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div class="principle-grid" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
                                <div class="principle-card" style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #1565c0; font-size: 0.9em;">Linear Width Progression</h5>
                                    <p style="font-size: 0.85em; margin: 10px 0;">$$c_j \approx c_0 + c_a \cdot j$$</p>
                                    <p style="font-size: 0.7em;">Width increases linearly with block index</p>
                                </div>
                                <div class="principle-card" style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #7b1fa2; font-size: 0.9em;">No Bottleneck</h5>
                                    <p style="font-size: 0.85em; margin: 10px 0;">$$k = 1$$</p>
                                    <p style="font-size: 0.7em;">Bottlenecks don't help in this design</p>
                                </div>
                                <div class="principle-card" style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #2e7d32; font-size: 0.9em;">Shared Group Width</h5>
                                    <p style="font-size: 0.85em; margin: 10px 0;">$$g_i = g \text{ for all } i$$</p>
                                    <p style="font-size: 0.7em;">Typically $g = 16$ works well</p>
                                </div>
                                <div class="principle-card" style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #ff6f00; font-size: 0.9em;">Progressive Structure</h5>
                                    <p style="font-size: 0.75em; margin: 10px 0;">$c_i \leq c_{i+1}$ and $d_i \leq d_{i+1}$</p>
                                    <p style="font-size: 0.7em;">Deeper and wider as we go</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">RegNetX32: A Practical Implementation</h2>
                    <div class="regnetx32-implementation" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4>32-Layer RegNetX Configuration</h4>
                            <p style="font-size: 0.8em;">Optimized for efficiency and performance</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Configuration</h5>
                            <div style="background: #f5f5f5; padding: 20px; border-radius: 8px;">
                                <ul style="font-size: 0.85em;">
                                    <li>Bottleneck ratio: $k = 1$ (no bottleneck)</li>
                                    <li>Group width: $g = 16$</li>
                                    <li>Stage 1: $d_1 = 4$ blocks, $c_1 = 32$ channels</li>
                                    <li>Stage 2: $d_2 = 6$ blocks, $c_2 = 80$ channels</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>PyTorch Implementation</h5>
                            <pre><code class="language-python">class RegNetX32(AnyNet):
    def __init__(self, lr=0.1, num_classes=10):
        stem_channels, groups, bot_mul = 32, 16, 1
        depths, channels = (4, 6), (32, 80)
        super().__init__(
            ((depths[0], channels[0], groups, bot_mul),
             (depths[1], channels[1], groups, bot_mul)),
            stem_channels, lr, num_classes)</code></pre>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Layer Summary</h5>
                            <div style="background: #262626; color: #00ff00; padding: 15px; border-radius: 5px; font-family: monospace; font-size: 0.75em;">
                                <div>RegNetX32().layer_summary((1, 1, 96, 96))</div>
                                <div style="margin-top: 10px;">
                                    Sequential output shape:     torch.Size([1, 32, 48, 48])<br>
                                    Sequential output shape:     torch.Size([1, 32, 24, 24])<br>
                                    Sequential output shape:     torch.Size([1, 80, 12, 12])<br>
                                    Sequential output shape:     torch.Size([1, 10])
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: RegNet Design</h2>
                    <div data-mcq='{
                        "question": "Why does RegNet use k=1 (no bottleneck) instead of the typical bottleneck design?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It reduces the number of parameters significantly",
                                "correct": false,
                                "explanation": "Actually, k=1 means no bottleneck, which typically uses more parameters than bottleneck designs."
                            },
                            {
                                "text": "Empirical evidence showed bottlenecks do not improve performance in this design space",
                                "correct": true,
                                "explanation": "Correct! Through systematic exploration, researchers found that bottlenecks (k>1) did not improve accuracy in the RegNet design space."
                            },
                            {
                                "text": "Bottlenecks are incompatible with grouped convolutions",
                                "correct": false,
                                "explanation": "Bottlenecks work fine with grouped convolutions (see ResNeXt). This is an empirical finding specific to RegNet."
                            },
                            {
                                "text": "It makes the network train faster",
                                "correct": false,
                                "explanation": "No bottleneck (k=1) actually increases computation compared to bottleneck designs."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Training and Results (Vertical) -->
            <section>
                <section data-sources='[{"text": "RegNet Training - d2l.ai", "url": "https://d2l.ai/chapter_convolutional-modern/cnn-design.html#training"}]'>
                    <h2 class="truncate-title">Training RegNetX32</h2>
                    <div class="regnet-training">
                        <div class="fragment">
                            <h4>Training on Fashion-MNIST</h4>
                            <p style="font-size: 0.8em;">Demonstrating RegNet's effectiveness on image classification</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Training Code</h5>
                            <pre><code class="language-python">model = RegNetX32(lr=0.05)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
trainer.fit(model, data)</code></pre>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px;">
                                <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h6 style="color: #1565c0;">Final Accuracy</h6>
                                    <p style="font-size: 1.2em; font-weight: bold;">~92%</p>
                                </div>
                                <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h6 style="color: #7b1fa2;">Training Time</h6>
                                    <p style="font-size: 1.2em; font-weight: bold;">10 epochs</p>
                                </div>
                                <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;">
                                    <h6 style="color: #2e7d32;">Parameters</h6>
                                    <p style="font-size: 1.2em; font-weight: bold;">~400K</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Performance Analysis</h2>
                    <div class="performance-analysis">

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Key Advantages of RegNet</h5>
                            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
                                <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h6 style="color: #1565c0;">Simplicity</h6>
                                    <ul style="font-size: 0.75em;">
                                        <li>Few hyperparameters</li>
                                        <li>Regular structure</li>
                                        <li>Easy to implement</li>
                                    </ul>
                                </div>
                                <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                    <h6 style="color: #7b1fa2;">Scalability</h6>
                                    <ul style="font-size: 0.75em;">
                                        <li>Works from mobile to server</li>
                                        <li>Predictable scaling</li>
                                        <li>Consistent performance</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p>RegNet achieves ResNet-level performance with simpler, more interpretable design</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Training and Performance</h2>
                    <div data-mcq='{
                        "question": "What makes RegNet particularly suitable for deployment across different computational budgets?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses special hardware-specific optimizations",
                                "correct": false,
                                "explanation": "RegNet does not rely on hardware-specific optimizations; its strength is in its design principles."
                            },
                            {
                                "text": "Its linear width progression and regular structure make scaling predictable",
                                "correct": true,
                                "explanation": "Correct! The linear relationship c_j ‚âà c_0 + c_a¬∑j and regular structure make it easy to scale RegNet up or down predictably for different computational budgets."
                            },
                            {
                                "text": "It requires less memory than all other CNN architectures",
                                "correct": false,
                                "explanation": "RegNet does not necessarily use less memory; its advantage is in design simplicity and scalability."
                            },
                            {
                                "text": "It can only run on GPUs",
                                "correct": false,
                                "explanation": "RegNet can run on any hardware; its advantage is consistent scaling across different computational resources."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Modern Context and Vision Transformers (Vertical) -->
            <section>
                <section data-sources='[{"text": "An Image is Worth 16x16 Words - Dosovitskiy et al., 2021", "url": "https://arxiv.org/abs/2010.11929"}, {"text": "A ConvNet for the 2020s - Liu et al., 2022", "url": "https://arxiv.org/abs/2201.03545"}]'>
                    <h2 class="truncate-title">From CNNs to Vision Transformers</h2>
                    <div class="cnn-to-transformers" style="font-size: 0.75em;">
                        <div class="fragment">
                            <h4>The Paradigm Shift in Computer Vision</h4>
                            <p style="font-size: 0.8em;">How Transformers challenged CNN dominance</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <div class="comparison-grid" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
                                <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #1565c0;">CNNs (Including RegNet)</h5>
                                    <ul style="font-size: 0.75em;">
                                        <li><strong>Inductive Bias:</strong> Locality, translation invariance</li>
                                        <li><strong>Data Efficiency:</strong> Good with smaller datasets</li>
                                        <li><strong>Architecture:</strong> Hierarchical, convolutions</li>
                                        <li><strong>Compute:</strong> Efficient for smaller models</li>
                                    </ul>
                                </div>
                                <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
                                    <h5 style="color: #7b1fa2;">Vision Transformers</h5>
                                    <ul style="font-size: 0.75em;">
                                        <li><strong>Inductive Bias:</strong> Minimal (learned from data)</li>
                                        <li><strong>Data Efficiency:</strong> Requires large datasets</li>
                                        <li><strong>Architecture:</strong> Self-attention, patches</li>
                                        <li><strong>Compute:</strong> Scales better with size</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Key Insight: "Scalability Trumps Inductive Biases"</h5>
                            <div style="background: #fff3e0; padding: 20px; border-radius: 8px;">
                                <p style="font-size: 0.8em;">With enough data (LAION-5B: 5 billion images), learned patterns outperform hand-designed biases</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Future Directions and Hybrid Approaches</h2>
                    <div class="future-directions" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4>The Best of Both Worlds</h4>
                            <p style="font-size: 0.8em;">Modern architectures combine CNN and Transformer insights</p>
                        </div>

                        <div class="fragment" style="margin-top: 20px;">
                            <h5>Emerging Trends</h5>
                            <div class="trend-grid" style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                                <div class="trend-card" style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #2e7d32; font-size: 0.9em;">ConvNeXt</h6>
                                    <p style="font-size: 0.7em;">Modernized CNNs with Transformer tricks</p>
                                </div>
                                <div class="trend-card" style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #ff6f00; font-size: 0.9em;">Hybrid Models</h6>
                                    <p style="font-size: 0.7em;">CNN stems with Transformer bodies</p>
                                </div>
                                <div class="trend-card" style="background: #ffebee; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #c62828; font-size: 0.9em;">MLP-Mixer</h6>
                                    <p style="font-size: 0.7em;">Neither CNN nor Transformer!</p>
                                </div>
                                <div class="trend-card" style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                    <h6 style="color: #1565c0; font-size: 0.9em;">Hardware Co-design</h6>
                                    <p style="font-size: 0.7em;">Architectures optimized for specific hardware</p>
                                </div>
                            </div>
                        </div>

                        <div class="fragment" style="margin-top: 30px;">
                            <h5>Lessons from Design Spaces</h5>
                            <ul style="font-size: 0.8em;">
                                <li>Systematic exploration beats pure intuition</li>
                                <li>Simple, regular designs often work best</li>
                                <li>Scalability is crucial for modern applications</li>
                                <li>Design principles transfer across architectures</li>
                            </ul>
                        </div>

                        <div class="fragment emphasis-box mt-lg">
                            <p>The design space methodology pioneered by RegNet continues to influence modern architecture research</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Modern Developments</h2>
                    <div data-mcq='{
                        "question": "What key factor enabled Vision Transformers to surpass CNNs in performance?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They are computationally more efficient than CNNs",
                                "correct": false,
                                "explanation": "Vision Transformers are generally more computationally expensive than CNNs, especially for smaller models."
                            },
                            {
                                "text": "They have better inductive biases for vision tasks",
                                "correct": false,
                                "explanation": "Vision Transformers actually have less inductive bias than CNNs, which have built-in locality and translation invariance."
                            },
                            {
                                "text": "Access to massive datasets and computational resources for training",
                                "correct": true,
                                "explanation": "Correct! Vision Transformers excel when trained on massive datasets (like LAION-5B) with substantial computational resources, where their flexibility allows them to learn patterns that outperform hand-designed CNN biases."
                            },
                            {
                                "text": "They require less training data than CNNs",
                                "correct": false,
                                "explanation": "Vision Transformers actually require much more training data than CNNs to achieve good performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js"></script>

    <!-- Shared scripts -->
    <script src="../shared/js/multiple-choice.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/title-handler.js"></script>

    <!-- Custom scripts -->
    <script src="js/architecture-timeline.js"></script>
    <script src="js/alexnet-visualizations.js"></script>
    <script src="js/vgg-visualizations.js"></script>
    <script src="js/nin-visualizations.js"></script>
    <script src="js/googlenet-visualizations.js"></script>
    <script src="js/batch-norm-visualizations.js"></script>
    <script src="js/resnet-visualizations.js"></script>
    <script src="js/densenet-visualizations.js"></script>
    <script src="js/cnn-design-visualizations.js"></script>

    <script>
        // Initialize Reveal.js
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes ]
        });
    </script>
</body>
</html>