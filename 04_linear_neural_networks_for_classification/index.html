<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Neural Networks for Classification - Introduction to Deep Learning</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="css/classification-custom.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Linear Neural Networks for Classification</h1>
                <p>Chapter 4: From Regression to Classification</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>
            
            <!-- Section 1: Introduction & Problem Setup (Enhanced) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 4", "url": "https://d2l.ai/chapter_linear-classification/index.html"}]'>
                    <h2 class="truncate-title">From Regression to Classification</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Regression: "How much?"</h4>
                            <ul>
                                <li class="fragment">Continuous outputs</li>
                                <li class="fragment">House prices: $234,567</li>
                                <li class="fragment">Baseball team wins: 87</li>
                                <li class="fragment">Hospital days: 5</li>
                                <li class="fragment">Minimize squared errors</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Classification: "Which category?"</h4>
                            <ul>
                                <li class="fragment">Discrete categories</li>
                                <li class="fragment">Email: spam or inbox</li>
                                <li class="fragment">Animal: cat, dog, bird</li>
                                <li class="fragment">Customer: sign up or not</li>
                                <li class="fragment">Cross-entropy loss</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>From predicting quantities to assigning categories</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Regression Isn't Always Simple</h2>
                    <div class="two-column">
                        <div class="column">
                            <div class="fragment">
                                <h4>House Prices</h4>
                                <ul>
                                    <li>Never negative values</li>
                                    <li>Changes often relative to baseline</li>
                                    <li>Solution: Regress on <span class="tooltip">log(price)<span class="tooltiptext">Logging house prices: (1) Creates symmetric distribution from skewed data, (2) Coefficients become percentage changes (more interpretable), (3) Stabilizes variance across price levels, (4) Can lead to better predictive performance</span></span></li>
                                </ul>
                            </div>
                        </div>
                        <div class="column">
                            <div class="fragment">
                                <h4>Hospital Days</h4>
                                <ul>
                                    <li>Discrete, non-negative random variable</li>
                                    <li>Least squares may not be ideal</li>
                                    <li><span class="tooltip">Survival modeling<span class="tooltiptext">A specialized field dealing with time-to-event data, considering censored observations and hazard rates</span></span>: specialized subfield</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>There's more to estimation than minimizing squared errors!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Classification Questions</h2>
                    <p>Moving from "how much?" to "which category?"</p>
                    <div class="grid-2x2 mt-md" style="font-size: 0.85em;">
                        <div class="fragment">
                            <h4>üìß Email Filtering</h4>
                            <p>Does this email belong in spam or inbox?</p>
                        </div>
                        <div class="fragment">
                            <h4>üõçÔ∏è Customer Behavior</h4>
                            <p>Will this customer sign up for our subscription?</p>
                        </div>
                        <div class="fragment">
                            <h4>üñºÔ∏è Image Recognition</h4>
                            <p>Is this a donkey, dog, cat, or rooster?</p>
                        </div>
                        <div class="fragment">
                            <h4>üé¨ Recommendations</h4>
                            <p>Which movie will Aston watch next?<br>
                            Which book section will you read next?</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Hard vs Soft Assignments</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Hard Assignment</h4>
                            <ul>
                                <li class="fragment">Definitive category placement</li>
                                <li class="fragment">Example: "This IS spam"</li>
                                <li class="fragment">Binary decision</li>
                                <li class="fragment">No uncertainty expressed</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Soft Assignment</h4>
                            <ul>
                                <li class="fragment">Probability for each category</li>
                                <li class="fragment">Example: "85% likely spam"</li>
                                <li class="fragment">Confidence scores</li>
                                <li class="fragment">Captures uncertainty</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.8em;">
                        <p><strong>Key Insight:</strong> Even when we want hard assignments, we often use models that make soft assignments and then threshold them</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why might we prefer soft assignments over hard assignments in classification?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "They provide confidence information",
                                "correct": true,
                                "explanation": "Correct! Soft assignments tell us how confident the model is about its prediction."
                            },
                            {
                                "text": "They allow for adjustable decision thresholds",
                                "correct": true,
                                "explanation": "Correct! We can adjust the threshold based on the cost of false positives vs false negatives."
                            },
                            {
                                "text": "They are always more accurate",
                                "correct": false,
                                "explanation": "Not necessarily. Accuracy depends on the problem and how we use the probabilities."
                            },
                            {
                                "text": "They capture model uncertainty",
                                "correct": true,
                                "explanation": "Correct! A 51% vs 99% prediction tells us very different things about model confidence."
                            }
                        ]
                    }'></div>
                </section>

                <section data-sources='[{"text": "Multi-label Classification Overview - Tsoumakas & Katakis 2007", "url": "https://doi.org/10.1504/IJDMMM.2007.016060"}, {"text": "Deep Learning for Multi-label Classification - Huang et al. 2015", "url": "https://arxiv.org/abs/1502.05988"}]'>
                    <h2 class="truncate-title">Multi-label Classification</h2>
                    <p>When one category isn't enough</p>
                    <div class="two-column">
                        <div class="column">
                            <h4>Example: News Article Categorization</h4>
                            <div class="fragment" style="display: flex; gap: 20px; justify-content: center; margin: 20px 0;">
                                <div style="padding: 10px; border: 2px solid #10099F; border-radius: 8px;">
                                    <strong>Article about SpaceX IPO</strong><br>
                                    <span style="font-size: 0.8em;">
                                    ‚úì Business<br>
                                    ‚úì Technology<br>
                                    ‚úì Space<br>
                                    ‚úó Medicine<br>
                                    ‚úó Sports
                                    </span>
                                </div>
                            </div>
                        </div>
                        <div class="column" style="font-size: 0.85em; text-align: left;">
                            <div class="fragment mt-md">
                                <p><strong>Challenge:</strong> Single category would lose important information</p>
                            </div>
                            <div class="fragment">
                                <p><strong>Applications:</strong></p>
                                <ul>
                                    <li>Image tagging (sunset + beach + vacation)</li>
                                    <li>Document classification</li>
                                    <li>Medical diagnosis (multiple conditions)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">A Simple Example: 2√ó2 Images</h2>
                    <p>Let's start with a toy problem to build intuition</p>
                    <div class="fragment">
                        <div style="display: flex; gap: 40px; justify-content: center; align-items: center; margin: 30px 0;">
                            <div>
                                <h4>Input Image</h4>
                                <div style="display: inline-grid; grid-template-columns: 60px 60px; grid-gap: 2px; background: #262626; padding: 2px; border-radius: 4px;">
                                    <div style="background: #888; height: 60px; display: flex; align-items: center; justify-content: center; color: white;">x‚ÇÅ</div>
                                    <div style="background: #aaa; height: 60px; display: flex; align-items: center; justify-content: center; color: white;">x‚ÇÇ</div>
                                    <div style="background: #666; height: 60px; display: flex; align-items: center; justify-content: center; color: white;">x‚ÇÉ</div>
                                    <div style="background: #ddd; height: 60px; display: flex; align-items: center; justify-content: center; color: #262626;">x‚ÇÑ</div>
                                </div>
                                <p style="font-size: 0.8em; margin-top: 10px;">2√ó2 grayscale pixels</p>
                            </div>
                            <div style="font-size: 2em;">‚Üí</div>
                            <div>
                                <h4>Features</h4>
                                <p style="font-family: monospace; background: #f5f5f5; padding: 10px; border-radius: 4px;">
                                    x = [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ]
                                </p>
                                <p style="font-size: 0.8em;">4 scalar values</p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>Categories to Classify</h4>
                        <div style="display: flex; gap: 30px; justify-content: center;">
                            <div>üê± Cat</div>
                            <div>üêî Chicken</div>
                            <div>üêï Dog</div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Label Representation: Integer Encoding</h2>
                    <p>The natural first choice</p>
                    <div class="two-column">
                        <div class="column">
                            <div class="fragment">
                                <table style="margin: 20px auto;">
                                    <thead>
                                        <tr>
                                            <th>Category</th>
                                            <th>Integer Label</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>üêï Dog</td>
                                            <td>1</td>
                                        </tr>
                                        <tr>
                                            <td>üê± Cat</td>
                                            <td>2</td>
                                        </tr>
                                        <tr>
                                            <td>üêî Chicken</td>
                                            <td>3</td>
                                        </tr>
                                    </tbody>
                                </table>
                                <p style="text-align: center; margin-top: 10px; font-size: 1.5em;">y ‚àà {1, 2, 3}</p>
                            </div>
                        </div>
                        <div class="column" style="text-align: left;">
                            <div class="fragment">
                                <h4>‚úì Good for:</h4>
                                <ul>
                                    <li>Storage efficiency</li>
                                    <li><span class="tooltip">Ordinal data<span class="tooltiptext">Data with a natural ordering, like age groups: baby < toddler < adolescent < adult</span></span> with natural ordering</li>
                                </ul>
                            </div>
                            <div class="fragment">
                                <h4>‚úó Problem:</h4>
                                <p>Implies dog < cat < chicken? No natural ordering exists!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Ordinal Regression Example</h2>
                    <p>When integer encoding makes sense</p>
                    <div class="fragment">
                        <h4>Age Group Classification</h4>
                        <table style="margin: 20px auto; font-size: 0.7em;">
                            <thead>
                                <tr>
                                    <th>Category</th>
                                    <th>Label</th>
                                    <th>Natural Order</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Baby</td>
                                    <td>1</td>
                                    <td rowspan="6" style="background: linear-gradient(to bottom, #e8f5e9, #ffebee); padding: 10px;">
                                        ‚Üì<br>
                                        Increasing<br>
                                        Age<br>
                                        ‚Üì
                                    </td>
                                </tr>
                                <tr>
                                    <td>Toddler</td>
                                    <td>2</td>
                                </tr>
                                <tr>
                                    <td>Adolescent</td>
                                    <td>3</td>
                                </tr>
                                <tr>
                                    <td>Young Adult</td>
                                    <td>4</td>
                                </tr>
                                <tr>
                                    <td>Adult</td>
                                    <td>5</td>
                                </tr>
                                <tr>
                                    <td>Geriatric</td>
                                    <td>6</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Here, 2 < 3 < 4 makes semantic sense!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When should you use integer encoding for categorical labels?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When categories have no relationship to each other",
                                "correct": false,
                                "explanation": "Integer encoding implies ordering. Use one-hot encoding for unrelated categories."
                            },
                            {
                                "text": "When categories have a natural ordering",
                                "correct": true,
                                "explanation": "Correct! Integer encoding is appropriate for ordinal data like sizes (S<M<L) or ratings (1-5 stars)."
                            },
                            {
                                "text": "When you want to save memory",
                                "correct": false,
                                "explanation": "While integers use less memory, this alone is not a good reason if it introduces false ordering."
                            },
                            {
                                "text": "When dealing with image classification",
                                "correct": false,
                                "explanation": "Most image categories (cat, dog, bird) have no natural ordering."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">One-Hot Encoding: The Solution</h2>
                    <p>Representing categories without implicit ordering</p>
                    <div class="fragment">
                        <h4>Converting Categories to Vectors</h4>
                        <table style="margin: 20px auto;">
                            <thead>
                                <tr>
                                    <th>Category</th>
                                    <th>One-Hot Vector</th>
                                    <th>Mathematical Notation</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>üê± Cat</td>
                                    <td style="font-family: monospace;">[1, 0, 0]</td>
                                    <td>(1, 0, 0)</td>
                                </tr>
                                <tr>
                                    <td>üêî Chicken</td>
                                    <td style="font-family: monospace;">[0, 1, 0]</td>
                                    <td>(0, 1, 0)</td>
                                </tr>
                                <tr>
                                    <td>üêï Dog</td>
                                    <td style="font-family: monospace;">[0, 0, 1]</td>
                                    <td>(0, 0, 1)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment two-column" style="font-size: 0.65em;">
                        <div class="column">
                            <div class="math-block">
                                $$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$
                            </div>
                        </div>
                        <div class="column">
                            <h4>Key Properties</h4>
                            <ul>
                                <li>Vectors are orthogonal (no implicit similarity)</li>
                                <li>Exactly one component = 1, rest = 0</li>
                                <li>Dimension = number of categories</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Learning Objectives (Updated)</h2>
                    <ul>
                        <li class="fragment">Understand when to use regression vs classification</li>
                        <li class="fragment">Distinguish between hard and soft assignments</li>
                        <li class="fragment">Recognize multi-label classification problems</li>
                        <li class="fragment">Master <span class="tooltip">one-hot encoding<span class="tooltiptext">A representation where categorical variables are converted to binary vectors with a single 1 and rest 0s</span></span> vs integer encoding</li>
                        <li class="fragment">Understand <span class="tooltip">softmax regression<span class="tooltiptext">A generalization of logistic regression for multi-class classification that outputs a probability distribution over classes</span></span></li>
                        <li class="fragment">Master the <span class="tooltip">cross-entropy loss<span class="tooltiptext">A loss function that measures the difference between two probability distributions: -Œ£ y log(≈∑)</span></span></li>
                        <li class="fragment">Implement classification from scratch</li>
                        <li class="fragment">Work with real image data (Fashion-MNIST)</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main difference between regression and classification?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Classification uses neural networks, regression does not",
                                "correct": false,
                                "explanation": "Both can use neural networks. The difference is in the output type."
                            },
                            {
                                "text": "Classification predicts discrete categories, regression predicts continuous values",
                                "correct": true,
                                "explanation": "Correct! Classification assigns inputs to discrete classes while regression outputs continuous numerical values."
                            },
                            {
                                "text": "Classification is always more accurate than regression",
                                "correct": false,
                                "explanation": "Accuracy depends on the problem and data, not the task type."
                            },
                            {
                                "text": "Classification requires more data than regression",
                                "correct": false,
                                "explanation": "Data requirements depend on problem complexity, not whether its classification or regression."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 2: Softmax Regression Architecture -->
            <section>
                <section>
                    <h2 class="truncate-title">Linear Model for Classification</h2>
                    <p>Multiple outputs for multiple classes</p>
                    <div class="fragment two-column" style="font-size: 0.75em;">
                        <div class="column">
                            <p>To estimate conditional probabilities for all classes, we need:</p>
                            <ul>
                                <li><strong>Multiple outputs:</strong> One per class</li>
                                <li><strong>Affine functions:</strong> One per output</li>
                                <li><strong>Redundant parametrization:</strong> For symmetry</li>
                            </ul>
                        </div>
                        <div class="column">
                            <p><strong>Our 2√ó2 image example:</strong></p>
                            <ul>
                                <li>4 features (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ)</li>
                                <li>3 output categories (cat, chicken, dog)</li>
                                <li>Need: 12 weights + 3 biases = 15 parameters</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Each output gets its own affine function</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mathematical Formulation</h2>
                    <p>Three affine functions for three classes:</p>
                    <div class="math-block" style="font-size: 0.7em;">
                        $$\begin{aligned}
                        o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1\\
                        o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2\\
                        o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3
                        \end{aligned}$$
                    </div>
                    <div class="fragment mt-md">
                        <p><strong>Key observations:</strong></p>
                        <ul>
                            <li>Each output o‚ÇÅ, o‚ÇÇ, o‚ÇÉ depends on ALL inputs</li>
                            <li>This is a <span class="tooltip">fully connected layer<span class="tooltiptext">Every input is connected to every output with its own weight</span></span></li>
                            <li>Single-layer neural network</li>
                        </ul>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Softmax Regression", "url": "https://d2l.ai/chapter_linear-classification/softmax-regression.html"}]'>
                    <h2 class="truncate-title">Network Architecture</h2>
                    <div class="network-diagram" id="softmax-architecture"></div>
                    <div class="fragment mt-lg">
                        <p><strong>Softmax regression = single-layer neural network</strong></p>
                        <ul>
                            <li>Input layer: 4 features (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ)</li>
                            <li>Output layer: 3 classes (o‚ÇÅ, o‚ÇÇ, o‚ÇÉ)</li>
                            <li>Fully connected: Every input ‚Üí every output</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Matrix Notation</h2>
                    <p>Compact representation for mathematics and code:</p>
                    <div class="math-block" style="font-size: 0.8em;">
                        $$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$$
                    </div>
                    <div class="fragment">
                        <div class="math-block" style="font-size: 0.8em;">
                            $$\mathbf{W} = \begin{bmatrix} 
                            w_{11} & w_{12} & w_{13} & w_{14} \\
                            w_{21} & w_{22} & w_{23} & w_{24} \\
                            w_{31} & w_{32} & w_{33} & w_{34}
                            \end{bmatrix} \in \mathbb{R}^{3 \times 4}
                            \quad\text{ and }\quad \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} \in \mathbb{R}^3$$
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.8em;">
                        <p>Much cleaner for both mathematics and implementation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Softmax Operation</h2>
                    <div class="interactive-demo" id="softmax-demo">
                        <div id="softmax-visualization"></div>
                        <div class="controls-fixed">
                            <label style="display: flex; align-items: center; gap: 8px; font-size: 0.9em;">
                                Temperature T: 
                                <input type="range" id="temperature" min="0.1" max="5" step="0.1" value="1" style="width: 150px; cursor: pointer;">
                                <span id="temp-value" style="font-family: monospace; min-width: 40px; color: #10099F; font-weight: bold;">1.0</span>
                            </label>
                        </div>
                    </div>
                    <div class="math-block mt-lg" style="font-size: 0.7em;">
                        $$\hat{y}_j = \frac{\exp(o_j/T)}{\sum_{k=1}^q \exp(o_k/T)}$$
                    </div>
                </section>

                <section data-sources='[{"text": "Statistical Mechanics - Gibbs 1902", "url": "https://archive.org/details/elementaryprinci00gibbgoog"}, {"text": "Boltzmann Distribution", "url": "https://en.wikipedia.org/wiki/Boltzmann_distribution"}]'>
                    <h2 class="truncate-title">Softmax Properties & Temperature</h2>
                    <ul style="font-size: 0.8em;">
                        <li class="fragment">
                            <strong>Order preservation:</strong> $\operatorname{argmax}_j \hat{y}_j = \operatorname{argmax}_j o_j$
                        </li>
                        <li class="fragment">
                            <strong>Normalization:</strong> $\sum_{j=1}^q \hat{y}_j = 1$
                        </li>
                        <li class="fragment">
                            <strong>Non-negativity:</strong> $\hat{y}_j \geq 0$ for all j
                        </li>
                    </ul>
                    <div class="fragment mt-md" style="font-size: 0.8em;">
                        <h4>Temperature from Statistical Physics</h4>
                        <ul>
                            <li><strong><span class="tooltip">Boltzmann<span class="tooltiptext">Ludwig Boltzmann (1844-1906), Austrian physicist who developed statistical mechanics and the Boltzmann distribution, which describes how particles distribute among energy states</span></span>:</strong> <span class="tooltip">Energy states ‚àù exp(-E/kT)<span class="tooltiptext">In statistical physics, the probability of a system being in a state with energy E is proportional to exp(-E/kT), where k is Boltzmann's constant and T is temperature. Energy typically represents the cost or "difficulty" of a state - lower energy states are more probable. Softmax uses this same form with logits as "energies" where higher logits (lower "energy") get higher probability</span></span></li>
                            <li><strong>Low T:</strong> Favor low energy (high confidence)</li>
                            <li><strong>High T:</strong> More uniform distribution</li>
                            <li><strong><span class="tooltip">Energy = Error<span class="tooltiptext">In machine learning, we often think of "energy" as representing error or cost. Higher logits correspond to lower "energy" (better predictions), just like how in physics, systems naturally move toward lower energy states. The softmax function assigns higher probabilities to lower energy (higher logit) states, mimicking how physical systems prefer low-energy configurations</span></span></strong> in machine learning</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we need multiple affine functions for multi-class classification?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the model more complex",
                                "correct": false,
                                "explanation": "Complexity is not the goal; we need separate functions to score each class."
                            },
                            {
                                "text": "Each class needs its own scoring function",
                                "correct": true,
                                "explanation": "Correct! Each affine function produces a score (logit) for one specific class."
                            },
                            {
                                "text": "To increase training speed",
                                "correct": false,
                                "explanation": "Multiple functions actually increase computation, not speed."
                            },
                            {
                                "text": "Because softmax requires it",
                                "correct": false,
                                "explanation": "Softmax operates on any vector; multiple functions are needed to produce class scores."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 3: Mathematics & Loss Function -->
            <section>
                <section>
                    <h2 class="truncate-title">Why Do We Need Softmax?</h2>
                    <p>Problems with raw logits as probabilities</p>
                    <div class="fragment">
                        <div class="three-column" style="font-size: 0.8em;">
                            <div class="column" style="text-align: left;">
                                <h4>Issue 1: No sum constraint</h4>
                                <p>Logits don't sum to 1 as probabilities should</p>
                                <p>Example: o = [2, 3, -1] ‚Üí sum = 4 ‚â† 1</p>
                            </div>
                            <div class="column" style="text-align: left;">
                                <h4>Issue 2: Can be negative</h4>
                                <p>Probabilities must be non-negative</p>
                                <p>Example: o‚ÇÉ = -1 < 0 (invalid probability!)</p>
                            </div>
                            <div class="column" style="text-align: left;">
                                <h4>Issue 3: Can exceed 1</h4>
                                <p>Individual values might be > 1</p>
                                <p>Example: "5 bedrooms ‚Üí P(buy) = 2.5" (!?)</p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>We need to "squish" the outputs into valid probabilities</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Vectorization for Minibatches</h2>
                    <p>Processing multiple examples efficiently</p>
                    <div class="fragment">
                        <div class="two-column">
                            <div class="column">
                                <h4>Minibatch Setup</h4>
                                <ul>
                                    <li><strong>Minibatch:</strong> $\mathbf{X} \in \mathbb{R}^{n \times d}$ (n examples, d features)</li>
                                    <li><strong>Weights:</strong> $\mathbf{W} \in \mathbb{R}^{d \times q}$ (d features, q classes)</li>
                                    <li><strong>Bias:</strong> $\mathbf{b} \in \mathbb{R}^{1 \times q}$ (broadcast to n rows)</li>
                                </ul>
                            </div>
                            <div class="column">
                                <h4>Batch Computation</h4>
                                <div class="math-block" style="font-size: 0.8em;">
                                    $$\begin{aligned}
                                    \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}\\
                                    \hat{\mathbf{Y}} &= \mathrm{softmax}(\mathbf{O})
                                    \end{aligned}$$
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment">
                        <p><strong>Key Insight:</strong> Softmax is applied <em>rowwise</em></p>
                        <p style="font-size: 0.85em;">Each row of $\hat{\mathbf{Y}}$ is a probability distribution</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Matrix Dimensions Example</h2>
                    <div class="interactive-demo" id="batch-matrix-demo">
                        <div id="batch-matrix-visualization"></div>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.55em;">
                        <p><strong>Example with batch size 32:</strong></p>
                        <ul>
                            <li>$\mathbf{X}$: [32 √ó 784] (32 images, 784 pixels each)</li>
                            <li>$\mathbf{W}$: [784 √ó 10] (784 inputs, 10 classes)</li>
                            <li>$\mathbf{O} = \mathbf{XW}$: [32 √ó 10] (32 examples, 10 logits each)</li>
                            <li>$\hat{\mathbf{Y}}$: [32 √ó 10] (32 probability distributions)</li>
                        </ul>
                    </div>
                </section>

                <section data-sources='[{"text": "Probit Model - Fechner 1860", "url": "https://en.wikipedia.org/wiki/Probit_model"}]'>
                    <h2 class="truncate-title">Alternative Approaches</h2>
                    <div class="fragment">
                        <h4>Probit Model (Fechner, 1860)</h4>
                        <ul>
                            <li>Assume: $\mathbf{y} = \mathbf{o} + \boldsymbol{\epsilon}$</li>
                            <li>Where: $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$</li>
                            <li>Add Gaussian noise to outputs</li>
                            <li>‚úó <span class="tooltip">Less convenient optimization
                                <span class="tooltiptext">
                                    The probit model uses the cumulative normal distribution (Œ¶) to map scores into probabilities. 
                                    While Œ¶ itself is easy to compute, its derivatives are less straightforward compared to the logistic 
                                    (softmax) function. This makes gradient-based optimization slower and less convenient in large-scale 
                                    machine learning.
                                </span>
                            </span></li>                            <li>‚úó <span class="tooltip">Not as effective as softmax<span class="tooltiptext">The probit model assumes Gaussian noise which may not match real data distributions. Softmax naturally handles multiple classes and provides better gradient flow during training, leading to faster convergence and better performance in practice.</span></span></li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Our Solution: Exponential + Normalization</h4>
                        <ul>
                            <li>Use exponential: $P(y = i) \propto \exp(o_i)$</li>
                            <li>Ensures non-negativity ‚úì</li>
                            <li>Monotonic (preserves ordering) ‚úì</li>
                            <li>Then normalize to sum to 1 ‚úì</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Softmax Function</h2>
                    <p>Combining exponential and normalization:</p>
                    <div class="math-block" style="font-size: 0.85em;">
                        $$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})$$
                    </div>
                    <div class="fragment">
                        <div class="math-block" style="font-size: 0.85em;">
                            $$\hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}$$
                        </div>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.8em;">
                        <p><strong>Key Properties:</strong></p>
                        <ul>
                            <li>‚úì Outputs sum to 1</li>
                            <li>‚úì All outputs non-negative</li>
                            <li>‚úì Differentiable everywhere</li>
                            <li>‚úì Preserves ordering: argmax(≈∑) = argmax(o)</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Maximum Likelihood Estimation</h2>
                    <p>Finding the best parameters through probability</p>
                    <div class="two-column" style="font-size: 0.85em;">
                        <div class="column fragment">
                            <h4>The Likelihood Function</h4>
                            <p>Probability of observing our data given parameters:</p>
                            <div class="math-block" style="font-size: 0.75em;">
                                $$P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$$
                            </div>
                        </div>
                        <div class="column fragment">
                            <h4>Log-Likelihood</h4>
                            <p>Products are hard ‚Üí Use logarithms for sums:</p>
                            <div class="math-block" style="font-size: 0.75em;">
                                $$\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n \log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$$
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box">
                        <p>Maximizing log-likelihood = Minimizing negative log-likelihood</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">From MLE to Cross-Entropy (Part 1)</h2>
                    <div class="fragment">
                        <h4>Step 1: Negative Log-Likelihood</h4>
                        <div class="math-block" style="font-size: 0.7em;">
                            $$-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})$$
                        </div>
                        <p style="font-size: 0.6em;">For a single example, we want to minimize $-\log P(\mathbf{y} \mid \mathbf{x})$</p>
                    </div>
                    
                    <div class="fragment mt-md">
                        <h4>Step 2: Multinomial Distribution</h4>
                        <p style="font-size: 0.6em;">Our model with parameters Œ∏ predicts probabilities $\hat{\mathbf{y}} = [\hat{y}_1, \hat{y}_2, ..., \hat{y}_q]$</p>
                        <div class="math-block" style="font-size: 0.7em;">
                            $$P(\mathbf{y} \mid \mathbf{x}) = \prod_{j=1}^q \hat{y}_j^{y_j}$$
                        </div>
                        <p style="font-size: 0.6em;">Here P represents the <strong>model's probability</strong> of observing label y given input x. Each class j contributes <span class="tooltip">$\hat{y}_j^{y_j}$<span class="tooltiptext">For one-hot labels: If j is the true class, then y_j = 1 and we get ≈∑_j^1 = ≈∑_j (the predicted probability). If j is not the true class, then y_j = 0 and we get ≈∑_j^0 = 1 (no contribution). This means only the predicted probability of the true class matters in the product.</span></span> to the probability</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">From MLE to Cross-Entropy (Part 2)</h2>
                    <div class="fragment">
                        <h4>Step 3: Taking the Logarithm</h4>
                        <div class="math-block" style="font-size: 0.6em;">
                            $$-\log P(\mathbf{y} \mid \mathbf{x}) = -\log \prod_{j=1}^q \hat{y}_j^{y_j} = -\sum_{j=1}^q y_j \log \hat{y}_j$$
                        </div>
                    </div>

                    <div class="fragment mt-md">
                        <h4>Step 4: One-Hot Simplification</h4>
                        <p style="font-size: 0.7em;">Since $\mathbf{y}$ is one-hot: $y_c = 1$ for true class c, $y_j = 0$ for all other j</p>
                        <div class="math-block" style="font-size: 0.7em;">
                            $$l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^q y_j \log \hat{y}_j = -\log \hat{y}_c$$
                        </div>
                    </div>

                    <div class="fragment emphasis-box mt-md" style="font-size: 0.6em;">
                        <p><strong>Result:</strong> Minimizing negative log-likelihood is exactly cross-entropy loss!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Statistical Mechanics - Gibbs 1902", "url": "https://archive.org/details/elementaryprinci00gibbgoog"}, {"text": "Energy-Based Models - Ranzato et al. 2007", "url": "https://yann.lecun.com/exdb/publis/pdf/ranzato-07.pdf"}]'>
                    <h2 class="truncate-title">Historical Context</h2>
                    <div class="two-column" style="font-size: 0.85em;">
                        <div class="column fragment">
                            <h4>Physics Origins</h4>
                            <ul>
                                <li><strong>Gibbs (1902):</strong> Adapted from physics</li>
                                <li><strong>Boltzmann:</strong> Energy state distribution</li>
                                <li>Probability ‚àù exp(-E/kT)</li>
                                <li>E = energy, T = temperature, k = Boltzmann constant</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Modern ML Interpretation</h4>
                            <ul>
                                <li><strong>Energy = Error:</strong> Lower is better</li>
                                <li><strong>Temperature:</strong> Controls distribution sharpness</li>
                                <li>High T ‚Üí more uniform (high entropy)</li>
                                <li>Low T ‚Üí peaked distribution (low entropy)</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Energy-based models use this physics analogy extensively</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Elements of Information Theory - Cover & Thomas", "url": "https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X"}]'>
                    <h2 class="truncate-title">Information Theory Basics</h2>
                    <div class="concept-grid">
                        <div class="fragment">
                            <h4>Entropy</h4>
                            <p>Measure of uncertainty</p>
                            <div class="math-block">
                                $$H[P] = -\sum_{j} P(j) \log P(j)$$
                            </div>
                            <p style="font-size: 0.75em;">Expected surprisal</p>
                        </div>
                        <div class="fragment">
                            <h4>Surprisal</h4>
                            <p>Information from an event</p>
                            <div class="math-block">
                                $$I(j) = -\log P(j)$$
                            </div>
                            <p style="font-size: 0.75em;">Bits to encode event j</p>
                        </div>
                    </div>
                    <div class="fragment mt-lg" style="font-size: 0.5em;">
                        <div class="two-column">
                            <div class="column">
                                <p><strong>Intuition:</strong> Rare events are more surprising!</p>
                                <ul>
                                    <li>P = 1: No surprise (0 bits)</li>
                                    <li>P = 0.5: Moderate surprise (1 bit)</li>
                                    <li>P ‚Üí 0: High surprise (many bits)</li>
                                </ul>
                            </div>
                            <div class="column">
                                <p><strong>Why -log(p) = bits needed?</strong></p>
                                <ul>
                                    <li>Common events: short codes</li>
                                    <li>Rare events: long codes</li>
                                    <li><span class="tooltip">Optimal code length ‚àù -log(probability)<span class="tooltiptext">Shannon's source coding theorem proves that the optimal average code length for encoding symbols from a distribution P is exactly the entropy H[P]. For individual symbols, the optimal code length is -log(P(symbol)). This means frequent events (high probability) get short codes, while rare events (low probability) get long codes. For example, if an event has probability 1/8, it needs -log‚ÇÇ(1/8) = 3 bits to encode optimally.</span></span></li>
                                    <li>Example: 50% event needs 1 bit (heads/tails)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Entropy: Quantifying Uncertainty</h2>
                    <p>The fundamental theorem of information theory</p>
                    <div class="fragment">
                        <h4>Shannon's Coding Theorem</h4>
                        <p style="font-size: 0.8em;">To encode data from distribution P, we need at least H[P] <span class="tooltip">"nats"<span class="tooltiptext">A "nat" (natural unit of information) is the unit of information entropy based on natural logarithms and powers of e, rather than powers of 2. One nat is the information content of an event when the probability of that event occurring is 1/e. One nat equals 1/ln(2) ‚âà 1.44 shannons (bits) or 1/ln(10) ‚âà 0.434 hartleys.</span></span></p>
                        <div class="math-block" style="font-size: 0.75em;">
                            $$H[P] = \sum_j -P(j) \log P(j)$$
                        </div>
                    </div>
                    <div class="fragment" style="font-size: 0.6em;">
                        <div class="two-column">
                            <div class="column">
                                <h4>Units of Information</h4>
                                <ul>
                                    <li><strong>Bits:</strong> Use log‚ÇÇ (binary encoding)</li>
                                    <li><strong>Nats:</strong> Use ln (natural logarithm)</li>
                                    <li>1 nat ‚âà 1.44 bits</li>
                                </ul>
                            </div>
                            <div class="column">
                                <h4>Examples</h4>
                                <ul>
                                    <li>Fair coin: H = 1 bit</li>
                                    <li><span class="tooltip">Biased coin (90% heads): H ‚âà 0.47 bits<span class="tooltiptext">With a 90% biased coin, an optimal coding scheme might use: '0' for heads (90% chance, 1 bit) and '10' for tails (10% chance, 2 bits). Average bits per flip = 0.9√ó1 + 0.1√ó2 = 1.1 bits. Even better schemes can approach the theoretical minimum of 0.47 bits by encoding multiple flips together, assigning shorter codes to common sequences like "HHH" and longer codes to rare sequences like "TTT".</span></span></li>
                                    <li>Uniform over 10 classes: H ‚âà 3.32 bits</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Entropy: Model vs Reality</h2>
                    <div class="fragment">
                        <h4>Definition</h4>
                        <p>Expected surprisal when using wrong distribution Q to encode P:</p>
                        <div class="math-block" style="font-size: 0.75em;">
                            $$H(P, Q) = -\sum_j P(j) \log Q(j)$$
                        </div>
                    </div>
                    <div class="fragment">
                        <div class="two-column" style="font-size: 0.65em;">
                            <div class="column">
                                <h4>Key Properties</h4>
                                <ul>
                                    <li>$H(P, Q) \geq H(P)$ (always)</li>
                                    <li>$H(P, Q) = H(P)$ only when $P = Q$</li>
                                    <li>Extra bits needed: $H(P, Q) - H(P)$ <span class="tooltip">(KL divergence)<span class="tooltiptext">The Kullback-Leibler (KL) divergence D_KL(P||Q) = H(P,Q) - H(P) measures the "extra bits" needed when using distribution Q to encode data from distribution P. It's always non-negative and equals zero only when P = Q. In machine learning, it quantifies how much our model's predicted distribution Q differs from the true distribution P.</span></span></li>
                                </ul>
                            </div>
                            <div class="column">
                                <h4>In Classification</h4>
                                <ul>
                                    <li>P = true distribution (one-hot labels)</li>
                                    <li>Q = model's predictions (softmax output)</li>
                                    <li>Loss = How many extra bits to encode truth using model</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Entropy Calculator</h2>
                    <div class="interactive-demo" id="entropy-demo">
                        <div id="entropy-visualization"></div>
                        <div class="controls-fixed" style="margin-top: 10px;">
                            <label style="display: flex; align-items: center; gap: 8px; font-size: 0.85em;">
                                Distribution type: 
                                <select id="dist-type" style="padding: 4px 8px; border: 2px solid #EEEEEE; border-radius: 4px; cursor: pointer;">
                                    <option value="uniform">Uniform</option>
                                    <option value="peaked">Peaked</option>
                                    <option value="binary">Binary</option>
                                    <option value="custom">Custom</option>
                                </select>
                            </label>
                            <span id="entropy-value" style="font-size: 0.9em; color: #10099F; font-weight: bold;">H = 2.30 nats</span>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.85em;">
                        <p>Maximum entropy = uniform distribution (maximum uncertainty)</p>
                        <p>Minimum entropy = deterministic (no uncertainty)</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Entropy Loss</h2>
                    <div class="interactive-demo" id="cross-entropy-demo" style="max-width: 900px; margin: 0 auto;">
                        <div id="loss-visualization" style="width: 100%;"></div>
                    </div>

                    <!-- Add three-column layout for insights -->
                    <div style="margin-top: 10px; display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 0.5em;">
                        <!-- Left column: Key Insights -->
                        <div style="padding: 8px; border-radius: 4px;">
                            <div style="font-weight: bold; font-size: 0.8em; margin-bottom: 6px;">Key Insights</div>
                            <ul style="font-size: 0.7em; line-height: 1.4;">
                                <li>Loss approaches 0 as P(true class) ‚Üí 1</li>
                                <li>Loss approaches ‚àû as P(true class) ‚Üí 0</li>
                                <li>Loss = -log(P(true class))</li>
                                <li>Green bar indicates the true class</li>
                            </ul>
                        </div>
                        
                        <!-- Middle column: Loss Values -->
                        <div style="padding: 8px; border-radius: 4px;">
                            <div style="font-weight: bold; font-size: 0.8em; margin-bottom: 6px;">Loss Values</div>
                            <ul style="font-size: 0.7em; line-height: 1.4;">
                                <li>Perfect prediction: L = 0</li>
                                <li>Random (3 classes): L ‚âà 1.10</li>
                                <li>Wrong with confidence: L ‚Üí ‚àû</li>
                            </ul>
                        </div>
                        
                        <!-- Right column: Formula -->
                        <div style="padding: 8px; border-radius: 4px;">
                            <div style="font-weight: bold; font-size: 0.8em; margin-bottom: 6px;">Formula</div>
                            <div class="math-block" style="font-size: 0.55em;">
                                $$L = -\sum_{j=1}^q y_j \log \hat{y}_j = -\log \hat{y}_c\quad \text{where } c \text{ is the true class}$$
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Loss Function Properties</h2>
                    <div class="two-column" style="font-size: 0.65em;">
                        <div class="column">
                            <h4>Why Cross-Entropy?</h4>
                            <ul>
                                <li class="fragment">Convex optimization</li>
                                <li class="fragment">Strong gradients for wrong predictions</li>
                                <li class="fragment">Probabilistic interpretation</li>
                                <li class="fragment">Maximum likelihood equivalence</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Loss Values</h4>
                            <ul>
                                <li>Perfect: L = 0</li>
                                <li>Random (3 classes): L ‚âà 1.10</li>
                                <li>Random (4 classes): L ‚âà 1.39</li>
                                <li>Wrong with confidence: L ‚Üí ‚àû</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Softmax and Cross-Entropy Loss</h2>
                    <p>Deriving the loss step by step</p>
                    <div class="fragment">
                        <p style="font-size: 0.75em;"><strong>Recall:</strong> $o_j$ are the raw outputs (logits) from our linear model before softmax</p>
                    </div>
                    <div class="fragment">
                        <h4>Starting from the definition:</h4>
                        <div class="math-block" style="font-size: 0.65em;">
                            $$\begin{aligned}
                            l(\mathbf{y}, \hat{\mathbf{y}}) &= -\sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)}\\
                            &= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
                            &= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j
                            \end{aligned}$$
                        </div>
                    </div>
                    <div class="fragment">
                        <p style="font-size: 0.85em;">Since $\sum_j y_j = 1$ for one-hot labels</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient Computation</h2>
                    <p>The gradient of cross-entropy loss with softmax:</p>
                    <div class="fragment">
                        <h4>Taking the derivative:</h4>
                        <div class="math-block" style="font-size: 0.55em;">
                            $$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j$$
                        </div>
                    </div>
                    <div class="fragment">
                        <div class="math-block" style="font-size: 0.55em;">
                            $$\frac{\partial L}{\partial o_j} = \mathrm{softmax}(\mathbf{o})_j - y_j = \hat{y}_j - y_j$$
                        </div>
                    </div>
                    <div class="two-column mt-md" style="font-size: 0.55em;">
                        <div class="column fragment">
                            <p><strong>Remarkably simple!</strong></p>
                            <ul>
                                <li>Gradient = predicted - actual</li>
                                <li>Same as in linear regression!</li>
                                <li>Enables efficient backpropagation</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <div class="emphasis-box">
                                <p>This is a <span class="tooltip">property of exponential family models<span class="tooltiptext">Exponential family distributions (like multinomial/categorical) have the special property that when combined with their natural loss functions (like cross-entropy), the gradient simplifies to prediction - target. This elegant mathematical relationship makes optimization particularly efficient and is why we see this same gradient form in logistic regression, softmax classification, and other exponential family models.</span></span>!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the gradient of cross-entropy loss with respect to logit o_j when using softmax?",
                        "type": "single",
                        "options": [
                            {
                                "text": "‚àÇL/‚àÇo_j = y_j",
                                "correct": false,
                                "explanation": "This would be just the label, missing the prediction component."
                            },
                            {
                                "text": "‚àÇL/‚àÇo_j = ≈∑_j - y_j",
                                "correct": true,
                                "explanation": "Correct! The gradient is simply the difference between predicted probability and true label."
                            },
                            {
                                "text": "‚àÇL/‚àÇo_j = -log(≈∑_j)",
                                "correct": false,
                                "explanation": "This is the loss value itself, not its gradient."
                            },
                            {
                                "text": "‚àÇL/‚àÇo_j = exp(o_j)",
                                "correct": false,
                                "explanation": "This is just the exponential of the logit, not the gradient."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When processing a minibatch with vectorization, how is softmax applied?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Applied once to the entire matrix",
                                "correct": false,
                                "explanation": "This would normalize across all examples, which is incorrect."
                            },
                            {
                                "text": "Applied rowwise to each example",
                                "correct": true,
                                "explanation": "Correct! Each row (example) gets its own probability distribution that sums to 1."
                            },
                            {
                                "text": "Applied columnwise to each class",
                                "correct": false,
                                "explanation": "This would normalize across examples for each class, which doesnt make sense."
                            },
                            {
                                "text": "Applied elementwise like ReLU",
                                "correct": false,
                                "explanation": "Softmax needs to normalize across classes, not elementwise."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we use exponential in the softmax function?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "It ensures all outputs are positive",
                                "correct": true,
                                "explanation": "Correct! exp(x) is always positive for any real x."
                            },
                            {
                                "text": "It preserves the relative ordering of inputs",
                                "correct": true,
                                "explanation": "Correct! exp() is monotonic, so argmax is preserved."
                            },
                            {
                                "text": "It makes computation faster",
                                "correct": false,
                                "explanation": "Exponential is actually computationally expensive."
                            },
                            {
                                "text": "It has nice gradient properties",
                                "correct": true,
                                "explanation": "Correct! Combined with cross-entropy, it gives simple gradients."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 4: Implementation Details -->
            <section>
                <section>
                    <h2 class="truncate-title">Vectorized Implementation</h2>
                    <p>Processing entire minibatches efficiently</p>
                    <div class="fragment">
                        <h4>Matrix-Matrix Product Dominance</h4>
                        <div class="math-block" style="font-size: 0.8em;">
                            $$\begin{aligned}
                            \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}\\
                            \hat{\mathbf{Y}} &= \mathrm{softmax}(\mathbf{O})
                            \end{aligned}$$
                        </div>
                    </div>
                    <div class="fragment two-column">
                        <div class="column">
                            <h4>Dimensions</h4>
                            <ul style="font-size: 0.65em;">
                                <li>$\mathbf{X} \in \mathbb{R}^{n \times d}$ (n samples, d features)</li>
                                <li>$\mathbf{W} \in \mathbb{R}^{d \times q}$ (d features, q classes)</li>
                                <li>$\mathbf{b} \in \mathbb{R}^{1 \times q}$ (broadcast to n rows)</li>
                                <li>$\mathbf{O} \in \mathbb{R}^{n \times q}$ (n samples, q logits each)</li>
                            </ul>
                        </div>
                        <div class="column" style="display: flex; align-items: center;">
                            <div class="emphasis-box">
                                <p>GPU acceleration: Matrix multiplication is highly optimized!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Efficiency</h2>
                    <div class="fragment">
                        <h4>Why Vectorization Matters</h4>
                        <ul>
                            <li><strong>BLAS/cuBLAS:</strong> <span class="tooltip">Optimized matrix libraries<span class="tooltiptext">BLAS (Basic Linear Algebra Subprograms) and cuBLAS (CUDA BLAS) are highly optimized libraries for matrix operations. They use assembly-level optimizations, vectorized instructions (SIMD), and specialized algorithms that can be 10-100x faster than naive implementations. cuBLAS specifically leverages GPU tensor cores for even greater acceleration.</span></span></li>
                            <li><strong>Cache efficiency:</strong> <span class="tooltip">Better memory access patterns<span class="tooltiptext">Vectorized operations access memory in contiguous blocks, maximizing CPU/GPU cache hits. Processing data in batches means we load weights once and reuse them across many examples, rather than loading weights repeatedly for individual samples. This dramatically reduces memory bandwidth bottlenecks.</span></span></li>
                            <li><strong>Parallelization:</strong> <span class="tooltip">GPU cores work simultaneously<span class="tooltiptext">Modern GPUs have thousands of cores that can process different elements of the matrix multiplication in parallel. For example, an RTX 4090 has 16,384 CUDA cores. Each core can handle different elements of the output matrix simultaneously, turning an O(n¬≥) sequential operation into an O(n) parallel one.</span></span></li>
                        </ul>
                    </div>
                    <div class="fragment">
                        <h4>Example: Fashion-MNIST</h4>
                        <table style="margin: 20px auto; font-size: 0.85em;">
                            <thead>
                                <tr>
                                    <th>Operation</th>
                                    <th>FLOPs</th>
                                    <th>Time (ms)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>XW (batch=256)</td>
                                    <td>~40M</td>
                                    <td>0.5</td>
                                </tr>
                                <tr>
                                    <td>Softmax</td>
                                    <td>~25K</td>
                                    <td>0.1</td>
                                </tr>
                                <tr>
                                    <td>Loss</td>
                                    <td>~5K</td>
                                    <td>0.05</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment">
                        <p><strong>Bottleneck:</strong> Matrix multiplication dominates computation!</p>
                    </div>
                </section>

                <section class="compact-slide">
                    <h2 class="truncate-title">Numerical Stability</h2>
                    <p>The log-sum-exp trick</p>
                    <div class="fragment">
                        <p><strong>Problem:</strong> exp(o) can overflow for large o</p>
                        <div style="background: #FFF3E0; padding: 10px; border-radius: 5px; margin: 10px 0;">
                            <code style="color: #E65100;">exp(100) ‚âà 2.7 √ó 10‚Å¥¬≥ ‚Üí overflow!</code><br>
                            <code style="color: #E65100;">exp(-100) ‚âà 3.7 √ó 10‚Åª‚Å¥‚Å¥ ‚Üí underflow!</code>
                        </div>
                    </div>
                    <div class="fragment">
                        <div class="two-column">
                            <div class="column" style="font-size: 0.65em;">
                                <p><strong>Solution:</strong> Subtract maximum before exponentiating</p>
                                <div class="math-block">
                                    $$\hat{y}_j = \frac{\exp(o_j - \max_k o_k)}{\sum_{i=1}^q \exp(o_i - \max_k o_k)}$$
                                </div>
                            </div>
                            <div class="column" style="font-size: 0.65em;">
                                <p><strong>Why it works:</strong></p>
                                <ul>
                                    <li>Mathematically equivalent (constant cancels)</li>
                                    <li>Largest exponential becomes exp(0) = 1</li>
                                    <li>All values in safe range [-‚àû, 0]</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Numerical Stability Deep Dive</h2>
                    <div class="interactive-demo" id="numerical-stability-demo">
                        <div id="stability-visualization"></div>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Mathematical Proof ($m = max(o)$)</h4>
                        <div class="math-block" style="font-size: 0.55em;">
                            $$\begin{aligned}
                            \frac{\exp(o_j)}{\sum_k \exp(o_k)} &= \frac{\exp(o_j) \cdot \exp(-m)}{\sum_k \exp(o_k) \cdot \exp(-m)}\\
                            &= \frac{\exp(o_j - m)}{\sum_k \exp(o_k - m)}
                            \end{aligned}$$
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Deep Fried Convnets - Yang et al. 2015", "url": "https://arxiv.org/abs/1412.7149"}, {"text": "Structured Matrix Methods - Sindhwani et al. 2015", "url": "https://arxiv.org/abs/1505.07570"}, {"text": "Quaternion Networks - Zhang et al. 2021", "url": "https://arxiv.org/abs/2101.08848"}]'>
                    <h2 class="truncate-title">Computational Optimizations</h2>
                    <p>Reducing the cost of large classifications</p>
                    <div class="fragment">
                        <h4>The Challenge</h4>
                        <ul>
                            <li>Cost: $\mathcal{O}(dq)$ for d inputs, q outputs</li>
                            <li>ImageNet: d=2048, q=1000 ‚Üí 2M parameters!</li>
                            <li>Language models: q can be 50,000+ (vocabulary size)</li>
                        </ul>
                    </div>
                    <div class="fragment">
                        <h4>Optimization Techniques</h4>
                        <table style="font-size: 0.75em;">
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Approach</th>
                                    <th>Cost</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Deep Fried Convnets</td>
                                    <td>Fourier transforms</td>
                                    <td>$\mathcal{O}(d \log q)$</td>
                                </tr>
                                <tr>
                                    <td>Structured Matrices</td>
                                    <td>Low-rank factorization</td>
                                    <td>$\mathcal{O}(dr + rq)$</td>
                                </tr>
                                <tr>
                                    <td>Quaternion Decomposition</td>
                                    <td>Alternative number system</td>
                                    <td>$\mathcal{O}(dq/n)$</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Softmax Jacobian Matrix</h2>
                    <p>Understanding the full gradient structure</p>
                    <div class="fragment">
                        <h4><span class="tooltip">Derivative of Softmax and Jacobian Matrix<span class="tooltiptext">The Jacobian matrix shows how each output probability changes with respect to each input logit. Understanding this structure helps us see why softmax + cross-entropy has such clean gradients and enables efficient backpropagation in neural networks.</span></span></h4>
                        <div class="math-block" style="font-size: 0.65em;">
                            $$\frac{\partial \hat{y}_i}{\partial o_j} = \begin{cases}
                            \hat{y}_i(1 - \hat{y}_i) & \text{if } i = j\\
                            -\hat{y}_i \hat{y}_j & \text{if } i \neq j
                            \end{cases}$$
                            
                            $$\mathbf{J} = \begin{bmatrix}
                            \hat{y}_1(1-\hat{y}_1) & -\hat{y}_1\hat{y}_2 & \cdots & -\hat{y}_1\hat{y}_q\\
                            -\hat{y}_2\hat{y}_1 & \hat{y}_2(1-\hat{y}_2) & \cdots & -\hat{y}_2\hat{y}_q\\
                            \vdots & \vdots & \ddots & \vdots\\
                            -\hat{y}_q\hat{y}_1 & -\hat{y}_q\hat{y}_2 & \cdots & \hat{y}_q(1-\hat{y}_q)
                            \end{bmatrix}$$
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="font-size: 0.75em;">
                        <p>But with cross-entropy loss, this <span class="tooltip">simplifies<span class="tooltiptext">We start with the chain rule: ‚àÇL/‚àÇo_j = Œ£·µ¢ (‚àÇL/‚àÇ≈∑·µ¢)(‚àÇ≈∑·µ¢/‚àÇo_j). The first term gives us ‚àÇL/‚àÇ≈∑·µ¢ = -y·µ¢/≈∑·µ¢ from cross-entropy loss. The second term is the complex Jacobian matrix shown above. When we multiply these together: for the diagonal terms (i=j), we get -y‚±º/≈∑‚±º √ó ≈∑‚±º(1-≈∑‚±º) = -y‚±º(1-≈∑‚±º). For off-diagonal terms (i‚â†j), we get -y·µ¢/≈∑·µ¢ √ó (-≈∑·µ¢≈∑‚±º) = y·µ¢≈∑‚±º. Summing over all i: ‚àÇL/‚àÇo_j = -y‚±º(1-≈∑‚±º) + Œ£·µ¢‚â†‚±º y·µ¢≈∑‚±º = -y‚±º + y‚±º≈∑‚±º + ≈∑‚±ºŒ£·µ¢‚â†‚±º y·µ¢ = -y‚±º + ≈∑‚±º(y‚±º + Œ£·µ¢‚â†‚±º y·µ¢) = -y‚±º + ≈∑‚±º(1) = ≈∑‚±º - y‚±º. The complex Jacobian terms perfectly cancel out because cross-entropy is the natural "conjugate" loss for softmax!</span></span> to $\hat{\mathbf{y}} - \mathbf{y}$!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Loop Structure</h2>
                    <pre><code class="python">for epoch in range(num_epochs):
    for X_batch, y_batch in data_loader:
        # Forward pass
        logits = X_batch @ W + b
        y_hat = softmax(logits)
        
        # Compute loss
        loss = cross_entropy(y_hat, y_batch)
        
        # Backward pass
        gradients = compute_gradients(loss)
        
        # Update parameters
        W -= learning_rate * grad_W
        b -= learning_rate * grad_b</code></pre>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.75em;">
                        <p>Minibatch SGD enables efficient training on large datasets</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Classification Demo</h2>
                    <div class="interactive-demo" id="classification-boundary-demo">
                        <div id="boundary-visualization"></div>
                        <div class="controls-fixed" style="margin-top: 12px;">
                            <label style="display: flex; align-items: center; gap: 6px; font-size: 0.75em; position: relative; z-index: 103;">
                                Add Class: 
                                <select id="class-selector" style="padding: 4px 8px; border: 2px solid #EEEEEE; border-radius: 4px; font-size: 0.75em; cursor: pointer; position: relative; z-index: 103;">
                                    <option value="0">Class 1 (Green)</option>
                                    <option value="1">Class 2 (Blue)</option>
                                </select>
                            </label>
                            <button id="add-points" style="background: #10099F; color: white; border: none; padding: 5px 10px; border-radius: 4px; cursor: pointer; font-size: 0.75em; position: relative; z-index: 103;">Add Random Points</button>
                            <button id="train-model" style="background: #10099F; color: white; border: none; padding: 5px 10px; border-radius: 4px; cursor: pointer; font-size: 0.75em; position: relative; z-index: 103;">Train Model</button>
                            <button id="reset-demo" style="background: #10099F; color: white; border: none; padding: 5px 10px; border-radius: 4px; cursor: pointer; font-size: 0.75em; position: relative; z-index: 103;">Reset</button>
                            <label style="display: flex; align-items: center; gap: 6px; font-size: 0.75em; position: relative; z-index: 103;">
                                Learning Rate: 
                                <input type="range" id="lr-slider" min="0.01" max="1" step="0.01" value="0.1" style="width: 100px; cursor: pointer;">
                                <span id="lr-value" style="font-family: monospace; min-width: 35px; color: #10099F; font-weight: bold; font-size: 0.9em;">0.1</span>
                            </label>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we use the log-sum-exp trick in softmax computation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make computation faster",
                                "correct": false,
                                "explanation": "It actually adds a subtraction step, so its not primarily for speed."
                            },
                            {
                                "text": "To prevent numerical overflow",
                                "correct": true,
                                "explanation": "Correct! Subtracting the maximum prevents exp() from producing infinity for large inputs."
                            },
                            {
                                "text": "To improve model accuracy",
                                "correct": false,
                                "explanation": "It maintains numerical precision but doesnt change the mathematical result."
                            },
                            {
                                "text": "To reduce memory usage",
                                "correct": false,
                                "explanation": "Memory usage remains similar; its about numerical stability."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which operation dominates the computational cost in softmax regression?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Computing the softmax function",
                                "correct": false,
                                "explanation": "Softmax is relatively cheap - just exp and division operations."
                            },
                            {
                                "text": "Matrix multiplication XW",
                                "correct": true,
                                "explanation": "Correct! The O(ndq) matrix multiplication dominates, especially for large d and q."
                            },
                            {
                                "text": "Computing the loss function",
                                "correct": false,
                                "explanation": "Loss computation is just a logarithm and sum - very cheap."
                            },
                            {
                                "text": "Adding the bias term",
                                "correct": false,
                                "explanation": "Bias addition is just broadcasting - O(nq) operations."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes the gradient ‚àÇL/‚àÇo = ≈∑ - y so special?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "It is computationally simple",
                                "correct": true,
                                "explanation": "Correct! Just a subtraction, no complex operations needed."
                            },
                            {
                                "text": "It is the same form as linear regression",
                                "correct": true,
                                "explanation": "Correct! Both have the form: prediction - target."
                            },
                            {
                                "text": "It requires the Jacobian matrix",
                                "correct": false,
                                "explanation": "The Jacobian simplifies away when combined with cross-entropy."
                            },
                            {
                                "text": "It is a property of exponential family models",
                                "correct": true,
                                "explanation": "Correct! This simple gradient form is characteristic of exponential family distributions."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 5: Fashion-MNIST Dataset & Practice -->
            <section>
                <section data-sources='[{"text": "Fashion-MNIST: a Novel Image Dataset", "url": "https://arxiv.org/abs/1708.07747"}, {"text": "MNIST Database", "url": "https://en.wikipedia.org/wiki/MNIST_database"}]'>
                    <h2 class="truncate-title">Fashion-MNIST Dataset</h2>
                    <div class="dataset-overview">
                        <div class="two-column">
                            <div class="column fragment" style="font-size: 0.65em;">
                                <p><strong>Dataset Statistics:</strong></p>
                                <ul>
                                    <li>60,000 training images</li>
                                    <li>10,000 test images</li>
                                    <li>28√ó28 grayscale pixels</li>
                                    <li>10 clothing categories</li>
                                    <li>Drop-in replacement for MNIST</li>
                                </ul>
                            </div>
                            <div class="column fragment">
                                <img src="images/fashion_mnist.png" alt="Fashion-MNIST sample images" style="width: 80%; height: auto; border-radius: 8px;">
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-md" style="font-size: 0.75em;">
                            <p>A more realistic benchmark than MNIST for modern algorithms</p>
                        </div>
                    </div>
                </section>

                <!-- Historical Context: MNIST -->
                <section data-sources='[{"text": "Gradient-Based Learning Applied to Document Recognition - LeCun et al. 1998", "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"}, {"text": "LeNet-5 Convolutional Neural Networks", "url": "http://yann.lecun.com/exdb/lenet/"}]'>
                    <h2 class="truncate-title">The Original MNIST Dataset</h2>
                    <div class="fragment">
                        <h4>Historical Context (1990s)</h4>
                        <ul>
                            <li>Released for handwritten digit recognition</li>
                            <li>60,000 training + 10,000 test images</li>
                            <li>28√ó28 pixel resolution</li>
                            <li>Key component in automating USPS letter sorting</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <div class="two-column">
                            <div class="column" style="font-size: 0.65em;">
                                <h4>Computing Environment of 1995</h4>
                                <div style="background: #f5f5f5; padding: 15px; border-radius: 8px; font-size: 0.85em;">
                                    <strong>Sun SPARCStation 5:</strong>
                                    <ul style="margin: 5px 0;">
                                        <li>64MB RAM (state of the art!)</li>
                                        <li>5 MFLOPs processing power</li>
                                        <li>AT&T Bell Laboratories standard</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="column" style="font-size: 0.65em;">
                                <h4>Key Achievements</h4>
                                <ul style="font-size: 0.85em;">
                                    <li><strong>LeNet-5:</strong> Convolutional networks</li>
                                    <li><strong>SVMs with invariances:</strong> Kernel methods</li>
                                    <li><strong>Tangent distance classifiers:</strong> Geometric transforms</li>
                                    <li>All achieved <strong>&lt;1% error rate</strong></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Why MNIST Became Obsolete -->
                <section>
                    <h2 class="truncate-title">Why MNIST Became Obsolete</h2>
                    <div class="fragment">
                        <h4>The Problem: Too Easy!</h4>
                        <ul>
                            <li>Simple models achieve &gt;95% accuracy</li>
                            <li>Cannot distinguish strong from weak algorithms</li>
                            <li>Allows unrealistically high accuracy levels</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <div class="two-column">
                            <div class="column" style="font-size: 0.65em;">
                                <h4>Algorithmic Bias</h4>
                                <p>MNIST skewed development toward:</p>
                                <ul>
                                    <li><span class="tooltip">Active set methods<span class="tooltiptext">Optimization techniques that iteratively work with a subset of constraints or variables, adding/removing them based on optimality conditions. Popular in quadratic programming and support vector machines, these methods were well-suited to MNIST's clean, separable data but may struggle with noisy, complex datasets.</span></span></li>
                                    <li><span class="tooltip">Boundary-seeking algorithms<span class="tooltiptext">Machine learning algorithms that primarily focus on identifying and refining decision boundaries between different classes. Examples include SVMs and perceptrons. While effective on MNIST's clear digit boundaries, they may overfit to simple patterns and fail to capture complex feature representations needed for real-world data.</span></span></li>
                                    <li>Methods that exploit clean datasets</li>
                                </ul>
                            </div>
                            <div class="column" style="font-size: 0.65em;">
                                <h4>Modern Alternative: ImageNet</h4>
                                <ul>
                                    <li>Much more challenging benchmark</li>
                                    <li>But too large for teaching (14+ million images)</li>
                                    <li>Training takes days/weeks</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>MNIST now serves as a sanity check, not a benchmark</p>
                    </div>
                </section>

                <!-- Fashion-MNIST as the Solution -->
                <section data-sources='[{"text": "Fashion-MNIST - Xiao, Rasul, Vollgraf 2017", "url": "https://arxiv.org/abs/1708.07747"}]'>
                    <h2 class="truncate-title">Fashion-MNIST: The Modern Alternative</h2>
                    <div class="fragment" style="font-size: 0.65em;">
                        <h4>Released in 2017</h4>
                        <ul>
                            <li>Authors: Xiao, Rasul, and Vollgraf</li>
                            <li>Zalando Research (fashion e-commerce)</li>
                            <li>Drop-in replacement for MNIST</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.65em;">
                        <h4>Design Philosophy</h4>
                        <ul>
                            <li><strong>Same structure:</strong> 60k/10k split, 28√ó28 pixels</li>
                            <li><strong>Same format:</strong> Grayscale images</li>
                            <li><strong>More challenging:</strong> Real-world clothing items</li>
                            <li><strong>Better benchmark:</strong> Distinguishes algorithms</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.65em;">
                        <h4>Why Clothing?</h4>
                        <ul>
                            <li>More complex patterns than digits</li>
                            <li>Higher intra-class variation</li>
                            <li>Practical e-commerce applications</li>
                            <li>Still manageable for teaching</li>
                        </ul>
                    </div>
                </section>

                <!-- MCQ for Historical Context -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why did MNIST become unsuitable as a benchmark dataset despite its historical importance?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Simple models achieve over 95% accuracy, making it hard to distinguish between algorithms",
                                "correct": true,
                                "explanation": "Correct! When even basic models get very high accuracy, the dataset cannot effectively benchmark algorithm improvements."
                            },
                            {
                                "text": "It skewed algorithm development toward methods that work well on clean datasets",
                                "correct": true,
                                "explanation": "Correct! MNIST led to algorithms optimized for unrealistically clean data, not real-world complexity."
                            },
                            {
                                "text": "The dataset was too small with only 60,000 training examples",
                                "correct": false,
                                "explanation": "The size wasnt the issue - Fashion-MNIST has the same number of examples and works well."
                            },
                            {
                                "text": "The 28√ó28 pixel resolution was too low for modern computer vision",
                                "correct": false,
                                "explanation": "Resolution wasnt the problem - Fashion-MNIST uses the same 28√ó28 resolution successfully."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Section 6: Implementation Details (Second Vertical Section) -->
            <section>
                <!-- Main Slide: Loading the Dataset -->
                <section>
                    <h2 class="truncate-title">Loading the Dataset</h2>
                    <p>PyTorch implementation using built-in utilities</p>
                    <pre class="fragment"><code class="python" style="font-size: 0.6em;">class FashionMNIST(d2l.DataModule):  #@save
    """The Fashion-MNIST dataset."""
    def __init__(self, batch_size=64, resize=(28, 28)):
        super().__init__()
        self.save_hyperparameters()
        trans = transforms.Compose([
            transforms.Resize(resize),
            transforms.ToTensor()
        ])
        self.train = torchvision.datasets.FashionMNIST(
            root=self.root, train=True, transform=trans, download=True)
        self.val = torchvision.datasets.FashionMNIST(
            root=self.root, train=False, transform=trans, download=True)</code></pre>
                    <div class="fragment">
                        <p style="font-size: 0.85em;"><strong>Key Features:</strong></p>
                        <ul style="font-size: 0.65em;">
                            <li>Inherits from <code>d2l.DataModule</code> for standardization</li>
                            <li>Automatic downloading and caching</li>
                            <li><span class="tooltip">Transform pipeline: Resize ‚Üí ToTensor<span class="tooltiptext">Resize images to specified dimensions, then convert PIL images to PyTorch tensors with values normalized to [0,1]</span></span></li>
                            <li>Separate train/validation splits</li>
                        </ul>
                    </div>
                </section>

                <!-- Data Shapes and Tensor Conventions -->
                <section>
                    <h2 class="truncate-title">Data Shapes and Tensor Conventions</h2>
                    <div class="fragment two-column">
                        <div class="column" style="font-size: 0.65em;">
                            <h4>Standard Image Tensor Format</h4>
                            <div class="math-block" style="font-size: 0.9em;">
                                $$\text{Tensor shape: } c \times h \times w$$
                            </div>
                            <ul>
                                <li><strong>c:</strong> Number of color channels</li>
                                <li><strong>h:</strong> Height in pixels</li>
                                <li><strong>w:</strong> Width in pixels</li>
                            </ul>
                        </div>
                        <div class="column" style="font-size: 0.65em;">
                            <h4>Channel Variations</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Grayscale (Fashion-MNIST):</strong> 1 channel</li>
                                <li><strong>RGB images:</strong> 3 channels (red, green, blue)</li>
                                <li><strong>Multispectral:</strong> 4-10 channels (e.g., Landsat: 8)</li>
                                <li><strong>Hyperspectral:</strong> 100+ channels (HyMap sensor: 126)</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Fashion-MNIST Specifics</h4>
                        <pre><code class="python">data = FashionMNIST(resize=(32, 32))
len(data.train), len(data.val)
# Output: (60000, 10000)

data.train[0][0].shape
# Output: torch.Size([1, 32, 32])</code></pre>
                    </div>
                </section>

                <!-- Text Labels and Categories -->
                <section>
                    <h2 class="truncate-title">Text Labels and Categories</h2>
                    <p>Converting numeric labels to human-readable names</p>
                    <pre class="fragment"><code class="python" style="font-size: 0.7em;">@d2l.add_to_class(FashionMNIST)  #@save
def text_labels(self, indices):
    """Return text labels."""
    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [labels[int(i)] for i in indices]</code></pre>
                    <div class="fragment mt-md">
                        <h4>Label Mapping</h4>
                        <table style="margin: 10px auto; font-size: 0.55em;">
                            <thead>
                                <tr>
                                    <th>Index</th>
                                    <th>0</th>
                                    <th>1</th>
                                    <th>2</th>
                                    <th>3</th>
                                    <th>4</th>
                                    <th>5</th>
                                    <th>6</th>
                                    <th>7</th>
                                    <th>8</th>
                                    <th>9</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Label</strong></td>
                                    <td>T-shirt</td>
                                    <td>Trouser</td>
                                    <td>Pullover</td>
                                    <td>Dress</td>
                                    <td>Coat</td>
                                    <td>Sandal</td>
                                    <td>Shirt</td>
                                    <td>Sneaker</td>
                                    <td>Bag</td>
                                    <td>Ankle boot</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.75em;">
                        <p>Human-readable labels essential for debugging and visualization</p>
                    </div>
                </section>

                <!-- DataLoader Implementation -->
                <section>
                    <h2 class="truncate-title">DataLoader Implementation</h2>
                    <p>Efficient minibatch iteration</p>
                    <div class="two-column">
                        <div class="column">
                            <h4>DataLoader Setup</h4>
                            <pre class="fragment"><code class="python" style="font-size: 0.65em;">@d2l.add_to_class(FashionMNIST)  #@save
def get_dataloader(self, train):
    data = self.train if train else self.val
    return torch.utils.data.DataLoader(
        data, 
        self.batch_size, 
        shuffle=train,
        num_workers=self.num_workers
    )</code></pre>
                        </div>
                        <div class="column">
                            <div class="fragment mt-md">
                                <h4>Reading a Minibatch</h4>
                                <pre><code class="python" style="font-size: 0.65em;">X, y = next(iter(data.train_dataloader()))
print(X.shape, y.shape)
# Output: torch.Size([64, 1, 32, 32]) torch.Size([64])</code></pre>
                            </div>
                        </div>
                    </div>
                    <div class="fragment">
                        <p><strong>Key Parameters:</strong></p>
                        <ul style="font-size: 0.85em;">
                            <li><strong>batch_size:</strong> 64 images per iteration</li>
                            <li><strong>shuffle:</strong> Randomize training data</li>
                            <li><strong>num_workers:</strong> Parallel data loading</li>
                        </ul>
                    </div>
                </section>

                <!-- Performance and Timing -->
                <section>
                    <h2 class="truncate-title">Performance and Timing</h2>
                    <p>I/O efficiency in deep learning pipelines</p>
                    <pre class="fragment"><code class="python" style="font-size: 0.7em;">tic = time.time()
for X, y in data.train_dataloader():
    continue
f'{time.time() - tic:.2f} sec'
# Output: '4.69 sec'</code></pre>
                    <div class="fragment mt-md" style="font-size: 0.55em;">
                        <div class="two-column">
                            <div class="column">
                                <h4>Performance Analysis</h4>
                                <ul>
                                    <li>~4.69 seconds to iterate 60,000 images</li>
                                    <li>~78 microseconds per image</li>
                                    <li>Built-in loader is reasonably fast</li>
                                </ul>
                            </div>
                            <div class="column">
                                <h4>Why I/O Isn't a Bottleneck</h4>
                                <ul>
                                    <li>Neural network processing takes much longer</li>
                                    <li>GPU computation dominates training time</li>
                                    <li>Data loading runs in parallel with training</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box">
                        <p>Good enough that training won't be I/O constrained</p>
                    </div>
                </section>

                <!-- MCQ: Tensor Conventions -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the correct tensor shape convention for images in PyTorch?",
                        "type": "single",
                        "options": [
                            {
                                "text": "height √ó width √ó channels",
                                "correct": false,
                                "explanation": "This is the TensorFlow/Keras convention, not PyTorch."
                            },
                            {
                                "text": "channels √ó height √ó width",
                                "correct": true,
                                "explanation": "Correct! PyTorch uses c √ó h √ó w convention, with channels first."
                            },
                            {
                                "text": "batch √ó pixels",
                                "correct": false,
                                "explanation": "This would be for flattened images, not the standard 2D format."
                            },
                            {
                                "text": "width √ó height √ó channels",
                                "correct": false,
                                "explanation": "Wrong order - width typically comes after height."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Visualization Utilities -->
                <section>
                    <h2 class="truncate-title">Visualization Utilities</h2>
                    <p>Inspecting data for quality and correctness</p>
                    <div class="fragment two-column" style="font-size: 0.8em;">
                        <div class="column">
                            <h4>The show_images Interface</h4>
                            <pre><code class="python" style="font-size: 0.55em;">def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """Plot a list of images."""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # Image tensor
            ax.imshow(img.numpy())
        else:
            # PIL image
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes</code></pre>
                        </div>
                        <div class="column">
                            <h4>Visualizing a Batch</h4>
                            <pre><code class="python" style="font-size: 0.55em;">@d2l.add_to_class(FashionMNIST)  #@save
def visualize(self, batch, nrows=1, ncols=8, labels=[]):
    X, y = batch
    if not labels:
        labels = self.text_labels(y)
    d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)

batch = next(iter(data.val_dataloader()))
data.visualize(batch)</code></pre>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.75em;">
                        <p><strong>Always visualize your data!</strong> Humans excel at spotting oddities</p>
                    </div>
                </section>

                <!-- Preparing the Data (moved from original) -->
                <section>
                    <h2 class="truncate-title">Data Preprocessing Pipeline</h2>
                    <div class="three-column" style="font-size: 0.6em; text-align: left;">
                        <div class="column fragment">
                            <h4>1. Normalization</h4>
                            <p>Scale pixel values to [0, 1]:</p>
                            <div class="math-block">$$x_{\text{normalized}} = \frac{x_{\text{pixel}}}{255}$$</div>
                        </div>
                        <div class="column fragment">
                            <h4>2. Flattening (for Linear Models)</h4>
                            <p>Convert 28√ó28 image to 784-dimensional vector</p>
                            <p style="font-size: 0.8em;">Note: CNNs preserve spatial structure</p>
                        </div>
                        <div class="column fragment">
                            <h4>3. Batching</h4>
                            <p>Group samples for efficient processing</p>
                            <p>Typical batch size: 64‚Äì256 samples</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Preprocessing is crucial for model performance</p>
                    </div>
                </section>

                <!-- MCQ: I/O Performance -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When training on Fashion-MNIST, why is I/O typically not a bottleneck?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Processing images with deep networks takes much longer than loading",
                                "correct": true,
                                "explanation": "Correct! Neural network computation, especially on GPUs, dominates the training time."
                            },
                            {
                                "text": "DataLoaders can run in parallel with model training",
                                "correct": true,
                                "explanation": "Correct! PyTorch DataLoaders use multiprocessing to load data while the GPU processes."
                            },
                            {
                                "text": "The entire dataset is loaded into memory at once",
                                "correct": false,
                                "explanation": "DataLoaders use iterators to load data in batches, not all at once."
                            },
                            {
                                "text": "Fashion-MNIST images are small (28√ó28 pixels)",
                                "correct": false,
                                "explanation": "While true, this alone doesnt prevent I/O bottlenecks - its the computation/loading ratio that matters."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Summary of Implementation -->
                <section>
                    <h2 class="truncate-title">Implementation Summary</h2>
                    <div class="three-column" style="font-size: 0.6em; text-align: left;">
                        <div class="column">
                            <h4>Data Structure</h4>
                            <ul style="font-size: 0.85em;">
                                <li>60,000 train / 10,000 test</li>
                                <li>Tensor shape: [1, 28, 28]</li>
                                <li>10 clothing categories</li>
                                <li>Grayscale images</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Key Components</h4>
                            <ul style="font-size: 0.85em;">
                                <li>FashionMNIST class</li>
                                <li>DataLoader for batching</li>
                                <li>Transform pipeline</li>
                                <li>Visualization utilities</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Performance Characteristics</h4>
                            <ul style="font-size: 0.85em;">
                                <li>~4.69 seconds to iterate full dataset</li>
                                <li>Efficient parallel data loading</li>
                                <li>GPU computation dominates training time</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Fashion-MNIST: A realistic yet manageable benchmark for classification</p>
                    </div>
                </section>
            </section>

            <!-- Section 7: The Base Classification Model -->
            <section>
                <!-- Main slide: Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 4: The Base Classification Model", "url": "https://d2l.ai/chapter_linear-classification/classification.html"}]'>
                    <h2 class="truncate-title">The Base Classification Model</h2>
                    <p>Building a foundation for all classification tasks</p>
                    <div class="fragment">
                        <div class="emphasis-box">
                            <p>From implementation details to reusable abstractions</p>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <h4>What we'll cover:</h4>
                        <ul>
                            <li>The <span class="tooltip">Classifier<span class="tooltiptext">A base class that provides common functionality for all classification models</span></span> base class</li>
                            <li>Validation step with loss and accuracy tracking</li>
                            <li>Accuracy computation and its importance</li>
                            <li>Why we optimize loss but care about accuracy</li>
                        </ul>
                    </div>
                </section>

                <!-- The Classifier Class -->
                <section>
                    <h2 class="truncate-title">The Classifier Class</h2>
                    <p>A base class for classification models</p>
                    <pre class="fragment"><code class="python">class Classifier(d2l.Module):  #@save
    """The base class of classification models."""
    def validation_step(self, batch):
        Y_hat = self(*batch[:-1])
        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)
        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)</code></pre>
                    <div class="fragment mt-md">
                        <h4>Key Features:</h4>
                        <ul>
                            <li>Inherits from <code>d2l.Module</code> for common functionality</li>
                            <li>Reports both loss and accuracy during validation</li>
                            <li>Updates metrics every <code>num_val_batches</code> batches</li>
                            <li>Provides averaged metrics over validation data</li>
                        </ul>
                    </div>
                </section>

                <!-- Validation Step Explained -->
                <section>
                    <h2 class="truncate-title">Understanding the Validation Step</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>What happens during validation?</h4>
                            <pre><code class="python">def validation_step(self, batch):
    # Forward pass
    Y_hat = self(*batch[:-1])
    
    # Track loss
    self.plot('loss', 
              self.loss(Y_hat, batch[-1]), 
              train=False)
    
    # Track accuracy
    self.plot('acc', 
              self.accuracy(Y_hat, batch[-1]), 
              train=False)</code></pre>
                        </div>
                        <div class="column">
                            <h4>Step-by-step breakdown:</h4>
                            <ol class="fragment" style="font-size: 0.6em;">
                                <li><strong>Forward pass:</strong> Compute predictions <code>Y_hat</code></li>
                                <li><strong>Calculate loss:</strong> Measure prediction error</li>
                                <li><strong>Calculate accuracy:</strong> Count correct predictions</li>
                                <li><strong>Plot metrics:</strong> Update visualization with <code>train=False</code></li>
                            </ol>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.6em;">
                        <p>Note: Validation metrics use <code>train=False</code> to distinguish from training metrics</p>
                    </div>
                </section>

                <!-- Optimizer Configuration -->
                <section>
                    <h2 class="truncate-title">Default Optimizer Configuration</h2>
                    <p>Stochastic Gradient Descent as the standard choice</p>
                    <pre class="fragment"><code class="python">@d2l.add_to_class(d2l.Module)  #@save
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), lr=self.lr)</code></pre>
                    <div class="fragment mt-lg">
                        <h4>Why SGD for classification?</h4>
                        <ul>
                            <li>Simple and effective for linear models</li>
                            <li>Works well with minibatches</li>
                            <li>Same optimizer we used for regression</li>
                            <li>Can be overridden for specific models</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                        <p>üí° This default can be customized in derived classes for more complex models</p>
                    </div>
                </section>

                <!-- Accuracy: The Key Metric -->
                <section>
                    <h2 class="truncate-title">Accuracy: The Metric We Care About</h2>
                    <div class="fragment" style="font-size: 0.6em;">
                        <h4>From probabilities to decisions</h4>
                        <p>Given predicted probabilities <code>y_hat</code>, we must make hard predictions</p>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.6em;">
                        <h4>Real-world examples:</h4>
                        <div class="grid-2x2" style="font-size: 0.9em;">
                            <div>
                                <strong>üìß Gmail</strong>
                                <p>Must categorize: "Primary", "Social", "Updates", "Spam"</p>
                            </div>
                            <div>
                                <strong>üè• Medical Diagnosis</strong>
                                <p>Must decide: benign or malignant</p>
                            </div>
                            <div>
                                <strong>üöó Autonomous Driving</strong>
                                <p>Must classify: pedestrian, car, cyclist, obstacle</p>
                            </div>
                            <div>
                                <strong>üè¶ Credit Approval</strong>
                                <p>Must determine: approve or reject</p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.6em;">
                        <p>Internal probabilities guide decisions, but applications need definitive choices</p>
                    </div>
                </section>

                <!-- Accuracy Computation -->
                <section>
                    <h2 class="truncate-title">Computing Accuracy</h2>
                    <p>The fraction of correct predictions</p>
                    <div class="interactive-demo" id="accuracy-demo">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; position: relative;">
                          <button id="accuracy-generate-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Generate New Batch</button>
                          <button id="accuracy-step-btn"      style="background: #2DD2C0; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Step Through</button>
                        </div>
                        <div id="accuracy-viz-container"></div>
                        <div class="accuracy-stats" style="display: flex; justify-content: space-around; margin-top: 20px; font-size: 1.1em;">
                          <div>Correct: <span id="correct-count" style="color: #2DD2C0; font-weight: bold;">0</span></div>
                          <div>Total: <span id="total-count" style="font-weight: bold;">0</span></div>
                          <div>Accuracy: <span id="accuracy-value" style="color: #10099F; font-weight: bold;">0.0%</span></div>
                        </div>
                      </div>
                      
                </section>

                <!-- Accuracy Implementation -->
                <section>
                    <h2 class="truncate-title">Accuracy Implementation</h2>
                    <pre><code class="python">@d2l.add_to_class(Classifier)  #@save
def accuracy(self, Y_hat, Y, averaged=True):
    """Compute the number of correct predictions."""
    # Reshape predictions to (n_samples, n_classes)
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    
    # Get predicted class (highest probability)
    preds = Y_hat.argmax(axis=1).type(Y.dtype)
    
    # Compare predictions with ground truth
    compare = (preds == Y.reshape(-1)).type(torch.float32)
    
    # Return mean (fraction correct) or raw comparisons
    return compare.mean() if averaged else compare</code></pre>
                    <div class="fragment mt-md">
                        <h4>Step-by-step process:</h4>
                        <ol style="font-size: 0.9em;">
                            <li><strong>Reshape:</strong> Ensure proper dimensions for batch processing</li>
                            <li><strong>Argmax:</strong> Find index of highest probability for each sample</li>
                            <li><strong>Compare:</strong> Check if predictions match ground truth</li>
                            <li><strong>Average:</strong> Compute fraction of correct predictions</li>
                        </ol>
                    </div>
                </section>

                <!-- Argmax Visualization -->
                <section>
                    <h2 class="truncate-title">The Argmax Operation</h2>
                    <p>From probability distribution to class prediction</p>
                    <div class="interactive-demo" id="argmax-demo">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100; font-size: 0.6em;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Class 0: 
                                <input type="range" id="prob-0" min="0" max="100" value="20" style="width: 100px;">
                                <span id="prob-0-value" style="font-family: monospace; width: 50px;">0.20</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Class 1: 
                                <input type="range" id="prob-1" min="0" max="100" value="50" style="width: 100px;">
                                <span id="prob-1-value" style="font-family: monospace; width: 50px;">0.50</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                Class 2: 
                                <input type="range" id="prob-2" min="0" max="100" value="30" style="width: 100px;">
                                <span id="prob-2-value" style="font-family: monospace; width: 50px;">0.30</span>
                            </label>
                        </div>
                        <div id="argmax-viz-container"></div>
                        <div class="argmax-result" style="text-align: center; margin-top: 20px; font-size: 0.5em;">
                            Predicted Class: <span id="predicted-class" style="color: #10099F; font-weight: bold; font-size: 1.5em;">1</span>$\quad\text{prediction} = \arg\max_i \, p_i$
                        </div>
                    </div>
                </section>

                <!-- Accuracy vs Loss -->
                <section>
                    <h2 class="truncate-title">Why Optimize Loss, Not Accuracy?</h2>
                    <div class="two-column" style="font-size: 0.6em;">
                        <div class="column">
                            <h4>Accuracy</h4>
                            <ul>
                                <li class="fragment">‚úÖ What we ultimately care about</li>
                                <li class="fragment">‚ùå Not differentiable</li>
                                <li class="fragment">‚ùå Discrete jumps (0 or 1)</li>
                                <li class="fragment">‚ùå No gradient information</li>
                                <li class="fragment">‚úÖ Easy to interpret</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Cross-Entropy Loss</h4>
                            <ul>
                                <li class="fragment">‚úÖ Smooth and differentiable</li>
                                <li class="fragment">‚úÖ Provides gradients</li>
                                <li class="fragment">‚úÖ Continuous optimization</li>
                                <li class="fragment">‚ùå Less interpretable</li>
                                <li class="fragment">‚úÖ Correlates with accuracy</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>We optimize what we can compute gradients for, but report what we care about</p>
                    </div>
                </section>

                <!-- MCQ 1: Classifier Class -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary purpose of the Classifier base class?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To implement the forward pass for all models",
                                "correct": false,
                                "explanation": "The Classifier class does not implement the forward pass; that is left to specific model implementations."
                            },
                            {
                                "text": "To provide common functionality like validation metrics tracking",
                                "correct": true,
                                "explanation": "Correct! The Classifier class provides reusable functionality like validation_step that tracks both loss and accuracy, which is common to all classification models."
                            },
                            {
                                "text": "To define the loss function for classification",
                                "correct": false,
                                "explanation": "The loss function is not defined in the Classifier base class; it is typically defined in the specific model implementation."
                            },
                            {
                                "text": "To handle data loading and preprocessing",
                                "correct": false,
                                "explanation": "Data loading and preprocessing are handled separately, not by the Classifier base class."
                            }
                        ]
                    }'></div>
                </section>

                <!-- MCQ 2: Accuracy Computation -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we use argmax when computing accuracy from predicted probabilities?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To normalize the probabilities to sum to 1",
                                "correct": false,
                                "explanation": "Argmax does not normalize probabilities; it finds the index of the maximum value. Softmax is used for normalization."
                            },
                            {
                                "text": "To convert probabilities to a hard class prediction",
                                "correct": true,
                                "explanation": "Correct! Argmax selects the class with the highest predicted probability, converting soft probabilities into a hard classification decision needed for accuracy computation."
                            },
                            {
                                "text": "To compute the gradient for backpropagation",
                                "correct": false,
                                "explanation": "Argmax is not differentiable and cannot be used for gradient computation. It is only used for making predictions."
                            },
                            {
                                "text": "To calculate the cross-entropy loss",
                                "correct": false,
                                "explanation": "Cross-entropy loss uses the actual probability values, not the argmax. It measures the difference between predicted and true probability distributions."
                            }
                        ]
                    }'></div>
                </section>

                <!-- MCQ 3: Loss vs Accuracy -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we optimize cross-entropy loss instead of directly optimizing accuracy?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Accuracy is not important for classification tasks",
                                "correct": false,
                                "explanation": "Accuracy is actually the most important metric for classification; we just cannot optimize it directly."
                            },
                            {
                                "text": "Cross-entropy loss is easier to compute than accuracy",
                                "correct": false,
                                "explanation": "Both metrics are easy to compute. The issue is not computational complexity but differentiability."
                            },
                            {
                                "text": "Accuracy is not differentiable, so we cannot compute gradients",
                                "correct": true,
                                "explanation": "Correct! Accuracy involves discrete decisions (right or wrong) that create a non-differentiable step function. Cross-entropy provides smooth gradients needed for optimization."
                            },
                            {
                                "text": "Cross-entropy loss always gives better results than accuracy",
                                "correct": false,
                                "explanation": "Cross-entropy is a proxy for what we want to optimize. We use it because it is differentiable, not because it gives better results than accuracy."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Summary -->
                <section>
                    <h2 class="truncate-title">Summary: The Base Classification Model</h2>
                    <div class="two-column" style="font-size: 0.6em;">
                        <div class="column">
                            <h4>Key Components</h4>
                            <ul>
                                <li><strong>Classifier base class:</strong> Reusable foundation</li>
                                <li><strong>Validation step:</strong> Tracks loss and accuracy</li>
                                <li><strong>SGD optimizer:</strong> Default choice for training</li>
                                <li><strong>Accuracy metric:</strong> What we ultimately care about</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Important Concepts</h4>
                            <ul>
                                <li><strong>Hard predictions:</strong> Applications need decisions</li>
                                <li><strong>Argmax operation:</strong> Probability ‚Üí class</li>
                                <li><strong>Loss vs Accuracy:</strong> Optimize smooth, report discrete</li>
                                <li><strong>Validation tracking:</strong> Monitor generalization</li>
                            </ul>
                        </div>
                    </div>
                    <div class="emphasis-box mt-lg">
                        <p><strong>Key Insight:</strong> Classification requires both statistical optimization (loss) and practical evaluation (accuracy)</p>
                    </div>
                </section>
            </section>

            <!-- Section: Softmax Regression Implementation from Scratch -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Softmax Regression Implementation", "url": "https://d2l.ai/chapter_linear-classification/softmax-regression-scratch.html"}]'>
                    <h2 class="truncate-title">Softmax Regression Implementation from Scratch</h2>
                    <p>Building our classifier step by step</p>
                    <div class="two-column mt-lg">
                        <div class="column" style="font-size: 0.8em;">
                            <h4>What We'll Implement</h4>
                            <ul>
                                <li class="fragment">The softmax function</li>
                                <li class="fragment">Model architecture</li>
                                <li class="fragment">Cross-entropy loss</li>
                                <li class="fragment">Training loop</li>
                            </ul>
                        </div>
                        <div class="column" style="font-size: 0.8em;">
                            <h4>Key Components</h4>
                            <ul>
                                <li class="fragment">784 input features (28√ó28)</li>
                                <li class="fragment">10 output classes</li>
                                <li class="fragment">Weight matrix: 784√ó10</li>
                                <li class="fragment">Bias vector: 1√ó10</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Understanding the implementation helps you build intuition for deep learning</p>
                    </div>
                </section>


                <section>
                    <h2 class="truncate-title">Matrix Operations with Softmax</h2>
                    <p>Working with batches efficiently</p>
                    <pre><code class="language-python">def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdims=True)
    return X_exp / partition  # Broadcasting applied here</code></pre>
                    <div class="fragment">
                        <h4>Axis Operations</h4>
                        <pre><code class="language-python">X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
X.sum(0, keepdims=True)  # Sum over rows: [[5., 7., 9.]]
X.sum(1, keepdims=True)  # Sum over columns: [[6.], [15.]]</code></pre>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p><strong>Note:</strong> keepdims=True preserves dimensions for broadcasting</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Model Architecture</h2>
                    <p>Defining the network structure</p>
                    <pre><code class="language-python">class SoftmaxRegressionScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),
                              requires_grad=True)
        self.b = torch.zeros(num_outputs, requires_grad=True)

    def forward(self, X):
        X = X.reshape((-1, self.W.shape[0]))  # Flatten images
        return softmax(torch.matmul(X, self.W) + self.b)</code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p><strong>Architecture:</strong> 784 inputs ‚Üí Linear transformation ‚Üí 10 outputs ‚Üí Softmax</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Entropy Loss Implementation</h2>
                    <p>Efficient indexing for one-hot labels</p>
                    <pre><code class="language-python">def cross_entropy(y_hat, y):
    return -torch.log(y_hat[list(range(len(y_hat))), y]).mean()</code></pre>
                    <div class="fragment">
                        <h4>Example: Selecting Correct Probabilities</h4>
                        <pre><code class="language-python">y = torch.tensor([0, 2])  # True labels
y_hat = torch.tensor([[0.1, 0.3, 0.6],  # Predictions for sample 1
                      [0.3, 0.2, 0.5]])  # Predictions for sample 2
y_hat[[0, 1], y]  # Returns tensor([0.1, 0.5])</code></pre>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p><strong>Trick:</strong> Use advanced indexing to avoid loops over one-hot vectors</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Process</h2>
                    <div id="training-loop-demo" class="demo-container"></div>
                    <pre class="fragment"><code class="language-python">data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Training Results</h2>
                    <p>Loss curve over 10 epochs</p>
                    <div style="text-align: center;">
                        <img src="images/training_loss_plot.svg" alt="Training Loss Plot" style="max-width: 70%; height: auto;">
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p><strong>Observation:</strong> Loss decreases smoothly, indicating proper convergence</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Making Predictions</h2>
                    <p>From probabilities to class labels</p>
                    <pre><code class="language-python">X, y = next(iter(data.val_dataloader()))
preds = model(X).argmax(axis=1)  # Get class with highest probability
preds.shape  # torch.Size([256])</code></pre>
                    <div class="fragment">
                        <h4>The Argmax Operation</h4>
                        <div class="two-column">
                            <div class="column">
                                <p><strong>Probabilities:</strong></p>
                                <pre><code>[0.05, 0.10, 0.70, 0.15]</code></pre>
                            </div>
                            <div class="column">
                                <p><strong>Prediction:</strong></p>
                                <pre><code>argmax ‚Üí 2 (index)</code></pre>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.6em;">
                        <p><strong>Note:</strong> We train with probabilities but predict with argmax</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Analyzing Misclassifications</h2>
                    <p>Understanding model errors</p>
                    <div style="text-align: center;">
                        <img src="images/misclassified_examples.svg" alt="Misclassified Examples" style="max-width: 90%; height: auto;">
                    </div>
                    <pre class="fragment"><code class="language-python">wrong = preds.type(y.dtype) != y
X, y, preds = X[wrong], y[wrong], preds[wrong]
labels = [a+'\n'+b for a, b in zip(
    data.text_labels(y), data.text_labels(preds))]</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Key Implementation Details</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>‚ö†Ô∏è Numerical Stability</h4>
                            <ul>
                                <li>Softmax can overflow with large inputs</li>
                                <li>Log can underflow with small probabilities</li>
                                <li>Use library implementations in practice</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>üéØ Optimization Tips</h4>
                            <ul>
                                <li>Vectorize operations</li>
                                <li>Use broadcasting</li>
                                <li>Avoid explicit loops</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>Remember:</strong> This implementation is for learning. Use framework functions in production!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we use advanced indexing in the cross-entropy loss implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the code run faster on GPUs",
                                "correct": false,
                                "explanation": "While it may be faster, the main reason is to avoid explicit loops over one-hot encoded labels."
                            },
                            {
                                "text": "To avoid creating one-hot encoded vectors explicitly",
                                "correct": true,
                                "explanation": "Correct! Advanced indexing lets us directly select the probabilities for true labels without creating memory-intensive one-hot vectors."
                            },
                            {
                                "text": "To reduce memory usage during backpropagation",
                                "correct": false,
                                "explanation": "The indexing technique is primarily about efficient forward computation, not backpropagation."
                            },
                            {
                                "text": "To handle variable batch sizes automatically",
                                "correct": false,
                                "explanation": "Advanced indexing works with any batch size, but thats not why we use it here."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Concise Implementation of Softmax Regression -->
            <section>
                <section>
                    <h1 class="truncate-title">Concise Implementation of Softmax Regression</h1>
                    <p>Leveraging High-Level Deep Learning Frameworks</p>
                    <div class="emphasis-box" style="margin-top: 40px;">
                        <p>Just as frameworks simplified linear regression, they make softmax regression implementation elegant and numerically stable</p>
                    </div>
                    <p class="fragment" style="margin-top: 30px; color: #666;">
                        Let's explore how PyTorch handles the complexities for us
                    </p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Concise Implementation", "url": "https://d2l.ai/chapter_linear-classification/softmax-regression-concise.html"}]'>
                    <h2 class="truncate-title">Defining the Model</h2>
                    <pre><code class="python" data-trim>
class SoftmaxRegression(d2l.Classifier):
    """The softmax regression model."""
    def __init__(self, num_outputs, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.Flatten(),           # 4D ‚Üí 2D tensor
            nn.LazyLinear(num_outputs)  # Linear layer
        )
    
    def forward(self, X):
        return self.net(X)
                    </code></pre>
                    <div class="fragment" style="margin-top: 20px;">
                        <p style="color: #666; font-size: 0.9em;">Key components:</p>
                        <ul style="font-size: 0.85em;">
                            <li><strong>nn.Flatten</strong>: Converts [batch, 1, 28, 28] ‚Üí [batch, 784]</li>
                            <li><strong>nn.LazyLinear</strong>: Infers input dimensions automatically</li>
                            <li><strong>nn.Sequential</strong>: Chains operations together</li>
                        </ul>
                    </div>
                </section>

                <section data-sources='[{"text": "LogSumExp Trick - Wikipedia", "url": "https://en.wikipedia.org/wiki/LogSumExp"}]'>
                    <h2 class="truncate-title">Numerical Stability: The Problem</h2>
                    <div style="background: #fff5f5; padding: 20px; border-radius: 10px; border-left: 4px solid #FC8484;">
                        <h4 style="color: #FC8484; margin-top: 0;">Overflow and Underflow Issues</h4>
                        <p>The softmax function: $$\hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$</p>
                        <ul style="font-size: 0.9em; margin-top: 15px;">
                            <li><strong>Overflow</strong>: When $o_k$ is large (e.g., > 90), $\exp(o_k) > 10^{38}$</li>
                            <li><strong>Underflow</strong>: When $o_k$ is very negative, $\exp(o_k) \approx 0$</li>
                            <li><strong>Result</strong>: NaN (Not a Number) in computations</li>
                        </ul>
                    </div>
                    <div class="fragment" style="margin-top: 30px;">
                        <p style="color: #666;">Single precision floats: approximately $10^{-38}$ to $10^{38}$</p>
                        <p style="color: #FC8484; font-weight: bold;">When logits fall outside [-90, 90], instability occurs!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Numerical Stability: The Solution</h2>
                    <div style="background: #f0fff9; padding: 20px; border-radius: 10px; border-left: 4px solid #2DD2C0; font-size: 0.8em;">
                        <h4 style="color: #2DD2C0; margin-top: 0;">The Log-Sum-Exp Trick</h4>
                        <p>Subtract the maximum logit $\bar{o} = \max_k o_k$ from all entries:</p>
                        <p>$$\hat{y}_j = \frac{\exp(o_j - \bar{o})}{\sum_k \exp(o_k - \bar{o})}$$</p>
                        <div class="fragment" style="margin-top: 20px;">
                            <p style="font-size: 0.9em;">For log probabilities:</p>
                            <p>$$\log \hat{y}_j = o_j - \bar{o} - \log \sum_k \exp(o_k - \bar{o})$$</p>
                        </div>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <ul style="font-size: 0.9em; color: #666;">
                            <li>All $o_j - \bar{o} \leq 0$ (prevents overflow)</li>
                            <li>At least one term equals 0 (prevents all underflow)</li>
                            <li>Mathematically equivalent but numerically stable</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Entropy Loss Implementation</h2>
                    <pre><code class="python" data-trim>
@d2l.add_to_class(d2l.Classifier)
def loss(self, Y_hat, Y, averaged=True):
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    Y = Y.reshape((-1,))
    return F.cross_entropy(
        Y_hat, Y, 
        reduction='mean' if averaged else 'none'
    )
                    </code></pre>
                    <div class="fragment" style="margin-top: 20px; background: #f9f9f9; padding: 15px; border-radius: 8px;">
                        <p style="color: #10099F; font-weight: bold; margin-top: 0;">
                            PyTorch's F.cross_entropy combines:
                        </p>
                        <ol style="font-size: 0.9em;">
                            <li>LogSoftmax (numerically stable)</li>
                            <li>Negative log-likelihood loss</li>
                            <li>Automatic averaging over batch</li>
                        </ol>
                    </div>
                    <div class="fragment" style="margin-top: 15px;">
                        <p style="color: #666; font-size: 0.85em;">
                            <strong>Important:</strong> Pass logits directly, not softmax probabilities!
                        </p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training on Fashion-MNIST</h2>
                    <pre><code class="python" data-trim>
# Load the dataset
data = d2l.FashionMNIST(batch_size=256)

# Initialize model
model = SoftmaxRegression(num_outputs=10, lr=0.1)

# Create trainer and train
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: High-Level vs Low-Level</h2>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px; font-size: 0.6em;">
                        <div style="background: #f0f9ff; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #10099F; margin-top: 0;">Benefits of High-Level APIs</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Numerical stability handled automatically</li>
                                <li>Concise, readable code</li>
                                <li>Optimized implementations</li>
                                <li>Less prone to bugs</li>
                            </ul>
                        </div>
                        <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #FC8484; margin-top: 0;">Trade-offs</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Less control over implementation details</li>
                                <li>Harder to customize novel components</li>
                                <li>Black-box feeling can hinder debugging</li>
                                <li>Need to understand internals for research</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 30px;">
                        <p><strong>Best Practice:</strong> Understand both implementations!</p>
                        <p style="font-size: 0.9em; margin-top: 10px;">Use high-level APIs for production, but know the fundamentals for innovation</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we subtract the maximum logit in the stable softmax implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make all values positive",
                                "correct": false,
                                "explanation": "Actually, subtracting the max makes all values non-positive (‚â§ 0)."
                            },
                            {
                                "text": "To prevent exponential overflow while preserving the probability distribution",
                                "correct": true,
                                "explanation": "Correct! Subtracting the max ensures exp(o_j - max) ‚â§ 1, preventing overflow while mathematically preserving the softmax output."
                            },
                            {
                                "text": "To normalize the logits to sum to 1",
                                "correct": false,
                                "explanation": "The logits dont sum to 1; the softmax probabilities do. Subtracting a constant doesnt change the softmax output."
                            },
                            {
                                "text": "To speed up the computation",
                                "correct": false,
                                "explanation": "The stable version has similar computational cost; its about numerical stability, not speed."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 11: Generalization in Classification -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 4: Generalization", "url": "https://d2l.ai/chapter_linear-classification/generalization-classification.html"}]'>
                    <h2 class="truncate-title">Generalization in Classification</h2>
                    <div style="text-align: center; margin-top: 40px;">
                        <p style="font-size: 1.2em; color: #10099F; font-weight: bold;">From Training to Testing</p>
                    </div>
                    <div class="fragment" style="margin-top: 30px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                            <div style="background: #f0f9ff; padding: 20px; border-radius: 10px;">
                                <h4 style="color: #10099F; margin-top: 0;">What We Want</h4>
                                <p style="font-size: 0.9em;">Models that perform well on <strong>unseen data</strong></p>
                            </div>
                            <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                                <h4 style="color: #FC8484; margin-top: 0;">What We Have</h4>
                                <p style="font-size: 0.9em;">Models trained on <strong>finite training sets</strong></p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 30px;">
                        <p><strong>Key Question:</strong> How can we ensure our classifier generalizes beyond memorization?</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Fundamental Challenge</h2>
                    <div style="margin-top: 30px;">
                        <p style="color: #666; font-size: 0.9em;">Consider a classifier trained on unique high-dimensional inputs...</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; background: #f9f9f9; padding: 20px; border-radius: 10px;">
                        <h4 style="color: #10099F; margin-top: 0;">Perfect Training Accuracy ‚â† Good Model</h4>
                        <ul style="font-size: 0.8em;">
                            <li class="fragment">Can memorize entire training set on first epoch</li>
                            <li class="fragment">Simply lookup labels for seen examples</li>
                            <li class="fragment">But what about new examples?</li>
                            <li class="fragment" style="color: #FC8484;"><strong>Result:</strong> Random guessing on test data!</li>
                        </ul>
                    </div>
                    <div class="fragment" style="margin-top: 30px;">
                        <h4>Three Burning Questions:</h4>
                        <ol style="font-size: 0.8em;">
                            <li class="fragment">How many test examples for good accuracy estimates?</li>
                            <li class="fragment">What happens with repeated test set evaluation?</li>
                            <li class="fragment">Why should fitting linear models work better than memorization?</li>
                        </ol>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Set Theory: Two Types of Error</h2>
                    <div style="margin-top: 30px;">
                        <div style="background: #f0f9ff; padding: 20px; border-radius: 10px; margin-bottom: 20px; font-size: 0.7em;">
                            <h4 style="color: #10099F; margin-top: 0;">Empirical Error</h4>
                            <p style="font-size: 0.9em;">Error on our finite test set $\mathcal{D}$</p>
                            <p style="margin-top: 15px;">$$\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)})$$</p>
                        </div>
                        <div class="fragment" style="background: #fff9f0; padding: 20px; border-radius: 10px; font-size: 0.7em;">
                            <h4 style="color: #FFA05F; margin-top: 0;">Population Error</h4>
                            <p style="font-size: 0.9em;">Expected error over entire distribution</p>
                            <p style="margin-top: 15px;">$$\epsilon(f) = E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y)$$</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px; font-size: 0.7em;">
                        <p>We observe $\epsilon_\mathcal{D}(f)$ but care about $\epsilon(f)$</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Error as a Statistical Estimator</h2>
                    <div style="display: flex; gap: 30px; align-items: flex-start; margin-top: 30px;">
                        <div id="test-error-convergence-demo" style="flex: 1;"></div>
                        <div class="fragment" style="flex: 1; background: #f9f9f9; padding: 20px; border-radius: 10px; font-size: 0.5em;">
                            <h4 style="color: #10099F; margin-top: 0;">Central Limit Theorem</h4>
                            <p style="font-size: 0.9em;">As test set size $n \to \infty$:</p>
                            <ul style="font-size: 0.85em;">
                                <li>Test error $\epsilon_\mathcal{D}(f) \to \epsilon(f)$</li>
                                <li>Convergence rate: $\mathcal{O}(1/\sqrt{n})$</li>
                                <li>Standard deviation: $\sigma/\sqrt{n}$</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Practical Implications of $\mathcal{O}(1/\sqrt{n})$</h2>
                    <div style="margin-top: 30px;">
                        <div id="sample-size-scaling-demo"></div>
                    </div>
                    <div class="fragment" style="margin-top: 30px; font-size: 0.6em;">
                        <table style="margin: 0 auto; font-size: 0.9em;">
                            <thead>
                                <tr>
                                    <th>Goal</th>
                                    <th>Required Samples</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td>2√ó precision</td>
                                    <td>4√ó samples</td>
                                </tr>
                                <tr class="fragment">
                                    <td>10√ó precision</td>
                                    <td>100√ó samples</td>
                                </tr>
                                <tr class="fragment">
                                    <td>100√ó precision</td>
                                    <td>10,000√ó samples</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px; font-size: 0.6em;">
                        <p>This $\mathcal{O}(1/\sqrt{n})$ rate is often the best we can hope for!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Bernoulli Variance and Classification</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em;">Classification error is a <span class="tooltip">Bernoulli random variable<span class="tooltiptext">A random variable that takes value 1 with probability p and 0 with probability 1-p</span></span></p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; display: flex; align-items: flex-start; gap: 30px;">
                        <div style="flex: 1;">
                            <div id="bernoulli-variance-demo"></div>
                        </div>
                        <div style="flex: 1; background: #f9f9f9; padding: 20px; border-radius: 10px; font-size: 0.5em;">
                            <h4 style="color: #10099F; margin-top: 0;">Variance Formula</h4>
                            <p>$$\sigma^2 = \epsilon(f)(1-\epsilon(f))$$</p>
                            <ul style="font-size: 0.85em; margin-top: 15px;">
                                <li class="fragment">Maximum at $\epsilon(f) = 0.5$: $\sigma^2 = 0.25$</li>
                                <li class="fragment">Lower near $\epsilon(f) = 0$ or $\epsilon(f) = 1$</li>
                                <li class="fragment">Asymptotic std deviation: $\leq \sqrt{0.25/n}$</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Confidence Intervals: How Many Samples?</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em;">For confidence interval $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$:</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <div id="confidence-interval-demo"></div>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <table style="margin: 0 auto; font-size: 0.85em;">
                            <thead>
                                <tr>
                                    <th>Confidence Level</th>
                                    <th>Standard Deviations</th>
                                    <th>Samples Needed</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td>68%</td>
                                    <td>¬±1œÉ</td>
                                    <td>~2,500</td>
                                </tr>
                                <tr class="fragment">
                                    <td>95%</td>
                                    <td>¬±2œÉ</td>
                                    <td>~10,000</td>
                                </tr>
                                <tr class="fragment">
                                    <td>99.7%</td>
                                    <td>¬±3œÉ</td>
                                    <td>~22,500</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px;">
                        <p>This <span class="tooltip">explains<span class="tooltiptext">Here's the connection: We just calculated that to be 95% confident our measured error rate is within ¬±0.01 of the true error rate, we need about 10,000 samples. MNIST's test set has exactly 10,000 samples! This wasn't coincidental - the dataset creators chose this size to provide statistically rigorous evaluation. When you see a paper report "98.5% accuracy on MNIST test set," you can be 95% confident the true accuracy is between 97.5% and 99.5% (within 1 percentage point).</span></span> why MNIST has 10,000 test samples!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Finite Sample Guarantees: Hoeffding's Inequality</h2>
                    <div class="fragment" style="margin-top: 20px; background: #f0f9ff; padding: 20px; border-radius: 10px;">
                        <h4 style="color: #10099F; margin-top: 0;"><span class="tooltip">Hoeffding's Inequality (1963)<span class="tooltiptext">For bounded random variables, we get valid finite sample bounds. Unlike asymptotic analysis which assumes infinite samples, Hoeffding's inequality provides exact probability bounds for any finite sample size n. The key insight is that when each sample is bounded (like 0/1 classification errors), we can derive exponentially tight concentration inequalities. This gives us rigorous statistical guarantees without needing large-sample approximations - crucial for practical machine learning where we have limited data.</span></span></h4>
                        <p>$$P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) < \exp\left( - 2n t^2 \right)$$</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                        <div id="hoeffding-bound-demo"></div>
                        <div style="background: #fff9f0; padding: 15px; border-radius: 8px; font-size: 0.5em;">
                            <p style="font-size: 0.85em;"><strong>Example:</strong> For 95% confidence that error gap ‚â§ 0.01:</p>
                            <ul style="font-size: 0.85em;">
                                <li>Asymptotic analysis: ~10,000 samples</li>
                                <li>Hoeffding bound: ~15,000 samples</li>
                                <li>Finite sample bounds are more conservative!</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Test Set Reuse Problem</h2>
                    <div style="margin-top: 30px;">
                        <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #FC8484; margin-top: 0;">Scenario: The 3am Insight</h4>
                            <ol style="font-size: 0.85em;">
                                <li class="fragment">Train model $f_1$, evaluate on test set ‚úì</li>
                                <li class="fragment">Get brilliant idea for model $f_2$ üí°</li>
                                <li class="fragment">Train $f_2$, want to evaluate... but wait!</li>
                                <li class="fragment" style="color: #FC8484;"><strong>Problem:</strong> Test set already used!</li>
                            </ol>
                        </div>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <h4>Two Major Issues:</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 15px;">
                            <div class="fragment" style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin-top: 0;">False Discovery</h5>
                                <p style="font-size: 0.8em;">With k models, probability of at least one misleading result increases</p>
                            </div>
                            <div class="fragment" style="background: #f9f9f9; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin-top: 0;">Adaptive Overfitting</h5>
                                <p style="font-size: 0.8em;">Model $f_2$ chosen after seeing $f_1$'s test performance</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Multiple Hypothesis Testing</h2>
                    <div style="margin-top: 30px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                        <div id="multiple-testing-demo"></div>
                        <div class="fragment" style="background: #f9f9f9; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #10099F; margin-top: 0; font-size: 0.5em;">The Problem</h4>
                            <ul style="font-size: 0.5em;">
                                <li class="fragment">Single model: 5% chance of misleading result</li>
                                <li class="fragment">20 models: ~64% chance at least one is misleading!</li>
                                <li class="fragment">Formula: $P(\text{at least one false}) = 1 - (0.95)^k$</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px;">
                        <p><strong>Best Practice:</strong> Reserve multiple test sets for benchmark challenges</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dwork et al. - Preserving Statistical Validity in Adaptive Data Analysis", "url": "https://arxiv.org/abs/1411.2664"}]'>
                    <h2 class="truncate-title">Adaptive Overfitting</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em; color: #666;">Once test set information leaks to the modeler...</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <div id="adaptive-overfitting-demo"></div>
                    </div>
                    <div class="fragment" style="margin-top: 20px; display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                        <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #FC8484; margin-top: 0; font-size: 0.7em;">The Contamination Effect</h4>
                            <ul style="font-size: 0.65em;">
                                <li>Test set no longer drawn independently</li>
                                <li>Model selection influenced by test performance</li>
                                <li>Guarantees become invalid</li>
                            </ul>
                        </div>
                        <div>
                            <h4 style="font-size: 0.7em;">Mitigation Strategies:</h4>
                            <ul style="font-size: 0.65em;">
                                <li class="fragment">Create true holdout sets</li>
                                <li class="fragment">Consult test sets infrequently</li>
                                <li class="fragment">Account for multiple testing in confidence intervals</li>
                                <li class="fragment">Higher vigilance with small datasets</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Statistical Learning Theory</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em; color: #666;">Can we guarantee generalization <em>a priori</em>?</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; background: #f0f9ff; padding: 20px; border-radius: 10px;">
                        <h4 style="color: #10099F; margin-top: 0;">The Promise</h4>
                        <p style="font-size: 0.85em;">For any model class and desired error bound $\epsilon$, determine required samples $n$ such that:</p>
                        <p style="margin-top: 10px; text-align: center;">empirical error ‚âà true error (within $\epsilon$)</p>
                        <p style="font-size: 0.85em; margin-top: 10px;">for <strong>any</strong> data distribution!</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; background: #fff5f5; padding: 20px; border-radius: 10px;">
                        <h4 style="color: #FC8484; margin-top: 0;">The Reality</h4>
                        <ul style="font-size: 0.85em;">
                            <li>Deep networks need absurd sample counts (trillions!)</li>
                            <li>Yet they generalize well with thousands of examples</li>
                            <li>Theory-practice gap remains mysterious</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Bias-Variance Trade-off</h2>
                    <div id="bias-variance-demo" style="margin-top: 30px;"></div>
                    <div class="fragment" style="margin-top: 20px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #f0f9ff; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #10099F; margin-top: 0;">High Bias (Rigid)</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>Simple models</li>
                                    <li>Underfitting risk</li>
                                    <li>Good generalization</li>
                                    <li>Example: Linear classifier</li>
                                </ul>
                            </div>
                            <div style="background: #fff5f5; padding: 15px; border-radius: 8px;">
                                <h5 style="color: #FC8484; margin-top: 0;">High Variance (Flexible)</h5>
                                <ul style="font-size: 0.8em;">
                                    <li>Complex models</li>
                                    <li>Overfitting risk</li>
                                    <li>Perfect training fit</li>
                                    <li>Example: Memorization</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px; font-size: 0.6em;">
                        <p>The art: Finding the sweet spot between flexibility and rigidity</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Vapnik & Chervonenkis - Theory of Pattern Recognition", "url": "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory"}]'>
                    <h2 class="truncate-title">VC Dimension: Measuring Model Complexity</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em;">The <span class="tooltip">VC dimension<span class="tooltiptext">Vapnik-Chervonenkis dimension: The largest number of points that can be shattered (assigned any labeling) by the model class</span></span> quantifies model flexibility</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <div id="vc-dimension-demo"></div>
                    </div>
                    <div class="fragment" style="margin-top: 20px; background: #f9f9f9; padding: 20px; border-radius: 10px;">
                        <h4 style="color: #10099F; margin-top: 0;">Examples</h4>
                        <ul style="font-size: 0.85em;">
                            <li>Linear models in $d$ dimensions: VC = $d + 1$</li>
                            <li>2D lines can shatter 3 points, but not 4</li>
                            <li>Higher VC ‚Üí more complex model class</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Uniform Convergence Bounds</h2>
                    <div style="margin-top: 30px;">
                        <p style="font-size: 0.9em;"><span class="tooltip">VC theory<span class="tooltiptext">R[p,f]: true risk (expected error), R_emp: empirical risk (training error), Œ±: generalization gap bound, Œ¥: confidence level</span></span> provides generalization guarantees:</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; background: #f0f9ff; padding: 20px; border-radius: 10px; font-size: 0.5em;">
                        <h4 style="color: #10099F; margin-top: 0;">VC Bound</h4>
                        <p style="margin-top: 15px;">$$P\left(R[p, f] - R_\textrm{emp}[\mathbf{X}, \mathbf{Y}, f] < \alpha\right) \geq 1-\delta$$</p>
                        <p style="margin-top: 15px; font-size: 0.85em;">for $\alpha \geq c \sqrt{(\textrm{VC} - \log \delta)/n}$</p>
                    </div>
                    <div class="fragment" style="margin-top: 20px; display: flex; align-items: flex-start; gap: 20px;">
                        <div style="flex: 1;">
                            <div id="vc-bound-demo"></div>
                        </div>
                        <div style="flex: 1;">
                            <ul style="font-size: 0.75em;">
                                <li>$\delta$: probability bound is violated</li>
                                <li>$\alpha$: generalization gap bound</li>
                                <li>$n$: dataset size</li>
                                <li>Still $\mathcal{O}(1/\sqrt{n})$ convergence!</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Deep Learning Paradox</h2>
                    <div style="margin-top: 30px;">
                        <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #FC8484; margin-top: 0;">Theory Says...</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Deep networks have huge VC dimensions</li>
                                <li>Can memorize random labels easily</li>
                                <li>Should need trillions of samples</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment" style="margin-top: 20px;">
                        <div style="background: #f0f9ff; padding: 20px; border-radius: 10px;">
                            <h4 style="color: #10099F; margin-top: 0;">Practice Shows...</h4>
                            <ul style="font-size: 0.85em;">
                                <li>Generalize well with thousands of examples</li>
                                <li>Larger networks often generalize better</li>
                                <li>Depth helps generalization</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 20px; font-size: 0.6em;">
                        <p><strong>Open Problem:</strong> Why do deep networks generalize so well?</p>
                        <p style="font-size: 0.85em; margin-top: 10px;">This mystery drives modern ML research!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: Theory vs Practice</h2>
                    <div style="margin-top: 30px;">
                        <table style="font-size: 0.85em;">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Theory</th>
                                    <th>Practice</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td><strong>Test Sets</strong></td>
                                    <td>Gold standard evaluation</td>
                                    <td>Often reused, contaminated</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Sample Size</strong></td>
                                    <td>$\mathcal{O}(1/\sqrt{n})$ convergence</td>
                                    <td>10K samples standard</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Guarantees</strong></td>
                                    <td>VC bounds available</td>
                                    <td>Too pessimistic for DNNs</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Deep Learning</strong></td>
                                    <td>Should need huge datasets</td>
                                    <td>Works with modest data</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box" style="margin-top: 30px;">
                        <p><strong>Takeaway:</strong> Use test sets carefully, understand the theory, but trust empirical validation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we need approximately 10,000 test samples for 95% confidence with ¬±0.01 error?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because 10,000 is a round number that is easy to work with",
                                "correct": false,
                                "explanation": "The sample size comes from statistical theory, not convenience."
                            },
                            {
                                "text": "Because the variance of a Bernoulli is at most 0.25, requiring ~10,000 samples for 2œÉ confidence",
                                "correct": true,
                                "explanation": "Correct! With maximum variance 0.25 and needing 2 standard deviations for 95% confidence, we need n such that 2√ó‚àö(0.25/n) ‚â§ 0.01, giving n ‚âà 10,000."
                            },
                            {
                                "text": "Because deep learning models have 10,000 parameters on average",
                                "correct": false,
                                "explanation": "Test set size is independent of model complexity in this analysis."
                            },
                            {
                                "text": "Because the VC dimension of most models is around 10,000",
                                "correct": false,
                                "explanation": "VC dimension affects generalization bounds but not test set size requirements."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main problem with evaluating multiple models on the same test set?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It takes too much computational time",
                                "correct": false,
                                "explanation": "Computation time is not the statistical concern here."
                            },
                            {
                                "text": "The test set gets worn out and provides less accurate results",
                                "correct": false,
                                "explanation": "Test sets dont wear out; the issue is statistical validity."
                            },
                            {
                                "text": "Risk of false discovery and adaptive overfitting increases",
                                "correct": true,
                                "explanation": "Correct! Multiple testing increases the chance that at least one model gets a misleadingly good score, and choosing models based on test performance violates independence assumptions."
                            },
                            {
                                "text": "The models start to memorize the test set",
                                "correct": false,
                                "explanation": "Models dont see the test set during training, so they cannot memorize it."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "According to VC theory, what determines the generalization gap for a model class?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "The VC dimension of the model class",
                                "correct": true,
                                "explanation": "Yes! VC dimension measures model complexity and appears in the bound."
                            },
                            {
                                "text": "The number of training samples",
                                "correct": true,
                                "explanation": "Yes! The gap decreases as O(1/‚àön) with more samples."
                            },
                            {
                                "text": "The desired confidence level (Œ¥)",
                                "correct": true,
                                "explanation": "Yes! Higher confidence (smaller Œ¥) requires larger bounds."
                            },
                            {
                                "text": "The number of epochs trained",
                                "correct": false,
                                "explanation": "Training epochs affect optimization but dont appear in VC bounds."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Environment and Distribution Shift -->
            <section>
                <section class="title-slide">
                    <h1 class="truncate-title">Environment and Distribution Shift</h1>
                    <p>From Training to Deployment: Challenges and Solutions</p>
                    <div class="emphasis-box mt-lg">
                        <p>Understanding how models behave when data distributions change</p>
                    </div>
                    <p class="fragment mt-md" style="color: #666;">
                        Real-world deployment often faces unexpected distribution changes
                    </p>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Environment and Distribution Shift", "url": "https://d2l.ai/chapter_linear-classification/environment.html"}]'>
                    <h2 class="truncate-title">Beyond Training Accuracy</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Questions We Often Ignore</h4>
                            <ul>
                                <li class="fragment">Where did the data come from?</li>
                                <li class="fragment">What will we do with model outputs?</li>
                                <li class="fragment">Will the deployment environment match training?</li>
                                <li class="fragment">Can our model influence the data distribution?</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Common Failure Modes</h4>
                            <ul>
                                <li>Perfect test accuracy, deployment failure</li>
                                <li>Model deployment changes user behavior</li>
                                <li>Feedback loops amplify biases</li>
                                <li>Distribution shifts break assumptions</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Footwear Loan Paradox</h2>
                    <div style="background: #fff5f5; padding: 20px; border-radius: 10px; border-left: 4px solid #FC8484;">
                        <h4>A Cautionary Tale</h4>
                        <p>Model finds: Oxfords ‚Üí Repayment, Sneakers ‚Üí Default</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>The Deployment Decision</h4>
                        <p>Grant loans to Oxford wearers, deny sneaker wearers</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4 style="color: #FC8484;">The Catastrophic Result</h4>
                        <ul>
                            <li>Customers catch on and change behavior</li>
                            <li>Everyone starts wearing Oxfords</li>
                            <li>No improvement in credit-worthiness</li>
                            <li><strong>Model-based decisions broke the model!</strong></li>
                        </ul>
                    </div>
                </section>

                <!-- Types of Distribution Shift -->
                <section>
                    <h2 class="truncate-title">Types of Distribution Shift</h2>
                    <p>Training distribution $p_S(\mathbf{x},y)$ vs Test distribution $p_T(\mathbf{x},y)$</p>
                    <div class="grid-2x2 mt-md" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4 style="color: #10099F;">Covariate Shift</h4>
                            <p>$P(\mathbf{x})$ changes</p>
                            <p>$P(y|\mathbf{x})$ stays same</p>
                            <p style="font-size: 0.8em;">Input distribution shifts</p>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #2DD2C0;">Label Shift</h4>
                            <p>$P(y)$ changes</p>
                            <p>$P(\mathbf{x}|y)$ stays same</p>
                            <p style="font-size: 0.8em;">Output distribution shifts</p>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #FFA05F;">Concept Shift</h4>
                            <p>$P(y|\mathbf{x})$ changes</p>
                            <p>Definitions evolve</p>
                            <p style="font-size: 0.8em;">Meaning changes over time</p>
                        </div>
                        <div class="fragment">
                            <h4 style="color: #FC8484;">Worst Case</h4>
                            <p>$P(y|\mathbf{x}) = 1 - P_{\text{old}}(y|\mathbf{x})$</p>
                            <p>Labels flip completely</p>
                            <p style="font-size: 0.8em;">
                                <span class="tooltip">Impossible to detect!
                                  <span class="tooltiptext">
                                    This pathological case shows why robust classification under arbitrary distribution shift is impossible. 
                                    The input images remain identical (p_S(x) = p_T(x)), but labels completely flip: what was "cat" becomes "dog" and vice versa. 
                                    Since inputs haven't changed, no algorithm can detect this shift occurred. 
                                    A model achieving 80% accuracy before the flip will still achieve 80% accuracy after, 
                                    even though it's now systematically wrong about the true (flipped) labels. 
                                    This indistinguishability motivates why ML requires specific assumptions about allowable distribution shifts.
                                  </span>
                                </span>
                              </p>
                              
                              
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Domain Adaptation in Computer Vision", "url": "https://arxiv.org/abs/1702.05374"}]'>
                    <h2 class="truncate-title">Covariate Shift</h2>
                    <div class="emphasis-box" style="font-size: 0.5em;">
                        <p><strong>Definition:</strong> Input distribution changes but labeling function remains constant</p>
                        <p>$$P_S(\mathbf{x}) \neq P_T(\mathbf{x}) \text{ but } P(y|\mathbf{x}) \text{ unchanged}$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Key Assumption</h4>
                        <p>We believe that $\mathbf{x}$ causes $y$ (causal direction matters!)</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Common Scenarios</h4>
                        <ul>
                            <li>Training on photos, testing on drawings</li>
                            <li>Lab conditions vs real-world deployment</li>
                            <li>Historical data vs current environment</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Covariate Shift Example: Cats vs Dogs</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Training Data (Photos)</h4>
                            <img src="images/cat-dog-train.png" style="width: 100%; border-radius: 10px;">
                            <p style="font-size: 0.8em; color: #666;">High-quality photographs</p>
                        </div>
                        <div class="column fragment">
                            <h4>Test Data (Cartoons)</h4>
                            <img src="images/cat-dog-test.png" style="width: 100%; border-radius: 10px;">
                            <p style="font-size: 0.8em; color: #666;">Stylized drawings</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Same task (cat vs dog), different input characteristics!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Label Shift</h2>
                    <div class="emphasis-box" style="font-size: 0.5em;">
                        <p><strong>Definition:</strong> Label distribution changes but class conditionals remain constant</p>
                        <p>$$P_S(y) \neq P_T(y) \text{ but } P(\mathbf{x}|y) \text{ unchanged}$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Key Assumption</h4>
                        <p>We believe that $y$ causes $\mathbf{x}$ (e.g., diseases cause symptoms)</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Medical Example</h4>
                        <ul>
                            <li>Disease prevalence changes seasonally (flu in winter)</li>
                            <li>Symptoms given disease remain constant</li>
                            <li>Need to adjust diagnostic thresholds</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Concept Shift</h2>
                    <img src="images/popvssoda.png" style="width: 50%; margin: 20px auto; display: block; border-radius: 10px;">
                    <p style="font-size: 0.9em; color: #666;">Same product, different names across US regions</p>
                    <div class="fragment mt-md">
                        <h4>Other Examples</h4>
                        <ul style="font-size: 0.6em;">
                            <li>Fashion trends: What's "stylish" changes over time</li>
                            <li>Job titles: "Data Scientist" meant different things in 2012 vs 2024</li>
                            <li>Medical diagnoses: DSM criteria evolve with research</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "A facial recognition system trained on professional headshots fails when deployed on security cameras. This is likely:",
                        "type": "single",
                        "options": [
                            {
                                "text": "Covariate shift",
                                "correct": true,
                                "explanation": "Correct! The input distribution changed (professional photos ‚Üí security footage) while the task of identifying people remains the same."
                            },
                            {
                                "text": "Label shift",
                                "correct": false,
                                "explanation": "Label shift would mean the distribution of people changes, but how they look given their identity stays the same."
                            },
                            {
                                "text": "Concept shift",
                                "correct": false,
                                "explanation": "Concept shift would mean the definition of identity itself changes, which is not the case here."
                            },
                            {
                                "text": "No distribution shift",
                                "correct": false,
                                "explanation": "There is clearly a shift - the visual characteristics of the input data have changed significantly."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Examples Section -->
                <section>
                    <h2 class="truncate-title">Medical Diagnostics: A Case Study</h2>
                    <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                        <h4>Blood Test for Elderly Disease</h4>
                        <ul style="font-size: 0.75em;">
                            <li>Disease affects older men predominantly</li>
                            <li>Sick patients: Easy to get samples (already in system)</li>
                            <li>Healthy controls: Recruited university students (!)</li>
                        </ul>
                    </div>
                    <div class="two-column fragment mt-md" style="font-size: 0.75em;">
                        <div class="column">
                            <h4 style="color: #FC8484;">The Problem</h4>
                            <p>Model learned to distinguish young from old, not healthy from sick!</p>
                            <ul>
                                <li>Age differences</li>
                                <li>Hormone levels</li>
                                <li>Physical activity</li>
                                <li>Diet and lifestyle</li>
                            </ul>
                        </div>
                        <div class="column fragment emphasis-box">
                            <p><strong>Result:</strong> Near-perfect accuracy in testing, complete failure in deployment</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Self-Driving Cars: Synthetic Data Pitfall</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>The Approach</h4>
                            <ul style="font-size: 0.75em;">
                                <li>Real annotated data expensive</li>
                                <li>Use game engine for training</li>
                                <li>Synthetic roadside textures</li>
                                <li>Perfect test accuracy on synthetic data</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4 style="color: #FC8484;">The Failure</h4>
                            <ul style="font-size: 0.75em;">
                                <li>All synthetic roadsides had same texture</li>
                                <li>Model learned texture, not concept</li>
                                <li>Real-world: Complete disaster</li>
                                <li>Similar to <span class="tooltip">US Army tank detector (shadows)<span class="tooltiptext">Classic ML failure: Model trained to distinguish tanks from empty fields learned to detect shadows instead of tanks, since tank photos were taken on sunny days while empty field photos were taken on cloudy days</span></span></li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Models often learn spurious correlations in controlled environments</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Nonstationary Distributions</h2>
                    <p>When distributions change slowly over time</p>
                    <div class="grid-2x2 mt-md" style="font-size: 0.85em;">
                        <div class="fragment">
                            <h4>üì± Ad Models</h4>
                            <p>New devices launch (iPad)</p>
                            <p>User preferences evolve</p>
                            <p>Need frequent updates</p>
                        </div>
                        <div class="fragment">
                            <h4>üìß Spam Filters</h4>
                            <p>Spammers adapt tactics</p>
                            <p>New spam patterns emerge</p>
                            <p>Arms race dynamics</p>
                        </div>
                        <div class="fragment">
                            <h4>üõçÔ∏è Recommendations</h4>
                            <p>Seasonal patterns</p>
                            <p>Santa hats in July?</p>
                            <p>Time-aware modeling needed</p>
                        </div>
                        <div class="fragment">
                            <h4>üì∑ Face Detection</h4>
                            <p>Works on benchmarks</p>
                            <p>Fails on close-ups</p>
                            <p>Training set bias</p>
                        </div>
                    </div>
                </section>

                <!-- Correction Methods -->
                <section>
                    <h2 class="truncate-title">Empirical Risk vs True Risk</h2>
                    <div class="emphasis-box" style="font-size: 0.5em;">
                        <p><strong>Empirical Risk</strong> (what we minimize):</p>
                        <p>$$\frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i)$$</p>
                    </div>
                    <div class="fragment mt-md emphasis-box" style="background: #f0fff9; font-size: 0.5em;">
                        <p><strong>True Risk</strong> (what we want to minimize):</p>
                        <p>$$E_{p(\mathbf{x}, y)} [l(f(\mathbf{x}), y)] = \int\int l(f(\mathbf{x}), y) p(\mathbf{x}, y) \,d\mathbf{x}dy$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <p style="color: #666; font-size: 0.5em;">Problem: We only have samples from training distribution, not true distribution!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Importance Weighted Risk Minimization", "url": "https://arxiv.org/abs/0901.2698"}]'>
                    <h2 class="truncate-title">Covariate Shift Correction</h2>
                    <div class="emphasis-box" style="font-size: 0.5em;">
                        <p><strong>Key Insight:</strong> Reweight samples by importance ratio</p>
                        <p style="font-size: 0.9em; color: #666;">where $p(\mathbf{x})$ = target distribution, $q(\mathbf{x})$ = source distribution</p>
                        <p>$$\beta_i = \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}$$</p>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.5em;">
                        <h4>Corrected Risk</h4>
                        <p>$$\int\int l(f(\mathbf{x}), y) p(y|\mathbf{x})p(\mathbf{x}) \,d\mathbf{x}dy =$$</p>
                        <p>$$\int\int l(f(\mathbf{x}), y) q(y|\mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} \,d\mathbf{x}dy$$</p>
                    </div>
                    <div class="fragment mt-md" style="font-size: 0.5em;">
                        <h4>Weighted Empirical Risk Minimization</h4>
                        <p style="background: #f0fff9; padding: 15px; border-radius: 10px;">
                            $$\text{minimize}_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i)$$
                        </p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Estimating Importance Weights</h2>
                    <div class="emphasis-box">
                        <p><strong>Problem:</strong> We don't know $\beta_i = \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}$ directly</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Solution: Binary Classification</h4>
                        <ol>
                            <li>Label source data as -1, target data as +1</li>
                            <li>Train logistic regression: $P(z=1|\mathbf{x}) = \frac{1}{1+\exp(-h(\mathbf{x}))}$</li>
                            <li>Extract weights: $\beta_i = \exp(h(\mathbf{x}_i))$</li>
                        </ol>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Intuition</h4>
                        <ul style="font-size: 0.9em;">
                            <li>If sample is typical of target ‚Üí high weight</li>
                            <li>If sample is typical of source only ‚Üí low weight</li>
                            <li>Cap weights at maximum $c$ for stability</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Covariate Shift Correction Algorithm</h2>
                    <pre><code class="python" data-trim>
def covariate_shift_correction(X_source, y_source, X_target):
    """
    Correct for covariate shift using importance weighting
    """
    # Step 1: Create binary classification dataset
    X_combined = np.vstack([X_source, X_target])
    z = np.hstack([-np.ones(len(X_source)), 
                    np.ones(len(X_target))])
    
    # Step 2: Train binary classifier
    classifier = LogisticRegression()
    classifier.fit(X_combined, z)
    
    # Step 3: Compute importance weights
    h_source = classifier.decision_function(X_source)
    beta = np.exp(h_source)
    
    # Step 4: Cap weights for stability
    beta = np.minimum(beta, c=10)
    
    # Step 5: Train weighted model
    model = WeightedRegressor()
    model.fit(X_source, y_source, sample_weight=beta)
    
    return model
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Label Shift Correction</h2>
                    <div class="emphasis-box" style="font-size: 0.7em;">
                        <p><strong>Setting:</strong> $P(y)$ changes but $P(\mathbf{x}|y)$ stays constant</p>
                        <p><strong>Weights:</strong> $\beta_i = \frac{p(y_i)}{q(y_i)}$</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Confusion Matrix Method</h4>
                        <ol style="font-size: 0.7em;">
                            <li>Compute confusion matrix $\mathbf{C}$ on validation set</li>
                            <li>Get mean predictions on target: $\mu(\hat{\mathbf{y}})$</li>
                            <li>Solve: $\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}})$</li>
                            <li>Compute: $p(\mathbf{y}) = \mathbf{C}^{-1} \mu(\hat{\mathbf{y}})$</li>
                            <li>Weight by: $\beta_i = \frac{p(y_i)}{q(y_i)}$</li>
                        </ol>
                    </div>
                    <div class="fragment mt-md">
                        <p style="color: #666; font-size: 0.7em;">
                            Works when classifier is reasonably accurate and confusion matrix is invertible
                        </p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Concept Shift: The Hard Problem</h2>
                    <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                        <h4>When Definitions Change</h4>
                        <p>No principled solution when $P(y|\mathbf{x})$ changes arbitrarily</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Practical Approaches</h4>
                        <ul>
                            <li><strong>Gradual adaptation:</strong> Fine-tune with new data</li>
                            <li><strong>Continual learning:</strong> Update without forgetting</li>
                            <li><strong>Domain detection:</strong> Identify when to switch models</li>
                            <li><strong>Human-in-the-loop:</strong> Flag uncertain cases</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Often requires collecting new labels and partial retraining</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Demo: Distribution Shift</h2>
                    <div id="distribution-shift-demo"></div>
                    <p style="margin-top: 20px; font-size: 0.9em; color: #666;">
                        Adjust the sliders to see how distribution changes affect model performance
                    </p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which method would you use to correct for seasonal changes in disease prevalence?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Covariate shift correction with importance weighting",
                                "correct": false,
                                "explanation": "Covariate shift assumes P(y|x) stays constant, but here P(x|y) is constant."
                            },
                            {
                                "text": "Label shift correction using confusion matrix",
                                "correct": true,
                                "explanation": "Correct! Disease prevalence changing means P(y) changes, while symptoms given disease P(x|y) remain constant."
                            },
                            {
                                "text": "Complete model retraining from scratch",
                                "correct": false,
                                "explanation": "This might work but is inefficient - we can adapt using the confusion matrix method."
                            },
                            {
                                "text": "No correction needed",
                                "correct": false,
                                "explanation": "Ignoring seasonal prevalence changes would lead to suboptimal diagnostic thresholds."
                            }
                        ]
                    }'></div>
                </section>

                <!-- Taxonomy Section -->
                <section>
                    <h2 class="truncate-title">A Taxonomy of Learning Problems</h2>
                    <div id="learning-taxonomy-viz"></div>
                </section>

                <section>
                    <h2 class="truncate-title">Batch Learning</h2>
                    <div class="emphasis-box">
                        <p><strong>Setup:</strong> Train on $\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$, deploy $f(\mathbf{x})$</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Characteristics</h4>
                        <ul>
                            <li>All training data available upfront</li>
                            <li>Model fixed after training</li>
                            <li>Assumes same distribution at test time</li>
                            <li>Default assumption in most ML</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Example: Cat Door System</h4>
                        <p style="font-size: 0.9em;">Train on cat/dog images ‚Üí Deploy in smart door ‚Üí Never update</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Online Learning</h2>
                    <div class="emphasis-box">
                        <p><strong>Cycle:</strong></p>
                        <p style="font-size: 0.6em;">
                        model $f_t$ ‚Üí data $\mathbf{x}_t$ ‚Üí predict $f_t(\mathbf{x}_t)$ ‚Üí 
                        observe $y_t$ ‚Üí loss $l(y_t, f_t(\mathbf{x}_t))$ ‚Üí model $f_{t+1}$
                        </p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Characteristics</h4>
                        <ul style="font-size: 0.6em;">
                            <li>Data arrives sequentially</li>
                            <li>Must predict before seeing label</li>
                            <li>Continuous model improvement</li>
                            <li>Adapts to changing distributions</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Example: Stock Price Prediction</h4>
                        <p style="font-size: 0.9em;">Predict tomorrow ‚Üí Trade ‚Üí Observe outcome ‚Üí Update model</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Bandits</h2>
                    <div class="emphasis-box">
                        <p><strong>Special case:</strong> Finite number of actions (arms to pull)</p>
                    </div>
                    <div class="two-column mt-md">
                        <div class="column fragment">
                            <h4>Setting</h4>
                            <ul style="font-size: 0.9em;">
                                <li>Choose action $a_t$ from finite set</li>
                                <li>Receive reward $r_t$</li>
                                <li>Balance exploration vs exploitation</li>
                                <li>No state, just actions and rewards</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Applications</h4>
                            <ul style="font-size: 0.9em;">
                                <li>A/B testing</li>
                                <li>Ad selection</li>
                                <li>Clinical trials</li>
                                <li>Resource allocation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Control & Reinforcement Learning</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Control (Memory)</h4>
                            <ul style="font-size: 0.9em;">
                                <li>Environment has state</li>
                                <li>Actions affect future states</li>
                                <li><span class="tooltip">PID controllers<span class="tooltiptext">Proportional-integral-derivative controller algorithms that form a model of the environment to make decisions appear less random</span></span></li>
                                <li><span class="tooltip">Coffee boiler example<span class="tooltiptext">A coffee boiler controller observes different temperatures depending on whether it was heating previously - the environment remembers past actions</span></span></li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Reinforcement Learning</h4>
                            <ul style="font-size: 0.9em;">
                                <li>Strategic environments</li>
                                <li>Other agents present</li>
                                <li>Games: Chess, Go, StarCraft</li>
                                <li>Autonomous driving</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Environment responds to our actions - can be cooperative or adversarial</p>
                    </div>
                </section>

                <!-- Fairness Section -->
                <section>
                    <h2 class="truncate-title">Beyond Optimization: Ethics in ML</h2>
                    <div class="emphasis-box" style="background: #fff5f5;">
                        <p><strong>Remember:</strong> We're not just optimizing models, we're automating decisions that affect lives</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Key Considerations</h4>
                        <ul>
                            <li>Which populations does it work for?</li>
                            <li>What are the costs of different errors?</li>
                            <li>Can the system create feedback loops?</li>
                            <li>Are we solving the right problem?</li>
                        </ul>
                    </div>
                </section>

                <section data-sources='[{"text": "Fairness and Machine Learning - Barocas et al.", "url": "https://fairmlbook.org/"}]'>
                    <h2 class="truncate-title">Runaway Feedback Loops</h2>
                    <div style="background: #fff5f5; padding: 20px; border-radius: 10px;">
                        <h4>Predictive Policing Example</h4>
                        <ol style="font-size: 0.9em;">
                            <li>High-crime areas get more patrols</li>
                            <li>More crimes discovered in patrolled areas</li>
                            <li>Data shows even more crime there</li>
                            <li>Model assigns even more patrols</li>
                            <li>Cycle continues...</li>
                        </ol>
                    </div>
                    <div class="fragment mt-md">
                        <p style="color: #FC8484; font-weight: bold; font-size: 0.8em;">
                            Model predictions become coupled to training data generation!
                        </p>
                    </div>
                    <div class="fragment emphasis-box mt-md">
                        <p>Similar issues in: Loan approval, hiring, content recommendation</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Summary: Key Takeaways</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Distribution Shift Types</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Covariate:</strong> $P(\mathbf{x})$ changes</li>
                                <li><strong>Label:</strong> $P(y)$ changes</li>
                                <li><strong>Concept:</strong> $P(y|\mathbf{x})$ changes</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Correction Methods</h4>
                            <ul style="font-size: 0.85em;">
                                <li><strong>Importance weighting</strong></li>
                                <li><strong>Confusion matrix method</strong></li>
                                <li><strong>Continuous adaptation</strong></li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment mt-md emphasis-box">
                        <p><strong>Critical Insight:</strong> Deployment can change the data distribution!</p>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Remember</h4>
                        <ul style="font-size: 0.9em;">
                            <li>Test accuracy ‚â† Deployment success</li>
                            <li>Consider feedback loops and fairness</li>
                            <li>Monitor and adapt to distribution changes</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes distribution shift particularly dangerous in deployment?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Models can appear to work perfectly on test sets",
                                "correct": true,
                                "explanation": "Yes! High test accuracy doesnt guarantee deployment success if distributions differ."
                            },
                            {
                                "text": "Deployment itself can change the data distribution",
                                "correct": true,
                                "explanation": "Yes! Model decisions can influence user behavior and create feedback loops."
                            },
                            {
                                "text": "Some shifts are impossible to detect or correct",
                                "correct": true,
                                "explanation": "Yes! Complete label flips or arbitrary concept shifts cannot be detected without ground truth."
                            },
                            {
                                "text": "All models eventually fail due to distribution shift",
                                "correct": false,
                                "explanation": "Not all models fail - with proper monitoring and adaptation, many systems remain robust."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Kaggle House Price Prediction -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Kaggle House Prices", "url": "https://d2l.ai/chapter_linear-regression/kaggle-house-price.html"}]'>
                    <h2 class="truncate-title">From Theory to Practice: Real-World ML Competition</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>What We've Learned</h4>
                            <ul>
                                <li class="fragment">Linear models for classification</li>
                                <li class="fragment">Softmax and cross-entropy</li>
                                <li class="fragment">Training and optimization</li>
                                <li class="fragment">Regularization techniques</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Real-World Application</h4>
                            <ul>
                                <li class="fragment">Kaggle competitions</li>
                                <li class="fragment">House price prediction</li>
                                <li class="fragment">Feature engineering</li>
                                <li class="fragment">Model validation</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>üí° Time to apply our knowledge to a real competition!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Kaggle Platform", "url": "https://www.kaggle.com"}]'>
                    <h2 class="truncate-title">Introduction to Kaggle</h2>
                    <p>The world's largest data science competition platform</p>
                    <div class="fragment">
                        <h4>What is Kaggle?</h4>
                        <ul>
                            <li>Platform for machine learning competitions</li>
                            <li>Real datasets from companies and organizations</li>
                            <li>Prizes for winning solutions</li>
                            <li>Community of data scientists sharing code and insights</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Benefits of Kaggle Competitions</h4>
                        <ul>
                            <li>Objective performance metrics</li>
                            <li>Learn from top practitioners</li>
                            <li>Build portfolio projects</li>
                            <li>Network with data science community</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The House Prices Competition</h2>
                    <p>Advanced Regression Techniques</p>
                    <div class="fragment">
                        <h4>Competition Overview</h4>
                        <ul>
                            <li>Dataset: House prices in Ames, Iowa (2006-2010)</li>
                            <li>Collected by De Cock (2011)</li>
                            <li>1460 training examples, 1459 test examples</li>
                            <li>80 features describing each house</li>
                            <li>Goal: Predict the sale price</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Why This Dataset?</h4>
                        <ul>
                            <li>Generic data without exotic structure</li>
                            <li>Mix of numerical and categorical features</li>
                            <li>Missing values (real-world challenge)</li>
                            <li>Good starting point for regression</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes Kaggle competitions valuable for learning?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Objective quantitative comparisons between approaches",
                                "correct": true,
                                "explanation": "Yes! Leaderboards provide clear, unbiased performance metrics."
                            },
                            {
                                "text": "Access to real-world datasets",
                                "correct": true,
                                "explanation": "Correct! Competitions use actual data from companies and organizations."
                            },
                            {
                                "text": "Community code sharing and collaboration",
                                "correct": true,
                                "explanation": "Yes! The platform encourages sharing solutions and learning from others."
                            },
                            {
                                "text": "Guaranteed job placement for winners",
                                "correct": false,
                                "explanation": "While winning can boost your profile, job placement is not guaranteed."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Data Loading and Exploration -->
            <section>
                <section>
                    <h2 class="truncate-title">Understanding Real-World Data</h2>
                    <p>Loading and exploring the Kaggle house dataset</p>
                    <div class="fragment">
                        <h4>Data Characteristics</h4>
                        <ul>
                            <li>Mixed data types: integers, floats, categories</li>
                            <li>Missing values marked as "na"</li>
                            <li>Features: street type, year built, roof type, etc.</li>
                            <li>Target: SalePrice (only in training set)</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Reality Check: Real data is messy and requires careful preprocessing!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Loading the Dataset</h2>
                    <pre><code class="python">class KaggleHouse(d2l.DataModule):
    def __init__(self, batch_size, train=None, val=None):
        super().__init__()
        self.save_hyperparameters()
        if self.train is None:
            self.raw_train = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,
                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))
            self.raw_val = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,
                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))

data = KaggleHouse(batch_size=64)
print(data.raw_train.shape)  # (1460, 81)
print(data.raw_val.shape)    # (1459, 80)</code></pre>
                    <div class="fragment mt-md">
                        <p>üìä Training: 1460 examples with 80 features + 1 label</p>
                        <p>üìä Validation: 1459 examples with 80 features (no labels)</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exploring the Data</h2>
                    <pre><code class="python"># First look at the data
print(data.raw_train.iloc[:4, [0, 1, 2, 3, -3, -2, -1]])

   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice
0   1          60       RL         65.0       WD        Normal     208500
1   2          20       RL         80.0       WD        Normal     181500
2   3          60       RL         68.0       WD        Normal     223500
3   4          70       RL         60.0       WD       Abnorml     140000</code></pre>
                    <div class="fragment mt-md">
                        <h4>Observations:</h4>
                        <ul>
                            <li>Id column: identifier (not useful for prediction)</li>
                            <li>Numerical features: LotFrontage (with decimals)</li>
                            <li>Categorical features: MSZoning, SaleType, SaleCondition</li>
                            <li>Target variable: SalePrice (what we predict)</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What challenges does this real-world dataset present?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Mixed data types requiring different preprocessing",
                                "correct": true,
                                "explanation": "Yes! Numerical and categorical features need different handling."
                            },
                            {
                                "text": "Missing values that need to be handled",
                                "correct": true,
                                "explanation": "Correct! Missing values are marked as na and must be addressed."
                            },
                            {
                                "text": "Features on different scales",
                                "correct": true,
                                "explanation": "Yes! Features like year (e.g., 2010) and size (e.g., 65.0) have very different ranges."
                            },
                            {
                                "text": "All features are equally important for prediction",
                                "correct": false,
                                "explanation": "Not true! Some features like Id provide no predictive value."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Data Preprocessing -->
            <section>
                <section>
                    <h2 class="truncate-title">Data Preprocessing Pipeline</h2>
                    <p>Transforming raw data into model-ready features</p>
                    <div class="fragment">
                        <h4>Preprocessing Steps</h4>
                        <ol>
                            <li class="fragment">Remove non-predictive features (Id)</li>
                            <li class="fragment">Handle missing values</li>
                            <li class="fragment">Standardize numerical features</li>
                            <li class="fragment">One-hot encode categorical features</li>
                        </ol>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>From 79 raw features to 331 processed features!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Handling Numerical Features</h2>
                    <p>Standardization for consistent scale</p>
                    <div class="two-column fragment" style="font-size: 0.6em;">
                        <div class="column">
                            <h4>Standardization Formula</h4>
                            <p>$$x \leftarrow \frac{x - \mu}{\sigma}$$</p>
                            <p>where $\mu$ = mean, $\sigma$ = standard deviation</p>
                        </div>
                        <div class="column">
                            <h4>Why Standardize?</h4>
                            <ul>
                                <li>Zero mean: $E[\frac{x-\mu}{\sigma}] = 0$</li>
                                <li>Unit variance: $E[(x-\mu)^2] = \sigma^2$</li>
                                <li>Convenient for optimization</li>
                                <li>Prevents feature scale bias</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>Missing Values</h4>
                        <p>Replace with feature mean (simple heuristic)</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Handling Categorical Features</h2>
                    <p>One-hot encoding for discrete values</p>
                    <div class="fragment">
                        <h4>Example: MSZoning Feature</h4>
                        <table style="margin: auto;">
                            <thead>
                                <tr>
                                    <th>Original</th>
                                    <th>MSZoning_RL</th>
                                    <th>MSZoning_RM</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>"RL"</td>
                                    <td>1</td>
                                    <td>0</td>
                                </tr>
                                <tr>
                                    <td>"RM"</td>
                                    <td>0</td>
                                    <td>1</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment mt-md">
                        <p>Each categorical value becomes a binary feature</p>
                        <p>pandas handles this automatically with <code>pd.get_dummies()</code></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Complete Preprocessing Code</h2>
                    <pre><code class="python">@d2l.add_to_class(KaggleHouse)
def preprocess(self):
    # Remove the ID and label columns
    label = 'SalePrice'
    features = pd.concat(
        (self.raw_train.drop(columns=['Id', label]),
         self.raw_val.drop(columns=['Id'])))
    
    # Standardize numerical columns
    numeric_features = features.dtypes[features.dtypes!='object'].index
    features[numeric_features] = features[numeric_features].apply(
        lambda x: (x - x.mean()) / (x.std()))
    
    # Replace NAN numerical features by 0
    features[numeric_features] = features[numeric_features].fillna(0)
    
    # Replace discrete features by one-hot encoding
    features = pd.get_dummies(features, dummy_na=True)
    
    # Save preprocessed features
    self.train = features[:self.raw_train.shape[0]].copy()
    self.train[label] = self.raw_train[label]
    self.val = features[self.raw_train.shape[0]:].copy()</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we standardize numerical features?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "To achieve zero mean and unit variance",
                                "correct": true,
                                "explanation": "Yes! Standardization transforms features to have mean=0 and variance=1."
                            },
                            {
                                "text": "To improve optimization convergence",
                                "correct": true,
                                "explanation": "Correct! Features on similar scales help gradient descent converge faster."
                            },
                            {
                                "text": "To prevent feature scale bias in coefficients",
                                "correct": true,
                                "explanation": "Yes! Without standardization, features with larger scales might dominate."
                            },
                            {
                                "text": "To reduce the number of features",
                                "correct": false,
                                "explanation": "Standardization doesnt reduce features; it transforms their scale."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Error Metrics -->
            <section>
                <section>
                    <h2 class="truncate-title">Choosing the Right Error Metric</h2>
                    <p>Why relative error matters for house prices</p>
                    <div class="fragment">
                        <h4>Absolute vs Relative Error</h4>
                        <ul>
                            <li>100,000 dollar error on 125,000 dollar house (Ohio): 80% error!</li>
                            <li>100,000 dollar error on 4,000,000 dollar house (California): 2.5% error</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>The Problem</h4>
                        <p style="font-size: 0.8em;">We care more about <span class="tooltip">relative error<span class="tooltiptext">The ratio of prediction error to actual value: (y - ≈∑) / y</span></span> $\frac{y - \hat{y}}{y}$ than absolute error $y - \hat{y}$</p>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>üí° Solution: Use logarithmic scale!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Logarithmic Error Metric</h2>
                    <p>The competition's official metric</p>
                    <div class="two-column fragment">
                        <div class="column">
                            <h4 style="font-size: 0.6em;">Root Mean Squared Log Error (RMSLE)</h4>
                            <p style="font-size: 0.6em;">$$\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log y_i -\log \hat{y}_i\right)^2}$$</p>
                        </div>
                        <div class="column">
                            <h4 style="font-size: 0.6em;">Why Log Scale?</h4>
                            <p style="font-size: 0.6em;">If $|\log y - \log \hat{y}| \leq \delta$, then:</p>
                            <p style="font-size: 0.6em;">$$e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^\delta$$</p>
                            <p style="font-size: 0.6em;">This bounds the relative error!</p>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>Benefits</h4>
                        <ul>
                            <li>Penalizes relative errors equally</li>
                            <li>Natural for prices (multiplicative changes)</li>
                            <li>Prevents large houses from dominating</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementing Log-Scale Training</h2>
                    <pre><code class="python">@d2l.add_to_class(KaggleHouse)
def get_dataloader(self, train):
    label = 'SalePrice'
    data = self.train if train else self.val
    if label not in data: return
    
    get_tensor = lambda x: torch.tensor(x.values.astype(float),
                                      dtype=torch.float32)
    
    # Key: Take logarithm of prices!
    tensors = (get_tensor(data.drop(columns=[label])),  # X
               torch.log(get_tensor(data[label])).reshape((-1, 1)))  # log(Y)
               
    return self.get_tensorloader(tensors, train)</code></pre>
                    <div class="fragment mt-md">
                        <p>‚ö†Ô∏è Remember to exponentiate predictions when submitting!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why use logarithmic error for house prices?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the math easier",
                                "correct": false,
                                "explanation": "Log scale actually makes computations slightly more complex."
                            },
                            {
                                "text": "It treats relative errors equally across price ranges",
                                "correct": true,
                                "explanation": "Correct! A 10% error is penalized equally whether on a cheap or expensive house."
                            },
                            {
                                "text": "It always produces smaller error values",
                                "correct": false,
                                "explanation": "Log scale doesnt necessarily produce smaller values, it changes the scale."
                            },
                            {
                                "text": "It eliminates the need for standardization",
                                "correct": false,
                                "explanation": "We still need to standardize features; log scale only affects the target."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: K-Fold Cross-Validation -->
            <section>
                <section>
                    <h2 class="truncate-title">K-Fold Cross-Validation</h2>
                    <p>Robust model evaluation with limited data</p>
                    <div class="fragment">
                        <h4>The Challenge</h4>
                        <ul>
                            <li>Limited training data (1460 examples)</li>
                            <li>Need reliable performance estimate</li>
                            <li>Want to use all data for training</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>The Solution: K-Fold CV</h4>
                        <ol>
                            <li>Split data into K equal folds</li>
                            <li>Train K models, each using K-1 folds</li>
                            <li>Validate on the held-out fold</li>
                            <li>Average performance across all K models</li>
                        </ol>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">K-Fold Visualization</h2>
                    <div class="interactive-demo" id="kfold-demo">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                K = 
                                <input type="range" id="k-slider" min="3" max="10" value="5" style="width: 100px;">
                                <span id="k-value" style="font-family: monospace;">5</span>
                            </label>
                            <button id="animate-kfold-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Animate Folds</button>
                            <button id="reset-kfold-btn" style="background: #FC8484; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Reset</button>
                        </div>
                        <div id="kfold-viz-container"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementing K-Fold CV</h2>
                    <pre><code class="python">def k_fold_data(data, k):
    rets = []
    fold_size = data.train.shape[0] // k
    for j in range(k):
        # Define validation fold indices
        idx = range(j * fold_size, (j+1) * fold_size)
        # Create new data module with train/val split
        rets.append(KaggleHouse(
            data.batch_size, 
            data.train.drop(index=idx),  # Training: all except fold j
            data.train.loc[idx]))         # Validation: fold j
    return rets</code></pre>
                    <div class="fragment mt-md">
                        <p>Each fold becomes validation set exactly once</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training with K-Fold</h2>
                    <pre><code class="python">def k_fold(trainer, data, k, lr):
    val_loss, models = [], []
    
    for i, data_fold in enumerate(k_fold_data(data, k)):
        model = d2l.LinearRegression(lr)
        model.board.yscale='log'
        
        # Only show first fold's training progress
        if i != 0: 
            model.board.display = False
            
        trainer.fit(model, data_fold)
        val_loss.append(float(model.board.data['val_loss'][-1].y))
        models.append(model)
    
    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')
    return models

# Train with 5-fold CV
trainer = d2l.Trainer(max_epochs=10)
models = k_fold(trainer, data, k=5, lr=0.01)</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Benefits of K-Fold CV</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Advantages</h4>
                            <ul>
                                <li class="fragment">More reliable performance estimate</li>
                                <li class="fragment">Uses all data for both training and validation</li>
                                <li class="fragment">Reduces overfitting to validation set</li>
                                <li class="fragment">Identifies unstable models</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Trade-offs</h4>
                            <ul>
                                <li class="fragment">K times more computation</li>
                                <li class="fragment">More complex implementation</li>
                                <li class="fragment">Need to manage K models</li>
                                <li class="fragment">Ensemble predictions for test set</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Worth it for small datasets and competitions!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are the key benefits of K-fold cross-validation?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Every sample is used for both training and validation",
                                "correct": true,
                                "explanation": "Yes! Each sample appears in K-1 training folds and 1 validation fold."
                            },
                            {
                                "text": "It provides a more robust performance estimate",
                                "correct": true,
                                "explanation": "Correct! Averaging across K folds reduces variance in the estimate."
                            },
                            {
                                "text": "It can detect if a model is sensitive to data splits",
                                "correct": true,
                                "explanation": "Yes! High variance across folds indicates instability."
                            },
                            {
                                "text": "It reduces total training time",
                                "correct": false,
                                "explanation": "Actually increases training time by factor of K."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Model Training and Submission -->
            <section>
                <section>
                    <h2 class="truncate-title">Model Selection and Training</h2>
                    <p>Putting it all together</p>
                    <div class="fragment">
                        <h4>Our Approach</h4>
                        <ul>
                            <li>Linear regression model (baseline)</li>
                            <li>Log-scale target transformation</li>
                            <li>5-fold cross-validation</li>
                            <li>Ensemble predictions from all folds</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Training Results</h4>
                        <pre><code>average validation log mse = 0.173</code></pre>
                        <p>Reasonable baseline performance!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Monitoring Training Progress</h2>
                    <div class="interactive-demo" id="training-progress-demo">
                        <div class="demo-controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; z-index: 100;">
                            <button id="start-training-btn" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Start Training</button>
                            <button id="pause-training-btn" style="background: #2DD2C0; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Pause</button>
                            <button id="reset-training-btn" style="background: #FC8484; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer;">Reset</button>
                        </div>
                        <div id="training-progress-viz-container"></div>
                    </div>
                    <div class="fragment mt-md">
                        <p>‚ö†Ô∏è Watch for overfitting: validation loss increasing while training loss decreases</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Making Predictions</h2>
                    <pre><code class="python"># Generate predictions from all K models
preds = [model(torch.tensor(data.val.values.astype(float), 
                           dtype=torch.float32))
         for model in models]

# Ensemble: average predictions from all folds
# Remember to exponentiate (we trained on log scale)!
ensemble_preds = torch.exp(torch.cat(preds, 1)).mean(1)

# Create submission file
submission = pd.DataFrame({
    'Id': data.raw_val.Id,
    'SalePrice': ensemble_preds.detach().numpy()
})

submission.to_csv('submission.csv', index=False)</code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>üéØ Ensemble averaging often improves performance!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Submission Process</h2>
                    <div class="fragment">
                        <h4>Steps to Submit</h4>
                        <ol>
                            <li>Log in to Kaggle</li>
                            <li>Navigate to competition page</li>
                            <li>Click "Submit Predictions"</li>
                            <li>Upload submission.csv</li>
                            <li>View leaderboard position</li>
                        </ol>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Leaderboard</h4>
                        <ul>
                            <li>Public: 50% of test data (visible during competition)</li>
                            <li>Private: 50% of test data (revealed after competition)</li>
                            <li>Prevents overfitting to public leaderboard</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What are key steps in our Kaggle pipeline?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Standardize features and handle missing values",
                                "correct": true,
                                "explanation": "Yes! Preprocessing is essential for real-world data."
                            },
                            {
                                "text": "Use logarithmic scale for house prices",
                                "correct": true,
                                "explanation": "Correct! Log scale handles relative errors better."
                            },
                            {
                                "text": "Apply K-fold cross-validation",
                                "correct": true,
                                "explanation": "Yes! K-fold provides robust performance estimates."
                            },
                            {
                                "text": "Always use the most complex model available",
                                "correct": false,
                                "explanation": "Start with simple baselines! Complex models arent always better."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section: Key Takeaways -->
            <section>
                <section>
                    <h2 class="truncate-title">Key Takeaways from Kaggle Competition</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Technical Skills</h4>
                            <ul>
                                <li class="fragment">Data preprocessing pipeline</li>
                                <li class="fragment">Feature engineering</li>
                                <li class="fragment">Cross-validation</li>
                                <li class="fragment">Ensemble methods</li>
                                <li class="fragment">Error metric selection</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Practical Insights</h4>
                            <ul>
                                <li class="fragment">Real data is messy</li>
                                <li class="fragment">Simple models can be effective</li>
                                <li class="fragment">Validation strategy matters</li>
                                <li class="fragment">Domain knowledge helps</li>
                                <li class="fragment">Iterative improvement</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>üéØ From theory to practice: You now have the tools to compete!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Next Steps</h2>
                    <div class="fragment">
                        <h4>Improvements to Try</h4>
                        <ul>
                            <li>Feature engineering (interactions, polynomials)</li>
                            <li>Advanced models (XGBoost, neural networks)</li>
                            <li>Hyperparameter tuning</li>
                            <li>Different preprocessing strategies</li>
                            <li>Stacking and blending</li>
                        </ul>
                    </div>
                    <div class="fragment mt-md">
                        <h4>Other Competitions</h4>
                        <ul>
                            <li>Image classification (CIFAR, ImageNet)</li>
                            <li>Natural language processing</li>
                            <li>Time series forecasting</li>
                            <li>Recommendation systems</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes Kaggle competitions valuable for learning deep learning?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "They provide real-world datasets with all their complexities",
                                "correct": true,
                                "explanation": "Yes! Real data teaches you practical preprocessing and feature engineering."
                            },
                            {
                                "text": "They offer objective benchmarks for model performance",
                                "correct": true,
                                "explanation": "Correct! Leaderboards provide clear, unbiased performance metrics."
                            },
                            {
                                "text": "They encourage sharing code and learning from others",
                                "correct": true,
                                "explanation": "Yes! The community aspect accelerates learning through shared solutions."
                            },
                            {
                                "text": "Simple models never work well in competitions",
                                "correct": false,
                                "explanation": "False! Well-tuned simple models often provide strong baselines and sometimes win!"
                            }
                        ]
                    }'></div>
                </section>
            </section>

        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>
    
    <!-- D3.js for visualizations -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    
    <!-- Shared utilities -->
    <script src="../shared/js/d3-utils.js"></script>
    <script src="../shared/js/animation-lib.js"></script>
    <script src="../shared/js/neural-viz.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/title-handler.js"></script>
    <script src="../shared/js/multiple-choice.js"></script>
    
    <!-- Custom visualization scripts -->
    <script src="js/softmax-viz.js"></script>
    <script src="js/cross-entropy-viz.js"></script>
    <script src="js/classification-boundary.js"></script>
    <script src="js/batch-matrix-viz.js"></script>
    <script src="js/entropy-calculator.js"></script>
    <script src="js/numerical-stability-demo.js"></script>
    <script src="js/classifier-base-viz.js"></script>
    <script src="js/softmax-implementation-viz.js"></script>
    <script src="js/training-loop-viz.js"></script>
    <script src="js/concise-implementation-viz.js"></script>
    <script src="js/generalization-viz.js"></script>
    <script src="js/distribution-shift-viz.js"></script>
    <script src="js/kaggle-viz.js"></script>
    
    <script>
        // Initialize Reveal.js
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full',
                TeX: {
                    Macros: {
                        R: '\\mathbb{R}',
                        set: ['\\left\\{#1\\right\\}', 1]
                    }
                }
            },
            plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
        });
        
        // Initialize visualizations when slides are ready
        Reveal.on('ready', () => {
            if (typeof initSoftmaxViz !== 'undefined') initSoftmaxViz();
            if (typeof initCrossEntropyViz !== 'undefined') initCrossEntropyViz();
            if (typeof initClassificationBoundary !== 'undefined') initClassificationBoundary();
            if (typeof initBatchMatrixViz !== 'undefined') initBatchMatrixViz();
            if (typeof initEntropyCalculator !== 'undefined') initEntropyCalculator();
            if (typeof initNumericalStabilityDemo !== 'undefined') initNumericalStabilityDemo();
            if (typeof initClassifierBaseViz !== 'undefined') initClassifierBaseViz();
            if (typeof initSoftmaxImplementationViz !== 'undefined') initSoftmaxImplementationViz();
            if (typeof initTrainingLoopViz !== 'undefined') initTrainingLoopViz();
            if (typeof initConciseImplementationViz !== 'undefined') initConciseImplementationViz();
            if (typeof initDistributionShiftViz !== 'undefined') initDistributionShiftViz();
        });
        
        // Reinitialize visualizations when returning to slides
        Reveal.on('slidechanged', event => {
            const currentSlide = event.currentSlide;
            if (currentSlide.querySelector('#softmax-demo') && typeof initSoftmaxViz !== 'undefined') {
                initSoftmaxViz();
            }
            if (currentSlide.querySelector('#cross-entropy-demo') && typeof initCrossEntropyViz !== 'undefined') {
                initCrossEntropyViz();
            }
            if (currentSlide.querySelector('#classification-boundary-demo') && typeof initClassificationBoundary !== 'undefined') {
                initClassificationBoundary();
            }
            if (currentSlide.querySelector('#batch-matrix-demo') && typeof initBatchMatrixViz !== 'undefined') {
                initBatchMatrixViz();
            }
            if (currentSlide.querySelector('#entropy-demo') && typeof initEntropyCalculator !== 'undefined') {
                initEntropyCalculator();
            }
            if (currentSlide.querySelector('#numerical-stability-demo') && typeof initNumericalStabilityDemo !== 'undefined') {
                initNumericalStabilityDemo();
            }
            if ((currentSlide.querySelector('#accuracy-demo') || 
                 currentSlide.querySelector('#argmax-demo') || 
                 currentSlide.querySelector('#training-dynamics-demo')) && 
                typeof initClassifierBaseViz !== 'undefined') {
                initClassifierBaseViz();
            }
            if (currentSlide.querySelector('#softmax-implementation-demo') && typeof initSoftmaxImplementationViz !== 'undefined') {
                initSoftmaxImplementationViz();
            }
            if (currentSlide.querySelector('#training-loop-demo') && typeof initTrainingLoopViz !== 'undefined') {
                initTrainingLoopViz();
            }
            if ((currentSlide.querySelector('#architecture-flow-demo') || 
                 currentSlide.querySelector('#stability-comparison-demo') || 
                 currentSlide.querySelector('#training-progress-demo')) && 
                typeof initConciseImplementationViz !== 'undefined') {
                initConciseImplementationViz();
            }
            if ((currentSlide.querySelector('#distribution-shift-examples') ||
                 currentSlide.querySelector('#distribution-shift-demo') ||
                 currentSlide.querySelector('#learning-taxonomy-viz')) &&
                typeof initDistributionShiftViz !== 'undefined') {
                initDistributionShiftViz();
            }
            // Initialize Kaggle visualizations
            if ((currentSlide.querySelector('#preprocessing-demo') ||
                 currentSlide.querySelector('#log-scale-demo') ||
                 currentSlide.querySelector('#kfold-demo') ||
                 currentSlide.querySelector('#training-progress-demo') ||
                 currentSlide.querySelector('#pipeline-demo')) &&
                typeof initKaggleViz !== 'undefined') {
                initKaggleViz();
            }
        });
    </script>
</body>
</html>
