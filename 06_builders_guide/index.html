<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Builder's Guide: Layers and Modules - Introduction to Deep Learning</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    <link rel="stylesheet" href="css/builders-custom.css">
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Builder's Guide: Layers and Modules</h1>
                <p>Chapter 6.1: From End User to Power User</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>
            
            <!-- Section 1: Introduction to Builder's Guide (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 6", "url": "https://d2l.ai/chapter_builders-guide/index.html"}]'>
                    <h2 class="truncate-title">The Evolution of Deep Learning Tools</h2>
                    <div class="timeline-container">
                        <div class="timeline">
                            <div class="timeline-item fragment">
                                <div class="year">2007</div>
                                <div class="event">Theano</div>
                                <div class="description">First major deep learning library</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2015</div>
                                <div class="event">TensorFlow</div>
                                <div class="description">Google's framework</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2016</div>
                                <div class="event">PyTorch</div>
                                <div class="description">Dynamic computation graphs</div>
                            </div>
                            <div class="timeline-item fragment">
                                <div class="year">2018+</div>
                                <div class="event">JAX, MXNet</div>
                                <div class="description">Modern frameworks</div>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Tools have evolved from low-level operations to high-level abstractions</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Abstraction Journey</h2>
                    <div class="abstraction-levels" style="font-size: 0.75em;">
                        <div class="level fragment">
                            <div class="icon">üîß</div>
                            <h4>Hardware Era</h4>
                            <p>Transistors ‚Üí Logic Circuits</p>
                        </div>
                        <div class="arrow fragment">‚Üí</div>
                        <div class="level fragment">
                            <div class="icon">üíª</div>
                            <h4>Software Era</h4>
                            <p>Logic Circuits ‚Üí Code</p>
                        </div>
                        <div class="arrow fragment">‚Üí</div>
                        <div class="level fragment">
                            <div class="icon">üß†</div>
                            <h4>Deep Learning Era</h4>
                            <p>Neurons ‚Üí Layers ‚Üí Blocks</p>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <p style="font-size: 0.9em;">Just as semiconductor designers evolved their abstractions, <br>
                        neural network researchers now think in terms of <span class="highlight">whole layers and architectural blocks</span></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">What You'll Master in This Lecture</h2>
                    <div class="grid-2x3" style="font-size: 0.75em;">
                        <div class="card fragment">
                            <h4>üèóÔ∏è Model Construction</h4>
                            <p>Build complex architectures from modular components</p>
                        </div>
                        <div class="card fragment">
                            <h4>üîë Parameter Access</h4>
                            <p>Navigate and inspect model parameters</p>
                        </div>
                        <div class="card fragment">
                            <h4>üé≤ Initialization</h4>
                            <p>Set up parameters for optimal training</p>
                        </div>
                        <div class="card fragment">
                            <h4>üé® Custom Layers</h4>
                            <p>Design your own neural network components</p>
                        </div>
                        <div class="card fragment">
                            <h4>üíæ Model I/O</h4>
                            <p>Save and load trained models</p>
                        </div>
                        <div class="card fragment">
                            <h4>‚ö° GPU Acceleration</h4>
                            <p>Achieve dramatic speedups</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">From End User to Power User</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>End User</h4>
                            <ul>
                                <li class="fragment">Uses pre-built models</li>
                                <li class="fragment">Calls high-level APIs</li>
                                <li class="fragment">Limited customization</li>
                                <li class="fragment">Standard architectures</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Power User</h4>
                            <ul>
                                <li class="fragment">Creates custom architectures</li>
                                <li class="fragment">Understands internals</li>
                                <li class="fragment">Full control over training</li>
                                <li class="fragment">Invents new models</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>This chapter bridges the gap between using and building</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What represents the evolution of abstractions in deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "From high-level blocks to individual neurons",
                                "correct": false,
                                "explanation": "The evolution went in the opposite direction - from low-level to high-level abstractions."
                            },
                            {
                                "text": "From individual neurons to layers to architectural blocks",
                                "correct": true,
                                "explanation": "Correct! Deep learning has evolved from thinking about individual neurons to whole layers and now architectural blocks."
                            },
                            {
                                "text": "From PyTorch to TensorFlow to Theano",
                                "correct": false,
                                "explanation": "This is backwards - Theano (2007) came first, then TensorFlow (2015), then PyTorch (2016)."
                            },
                            {
                                "text": "From software to hardware to neural networks",
                                "correct": false,
                                "explanation": "The analogy shows parallel evolution in different fields, not a sequence."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 2: Understanding Modules (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 6.1", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html"}]'>
                    <h2 class="truncate-title">What Is a Module?</h2>
                    <div class="module-definition">
                        <p class="fragment">A <span class="tooltip">module<span class="tooltiptext">The fundamental abstraction in deep learning frameworks for encapsulating computation</span></span> is the basic unit of deep learning</p>
                        <div class="module-types fragment">
                            <div class="type-card">
                                <div class="icon">‚Ä¢</div>
                                <p>Single Neuron</p>
                            </div>
                            <div class="type-card">
                                <div class="icon">‚îÅ</div>
                                <p>Layer</p>
                            </div>
                            <div class="type-card">
                                <div class="icon">‚ñ¶</div>
                                <p>Block</p>
                            </div>
                            <div class="type-card">
                                <div class="icon">‚ñ£</div>
                                <p>Model</p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Modules can represent <strong>any</strong> level of abstraction</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Module Abstraction Spectrum</h2>
                    <div id="module-hierarchy" style="width: 100%; height: 400px;"></div>
                    <div class="fragment mt-md">
                        <p style="font-size: 0.9em;">Modules are <span class="highlight">composable</span> - they can contain other modules</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Five Key Module Responsibilities</h2>
                    <div class="responsibilities-grid" style="font-size: 0.75em;">
                        <div class="responsibility fragment" data-fragment-index="1">
                            <div class="number">1</div>
                            <h4>Ingest Input</h4>
                            <p>Accept data tensors</p>
                            <code>forward(X)</code>
                        </div>
                        <div class="responsibility fragment" data-fragment-index="2">
                            <div class="number">2</div>
                            <h4>Generate Output</h4>
                            <p>Produce predictions</p>
                            <code>return Y</code>
                        </div>
                        <div class="responsibility fragment" data-fragment-index="3">
                            <div class="number">3</div>
                            <h4>Calculate Gradients</h4>
                            <p>Enable backpropagation</p>
                            <code>backward()</code>
                        </div>
                        <div class="responsibility fragment" data-fragment-index="4">
                            <div class="number">4</div>
                            <h4>Store Parameters</h4>
                            <p>Weights and biases</p>
                            <code>self.weight</code>
                        </div>
                        <div class="responsibility fragment" data-fragment-index="5">
                            <div class="number">5</div>
                            <h4>Initialize Parameters</h4>
                            <p>Set starting values</p>
                            <code>nn.init</code>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Module Hierarchy Example</h2>
                    <pre><code class="python" data-trim data-line-numbers>
import torch.nn as nn

# A module can be as simple as a single layer
linear_module = nn.Linear(20, 10)

# Or a complex architecture
class DeepModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        self.decoder = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 784)
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
                    </code></pre>
                    <p class="fragment mt-md">Every component is a module - from <code>Linear</code> to <code>DeepModel</code></p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which of the following are key responsibilities of a module?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Accepting input data and producing output",
                                "correct": true,
                                "explanation": "Correct! Input ingestion and output generation are core module responsibilities."
                            },
                            {
                                "text": "Calculating gradients through backpropagation",
                                "correct": true,
                                "explanation": "Correct! Modules must support gradient calculation for training."
                            },
                            {
                                "text": "Visualizing the neural network architecture",
                                "correct": false,
                                "explanation": "Visualization is not a core module responsibility - it is handled by external tools."
                            },
                            {
                                "text": "Storing and initializing parameters",
                                "correct": true,
                                "explanation": "Correct! Parameter management is essential for learnable modules."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 3: Building a Custom Module (Vertical) -->
            <section>
                <section data-sources='[{"text": "Custom Modules in PyTorch", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html#a-custom-module"}]'>
                    <h2 class="truncate-title">Creating Your First Custom Module</h2>
                    <p>Let's build a simple MLP module from scratch</p>
                    <pre><code class="python" data-trim data-line-numbers="1-11|1-2|3-6|7-8|10-11">
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.LazyLinear(256)  # Hidden layer
        self.out = nn.LazyLinear(10)      # Output layer
    
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
                    </code></pre>
                    <div class="fragment">
                        <p style="font-size: 0.9em;">Key components:</p>
                        <ul style="font-size: 0.85em;">
                            <li>Inherit from <code>nn.Module</code></li>
                            <li>Initialize layers in <code>__init__</code></li>
                            <li>Define computation in <code>forward</code></li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding LazyLinear</h2>
                    <div class="lazy-explanation">
                        <p><span class="tooltip">LazyLinear<span class="tooltiptext">A linear layer that infers input dimensions from the first forward pass</span></span> defers weight initialization until first use</p>
                        <div class="comparison-grid mt-md">
                            <div class="comparison-item fragment">
                                <h4>Regular Linear</h4>
                                <pre><code class="python">nn.Linear(784, 256)</code></pre>
                                <p>Must specify input dimension</p>
                            </div>
                            <div class="comparison-item fragment">
                                <h4>LazyLinear</h4>
                                <pre><code class="python">nn.LazyLinear(256)</code></pre>
                                <p>Infers input dimension automatically</p>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>LazyLinear simplifies architecture changes - no need to update dimensions!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Testing Our Custom Module</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-3|5-7|9-11|13-14">
# Create an instance of our module
net = MLP()
print(f"Module created: {net}")

# Create dummy input
X = torch.rand(2, 20)  # Batch size 2, 20 features
print(f"Input shape: {X.shape}")

# Forward pass
output = net(X)
print(f"Output shape: {output.shape}")

# The module now has initialized parameters
print(f"Hidden layer shape: {net.hidden.weight.shape}")
                    </code></pre>
                    <div class="fragment output-box">
                        <pre>Module created: MLP(
  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)
  (out): LazyLinear(in_features=0, out_features=10, bias=True)
)
Input shape: torch.Size([2, 20])
Output shape: torch.Size([2, 10])
Hidden layer shape: torch.Size([256, 20])</pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens when you first call forward() on a module with LazyLinear layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "An error occurs because dimensions are not specified",
                                "correct": false,
                                "explanation": "LazyLinear is designed to work without pre-specified input dimensions."
                            },
                            {
                                "text": "The weights are initialized based on the input shape",
                                "correct": true,
                                "explanation": "Correct! LazyLinear infers the input dimension from the first forward pass and initializes weights accordingly."
                            },
                            {
                                "text": "The module ignores the LazyLinear layers",
                                "correct": false,
                                "explanation": "LazyLinear layers are fully functional - they just defer initialization."
                            },
                            {
                                "text": "The input dimensions must be manually set first",
                                "correct": false,
                                "explanation": "The whole point of LazyLinear is automatic dimension inference."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 4: The Sequential Module (Vertical) -->
            <section>
                <section data-sources='[{"text": "Sequential Modules", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html#the-sequential-module"}]'>
                    <h2 class="truncate-title">The Sequential Module</h2>
                    <p>Chain modules together in sequence</p>
                    <pre><code class="python" data-trim>
# Using the built-in Sequential
net = nn.Sequential(
    nn.LazyLinear(256),
    nn.ReLU(),
    nn.LazyLinear(10)
)

# Data flows through each module in order
X = torch.rand(2, 20)
output = net(X)  # X ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí output
                    </code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p>Sequential automatically chains the forward methods</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Building MySequential from Scratch</h2>
                    <p>Let's understand how Sequential works internally</p>
                    <pre><code class="python" data-trim data-line-numbers="1-12|3-6|8-11|13-17">
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        # Register each module with a numeric key
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)
    
    def forward(self, X):
        # Pass data through each module in sequence
        for module in self.children():
            X = module(X)
        return X

# Usage is identical to nn.Sequential
net = MySequential(
    nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10)
)
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding Module Registration</h2>
                    <div class="registration-explanation">
                        <p>The <code>add_module</code> method does three critical things:</p>
                        <ol class="spaced-list">
                            <li class="fragment">Registers the module in the <code>_modules</code> OrderedDict</li>
                            <li class="fragment">Makes parameters accessible for optimization</li>
                            <li class="fragment">Enables automatic gradient computation</li>
                        </ol>
                    </div>
                    <pre class="fragment"><code class="python" data-trim>
# Behind the scenes
print(net._modules)
# OrderedDict([('0', Linear(...)), ('1', ReLU()), ('2', Linear(...))])

# All parameters are accessible
for name, param in net.named_parameters():
    print(f"{name}: {param.shape}")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Sequential vs Custom Modules</h2>
                    <div class="comparison-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Sequential</th>
                                    <th>Custom Module</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td>‚úÖ Quick to define</td>
                                    <td>‚úÖ Full control</td>
                                </tr>
                                <tr class="fragment">
                                    <td>‚úÖ Clean syntax</td>
                                    <td>‚úÖ Complex logic</td>
                                </tr>
                                <tr class="fragment">
                                    <td>‚ùå Linear flow only</td>
                                    <td>‚úÖ Branching/merging</td>
                                </tr>
                                <tr class="fragment">
                                    <td>‚ùå No conditionals</td>
                                    <td>‚úÖ Dynamic behavior</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.65em;">
                        <p>Use Sequential for simple chains, custom modules for complex architectures</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main difference between Sequential and a custom module?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Sequential is faster than custom modules",
                                "correct": false,
                                "explanation": "Performance is similar - the difference is in flexibility and ease of use."
                            },
                            {
                                "text": "Sequential only supports linear data flow, while custom modules allow complex logic",
                                "correct": true,
                                "explanation": "Correct! Sequential chains modules linearly, while custom modules can implement branching, conditionals, and complex architectures."
                            },
                            {
                                "text": "Custom modules cannot use built-in layers",
                                "correct": false,
                                "explanation": "Custom modules can use any built-in layers or other modules."
                            },
                            {
                                "text": "Sequential requires manual gradient calculation",
                                "correct": false,
                                "explanation": "Both Sequential and custom modules handle gradients automatically."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 5: Advanced Forward Propagation (Vertical) -->
            <section>
                <section data-sources='[{"text": "Executing Code in Forward", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html#executing-code-in-the-forward-propagation-method"}]'>
                    <h2 class="truncate-title">Beyond Simple Forward Passes</h2>
                    <p>The <code>forward</code> method can contain <strong>any</strong> Python code!</p>
                    <pre><code class="python" data-trim data-line-numbers="1-15|7-14">
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.rand_weight = torch.rand((20, 20))  # Fixed random
        self.linear = nn.LazyLinear(20)          # Learnable
    
    def forward(self, X):
        X = self.linear(X)
        X = F.relu(X @ self.rand_weight + 1)  # Fixed transform
        X = self.linear(X)  # Reuse the same linear layer!
        
        # Python control flow is allowed
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
                    </code></pre>
                    <div class="fragment">
                        <p style="font-size: 0.9em;">This module combines fixed weights, layer reuse, and control flow!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Mixing Learnable and Fixed Parameters</h2>
                    <div class="parameter-types" style="font-size: 0.7em;">
                        <div class="param-card fragment">
                            <h4>üé≤ Fixed Parameters</h4>
                            <pre><code class="python">self.rand_weight = torch.rand((20, 20))</code></pre>
                            <ul>
                                <li>Not updated during training</li>
                                <li>Useful for random projections</li>
                                <li>Saves memory and computation</li>
                            </ul>
                        </div>
                        <div class="param-card fragment">
                            <h4>üìö Learnable Parameters</h4>
                            <pre><code class="python">self.linear = nn.LazyLinear(20)</code></pre>
                            <ul>
                                <li>Updated via backpropagation</li>
                                <li>Registered as parameters</li>
                                <li>Standard neural network weights</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Control Flow in Forward Propagation</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-16|6-8|10-12|14-16">
class DynamicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([nn.LazyLinear(64) for _ in range(5)])
    
    def forward(self, X, depth=None):
        # Dynamic depth based on input
        if depth is None:
            depth = min(3 + int(X.abs().mean() * 2), 5)
        
        # Apply variable number of layers
        for i in range(depth):
            X = F.relu(self.layers[i](X))
        
        # Conditional processing
        if X.shape[0] > 10:  # Large batch
            X = F.dropout(X, 0.5)
        
        return X
                    </code></pre>
                    <div class="fragment emphasis-box">
                        <p>The computation graph can change based on input!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Layer Reuse Pattern</h2>
                    <pre><code class="python" data-trim>
class SharedWeightNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared = nn.LazyLinear(256)
        self.relu = nn.ReLU()
    
    def forward(self, X):
        # Use the same layer multiple times
        X = self.relu(self.shared(X))
        X = self.relu(self.shared(X))  # Reuse!
        return self.shared(X)          # Reuse again!

# The shared layer's weights are used 3 times
# but only stored once in memory
                    </code></pre>
                    <div class="fragment" style="font-size: 0.75em;">
                        <p>Benefits: Memory efficiency, parameter tying, weight sharing patterns</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which of the following are valid in a module forward method?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Python while loops and if statements",
                                "correct": true,
                                "explanation": "Correct! Any Python control flow is allowed in forward methods."
                            },
                            {
                                "text": "Using the same layer multiple times",
                                "correct": true,
                                "explanation": "Correct! Layers can be reused, sharing their parameters across multiple applications."
                            },
                            {
                                "text": "Mixing fixed and learnable parameters",
                                "correct": true,
                                "explanation": "Correct! You can combine learnable nn.Module parameters with fixed tensors."
                            },
                            {
                                "text": "Only sequential layer application",
                                "correct": false,
                                "explanation": "Forward methods are not limited to sequential application - they can contain complex logic."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 6: Module Composition and Nesting (Vertical) -->
            <section>
                <section data-sources='[{"text": "Nested Modules", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html"}]'>
                    <h2 class="truncate-title">Module Composition: Building Complex Architectures</h2>
                    <p>Modules can contain other modules, creating hierarchies</p>
                    <pre><code class="python" data-trim data-line-numbers="1-18|3-8|10-17">
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.LazyLinear(64), nn.ReLU(),
            nn.LazyLinear(32), nn.ReLU()
        )
        self.linear = nn.LazyLinear(16)
    
    def forward(self, X):
        return self.linear(self.net(X))

# Create a chimera with multiple nested modules
chimera = nn.Sequential(
    NestMLP(), nn.LazyLinear(20), FixedHiddenMLP()
)
                    </code></pre>
                    <div class="fragment">
                        <p>This creates a deep hierarchy of modules!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Accessing Nested Components</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-3|5-8|10-13|15-17">
# Direct child access
print(chimera[0])  # NestMLP module
print(chimera[1])  # Linear layer

# Nested access using indices
print(chimera[0].net[0])  # First linear in NestMLP
print(chimera[0].net[1])  # First ReLU in NestMLP
print(chimera[0].linear)  # Final linear in NestMLP

# Iterate through all modules
for name, module in chimera.named_modules():
    if isinstance(module, nn.Linear):
        print(f"Found Linear layer: {name}")

# Access all parameters
for name, param in chimera.named_parameters():
    print(f"Parameter {name}: shape {param.shape}")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Module Inspection and Debugging</h2>
                    <pre><code class="python" data-trim>
# Pretty print the entire structure
print(chimera)

# Output:
# Sequential(
#   (0): NestMLP(
#     (net): Sequential(
#       (0): LazyLinear(in_features=0, out_features=64, bias=True)
#       (1): ReLU()
#       (2): LazyLinear(in_features=0, out_features=32, bias=True)
#       (3): ReLU()
#     )
#     (linear): LazyLinear(in_features=0, out_features=16, bias=True)
#   )
#   (1): LazyLinear(in_features=0, out_features=20, bias=True)
#   (2): FixedHiddenMLP(
#     (linear): LazyLinear(in_features=0, out_features=20, bias=True)
#   )
# )
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Management in Nested Modules</h2>
                    <div class="param-management">
                        <p>Parameters are automatically collected from all nested modules</p>
                        <pre><code class="python" data-trim>
# Count parameters at different levels
def count_parameters(module):
    return sum(p.numel() for p in module.parameters())

print(f"Total parameters: {count_parameters(chimera)}")
print(f"NestMLP parameters: {count_parameters(chimera[0])}")
print(f"NestMLP.net parameters: {count_parameters(chimera[0].net)}")

# Check if parameters are shared
params = list(chimera.parameters())
print(f"Total parameter tensors: {len(params)}")
print(f"Unique parameter tensors: {len(set(id(p) for p in params))}")
                        </code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "How are parameters managed in nested modules?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Each module manages only its direct parameters",
                                "correct": false,
                                "explanation": "Modules automatically collect parameters from all their children recursively."
                            },
                            {
                                "text": "Parameters are automatically collected recursively from all nested modules",
                                "correct": true,
                                "explanation": "Correct! When you call .parameters() on a module, it recursively collects all parameters from nested modules."
                            },
                            {
                                "text": "Nested module parameters must be manually registered",
                                "correct": false,
                                "explanation": "Adding modules with add_module() or as attributes automatically registers their parameters."
                            },
                            {
                                "text": "Only the top-level module parameters are accessible",
                                "correct": false,
                                "explanation": "All parameters at all nesting levels are accessible through various methods."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 7: Practical Examples and Exercises (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Exercises", "url": "https://d2l.ai/chapter_builders-guide/model-construction.html#exercises"}]'>
                    <h2 class="truncate-title">Bringing It All Together</h2>
                    <p>Let's build a real-world module using everything we've learned</p>
                    <pre><code class="python" data-trim data-line-numbers="1-25|3-10|12-24">
class ResidualBlock(nn.Module):
    """A residual block with skip connections"""
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.LazyConv2d(channels, 3, padding=1)
        self.bn1 = nn.LazyBatchNorm2d()
        self.conv2 = nn.LazyConv2d(channels, 3, padding=1)
        self.bn2 = nn.LazyBatchNorm2d()
        self.skip = nn.LazyConv2d(channels, 1)  # 1x1 conv
    
    def forward(self, X):
        # Main path
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        
        # Skip connection
        if X.shape[1] != Y.shape[1]:
            X = self.skip(X)
        
        # Residual addition
        return F.relu(Y + X)

# Stack multiple residual blocks
model = nn.Sequential(*[ResidualBlock(64) for _ in range(4)])
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Common Module Design Patterns</h2>
                    <div class="patterns-grid">
                        <div class="pattern-card fragment">
                            <h4>üîÑ Skip Connections</h4>
                            <pre><code class="python">Y = F.relu(Y + X)</code></pre>
                            <p>Helps with gradient flow</p>
                        </div>
                        <div class="pattern-card fragment">
                            <h4>üì¶ Module Lists</h4>
                            <pre><code class="python">nn.ModuleList([...])</code></pre>
                            <p>Dynamic number of layers</p>
                        </div>
                        <div class="pattern-card fragment">
                            <h4>üéØ Lazy Initialization</h4>
                            <pre><code class="python">nn.LazyLinear(out)</code></pre>
                            <p>Flexible input dimensions</p>
                        </div>
                        <div class="pattern-card fragment">
                            <h4>üå≥ Hierarchical Design</h4>
                            <pre><code class="python">self.encoder = Block()</code></pre>
                            <p>Modular architectures</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Comparison</h2>
                    <div class="framework-comparison" style="font-size: 0.75em;">
                        <table>
                            <thead>
                                <tr>
                                    <th>Concept</th>
                                    <th>PyTorch</th>
                                    <th>TensorFlow</th>
                                    <th>JAX</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td>Base Class</td>
                                    <td><code>nn.Module</code></td>
                                    <td><code>tf.keras.Model</code></td>
                                    <td><code>flax.linen.Module</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td>Forward Method</td>
                                    <td><code>forward()</code></td>
                                    <td><code>call()</code></td>
                                    <td><code>__call__()</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td>Sequential</td>
                                    <td><code>nn.Sequential</code></td>
                                    <td><code>tf.keras.Sequential</code></td>
                                    <td><code>nn.Sequential</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td>Lazy Init</td>
                                    <td><code>LazyLinear</code></td>
                                    <td>Automatic</td>
                                    <td>Via <code>setup()</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exercise 1: Module Storage</h2>
                    <div class="exercise">
                        <h4>Investigate how modules are stored internally</h4>
                        <p>PyTorch modules use special dictionaries to organize their components:</p>
                        <pre><code class="python" data-line-numbers="1-3|1|2|3">module._modules  # OrderedDict of child modules
module._parameters  # OrderedDict of parameters
module._buffers  # OrderedDict of buffers</code></pre>
                        <div class="fragment">
                            <h5>Try this exploration:</h5>
                            <pre><code class="python">net = nn.Sequential(nn.Linear(10, 5), nn.ReLU())
print("Modules:", list(net._modules.keys()))
print("Parameters:", list(net._parameters.keys()))
for name, param in net.named_parameters():
    print(f"{name}: {param.shape}")</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exercise 2: Parallel Blocks</h2>
                    <div class="exercise">
                        <h4>Create a module that processes input through parallel paths</h4>
                        <p>Build architectures that split and merge data flows:</p>
                        <pre><code class="python" data-line-numbers="1-6|2-3|4-5|6|7-10">class ParallelBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_path = nn.Conv2d(3, 16, 3)
        self.pool_path = nn.AdaptiveAvgPool2d((1, 1))
    
    def forward(self, X):
        path1 = self.conv_path(X)
        path2 = self.pool_path(X)
        return torch.cat([path1, path2], dim=1)</code></pre>
                        <div class="fragment">
                            <h5>Challenge Extension:</h5>
                            <p style="font-size: 0.75em;">Add residual connections and different activation functions per path</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exercise 3: Module Factory</h2>
                    <div class="exercise" style="font-size: 0.75em;">
                        <h4>Build a function that creates modules based on config</h4>
                        <p>Dynamic module creation from configuration:</p>
                        <pre><code class="python" data-line-numbers="1-6|1-2|3-6">layer_map = {
    'linear': nn.Linear,
    'relu': nn.ReLU,
    'dropout': nn.Dropout
}

def create_module(config):
    return nn.Sequential(*[
        layer_map[layer_type](**params)
        for layer_type, params in config
    ])</code></pre>
                        <div class="fragment">
                            <h5>Usage Example:</h5>
                            <pre><code class="python">config = [
    ('linear', {'in_features': 784, 'out_features': 128}),
    ('relu', {}),
    ('dropout', {'p': 0.2}),
    ('linear', {'in_features': 128, 'out_features': 10})
]
model = create_module(config)</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Key Takeaways</h2>
                    <div class="takeaways" style="font-size: 0.75em;">
                        <div class="takeaway fragment">
                            <div class="icon">üéØ</div>
                            <p>Modules are the <strong>fundamental abstraction</strong> in deep learning</p>
                        </div>
                        <div class="takeaway fragment">
                            <div class="icon">üîß</div>
                            <p>You can create <strong>any architecture</strong> with custom modules</p>
                        </div>
                        <div class="takeaway fragment">
                            <div class="icon">üå≥</div>
                            <p>Modules naturally form <strong>hierarchies</strong> through composition</p>
                        </div>
                        <div class="takeaway fragment">
                            <div class="icon">üöÄ</div>
                            <p>The forward method can contain <strong>arbitrary Python code</strong></p>
                        </div>
                        <div class="takeaway fragment">
                            <div class="icon">üí°</div>
                            <p>Understanding modules makes you a <strong>power user</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Final Test: Comprehensive Understanding</h2>
                    <div data-mcq='{
                        "question": "Which statements about modules in deep learning are TRUE?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Modules can contain other modules, forming hierarchies",
                                "correct": true,
                                "explanation": "Correct! Module composition is a fundamental feature allowing complex architectures."
                            },
                            {
                                "text": "The forward method must only contain matrix operations",
                                "correct": false,
                                "explanation": "The forward method can contain any Python code including control flow and complex logic."
                            },
                            {
                                "text": "Parameters from nested modules are automatically accessible",
                                "correct": true,
                                "explanation": "Correct! Parameters are recursively collected from all nested modules."
                            },
                            {
                                "text": "LazyLinear requires input dimensions to be specified",
                                "correct": false,
                                "explanation": "LazyLinear automatically infers input dimensions from the first forward pass."
                            },
                            {
                                "text": "Modules handle both forward and backward propagation",
                                "correct": true,
                                "explanation": "Correct! Modules manage forward computation and automatic gradient calculation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 8: Introduction to Parameter Management (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 6.2", "url": "https://d2l.ai/chapter_builders-guide/parameters.html"}]'>
                    <h2 class="truncate-title">Parameter Management: The Heart of Deep Learning</h2>
                    <p>Once we've built our architecture, we need to manage its parameters</p>
                    <div class="parameter-overview" style="font-size: 0.5em;">
                        <div class="param-aspect fragment">
                            <div class="icon">üéØ</div>
                            <h4>Training</h4>
                            <p>Find parameter values that minimize loss</p>
                        </div>
                        <div class="param-aspect fragment">
                            <div class="icon">üíæ</div>
                            <h4>Persistence</h4>
                            <p>Save and load trained models</p>
                        </div>
                        <div class="param-aspect fragment">
                            <div class="icon">üîç</div>
                            <h4>Inspection</h4>
                            <p>Debug and understand models</p>
                        </div>
                        <div class="param-aspect fragment">
                            <div class="icon">üîÑ</div>
                            <h4>Transfer</h4>
                            <p>Reuse parameters across models</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Parameters are more than just numbers - they're complex objects with values, gradients, and metadata</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Parameter Management Matters</h2>
                    <div class="importance-grid" style="font-size: 0.5em;">
                        <div class="importance-card fragment">
                            <h4>üß™ Scientific Understanding</h4>
                            <p>Examine learned representations to gain insights into what the model has learned</p>
                        </div>
                        <div class="importance-card fragment">
                            <h4>üêõ Debugging</h4>
                            <p>Inspect parameter values and gradients to diagnose training issues</p>
                        </div>
                        <div class="importance-card fragment">
                            <h4>üöÄ Deployment</h4>
                            <p>Export models to production systems in different formats</p>
                        </div>
                        <div class="importance-card fragment">
                            <h4>üé® Fine-tuning</h4>
                            <p>Selectively update specific layers while freezing others</p>
                        </div>
                        <div class="importance-card fragment">
                            <h4>‚ö° Optimization</h4>
                            <p>Apply different learning rates to different parameter groups</p>
                        </div>
                        <div class="importance-card fragment">
                            <h4>üîó Parameter Sharing</h4>
                            <p>Reuse parameters across layers for efficiency</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Lifecycle</h2>
                    <div class="lifecycle-stages mt-md" style="font-size: 0.5em;">
                        <span class="stage fragment">Creation</span>
                        <span class="arrow fragment">‚Üí</span>
                        <span class="stage fragment">Initialization</span>
                        <span class="arrow fragment">‚Üí</span>
                        <span class="stage fragment">Training</span>
                        <span class="arrow fragment">‚Üí</span>
                        <span class="stage fragment">Saving</span>
                        <span class="arrow fragment">‚Üí</span>
                        <span class="stage fragment">Loading</span>
                        <span class="arrow fragment">‚Üí</span>
                        <span class="stage fragment">Inference</span>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">What's Inside a Parameter?</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-7|10-13|15-17|19-21">
# Parameters are not just tensors!
import torch
from torch import nn

# Create a simple layer
layer = nn.Linear(10, 5)

# A parameter contains:
param = layer.weight

# 1. The data shape and actual values
print(f"Data shape: {param.data.shape}")  # torch.Size([5, 10])
print(f"Data values: {param.data}")  # tensor([[...], [...], [...], [...], [...]])

# 2. Gradients (after backward pass)
print(f"Gradient: {param.grad}")  # None (not computed yet)

# 3. Additional metadata
print(f"Requires grad: {param.requires_grad}")  # True, requires_grad means it is a learnable parameter
print(f"Is leaf: {param.is_leaf}")  # True, is_leaf means it is a leaf node in the computation graph
print(f"Device: {param.device}")  # cpu, device means the device on which the parameter is stored
                    </code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p>Parameters are <span class="tooltip">Parameter<span class="tooltiptext">A special tensor subclass that signals to autograd that they should be treated as learnable parameters</span></span> objects, not plain tensors!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What information does a PyTorch parameter contain?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "The actual weight/bias values (data)",
                                "correct": true,
                                "explanation": "Correct! Parameters store the actual numerical values as their data attribute."
                            },
                            {
                                "text": "Gradient information from backpropagation",
                                "correct": true,
                                "explanation": "Correct! Parameters store gradients computed during backpropagation in their grad attribute."
                            },
                            {
                                "text": "The learning rate for optimization",
                                "correct": false,
                                "explanation": "Learning rates are stored in the optimizer, not in the parameters themselves."
                            },
                            {
                                "text": "Metadata like requires_grad and device",
                                "correct": true,
                                "explanation": "Correct! Parameters include metadata about gradient requirements and device placement."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 9: Parameter Access Fundamentals (Vertical) -->
            <section>
                <section data-sources='[{"text": "Parameter Access", "url": "https://d2l.ai/chapter_builders-guide/parameters.html#parameter-access"}]'>
                    <h2 class="truncate-title">Accessing Parameters: Layer by Layer</h2>
                    <p>Let's explore how to access parameters from neural networks</p>
                    <pre><code class="python" data-trim data-line-numbers="1-8|10-12|14-16">
# Build a simple network
net = nn.Sequential(
    nn.LazyLinear(8),
    nn.ReLU(),
    nn.LazyLinear(1)
)

X = torch.rand(2, 4)
output = net(X)  # Initialize lazy layers

# Access layer by index (like a list!)
print(net[0])  # First linear layer
print(net[2])  # Second linear layer (index 2, not 1!)

# Access parameters of a specific layer
print(net[2].state_dict())
# OrderedDict([('weight', tensor([[...]]), ('bias', tensor([...]))])
                    </code></pre>
                    <div class="fragment">
                        <p style="font-size: 0.9em;">üí° Sequential models can be indexed like lists to access individual layers</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Targeted Parameter Access</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-6|7-11|13-17|19-22">
# Access specific parameters directly
layer = net[2]  # Get the output layer

# Access weight parameter
print(f"Weight type: {type(layer.weight)}")
# <class 'torch.nn.parameter.Parameter'>

# Get the actual tensor data
weight_data = layer.weight.data
print(f"Weight shape: {weight_data.shape}")
# torch.Size([1, 8])

# Access bias parameter
bias_data = layer.bias.data
print(f"Bias shape: {bias_data.shape}")
print(f"Bias value: {bias_data.item()}")
# torch.Size([1]) and actual value

# Check gradient (None before backward)
print(f"Weight gradient: {layer.weight.grad}")
# None
print(f"Gradient exists: {layer.weight.grad is not None}")
# False</code></pre>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.5em;">
                        <p>Use <code>.data</code> to access raw tensor values, but be careful - direct modifications to <code>.data</code> bypass autograd tracking, which means gradients won't be computed correctly during backpropagation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding Parameter Properties</h2>
                    <div class="properties-table" style="font-size: 0.5em;">
                        <table>
                            <thead>
                                <tr>
                                    <th>Property</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td><code>.data</code></td>
                                    <td>Raw tensor values</td>
                                    <td><code>param.data.fill_(0)</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td><code>.grad</code></td>
                                    <td>Accumulated gradients</td>
                                    <td><code>param.grad.zero_()</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td><code>.requires_grad</code></td>
                                    <td>Whether to compute gradients</td>
                                    <td><code>param.requires_grad = False</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td><code>.shape</code></td>
                                    <td>Tensor dimensions</td>
                                    <td><code>param.shape</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td><code>.device</code></td>
                                    <td>CPU or GPU location</td>
                                    <td><code>param.to('cuda')</code></td>
                                </tr>
                                <tr class="fragment">
                                    <td><code>.dtype</code></td>
                                    <td>Data type</td>
                                    <td><code>param.dtype</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "How do you properly access the raw numerical values of a parameter?",
                        "type": "single",
                        "options": [
                            {
                                "text": "param.values",
                                "correct": false,
                                "explanation": "There is no .values attribute. Use .data to access the raw tensor."
                            },
                            {
                                "text": "param.data",
                                "correct": true,
                                "explanation": "Correct! The .data attribute gives you access to the underlying tensor values."
                            },
                            {
                                "text": "param.tensor",
                                "correct": false,
                                "explanation": "Parameters don not have a .tensor attribute. Use .data instead."
                            },
                            {
                                "text": "param.numpy()",
                                "correct": false,
                                "explanation": "While you can convert to numpy, you should first access .data, then optionally convert."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 10: All Parameters at Once (Vertical) -->
            <section>
                <section data-sources='[{"text": "All Parameters at Once", "url": "https://d2l.ai/chapter_builders-guide/parameters.html#all-parameters-at-once"}]'>
                    <h2 class="truncate-title">Accessing All Parameters Simultaneously</h2>
                    <p>Working with complex models requires efficient parameter access</p>
                    <pre><code class="python" data-trim data-line-numbers="1-11|13-18|19-23">
# Create a more complex network
class ComplexNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        self.decoder = nn.Linear(128, 10)
        
model = ComplexNet()

# Method 1: named_parameters() - most common
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")
    # encoder.0.weight: torch.Size([256, 784])
    # encoder.0.bias: torch.Size([256])...

# Method 2: parameters() - just the tensors
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")
# Total parameters: 218,762
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Navigating Nested Module Hierarchies</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-19|21-26|28-32">
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        
class ResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_stem = nn.Conv2d(3, 64, 7, stride=2, padding=3)
        self.blocks = nn.ModuleList([
            ResidualBlock(64) for _ in range(4)
        ])
        self.classifier = nn.Linear(64, 10)
        
model = ResNet()

# Access nested parameters with full path
for name, param in model.named_parameters():
    if "blocks.0.conv1" in name:
        print(f"First block's first conv: {name}")
        print(f"  Shape: {param.shape}")
        print(f"  Total elements: {param.numel()}")
        break

# Count parameters by module
for name, module in model.named_children():
    params = sum(p.numel() for p in module.parameters())
    print(f"{name}: {params:,} parameters")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Collection Methods</h2>
                    <div class="methods-comparison" style="font-size: 0.5em;">
                        <div class="method-card fragment">
                            <h4><code>parameters()</code></h4>
                            <pre><code class="python">for p in model.parameters():
    print(p.shape)</code></pre>
                            <p>Returns parameter tensors only</p>
                        </div>
                        <div class="method-card fragment">
                            <h4><code>named_parameters()</code></h4>
                            <pre><code class="python">for name, p in model.named_parameters():
    print(f"{name}: {p.shape}")</code></pre>
                            <p>Returns (name, parameter) pairs</p>
                        </div>
                        <div class="method-card fragment">
                            <h4><code>state_dict()</code></h4>
                            <pre><code class="python">state = model.state_dict()
for key in state:
    print(f"{key}: {state[key].shape}")</code></pre>
                            <p>Returns OrderedDict of all parameters</p>
                        </div>
                        <div class="method-card fragment">
                            <h4><code>named_modules()</code></h4>
                            <pre><code class="python">for name, module in model.named_modules():
    print(f"{name}: {type(module)}")</code></pre>
                            <p>Returns all submodules recursively</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Filtering and Selecting Parameters</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-5|7-12|13-18|19-27">
# Filter parameters by name pattern
conv_params = [p for name, p in model.named_parameters() 
               if 'conv' in name]
print(f"Convolution parameters: {len(conv_params)}")

# Filter by parameter size
large_params = [(name, p) for name, p in model.named_parameters() 
                if p.numel() > 10000]
print(f"Large parameters (>10k elements):")
for name, p in large_params:
    print(f"  {name}: {p.numel():,} elements")

# Get parameters from specific modules
encoder_params = list(model.encoder.parameters())
decoder_params = list(model.decoder.parameters())
print(f"Encoder params: {len(encoder_params)}")
print(f"Decoder params: {len(decoder_params)}")

# Check for frozen parameters
trainable = []
frozen = []
for name, p in model.named_parameters():
    if p.requires_grad:
        trainable.append(name)
    else:
        frozen.append(name)
print(f"Trainable: {len(trainable)}, Frozen: {len(frozen)}")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the difference between parameters() and named_parameters()?",
                        "type": "single",
                        "options": [
                            {
                                "text": "parameters() returns a dictionary, named_parameters() returns a list",
                                "correct": false,
                                "explanation": "Both return iterators. parameters() yields tensors, named_parameters() yields (name, tensor) pairs."
                            },
                            {
                                "text": "parameters() returns only tensors, named_parameters() returns (name, tensor) pairs",
                                "correct": true,
                                "explanation": "Correct! parameters() gives you just the parameter tensors, while named_parameters() includes their hierarchical names."
                            },
                            {
                                "text": "named_parameters() includes gradients, parameters() does not",
                                "correct": false,
                                "explanation": "Both methods return the same Parameter objects which may contain gradients."
                            },
                            {
                                "text": "There is no difference, they are aliases",
                                "correct": false,
                                "explanation": "They are different methods with distinct return formats."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 11: Parameter Initialization (Vertical) -->
            <section>
                <section data-sources='[{"text": "Parameter Initialization - Dive into Deep Learning", "url": "https://d2l.ai/chapter_builders-guide/init-param.html"}]'>
                    <h2 class="truncate-title">Parameter Initialization: The Foundation of Learning</h2>
                    <p style="font-size: 0.5em;">Now that we know how to access parameters, let's look at how to initialize them properly</p>
                    <div class="init-importance">
                        <div class="init-problem fragment">
                            <h4>‚ùå Poor Initialization</h4>
                            <ul>
                                <li><span class="tooltip">Vanishing gradients<span class="tooltiptext">Gradients become too small to effectively update parameters</span></span></li>
                                <li><span class="tooltip">Exploding gradients<span class="tooltiptext">Gradients grow uncontrollably, causing unstable training</span></span></li>
                                <li>Dead neurons (ReLU)</li>
                                <li>Slow or failed convergence</li>
                            </ul>
                        </div>
                        <div class="init-solution fragment">
                            <h4>‚úÖ Proper Initialization</h4>
                            <ul>
                                <li>Stable gradient flow</li>
                                <li>Active neurons throughout network</li>
                                <li>Fast convergence</li>
                                <li>Better final performance</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.5em;">
                        <p>We discussed the need for proper initialization in <strong>Section 5.4</strong> - now let's master it!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Default Initialization", "url": "https://d2l.ai/chapter_builders-guide/init-param.html"}]'>
                    <h2 class="truncate-title">Default Initialization Across Frameworks</h2>
                    <p>Each framework has its own default initialization strategy</p>
                    <table class="framework-defaults" style="font-size: 0.5em;">
                        <thead>
                            <tr>
                                <th>Framework</th>
                                <th>Weight Initialization</th>
                                <th>Bias Initialization</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td><strong>PyTorch</strong></td>
                                <td>Uniform based on <span class="tooltip">fan_in/fan_out<span class="tooltiptext">Number of input and output connections</span></span></td>
                                <td>Uniform (same range)</td>
                            </tr>
                            <tr class="fragment">
                                <td><strong>MXNet</strong></td>
                                <td>$$U(-0.07, 0.07)$$</td>
                                <td>Zero</td>
                            </tr>
                            <tr class="fragment">
                                <td><strong>JAX/Flax</strong></td>
                                <td><span class="tooltip">LeCun Normal<span class="tooltiptext">Truncated normal with std = sqrt(1/fan_in)</span></span></td>
                                <td>Zero</td>
                            </tr>
                            <tr class="fragment">
                                <td><strong>TensorFlow/Keras</strong></td>
                                <td>Uniform (Glorot)</td>
                                <td>Zero</td>
                            </tr>
                        </tbody>
                    </table>
                    <pre class="fragment"><code class="python" data-trim data-line-numbers="1-5|6-8">
# PyTorch example - automatic initialization
net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))
X = torch.rand(size=(2, 4))
net(X)  # Lazy layers initialize here

# The framework provides default random initializations
# But we often want to use specific protocols...
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Built-in Initialization: Normal Distribution</h2>
                    <p>Initialize all weights as Gaussian random variables</p>
                    <pre><code class="python" data-trim data-line-numbers="1-6|7-13|15-20|22-25">
# PyTorch implementation
def init_normal(module):
    if type(module) == nn.Linear:
        nn.init.normal_(module.weight, mean=0, std=0.01)
        nn.init.zeros_(module.bias)

# Apply to network
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
# Output: (tensor([-0.0129, -0.0007, -0.0033, 0.0276]), tensor(0.))

# MXNet version
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)

# JAX/Flax version
weight_init = nn.initializers.normal(0.01)
bias_init = nn.initializers.zeros
net = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),
                     nn.relu,
                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])

# TensorFlow/Keras version
tf.keras.layers.Dense(4, 
    kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),
    bias_initializer=tf.zeros_initializer())
                    </code></pre>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.5em;">
                        <p>üí° Small standard deviation (0.01) keeps initial values near zero</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Built-in Initialization: Constant Values</h2>
                    <p>Initialize all parameters to a given constant value</p>
                    <pre><code class="python" data-trim data-line-numbers="1-6|8-11|12-19">
# PyTorch - Initialize to constant value (e.g., 1)
def init_constant(module):
    if type(module) == nn.Linear:
        nn.init.constant_(module.weight, 1)
        nn.init.zeros_(module.bias)

# Apply to network
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
# Output: (tensor([1., 1., 1., 1.]), tensor(0.))

# MXNet version
net.initialize(init=init.Constant(1), force_reinit=True)

# JAX/Flax version
weight_init = nn.initializers.constant(1)
net = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),
                     nn.relu,
                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])
                    </code></pre>
                    <div class="fragment" style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px; font-size: 0.5em;">
                        <p>‚ö†Ô∏è <strong>Warning:</strong> Constant initialization can cause symmetry problems!</p>
                        <p style="font-size: 0.9em;">All neurons compute the same function initially</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Xavier/Glorot Initialization</h2>
                    <p>Maintain variance across layers for <span class="tooltip">linear activations<span class="tooltiptext">Works best with tanh, sigmoid, or no activation</span></span></p>
                    <div class="math-section" style="font-size: 0.5em;">
                        <div class="fragment">
                            <h4>Xavier Uniform:</h4>
                            $$W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)$$
                        </div>
                        <div class="fragment">
                            <h4>Xavier Normal:</h4>
                            $$W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)$$
                        </div>
                    </div>
                    <pre class="fragment" style="font-size: 0.5em;"><code class="python" data-trim data-line-numbers="1-5|6-10">
# PyTorch Xavier initialization
def init_xavier(module):
    if type(module) == nn.Linear:
        nn.init.xavier_uniform_(module.weight)

# Apply Xavier to first layer
net[0].apply(init_xavier)
print(net[0].weight.data[0])
# Output: tensor([-0.0974, 0.1707, 0.5840, -0.5032])
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Different Initialization for Different Layers</h2>
                    <p>Apply different initializers to specific blocks</p>
                    <pre><code class="python" data-trim data-line-numbers="1-8|10-14|16-19">
# Define different initialization strategies
def init_xavier(module):
    if type(module) == nn.Linear:
        nn.init.xavier_uniform_(module.weight)

def init_42(module):
    if type(module) == nn.Linear:
        nn.init.constant_(module.weight, 42)

# Apply to different layers
net[0].apply(init_xavier)    # First layer: Xavier
net[2].apply(init_42)         # Output layer: constant 42
print(net[0].weight.data[0])
print(net[2].weight.data)

# Output:
# tensor([-0.0974, 0.1707, 0.5840, -0.5032])
# tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])
                    </code></pre>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.65em;">
                        <p>üí° Different layers may benefit from different initialization strategies</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Custom Initialization", "url": "https://d2l.ai/chapter_builders-guide/init-param.html#custom-initialization"}]'>
                    <h2 class="truncate-title">Custom Initialization: The Strange Distribution</h2>
                    <p style="font-size: 0.5em;">Sometimes we need initialization methods not provided by frameworks</p>
                    <div class="custom-dist-formula fragment" style="font-size: 0.5em;">
                        <h4>Custom Distribution:</h4>
                        $$w \sim \begin{cases}
                        U(5, 10) & \text{with probability } \frac{1}{4} \\
                        0 & \text{with probability } \frac{1}{2} \\
                        U(-10, -5) & \text{with probability } \frac{1}{4}
                        \end{cases}$$
                    </div>
                    <pre class="fragment" style="font-size: 0.5em;"><code class="python" data-trim data-line-numbers="1-8|9-13">
# PyTorch implementation
def my_init(module):
    if type(module) == nn.Linear:
        print("Init", *[(name, param.shape) 
                        for name, param in module.named_parameters()][0])
        nn.init.uniform_(module.weight, -10, 10)
        module.weight.data *= module.weight.data.abs() >= 5

# Apply custom initialization
net.apply(my_init)
net[0].weight[:2]
# Output: tensor([[0.0000, -7.6364, -0.0000, -6.1206],
#                 [9.3516, -0.0000, 5.1208, -8.4003]])
                    </code></pre>
                    <div class="fragment" style="font-size: 0.5em;">
                        <p>This creates a distribution with:</p>
                        <ul>
                            <li>Half the values are zero (between -5 and 5)</li>
                            <li>Quarter are positive (5 to 10)</li>
                            <li>Quarter are negative (-10 to -5)</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Direct Parameter Manipulation</h2>
                    <p>We always have the option of setting parameters directly</p>
                    <pre><code class="python" data-trim data-line-numbers="1-5|7-11|13-17">
# PyTorch: Direct manipulation
net[0].weight.data[:] += 1           # Add 1 to all weights
net[0].weight.data[0, 0] = 42       # Set specific value
net[0].weight.data[0]
# Output: tensor([42.0000, -6.6364, 1.0000, -5.1206])

# MXNet: Direct manipulation
net[0].weight.data()[:] += 1
net[0].weight.data()[0, 0] = 42
net[0].weight.data()[0]
# array([42., 9.991421, 1., 1.])

# TensorFlow: Direct manipulation (using assign)
net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)
net.layers[1].weights[0][0, 0].assign(42)
net.layers[1].weights[0]
# <tf.Variable shape=(4, 4) ... [[42., 1., 1., 1.], ...]>
                    </code></pre>
                    <div class="fragment" style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px; font-size: 0.5em;">
                        <p>‚ö†Ô∏è <strong>Caution:</strong> Direct manipulation bypasses gradient tracking!</p>
                        <p style="font-size: 0.9em;">Use <code>.data</code> carefully - it can break autograd</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Initialization Best Practices</h2>
                    <div class="best-practices">
                        <table style="font-size: 0.4em;">
                            <thead>
                                <tr>
                                    <th>Activation</th>
                                    <th>Recommended Init</th>
                                    <th>Variance</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td><strong>ReLU</strong></td>
                                    <td>He/Kaiming</td>
                                    <td>$$\text{Var} = \frac{2}{n_{\text{in}}}$$</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Tanh</strong></td>
                                    <td>Xavier/Glorot</td>
                                    <td>$$\text{Var} = \frac{1}{n_{\text{in}}}$$</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Sigmoid</strong></td>
                                    <td>Xavier/Glorot</td>
                                    <td>$$\text{Var} = \frac{2}{n_{\text{in}} + n_{\text{out}}}$$</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>SELU</strong></td>
                                    <td>LeCun</td>
                                    <td>$$\text{Var} = \frac{1}{n_{\text{in}}}$$</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>Leaky ReLU</strong></td>
                                    <td>He (adjusted)</td>
                                    <td>$$\text{Var} = \frac{2}{(1 + \alpha^2) \cdot n_{\text{in}}}$$</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.5em;">
                        <p>Match your initialization to your activation function for optimal results!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens when you use the custom initialization function with uniform(-10, 10) and then multiply by (abs(data) >= 5)?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Values between -5 and 5 become zero",
                                "correct": true,
                                "explanation": "Correct! The condition abs(data) >= 5 is False for values in (-5, 5), so they are multiplied by 0."
                            },
                            {
                                "text": "About 50% of values will be zero",
                                "correct": true,
                                "explanation": "Correct! Uniform(-10, 10) means 50% of values fall in (-5, 5) and become zero."
                            },
                            {
                                "text": "All negative values become positive",
                                "correct": false,
                                "explanation": "The multiplication preserves the sign - negative values outside (-5, 0) remain negative."
                            },
                            {
                                "text": "The resulting distribution has three distinct regions",
                                "correct": true,
                                "explanation": "Correct! The distribution has zeros in the middle, positive values (5 to 10), and negative values (-10 to -5)."
                            },
                            {
                                "text": "This prevents the vanishing gradient problem",
                                "correct": false,
                                "explanation": "This unusual distribution may actually cause training difficulties due to many zero weights."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 12: Tied/Shared Parameters (Vertical) -->
            <section>
                <section data-sources='[{"text": "Tied Parameters", "url": "https://d2l.ai/chapter_builders-guide/parameters.html#tied-parameters"}]'>
                    <h2 class="truncate-title">Parameter Sharing: Elegant Efficiency</h2>
                    <p>Sometimes we want multiple layers to share the same parameters</p>
                    <div class="sharing-benefits" style="font-size: 0.5em;">
                        <div class="benefit fragment">
                            <div class="icon">üíæ</div>
                            <h4>Memory Efficiency</h4>
                            <p>Reduce parameter count</p>
                        </div>
                        <div class="benefit fragment">
                            <div class="icon">üîÑ</div>
                            <h4>Weight Tying</h4>
                            <p>Enforce architectural constraints</p>
                        </div>
                        <div class="benefit fragment">
                            <div class="icon">üßÆ</div>
                            <h4>Regularization</h4>
                            <p>Implicit constraint on model capacity</p>
                        </div>
                        <div class="benefit fragment">
                            <div class="icon">‚ö°</div>
                            <h4>Faster Training</h4>
                            <p>Fewer parameters to update</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementing Shared Parameters</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-13|14-21|23-27|29-31">
# Create a shared layer
shared = nn.LazyLinear(8)

# Use it multiple times in a network
net = nn.Sequential(
    nn.LazyLinear(8), nn.ReLU(),
    shared, nn.ReLU(),  # First use
    shared, nn.ReLU(),  # Second use (same parameters!)
    nn.LazyLinear(1)
)

X = torch.rand(2, 20)
net(X)  # Initialize lazy layers

# Verify parameters are the same object
print(net[2].weight.data[0] == net[4].weight.data[0])
# tensor([True, True, True, True, True, True, True, True])

print(f"Layer 2 weight id: {id(net[2].weight)}")
print(f"Layer 4 weight id: {id(net[4].weight)}")
# Same memory address!

# Modify one affects the other
net[2].weight.data[0, 0] = 100
print(f"Layer 2 weight[0,0]: {net[2].weight.data[0, 0]}")
print(f"Layer 4 weight[0,0]: {net[4].weight.data[0, 0]}")
# Both show 100

# They are literally the same object
print(net[2].weight is net[4].weight)
# True
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">How Gradients Work with Shared Parameters</h2>
                    <div class="gradient-sharing">
                        <p style="font-size: 0.5em;">When parameters are shared, gradients are <strong>accumulated</strong>:</p>
                        <div class="gradient-flow fragment">
                            <pre><code class="python" data-trim>
# During backpropagation:
# grad(shared_weight) = grad_from_layer2 + grad_from_layer4

# Example:
loss = net(X).sum()
loss.backward()

# The shared layer's gradient is the sum of gradients
# from all positions where it's used
print(shared.weight.grad.shape)
# Shows accumulated gradients
                            </code></pre>
                        </div>
                        <div class="fragment emphasis-box mt-md" style="font-size: 0.75em;">
                            <p>$$\frac{\partial L}{\partial W_{\text{shared}}} = \sum_{i \in \text{uses}} \frac{\partial L}{\partial W_i}$$</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Real-World Applications of Parameter Sharing</h2>
                    <div class="applications-grid" style="font-size: 0.5em;">
                        <div class="app-card fragment">
                            <h4>üî§ Autoencoders</h4>
                            <p>Tie encoder and decoder weights</p>
                            <pre><code class="python">decoder.weight = encoder.weight.T</code></pre>
                        </div>
                        <div class="app-card fragment">
                            <h4>üåç Siamese Networks</h4>
                            <p>Process multiple inputs identically</p>
                            <pre><code class="python">out1 = shared_net(input1)
out2 = shared_net(input2)</code></pre>
                        </div>
                        <div class="app-card fragment">
                            <h4>üìö Language Models</h4>
                            <p>Share embedding and output weights</p>
                            <pre><code class="python">output.weight = embedding.weight</code></pre>
                        </div>
                        <div class="app-card fragment">
                            <h4>üîÅ Recurrent Networks</h4>
                            <p>Same weights across time steps</p>
                            <pre><code class="python">for t in range(seq_len):
    h = cell(x[t], h)</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Advanced: Custom Parameter Sharing Patterns</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-14|15-27">
class SymmetricAutoencoder(nn.Module):
    """Autoencoder with tied weights"""
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim)
        # Decoder shares transposed encoder weights
        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)
        
    def forward(self, x):
        # Tie weights dynamically
        self.decoder.weight = nn.Parameter(self.encoder.weight.T)
        encoded = F.relu(self.encoder(x))
        decoded = self.decoder(encoded)
        return decoded

class RecurrentCell(nn.Module):
    """RNN cell with explicit parameter sharing"""
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.weight_ih = nn.Parameter(torch.randn(hidden_size, input_size))
        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
    
    def forward(self, input, hidden):
        # Same parameters used for all time steps
        return torch.tanh(input @ self.weight_ih.T + 
                         hidden @ self.weight_hh.T + self.bias)
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens to gradients when parameters are shared between layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Only the gradient from the last use is kept",
                                "correct": false,
                                "explanation": "All gradients from all uses are considered, not just the last one."
                            },
                            {
                                "text": "Gradients are accumulated (summed) from all uses",
                                "correct": true,
                                "explanation": "Correct! When a parameter is used multiple times, its gradient is the sum of gradients from all positions."
                            },
                            {
                                "text": "Gradients are averaged across all uses",
                                "correct": false,
                                "explanation": "Gradients are summed, not averaged, which can lead to larger gradient magnitudes."
                            },
                            {
                                "text": "An error occurs during backpropagation",
                                "correct": false,
                                "explanation": "PyTorch handles shared parameters gracefully by accumulating gradients."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 13: Parameter Persistence (Vertical) -->
            <section>
                <section data-sources='[{"text": "Saving and Loading Models", "url": "https://d2l.ai/chapter_builders-guide/parameters.html"}]'>
                    <h2 class="truncate-title">Saving and Loading: Model Persistence</h2>
                    <p>Preserving trained models for deployment and reuse</p>
                    <div class="persistence-scenarios" style="font-size: 0.5em;">
                        <div class="scenario fragment">
                            <h4>üîÑ Checkpointing</h4>
                            <p>Save progress during long training runs</p>
                        </div>
                        <div class="scenario fragment">
                            <h4>üöÄ Deployment</h4>
                            <p>Export models for production</p>
                        </div>
                        <div class="scenario fragment">
                            <h4>üî¨ Research</h4>
                            <p>Share models with the community</p>
                        </div>
                        <div class="scenario fragment">
                            <h4>üé® Fine-tuning</h4>
                            <p>Load pretrained models for transfer learning</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Basic Save and Load Operations</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-8|9-12|13-16|18-26">
# Create and train a model
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
# ... training code ...

# Method 1: Save only parameters (recommended)
torch.save(model.state_dict(), 'model_weights.pth')
# Small file, portable, flexible

# Load parameters
model = ModelClass()  # Create model structure
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()  # Set to evaluation mode

# Method 2: Save entire model (less portable)
torch.save(model, 'complete_model.pth')
# Larger file, includes architecture

# Load entire model
model = torch.load('complete_model.pth')
model.eval()
                    </code></pre>
                    <div class="fragment emphasis-box mt-md" style="font-size: 0.5em;">
                        <p>Always prefer saving <code>state_dict()</code> over the entire model!</p>
                        <ul style="font-size: 0.9em; margin-top: 10px;">
                            <li><strong>Portability:</strong> Works across different PyTorch versions</li>
                            <li><strong>Flexibility:</strong> Can load into modified architectures</li>
                            <li><strong>Size:</strong> Smaller files (only parameters, not code)</li>
                            <li><strong>Security:</strong> Avoids potential pickle vulnerabilities</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding State Dictionaries</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-4|6-12|14-19|21-29">
# Examine state dictionary structure
state_dict = model.state_dict()
print(type(state_dict))
# OrderedDict

# View keys (parameter names)
for key in state_dict.keys():
    print(f"{key}: {state_dict[key].shape}")
# 0.weight: torch.Size([256, 784])
# 0.bias: torch.Size([256])
# 2.weight: torch.Size([10, 256])
# 2.bias: torch.Size([10])

# State dict is just a dictionary of tensors
print(type(state_dict['0.weight']))
# <class 'torch.Tensor'>

# Can be manipulated like any dictionary
state_dict['0.bias'].fill_(0)  # Reset bias

# Partial loading (useful for transfer learning)
pretrained_dict = torch.load('pretrained.pth')
model_dict = model.state_dict()
# Filter out unnecessary keys
pretrained_dict = {k: v for k, v in pretrained_dict.items() 
                   if k in model_dict}
model_dict.update(pretrained_dict)
model.load_state_dict(model_dict)
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Complete Training Checkpoint</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-13|15-27|28-42">
# Save complete training state
def save_checkpoint(model, optimizer, epoch, loss, path):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'rng_state': torch.get_rng_state(),
        # Add any other info you need
        'learning_rate': optimizer.param_groups[0]['lr'],
        'training_time': time.time()
    }
    torch.save(checkpoint, path)

# Load and resume training
def load_checkpoint(path, model, optimizer=None):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    torch.set_rng_state(checkpoint['rng_state'])
    
    return model, optimizer, epoch, loss

# Usage during training
for epoch in range(start_epoch, num_epochs):
    train(model, train_loader, optimizer, epoch)
    
    if epoch % save_interval == 0:
        save_checkpoint(
            model, optimizer, epoch, loss,
            f'checkpoint_epoch_{epoch}.pth'
        )
    
    if interrupted:
        model, optimizer, epoch, loss = load_checkpoint(
            'latest_checkpoint.pth', model, optimizer
        )
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Device and Cross-Framework Saving</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-7|9-15|17-25">
# Save for CPU/GPU compatibility
# Save from GPU
torch.save(model.state_dict(), 'model.pth')

# Load on CPU
device = torch.device('cpu')
model.load_state_dict(torch.load('model.pth', map_location=device))

# Load on different GPU
device = torch.device('cuda:1')  # Different GPU
model.load_state_dict(torch.load('model.pth', map_location=device))

# Or use lambda for complex mapping
model.load_state_dict(torch.load('model.pth', 
    map_location=lambda storage, loc: storage))

# Export to ONNX for cross-framework compatibility
dummy_input = torch.randn(1, 784)
torch.onnx.export(model, 
                  dummy_input,
                  "model.onnx",
                  export_params=True,
                  input_names=['input'],
                  output_names=['output'],
                  dynamic_axes={'input': {0: 'batch_size'}})
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Best Practices for Model Persistence</h2>
                    <div class="best-practices" style="font-size: 0.5em;">
                        <div class="practice fragment">
                            <h4>‚úÖ Do:</h4>
                            <ul>
                                <li>Save <code>state_dict()</code> not entire model - preserves portability across PyTorch versions and avoids serializing class definitions</li>
                                <li>Include optimizer state for resuming - maintains learning rate schedules, momentum, and adaptive parameter histories</li>
                                <li>Version your model saves - enables rollback to stable checkpoints and tracks experimental changes</li>
                                <li>Test loading immediately after saving - catches serialization issues early before they corrupt training runs</li>
                                <li>Use descriptive filenames with metadata - includes epoch, loss, timestamp for easy identification and comparison</li>
                            </ul>
                        </div>
                        <div class="practice fragment">
                            <h4>‚ùå Don't:</h4>
                            <ul>
                                <li>Save models during active training step - can corrupt state or save incomplete gradients</li>
                                <li>Forget <code>model.eval()</code> after loading - keeps dropout/batch norm in training mode</li>
                                <li>Mix model versions carelessly - architecture mismatches cause loading failures</li>
                                <li>Ignore device compatibility - leads to CUDA/CPU tensor location errors</li>
                                <li>Rely on pickle for production - creates security risks and version dependencies</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the recommended way to save a PyTorch model?",
                        "type": "single",
                        "options": [
                            {
                                "text": "torch.save(model, \"model.pth\")",
                                "correct": false,
                                "explanation": "Saving the entire model is less portable and can cause version compatibility issues."
                            },
                            {
                                "text": "torch.save(model.state_dict(), \"model.pth\")",
                                "correct": true,
                                "explanation": "Correct! Saving the state dictionary is more portable and flexible for loading."
                            },
                            {
                                "text": "pickle.dump(model, file)",
                                "correct": false,
                                "explanation": "While possible, using pickle directly is not recommended for PyTorch models."
                            },
                            {
                                "text": "model.save(\"model.pth\")",
                                "correct": false,
                                "explanation": "PyTorch models do not have a built-in save() method."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 14: Advanced Parameter Operations (Vertical) -->
            <section>
                <section data-sources='[{"text": "Advanced Parameter Management", "url": "https://d2l.ai/chapter_builders-guide/parameters.html"}]'>
                    <h2 class="truncate-title">Advanced Parameter Operations</h2>
                    <p>Master-level techniques for parameter manipulation</p>
                    <div class="advanced-techniques" style="font-size: 0.5em;">
                        <div class="technique fragment">
                            <h4>üîí Freezing</h4>
                            <p>Selective parameter updates</p>
                        </div>
                        <div class="technique fragment">
                            <h4>üéØ Groups</h4>
                            <p>Different learning rates</p>
                        </div>
                        <div class="technique fragment">
                            <h4>‚úÇÔ∏è Pruning</h4>
                            <p>Remove unnecessary parameters</p>
                        </div>
                        <div class="technique fragment">
                            <h4>üìä Quantization</h4>
                            <p>Reduce precision for efficiency</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Freezing and Unfreezing Parameters</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-8|10-17|18-25|26-32">
# Freeze all parameters
for param in model.parameters():
    param.requires_grad = False

# Freeze specific layers
for name, param in model.named_parameters():
    if 'encoder' in name:
        param.requires_grad = False

# Selective unfreezing for fine-tuning
# Freeze everything first
for param in model.parameters():
    param.requires_grad = False
    
# Unfreeze last few layers
for param in model.classifier.parameters():
    param.requires_grad = True

# Context manager for temporary freezing
from torch import no_grad

with no_grad():
    # Parameters won't track gradients here
    output = model(input)
    # Useful for inference or feature extraction

# Check which parameters are trainable
trainable_params = sum(p.numel() for p in model.parameters() 
                      if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable_params:,} / {total_params:,}")
print(f"Frozen: {total_params - trainable_params:,}")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Groups with Different Learning Rates</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-11|13-22|23-34">
# Create parameter groups for different learning rates
param_groups = [
    {'params': model.encoder.parameters(), 'lr': 1e-4},
    {'params': model.decoder.parameters(), 'lr': 1e-3},
    {'params': model.classifier.parameters(), 'lr': 1e-2}
]

optimizer = torch.optim.Adam(param_groups)

# Each group can have different hyperparameters
print(f"Number of param groups: {len(optimizer.param_groups)}")

# More complex grouping based on parameter properties
fast_params = []
slow_params = []

for name, param in model.named_parameters():
    if 'bias' in name:
        fast_params.append(param)  # Biases learn faster
    else:
        slow_params.append(param)  # Weights learn slower

optimizer = torch.optim.SGD([
    {'params': slow_params, 'lr': 1e-3, 'weight_decay': 1e-4},
    {'params': fast_params, 'lr': 1e-2, 'weight_decay': 0}
])

# Dynamic learning rate adjustment per group
for i, param_group in enumerate(optimizer.param_groups):
    old_lr = param_group['lr']
    new_lr = old_lr * 0.9  # Decay by 10%
    param_group['lr'] = new_lr
    print(f"Group {i}: {old_lr:.5f} -> {new_lr:.5f}")
                    </code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Pruning: Making Networks Sparse</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-5|7-14|16-27|29-32">
import torch.nn.utils.prune as prune

# Prune 30% of connections with smallest magnitude
layer = model.encoder[0]
prune.l1_unstructured(layer, name='weight', amount=0.3)

# Check pruning mask
print(f"Pruned weight shape: {layer.weight.shape}")
print(f"Mask shape: {layer.weight_mask.shape}")
print(f"Sparsity: {(layer.weight_mask == 0).sum().item() / layer.weight_mask.numel():.2%}")

# Structured pruning (remove entire channels)
prune.ln_structured(layer, name='weight', amount=0.5, n=2, dim=0)
# Removes 50% of channels based on L2 norm

# Global pruning across multiple layers
parameters_to_prune = [
    (model.encoder[0], 'weight'),
    (model.encoder[2], 'weight'),
    (model.decoder, 'weight'),
]

prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.2  # Prune 20% globally
)

# Make pruning permanent
for module, param_name in parameters_to_prune:
    prune.remove(module, param_name)
# Now the weights are actually sparse
                    </code></pre>
                </section>

                

                

                <section>
                    <h2 class="truncate-title">Final Test: Comprehensive Parameter Mastery</h2>
                    <div data-mcq='{
                        "question": "Which statements about parameter management are TRUE?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Freezing parameters prevents gradient computation and updates",
                                "correct": true,
                                "explanation": "Correct! Setting requires_grad=False prevents both gradient computation and parameter updates."
                            },
                            {
                                "text": "Parameter groups allow different learning rates for different layers",
                                "correct": true,
                                "explanation": "Correct! Optimizers can handle multiple parameter groups with different hyperparameters."
                            },
                            {
                                "text": "Pruning always reduces model accuracy significantly",
                                "correct": false,
                                "explanation": "Modern pruning techniques can remove 90%+ of parameters with minimal accuracy loss."
                            },
                            {
                                "text": "Shared parameters accumulate gradients from all their uses",
                                "correct": true,
                                "explanation": "Correct! Gradients are summed across all positions where shared parameters are used."
                            },
                            {
                                "text": "State dictionaries contain only weight values",
                                "correct": false,
                                "explanation": "State dictionaries contain all learnable parameters including weights, biases, and other registered parameters."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 15: Introduction to Lazy Initialization (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 6.4: Lazy Initialization", "url": "https://d2l.ai/chapter_builders-guide/lazy-init.html"}]'>
                    <h2 class="truncate-title">Lazy Initialization: Deferred Parameter Setup</h2>
                    <div class="paradox-container">
                        <h3>The Initialization Paradox ü§î</h3>
                        <div class="paradox-list fragment" style="font-size: 0.75em;">
                            <div class="paradox-item">
                                <span class="icon">‚ùì</span>
                                <p>We define networks <strong>without</strong> specifying input dimensions</p>
                            </div>
                            <div class="paradox-item">
                                <span class="icon">üîó</span>
                                <p>We add layers <strong>without</strong> knowing previous layer sizes</p>
                            </div>
                            <div class="paradox-item">
                                <span class="icon">‚öôÔ∏è</span>
                                <p>We "initialize" parameters <strong>before</strong> knowing their shapes</p>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p><strong>Yet it all works!</strong> Thanks to <span class="tooltip">lazy initialization<span class="tooltiptext">A technique where parameter initialization is deferred until the first forward pass, allowing automatic shape inference</span></span></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">What is Lazy Initialization?</h2>
                    <div class="concept-explanation" style="font-size: 0.5em;">
                        <div class="definition-box">
                            <h4>Definition</h4>
                            <p><span class="tooltip">Lazy initialization<span class="tooltiptext">Also called deferred initialization - parameters are not allocated until first data pass</span></span> is a technique where deep learning frameworks <strong>defer parameter initialization</strong> until the first time data passes through the model.</p>
                        </div>
                        <div class="process-flow fragment">
                            <h4>The Process</h4>
                            <div class="flow-steps">
                                <div class="step">
                                    <div class="step-number">1</div>
                                    <div class="step-content">Define architecture with unknown dimensions</div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="step">
                                    <div class="step-number">2</div>
                                    <div class="step-content">Pass first batch through network</div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="step">
                                    <div class="step-number">3</div>
                                    <div class="step-content">Framework infers all shapes</div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="step">
                                    <div class="step-number">4</div>
                                    <div class="step-content">Parameters initialized with correct dimensions</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Benefits of Lazy Initialization</h2>
                    <div class="benefits-grid" style="font-size: 0.5em;">
                        <div class="benefit-card fragment">
                            <div class="icon">üéØ</div>
                            <h4>Simplified Architecture</h4>
                            <p>No need to manually track dimensions through the network</p>
                        </div>
                        <div class="benefit-card fragment">
                            <div class="icon">üîÑ</div>
                            <h4>Flexible Models</h4>
                            <p>Easy to modify architectures without recalculating dimensions</p>
                        </div>
                        <div class="benefit-card fragment">
                            <div class="icon">üõ°Ô∏è</div>
                            <h4>Error Prevention</h4>
                            <p>Eliminates dimension mismatch errors</p>
                        </div>
                        <div class="benefit-card fragment">
                            <div class="icon">üñºÔ∏è</div>
                            <h4>Especially for CNNs</h4>
                            <p>Input resolution affects all subsequent layers automatically</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg" style="font-size: 0.5em;">
                        <p>Lazy initialization is particularly powerful for <span class="tooltip">convolutional networks<span class="tooltiptext">Networks where spatial dimensions change through pooling and convolution operations</span></span> where dimension calculations can be complex</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of lazy initialization?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes training faster",
                                "correct": false,
                                "explanation": "Lazy initialization does not affect training speed; it only changes when parameters are allocated."
                            },
                            {
                                "text": "It automatically infers parameter shapes from data",
                                "correct": true,
                                "explanation": "Correct! Lazy initialization defers parameter allocation until the first forward pass, automatically inferring all dimensions."
                            },
                            {
                                "text": "It reduces memory usage",
                                "correct": false,
                                "explanation": "The same amount of memory is used; parameters are just allocated later."
                            },
                            {
                                "text": "It improves model accuracy",
                                "correct": false,
                                "explanation": "Lazy initialization is about convenience and error prevention, not model performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 16: How Lazy Initialization Works (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Step-by-Step Initialization Process</h2>
                    <div class="initialization-steps" style="font-size: 0.5em;">
                        <div class="step-detail fragment">
                            <h4>Step 1: Network Definition</h4>
                            <pre><code class="python">net = nn.Sequential(
    nn.LazyLinear(256),  # Input dimension unknown
    nn.ReLU(),
    nn.LazyLinear(10)    # Previous layer dimension unknown
)</code></pre>
                            <p>Parameters exist but dimensions are marked as <code>-1</code> or uninitialized</p>
                        </div>
                        <div class="step-detail fragment">
                            <h4>Step 2: First Forward Pass</h4>
                            <pre><code class="python">X = torch.rand(2, 20)  # Batch size 2, features 20
output = net(X)</code></pre>
                            <p>Data flows through, triggering initialization</p>
                        </div>
                        <div class="step-detail fragment">
                            <h4>Step 3: Automatic Shape Inference</h4>
                            <div class="shape-inference">
                                <p>Layer 1: Input shape (2, 20) ‚Üí Weight shape: (256, 20)</p>
                                <p>Layer 2: Input shape (2, 256) ‚Üí Weight shape: (10, 256)</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Under the Hood: Deferred Initialization</h2>
                    <div class="technical-details" style="font-size: 0.5em;">
                        <h4>What Actually Happens?</h4>
                        <div class="timeline-vertical">
                            <div class="timeline-event fragment">
                                <div class="event-marker">1</div>
                                <div class="event-content">
                                    <strong>Parameter Placeholder Creation</strong>
                                    <p>Framework creates parameter objects with unknown dimensions</p>
                                </div>
                            </div>
                            <div class="timeline-event fragment">
                                <div class="event-marker">2</div>
                                <div class="event-content">
                                    <strong>Forward Hook Registration</strong>
                                    <p>Hooks monitor the first forward pass</p>
                                </div>
                            </div>
                            <div class="timeline-event fragment">
                                <div class="event-marker">3</div>
                                <div class="event-content">
                                    <strong>Shape Detection</strong>
                                    <p>Input tensor shape is captured</p>
                                </div>
                            </div>
                            <div class="timeline-event fragment">
                                <div class="event-marker">4</div>
                                <div class="event-content">
                                    <strong>Memory Allocation</strong>
                                    <p>Parameters allocated with correct shapes</p>
                                </div>
                            </div>
                            <div class="timeline-event fragment">
                                <div class="event-marker">5</div>
                                <div class="event-content">
                                    <strong>Weight Initialization</strong>
                                    <p>Values initialized (Xavier, He, etc.)</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When are parameters actually allocated in memory with lazy initialization?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When the model is defined",
                                "correct": false,
                                "explanation": "With lazy initialization, only parameter placeholders are created during model definition."
                            },
                            {
                                "text": "During the first forward pass",
                                "correct": true,
                                "explanation": "Correct! Parameters are allocated when the first batch of data passes through the network."
                            },
                            {
                                "text": "When .initialize() is called",
                                "correct": false,
                                "explanation": "Calling initialize() may register initialization preferences but does not allocate memory until data passes through."
                            },
                            {
                                "text": "During the first backward pass",
                                "correct": false,
                                "explanation": "Parameters must be allocated during forward pass to compute outputs; backward pass requires already-initialized parameters."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 17: Implementation Across Frameworks (Vertical) -->
            <section>
                <section data-sources='[{"text": "PyTorch Lazy Modules Documentation", "url": "https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html"}, {"text": "MXNet Gluon Documentation", "url": "https://mxnet.apache.org/versions/1.9.1/api/python/docs/api/gluon/index.html"}]'>
                    <h2 class="truncate-title">PyTorch: LazyLinear Implementation</h2>
                    <div class="framework-implementation">
                        <h4>PyTorch Approach</h4>
                        <pre><code class="python"># PyTorch uses LazyLinear for deferred initialization
import torch
from torch import nn

# Define network with lazy layers
net = nn.Sequential(
    nn.LazyLinear(256),  # Will infer input dimension
    nn.ReLU(),
    nn.LazyLinear(10)    # Will infer from previous layer
)

# Before first forward pass - dimensions unknown
print(net[0].weight)  # Uninitialized parameter

# First forward pass triggers initialization
X = torch.rand(2, 20)
output = net(X)

# After forward pass - dimensions are set
print(net[0].weight.shape)  # torch.Size([256, 20])</code></pre>
                        <div class="note-box fragment">
                            <p><strong>Note:</strong> <code>LazyLinear</code> replaces the older manual approach of using <code>nn.Linear</code> with calculated dimensions</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Comparison</h2>
                    <div class="comparison-table-container">
                        <table class="framework-comparison" style="font-size: 0.5em;">
                            <thead>
                                <tr>
                                    <th>Framework</th>
                                    <th>Lazy Init Method</th>
                                    <th>Key Feature</th>
                                    <th>Philosophy</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment">
                                    <td><strong>PyTorch</strong></td>
                                    <td><code>LazyLinear</code></td>
                                    <td>Explicit lazy modules</td>
                                    <td>Opt-in convenience</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>MXNet</strong></td>
                                    <td><code>-1</code> dimensions</td>
                                    <td>Special marker value</td>
                                    <td>Implicit handling</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>JAX/Flax</strong></td>
                                    <td>Manual <code>init()</code></td>
                                    <td>Explicit control</td>
                                    <td>No magic</td>
                                </tr>
                                <tr class="fragment">
                                    <td><strong>TensorFlow</strong></td>
                                    <td>Automatic</td>
                                    <td>Transparent inference</td>
                                    <td>User-friendly</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="emphasis-box fragment mt-lg" style="font-size: 0.5em;">
                        <p>Despite different approaches, all frameworks achieve the same goal: <br>
                        <strong>Simplifying network definition by deferring parameter initialization</strong></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which framework requires the most explicit control over parameter initialization?",
                        "type": "single",
                        "options": [
                            {
                                "text": "PyTorch with LazyLinear",
                                "correct": false,
                                "explanation": "PyTorch LazyLinear handles initialization automatically after the first forward pass."
                            },
                            {
                                "text": "JAX/Flax",
                                "correct": true,
                                "explanation": "Correct! JAX/Flax requires manual initialization with explicit dummy inputs and key management."
                            },
                            {
                                "text": "MXNet with -1 dimensions",
                                "correct": false,
                                "explanation": "MXNet handles initialization implicitly using -1 as a placeholder for unknown dimensions."
                            },
                            {
                                "text": "TensorFlow/Keras",
                                "correct": false,
                                "explanation": "TensorFlow handles lazy initialization completely transparently without user intervention."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 18: Practical Examples and Exercises (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Common Pitfalls and Solutions</h2>
                    <div class="pitfalls-grid" style="font-size: 0.5em;">
                        <div class="pitfall-card fragment">
                            <div class="pitfall-header error">‚ö†Ô∏è Pitfall 1: Accessing Uninitialized Parameters</div>
                            <pre><code class="python"># Wrong - parameters not ready yet
net = nn.Sequential(nn.LazyLinear(256))
print(net[0].weight.data())  # Error!</code></pre>
                            <div class="solution">
                                <strong>‚úÖ Solution:</strong>
                                <pre><code class="python"># Pass data first
X = torch.rand(1, 20)
net(X)
print(net[0].weight.shape)  # Now works!</code></pre>
                            </div>
                        </div>
                        <div class="pitfall-card fragment">
                            <div class="pitfall-header error">‚ö†Ô∏è Pitfall 2: Mismatched Dimensions</div>
                            <pre><code class="python"># Network initialized with shape (batch, 20)
net(torch.rand(2, 20))
# Later, wrong shape
net(torch.rand(2, 30))  # Error!</code></pre>
                            <div class="solution">
                                <strong>‚úÖ Solution:</strong>
                                <p>Ensure consistent input dimensions after initialization</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exercise Solutions from the Book</h2>
                    <div class="exercise-container" style="font-size: 0.5em;">
                        <div class="exercise fragment">
                            <h4>Exercise 1: Partial Dimension Specification</h4>
                            <p><em>What happens if you specify input dimensions to the first layer but not subsequent layers?</em></p>
                            <div class="answer">
                                <pre><code class="python"># First layer knows input dimension
net = nn.Sequential(
    nn.Linear(20, 256),  # Fully specified
    nn.ReLU(),
    nn.LazyLinear(10)    # Still lazy!
)
# First layer initializes immediately
# Second layer waits for forward pass</code></pre>
                                <p><strong>Answer:</strong> Only layers with complete dimension info initialize immediately</p>
                            </div>
                        </div>
                        <div class="exercise fragment">
                            <h4>Exercise 2: Varying Input Dimensions</h4>
                            <p><em>How to handle varying input dimensionality?</em></p>
                            <div class="answer">
                                <p><strong>Solution:</strong> Use adaptive layers or padding/truncation strategies</p>
                                <pre><code class="python"># Adaptive pooling for varying spatial dimensions
nn.AdaptiveAvgPool2d((1, 1))  # Always outputs 1x1</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Best Practices Summary</h2>
                    <div class="best-practices" style="font-size: 0.5em;">
                        <div class="practice-item fragment">
                            <div class="icon">‚úÖ</div>
                            <div class="content">
                                <h4>Use Lazy Initialization for Prototyping</h4>
                                <p>Speeds up experimentation and reduces errors during architecture exploration</p>
                            </div>
                        </div>
                        <div class="practice-item fragment">
                            <div class="icon">üìè</div>
                            <div class="content">
                                <h4>Document Expected Input Shapes</h4>
                                <p>Even with lazy init, document expected dimensions for clarity</p>
                            </div>
                        </div>
                        <div class="practice-item fragment">
                            <div class="icon">üß™</div>
                            <div class="content">
                                <h4>Test with Representative Data Early</h4>
                                <p>Initialize with realistic data shapes to catch issues early</p>
                            </div>
                        </div>
                        <div class="practice-item fragment">
                            <div class="icon">üîÑ</div>
                            <div class="content">
                                <h4>Consider Framework Philosophy</h4>
                                <p>Choose frameworks based on your preference for explicit vs implicit behavior</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "After a network with lazy initialization has been initialized with input shape (32, 784), what happens if you later pass input with shape (64, 784)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The network reinitializes its parameters",
                                "correct": false,
                                "explanation": "Parameters are only initialized once; after that, shapes are fixed."
                            },
                            {
                                "text": "It works fine - only batch size changed",
                                "correct": true,
                                "explanation": "Correct! Changing batch size is fine; the feature dimension (784) remains the same."
                            },
                            {
                                "text": "It throws a dimension mismatch error",
                                "correct": false,
                                "explanation": "Batch size can vary; only feature dimensions must match after initialization."
                            },
                            {
                                "text": "The network automatically adapts its weights",
                                "correct": false,
                                "explanation": "Once initialized, weight shapes are fixed and cannot automatically adapt."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 19: Introduction to Custom Layers (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Custom Layers", "url": "https://d2l.ai/chapter_builders-guide/custom-layer.html"}]'>
                    <h2 class="truncate-title">Why Create Custom Layers?</h2>
                    <div class="custom-layer-intro">
                        <div class="scenario-grid" style="font-size: 0.5em; display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 0.5rem;">
                            <div class="scenario fragment">
                                <div class="icon">üî¨</div>
                                <h4>Novel Research</h4>
                                <p>Implement cutting-edge activation functions or architectures not yet in libraries</p>
                            </div>
                            <div class="scenario fragment">
                                <div class="icon">üéØ</div>
                                <h4>Domain-Specific Operations</h4>
                                <p>Create specialized transformations for your particular problem domain</p>
                            </div>
                            <div class="scenario fragment">
                                <div class="icon">‚ö°</div>
                                <h4>Performance Optimization</h4>
                                <p>Combine multiple operations into a single efficient layer</p>
                            </div>
                            <div class="scenario fragment">
                                <div class="icon">üß™</div>
                                <h4>Experimentation</h4>
                                <p>Rapidly prototype new architectural ideas without waiting for library updates</p>
                            </div>
                        </div>
                    </div>
                    <div class="emphasis-box fragment mt-lg">
                        <p>Custom layers give you complete control over the <span class="tooltip">forward pass<span class="tooltiptext">The computation that transforms input to output during inference</span></span> and <span class="tooltip">gradient flow<span class="tooltiptext">How gradients propagate backwards through your layer during training</span></span></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Two Types of Custom Layers</h2>
                    <div class="layer-types-comparison" style="font-size: 0.5em;">
                        <div class="layer-type fragment">
                            <h3>Layers Without Parameters</h3>
                            <div class="layer-card">
                                <div class="icon">üìä</div>
                                <h4>Stateless Transformations</h4>
                                <ul>
                                    <li>No learnable weights or biases</li>
                                    <li>Deterministic operations on input</li>
                                    <li>Examples: Normalization, centering, clipping</li>
                                </ul>
                                <pre><code class="language-python">class CenteredLayer(nn.Module):
    def forward(self, X):
        return X - X.mean()</code></pre>
                            </div>
                        </div>
                        <div class="layer-type fragment">
                            <h3>Layers With Parameters</h3>
                            <div class="layer-card">
                                <div class="icon">üß†</div>
                                <h4>Learnable Transformations</h4>
                                <ul>
                                    <li>Contains trainable weights and biases</li>
                                    <li>Parameters updated during backprop</li>
                                    <li>Examples: Custom linear layers, attention</li>
                                </ul>
                                <pre><code class="language-python">class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        self.weight = nn.Parameter(...)
        self.bias = nn.Parameter(...)</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which scenario would most benefit from a custom layer implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Using a standard ReLU activation function",
                                "correct": false,
                                "explanation": "ReLU is already available in all frameworks; no custom layer needed."
                            },
                            {
                                "text": "Implementing a novel activation function from recent research",
                                "correct": true,
                                "explanation": "Correct! Novel research ideas often require custom implementations before library support."
                            },
                            {
                                "text": "Adding a standard convolutional layer",
                                "correct": false,
                                "explanation": "Standard convolutions are well-supported; use existing implementations."
                            },
                            {
                                "text": "Applying dropout regularization",
                                "correct": false,
                                "explanation": "Dropout is a standard operation available in all deep learning frameworks."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 20: Layers Without Parameters (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">CenteredLayer: A Simple Custom Layer</h2>
                    <div class="centered-layer-concept">
                        <h3>Mathematical Definition</h3>
                        <div class="math-display">
                            $$\text{output} = \text{input} - \text{mean}(\text{input})$$
                        </div>
                        <div class="visual-explanation fragment">
                            <div id="centered-layer-visual" style="width: 100%; height: 300px;"></div>
                        </div>
                        <div class="explanation fragment" style="font-size: 0.5em;">
                            <p>The <span class="tooltip">CenteredLayer<span class="tooltiptext">A custom layer that centers data by subtracting the mean</span></span> performs mean centering:</p>
                            <ul>
                                <li>Computes the mean of all input elements</li>
                                <li>Subtracts this mean from each element</li>
                                <li>Results in zero-centered output</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementing CenteredLayer in PyTorch</h2>
                    <pre><code class="language-python">import torch
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, X):
        return X - X.mean()</code></pre>
                    
                    <div class="code-explanation fragment" style="font-size: 0.75em;">
                        <h4>Key Components:</h4>
                        <ul>
                            <li><strong>Inheritance:</strong> Extends <span class="tooltip">nn.Module<span class="tooltiptext">Base class for all neural network modules in PyTorch</span></span></li>
                            <li><strong>Constructor:</strong> Calls parent's __init__ (no parameters to initialize)</li>
                            <li><strong>Forward Method:</strong> Defines the computation performed</li>
                            <li><strong>Automatic Differentiation:</strong> PyTorch handles gradients automatically</li>
                        </ul>
                    </div>
                    
                    <div class="usage-example fragment" style="font-size: 0.75em;">
                        <h4>Testing the Layer:</h4>
                        <pre><code class="language-python">layer = CenteredLayer()
Y = layer(torch.FloatTensor([1, 2, 3, 4, 5]))
print(Y)  # tensor([-2., -1.,  0.,  1.,  2.])</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Using CenteredLayer in Models</h2>
                    <pre><code class="language-python" data-line-numbers="1-6|8-9|11-16">net = nn.Sequential(
    nn.Linear(8, 128),
    CenteredLayer(),  # Our custom layer
    nn.ReLU(),
    nn.Linear(128, 10)
)

# Test with random data
X = torch.rand(4, 8)

# Forward pass
Y = net(X)
print(f"Input shape: {X.shape}")
print(f"Output shape: {Y.shape}")
print(f"Output mean: {Y.mean():.6f}")
# Output mean will be close to 0</code></pre>
                    
                    <div class="integration-notes fragment" style="font-size: 0.75em;">
                        <h4>Integration Benefits:</h4>
                        <ul>
                            <li>‚úÖ Seamlessly integrates with <span class="tooltip">nn.Sequential<span class="tooltiptext">Container module that chains layers in sequence</span></span></li>
                            <li>‚úÖ Works with automatic differentiation</li>
                            <li>‚úÖ Compatible with all PyTorch optimizers</li>
                            <li>‚úÖ Can be saved and loaded like standard layers</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens to the gradients when backpropagating through a CenteredLayer?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Gradients are blocked and set to zero",
                                "correct": false,
                                "explanation": "The centering operation allows gradients to flow through, just modified by the operation."
                            },
                            {
                                "text": "Each gradient is reduced by the mean of all gradients",
                                "correct": true,
                                "explanation": "Correct! The derivative of (x - mean(x)) distributes the gradient adjustment across all elements."
                            },
                            {
                                "text": "Gradients pass through unchanged",
                                "correct": false,
                                "explanation": "The mean subtraction affects gradient flow; each element influences all others through the mean."
                            },
                            {
                                "text": "Only the largest gradient is preserved",
                                "correct": false,
                                "explanation": "All gradients are preserved but adjusted based on the centering operation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 21: Layers With Parameters (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Custom Linear Layer with Parameters</h2>
                    <div class="mylinear-concept">
                        <h3>Mathematical Definition</h3>
                        <div class="math-display">
                            $$y = \text{ReLU}(Wx + b)$$
                            <div class="math-components fragment">
                                <span style="color: #10099F;">$W \in \mathbb{R}^{m \times n}$</span>: Weight matrix<br>
                                <span style="color: #2DD2C0;">$b \in \mathbb{R}^m$</span>: Bias vector<br>
                                <span style="color: #FC8484;">$\text{ReLU}(z) = \max(0, z)$</span>: Activation
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Implementing MyLinear Layer</h2>
                    <pre><code class="language-python">import torch
from torch import nn
import torch.nn.functional as F

class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)</code></pre>
                    
                    <div class="parameter-explanation fragment" style="font-size: 0.5em;">
                        <h4>Understanding <span class="tooltip">nn.Parameter<span class="tooltiptext">Wrapper that tells PyTorch this tensor should be treated as a learnable parameter</span></span>:</h4>
                        <ul>
                            <li>Automatically registered as model parameter</li>
                            <li>Included in model.parameters() for optimizer</li>
                            <li>Saved when serializing the model</li>
                            <li>Moved to GPU with model.cuda()</li>
                        </ul>
                    </div>
                    
                    <div class="initialization-note fragment" style="font-size: 0.5em;">
                        <div class="emphasis-box">
                            <p>‚ö†Ô∏è Using <code>torch.randn</code> for initialization is simple but not optimal. 
                            Production code should use proper initialization like <span class="tooltip">Xavier/He initialization<span class="tooltiptext">Methods to initialize weights based on layer dimensions to maintain gradient magnitudes</span></span></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Management in Custom Layers</h2>
                    <pre><code class="language-python" data-line-numbers="1-5|7-11|12-19">linear = MyLinear(5, 3)

# Access parameters
print(linear.weight.shape)  # torch.Size([5, 3])
print(linear.bias.shape)    # torch.Size([3])

# Parameters are automatically tracked
for name, param in linear.named_parameters():
    print(f"{name}: {param.shape}")
# weight: torch.Size([5, 3])
# bias: torch.Size([3])

# Use in a model
net = nn.Sequential(
    MyLinear(64, 8),
    MyLinear(8, 1)
)

print(f"Total parameters: {sum(p.numel() for p in net.parameters())}")</code></pre>
                    
                    <div class="parameter-features fragment" style="font-size: 0.5em;">
                        <h4>Automatic Features:</h4>
                        <div class="feature-grid" style="display: flex; justify-content: space-between; gap: 1rem;">
                            <div class="feature" style="flex: 1; text-align: center;">
                                <span class="icon">üéØ</span>
                                <p>Gradient computation</p>
                            </div>
                            <div class="feature" style="flex: 1; text-align: center;">
                                <span class="icon">üíæ</span>
                                <p>Model serialization</p>
                            </div>
                            <div class="feature" style="flex: 1; text-align: center;">
                                <span class="icon">üöÄ</span>
                                <p>Device placement</p>
                            </div>
                            <div class="feature" style="flex: 1; text-align: center;">
                                <span class="icon">üîß</span>
                                <p>Optimizer updates</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Advanced: Tensor Reduction Layer</h2>
                    <div class="tensor-reduction-concept" style="font-size: 0.75em;">
                        <h3>Mathematical Formulation</h3>
                        <div class="math-display">
                            $$y_k = \sum_{i,j} W_{ijk} \cdot x_i \cdot x_j$$
                        </div>
                        <div class="explanation fragment" style="font-size: 0.75em;">
                            <p>This creates a <span class="tooltip">bilinear form<span class="tooltiptext">A function that is linear in each of its arguments when the others are held fixed</span></span> with learnable weights:</p>
                            <ul>
                                <li><strong>Input:</strong> Vector x ‚àà ‚Ñù‚Åø</li>
                                <li><strong>Weights:</strong> Tensor W ‚àà ‚Ñù‚ÅøÀ£‚ÅøÀ£·µè</li>
                                <li><strong>Output:</strong> Vector y ‚àà ‚Ñù·µè</li>
                            </ul>
                        </div>
                        <pre class="fragment"><code class="language-python">class ReductionLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.W = nn.Parameter(
            torch.randn(input_dim, input_dim, output_dim) * 0.01
        )
    
    def forward(self, x):
        # x: [batch_size, input_dim]
        batch_size = x.shape[0]
        y = []
        for k in range(self.W.shape[2]):
            # Compute y_k = sum_ij W_ijk * x_i * x_j
            y_k = torch.sum(
                self.W[:, :, k] * torch.outer(x[0], x[0])
            )
            y.append(y_k)
        return torch.stack(y)</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we use nn.Parameter instead of regular tensors for weights in custom layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "nn.Parameter makes computations faster",
                                "correct": false,
                                "explanation": "nn.Parameter does not affect computation speed; it affects how PyTorch treats the tensor."
                            },
                            {
                                "text": "It automatically registers the tensor for gradient updates and model operations",
                                "correct": true,
                                "explanation": "Correct! nn.Parameter tells PyTorch this tensor should be trained and included in model operations."
                            },
                            {
                                "text": "Regular tensors cannot be used in forward() methods",
                                "correct": false,
                                "explanation": "Regular tensors can be used in forward(), but they will not be trained."
                            },
                            {
                                "text": "nn.Parameter provides better random initialization",
                                "correct": false,
                                "explanation": "nn.Parameter is just a wrapper; initialization is done separately."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 22: Practical Examples and Exercises (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Building Complex Models with Custom Layers</h2>
                    <pre><code class="language-python">class CustomNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(784, 128)
        self.centered = CenteredLayer()
        self.custom_linear = MyLinear(128, 64)
        self.output = nn.Linear(64, 10)
    
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.centered(x)  # Center activations
        x = self.custom_linear(x)  # Custom transformation
        x = self.output(x)
        return F.log_softmax(x, dim=1)

# Create and test the model
model = CustomNet()
test_input = torch.randn(32, 784)
output = model(test_input)
print(f"Output shape: {output.shape}")  # [32, 10]</code></pre>
                    
                    <div class="design-principles fragment" style="font-size: 0.5em;">
                        <h4>Best Practices for Custom Layers:</h4>
                        <ul>
                            <li>üéØ <strong>Single Responsibility:</strong> Each layer should do one thing well</li>
                            <li>üìê <strong>Shape Preservation:</strong> Document expected input/output shapes</li>
                            <li>üîÑ <strong>Gradient Flow:</strong> Ensure operations are differentiable</li>
                            <li>üß™ <strong>Testing:</strong> Verify forward and backward passes</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Exercise: Fourier Coefficient Layer</h2>
                    <div class="exercise-box" style="font-size: 0.5em;">
                        <h3>Challenge: Implement a layer that returns Fourier coefficients</h3>
                        <div class="exercise-requirements">
                            <p><strong>Requirements:</strong></p>
                            <ul>
                                <li>Input: Signal of length n</li>
                                <li>Output: First n/2 <span class="tooltip">Fourier coefficients<span class="tooltiptext">Frequency domain representation of the signal</span></span></li>
                                <li>Should handle batch processing</li>
                            </ul>
                        </div>
                        <pre class="fragment"><code class="language-python">class FourierLayer(nn.Module):
    def __init__(self, return_complex=False):
        super().__init__()
        self.return_complex = return_complex
    
    def forward(self, x):
        # Compute FFT along last dimension
        fft = torch.fft.rfft(x, dim=-1)
        
        # Return first half of coefficients
        n = x.shape[-1]
        coeffs = fft[..., :n//2]
        
        if self.return_complex:
            return coeffs
        else:
            # Return magnitude
            return torch.abs(coeffs)</code></pre>
                        <div class="hint fragment">
                            <p>üí° <strong>Hint:</strong> Use <code>torch.fft.rfft</code> for real-valued inputs</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Common Pitfalls and Solutions</h2>
                    <div class="pitfalls-grid" style="font-size: 0.5em;">
                        <div class="pitfall fragment">
                            <h4>‚ùå Pitfall: Breaking Gradient Flow</h4>
                            <pre><code class="language-python"># Bad: Using .numpy() breaks gradients
def forward(self, x):
    return torch.tensor(x.numpy() * 2)</code></pre>
                            <h4>‚úÖ Solution:</h4>
                            <pre><code class="language-python"># Good: Keep operations in PyTorch
def forward(self, x):
    return x * 2</code></pre>
                        </div>
                        
                        <div class="pitfall fragment">
                            <h4>‚ùå Pitfall: Wrong Device Placement</h4>
                            <pre><code class="language-python"># Bad: Creating tensors on CPU
def forward(self, x):
    bias = torch.zeros(x.shape[-1])
    return x + bias  # Error if x is on GPU</code></pre>
                            <h4>‚úÖ Solution:</h4>
                            <pre><code class="language-python"># Good: Match device
def forward(self, x):
    bias = torch.zeros(x.shape[-1], device=x.device)
    return x + bias</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Comparison: Custom Layers</h2>
                    <div class="framework-comparison" style="font-size: 0.5em;">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>PyTorch</th>
                                    <th>TensorFlow/Keras</th>
                                    <th>JAX</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Base Class</td>
                                    <td><code>nn.Module</code></td>
                                    <td><code>tf.keras.Layer</code></td>
                                    <td>Functions + <code>@jax.jit</code></td>
                                </tr>
                                <tr>
                                    <td>Parameters</td>
                                    <td><code>nn.Parameter</code></td>
                                    <td><code>self.add_weight()</code></td>
                                    <td>Explicit state</td>
                                </tr>
                                <tr>
                                    <td>Forward Pass</td>
                                    <td><code>forward()</code></td>
                                    <td><code>call()</code></td>
                                    <td>Regular function</td>
                                </tr>
                                <tr>
                                    <td>Auto-diff</td>
                                    <td>Automatic</td>
                                    <td>Automatic</td>
                                    <td><code>jax.grad()</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="framework-choice fragment">
                        <p><strong>Choose based on:</strong> Your ecosystem, team expertise, and deployment requirements</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When implementing a custom layer, which of these practices should you follow for production code?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Use proper weight initialization (Xavier/He) instead of random",
                                "correct": true,
                                "explanation": "Proper initialization helps with training stability and convergence."
                            },
                            {
                                "text": "Always include batch normalization in every custom layer",
                                "correct": false,
                                "explanation": "Batch normalization should be added based on need, not automatically."
                            },
                            {
                                "text": "Test gradient flow with a small example",
                                "correct": true,
                                "explanation": "Testing gradients ensures your layer is properly differentiable."
                            },
                            {
                                "text": "Document expected input and output shapes",
                                "correct": true,
                                "explanation": "Clear documentation helps users understand how to use your layer."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Section 7: File I/O (Vertical) -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 6.6: File I/O", "url": "https://d2l.ai/chapter_builders-guide/read-write.html"}]'>
                    <h2 class="truncate-title">File I/O: Saving and Loading Models</h2>
                    <div class="io-overview">
                        <div class="fragment">
                            <h3>Why Model Persistence Matters</h3>
                            <div class="reasons-grid" style="font-size: 0.5em;">
                                <div class="reason-card">
                                    <div class="icon">üíæ</div>
                                    <h4>Training Checkpoints</h4>
                                    <p>Save progress during long training runs</p>
                                </div>
                                <div class="reason-card">
                                    <div class="icon">üöÄ</div>
                                    <h4>Deployment</h4>
                                    <p>Use trained models in production</p>
                                </div>
                                <div class="reason-card">
                                    <div class="icon">üîÑ</div>
                                    <h4>Transfer Learning</h4>
                                    <p>Reuse pretrained weights</p>
                                </div>
                                <div class="reason-card">
                                    <div class="icon">üìä</div>
                                    <h4>Reproducibility</h4>
                                    <p>Share models with others</p>
                                </div>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-lg">
                            <p>‚ö†Ô∏è We save <strong>parameters</strong>, not architecture! Models must be reconstructed in code.</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Loading and Saving Tensors</h2>
                    <div class="tensor-io-section">
                        <h3>Single Tensor Operations</h3>
                        <div class="code-tabs">
                            <div class="tab-headers">
                                <button class="tab-header active" data-framework="pytorch">PyTorch</button>
                                <button class="tab-header" data-framework="mxnet">MXNet</button>
                                <button class="tab-header" data-framework="jax">JAX</button>
                                <button class="tab-header" data-framework="tensorflow">TensorFlow</button>
                            </div>
                            <div class="tab-content">
                                <div class="tab-pane active" data-framework="pytorch">
                                    <pre><code class="language-python">import torch

# Save a tensor
x = torch.arange(4)
torch.save(x, 'x-file')

# Load the tensor back
x2 = torch.load('x-file')
print(x2)  # tensor([0, 1, 2, 3])</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="mxnet">
                                    <pre><code class="language-python">from mxnet import np, npx
npx.set_np()

# Save a tensor
x = np.arange(4)
npx.save('x-file', x)

# Load the tensor back
x2 = npx.load('x-file')
print(x2)  # [array([0., 1., 2., 3.])]</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="jax">
                                    <pre><code class="language-python">from jax import numpy as jnp

# Save a tensor
x = jnp.arange(4)
jnp.save('x-file.npy', x)

# Load the tensor back
x2 = jnp.load('x-file.npy', allow_pickle=True)
print(x2)  # Array([0, 1, 2, 3], dtype=int32)</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="tensorflow">
                                    <pre><code class="language-python">import tensorflow as tf
import numpy as np

# Save a tensor
x = tf.range(4)
np.save('x-file.npy', x)

# Load the tensor back
x2 = np.load('x-file.npy', allow_pickle=True)
print(x2)  # array([0, 1, 2, 3], dtype=int32)</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Saving Tensor Collections</h2>
                    <div class="collections-section" style="font-size: 0.5em;">
                        <h3>Lists and Dictionaries</h3>
                        <div class="code-comparison">
                            <div class="code-block">
                                <h4>Saving Lists of Tensors</h4>
                                <pre><code class="language-python"># PyTorch
y = torch.zeros(4)
torch.save([x, y], 'x-files')
x2, y2 = torch.load('x-files')

# MXNet
y = np.zeros(4)
npx.save('x-files', [x, y])
x2, y2 = npx.load('x-files')</code></pre>
                            </div>
                            <div class="code-block">
                                <h4>Saving Dictionaries</h4>
                                <pre><code class="language-python"># PyTorch
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')

# JAX
mydict = {'x': x, 'y': y}
jnp.save('mydict.npy', mydict)
mydict2 = jnp.load('mydict.npy', 
                   allow_pickle=True)</code></pre>
                            </div>
                        </div>
                        <div class="fragment emphasis-box mt-md">
                            <p>üí° Dictionaries are perfect for saving named parameters in models!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Tensor I/O</h2>
                    <div data-mcq='{
                        "question": "What is saved when you save a tensor to disk?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The tensor values and computation graph",
                                "correct": false,
                                "explanation": "The computation graph is not saved - only the tensor values are persisted."
                            },
                            {
                                "text": "Only the tensor values and shape",
                                "correct": true,
                                "explanation": "Correct! Tensors are saved as raw numerical data with shape information."
                            },
                            {
                                "text": "The tensor values and gradient information",
                                "correct": false,
                                "explanation": "Gradients are not saved - they are recomputed during training."
                            },
                            {
                                "text": "The entire Python object including methods",
                                "correct": false,
                                "explanation": "Only the numerical data is saved, not the Python object structure."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Model Parameter Management</h2>
                    <div class="parameter-management">
                        <h3>The MLP Example</h3>
                        <pre><code class="language-python">class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.LazyLinear(256)
        self.output = nn.LazyLinear(10)
    
    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

# Create and initialize model
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)  # Initialize lazy layers</code></pre>
                        <div class="fragment emphasis-box mt-md">
                            <p>The <span class="tooltip">state_dict<span class="tooltiptext">A Python dictionary that maps layer names to their parameter tensors</span></span> contains all learnable parameters</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Saving Model Parameters</h2>
                    <div class="saving-params-section">
                        <div class="code-tabs">
                            <div class="tab-headers">
                                <button class="tab-header active" data-framework="pytorch">PyTorch</button>
                                <button class="tab-header" data-framework="mxnet">MXNet</button>
                                <button class="tab-header" data-framework="jax">JAX/Flax</button>
                                <button class="tab-header" data-framework="tensorflow">TensorFlow</button>
                            </div>
                            <div class="tab-content">
                                <div class="tab-pane active" data-framework="pytorch">
                                    <pre><code class="language-python"># Save model parameters
torch.save(net.state_dict(), 'mlp.params')

# The state_dict contains:
# - hidden.weight
# - hidden.bias  
# - output.weight
# - output.bias</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="mxnet">
                                    <pre><code class="language-python"># Save model parameters
net.save_parameters('mlp.params')

# Saves all parameters from:
# - net.hidden
# - net.output</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="jax">
                                    <pre><code class="language-python">from flax.training import checkpoints

# Save parameters
checkpoints.save_checkpoint(
    'ckpt_dir', 
    params, 
    step=1, 
    overwrite=True
)</code></pre>
                                </div>
                                <div class="tab-pane" data-framework="tensorflow">
                                    <pre><code class="language-python"># Save model weights
net.save_weights('mlp.params')

# Saves weights for:
# - flatten layer
# - hidden layer
# - output layer</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="fragment file-format-info mt-md">
                            <h4>Common File Formats</h4>
                            <div class="format-grid">
                                <div class="format-card">
                                    <code>.pth, .pt</code>
                                    <p>PyTorch</p>
                                </div>
                                <div class="format-card">
                                    <code>.params</code>
                                    <p>MXNet</p>
                                </div>
                                <div class="format-card">
                                    <code>.npy</code>
                                    <p>NumPy/JAX</p>
                                </div>
                                <div class="format-card">
                                    <code>.h5</code>
                                    <p>TensorFlow/Keras</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Loading Model Parameters</h2>
                    <div class="loading-params-section" style="font-size: 0.5em;">
                        <h3>The Two-Step Process</h3>
                        <div class="loading-steps">
                            <div class="step fragment">
                                <div class="step-number">1</div>
                                <div class="step-content">
                                    <h4>Recreate Architecture</h4>
                                    <pre><code class="language-python"># Must define the same architecture
clone = MLP()</code></pre>
                                </div>
                            </div>
                            <div class="step fragment">
                                <div class="step-number">2</div>
                                <div class="step-content">
                                    <h4>Load Parameters</h4>
                                    <pre><code class="language-python"># Load the saved parameters
clone.load_state_dict(
    torch.load('mlp.params')
)
clone.eval()  # Set to evaluation mode</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="fragment verification-section mt-lg">
                            <h4>Verify the Loaded Model</h4>
                            <pre><code class="language-python"># Both models should produce identical outputs
Y_clone = clone(X)
assert torch.all(Y_clone == Y)  # True!</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Model Parameters</h2>
                    <div data-mcq='{
                        "question": "Why must we recreate the model architecture before loading parameters?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Parameters files dont contain architecture information",
                                "correct": true,
                                "explanation": "Correct! Only the numerical values of parameters are saved, not the model structure."
                            },
                            {
                                "text": "It improves loading speed",
                                "correct": false,
                                "explanation": "Speed is not the reason - its because architecture isnt saved."
                            },
                            {
                                "text": "To prevent security vulnerabilities",
                                "correct": false,
                                "explanation": "While security is a concern with arbitrary code, the main reason is that architecture isnt serialized."
                            },
                            {
                                "text": "The frameworks require it for backward compatibility",
                                "correct": false,
                                "explanation": "This is a fundamental design choice, not a compatibility issue."
                            }
                        ]
                    }'></div>
                </section>

                <section data-sources='[{"text": "PyTorch Saving and Loading Tutorial", "url": "https://pytorch.org/tutorials/beginner/saving_loading_models.html"}, {"text": "Best Practices for Model Checkpointing", "url": "https://www.tensorflow.org/guide/checkpoint"}]'>
                    <h2 class="truncate-title">Practical Checkpointing Strategies</h2>
                    <div class="checkpointing-section">
                        <h3>Training with Checkpoints</h3>
                        <pre><code class="language-python">def train_with_checkpoints(model, dataloader, epochs=100):
    optimizer = torch.optim.Adam(model.parameters())
    
    for epoch in range(epochs):
        # Training loop
        for batch in dataloader:
            loss = compute_loss(model, batch)
            loss.backward()
            optimizer.step()
        
        # Save checkpoint every 10 epochs
        if epoch % 10 == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': loss,
            }
            torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pt')
            print(f"Checkpoint saved at epoch {epoch}")</code></pre>
                        <div class="fragment checkpoint-timeline mt-md">
                            <div id="checkpoint-timeline-viz"></div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Resuming from Checkpoints</h2>
                    <div class="resume-training-section">
                        <pre><code class="language-python">def resume_training(model, checkpoint_path):
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path)
    
    # Restore model state
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Restore optimizer state
    optimizer = torch.optim.Adam(model.parameters())
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # Get training progress
    start_epoch = checkpoint['epoch'] + 1
    last_loss = checkpoint['loss']
    
    print(f"Resuming from epoch {start_epoch}")
    print(f"Last loss: {last_loss:.4f}")
    
    return model, optimizer, start_epoch</code></pre>
                        <div class="fragment emphasis-box mt-md">
                            <p>üí° Always save the <span class="tooltip">optimizer state<span class="tooltiptext">Includes momentum buffers and learning rate schedules</span></span> for perfect resumption!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Transfer Learning Workflow</h2>
                    <div class="transfer-learning-section">
                        <h3>Loading Pretrained Weights</h3>
                        <div class="transfer-steps">
                            <div class="transfer-card fragment">
                                <h4>1. Load Pretrained Model</h4>
                                <pre><code class="language-python"># Load a pretrained ResNet
pretrained = torch.load('resnet50.pth')
model.load_state_dict(pretrained, 
                     strict=False)</code></pre>
                            </div>
                            <div class="transfer-card fragment">
                                <h4>2. Freeze Base Layers</h4>
                                <pre><code class="language-python"># Freeze all but last layer
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad = True</code></pre>
                            </div>
                            <div class="transfer-card fragment">
                                <h4>3. Fine-tune</h4>
                                <pre><code class="language-python"># Train only the new layers
optimizer = torch.optim.Adam(
    model.fc.parameters(),
    lr=0.001
)</code></pre>
                            </div>
                        </div>
                        <div class="fragment transfer-viz mt-md">
                            <div id="transfer-learning-viz"></div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Best Practices for File I/O</h2>
                    <div class="best-practices">
                        <div class="practice-grid">
                            <div class="practice-card fragment">
                                <div class="icon">üìù</div>
                                <h4>Version Your Models</h4>
                                <pre><code class="language-python">torch.save({
    'version': '1.2.0',
    'model_state': state_dict,
    'config': model_config
}, 'model_v1.2.0.pt')</code></pre>
                            </div>
                            <div class="practice-card fragment">
                                <div class="icon">üîç</div>
                                <h4>Validate After Loading</h4>
                                <pre><code class="language-python"># Always verify loaded models
test_input = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output = model(test_input)
assert output.shape == (1, 1000)</code></pre>
                            </div>
                            <div class="practice-card fragment">
                                <div class="icon">üóÇÔ∏è</div>
                                <h4>Organize Checkpoints</h4>
                                <pre><code class="language-python">checkpoint_dir/
‚îú‚îÄ‚îÄ best_model.pt
‚îú‚îÄ‚îÄ latest_checkpoint.pt
‚îî‚îÄ‚îÄ epoch_10/
    ‚îú‚îÄ‚îÄ model.pt
    ‚îî‚îÄ‚îÄ optimizer.pt</code></pre>
                            </div>
                            <div class="practice-card fragment">
                                <div class="icon">‚ö°</div>
                                <h4>Use Efficient Formats</h4>
                                <pre><code class="language-python"># Save with compression
torch.save(state_dict, 'model.pt',
          _use_new_zipfile_serialization=True)</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Checkpointing</h2>
                    <div data-mcq='{
                        "question": "What should be included in a training checkpoint for perfect resumption?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "Model state dictionary",
                                "correct": true,
                                "explanation": "Essential for restoring model parameters."
                            },
                            {
                                "text": "Optimizer state dictionary",
                                "correct": true,
                                "explanation": "Needed to restore momentum buffers and learning rate schedules."
                            },
                            {
                                "text": "Current epoch number",
                                "correct": true,
                                "explanation": "Required to know where to resume training."
                            },
                            {
                                "text": "Training data samples",
                                "correct": false,
                                "explanation": "Data is typically not saved in checkpoints - its loaded separately."
                            },
                            {
                                "text": "Random number generator state",
                                "correct": true,
                                "explanation": "Important for reproducible training resumption."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Advanced: Partial Model Loading</h2>
                    <div class="partial-loading-section">
                        <h3>Selective Parameter Loading</h3>
                        <pre><code class="language-python"># Load only matching parameters
def load_partial_state_dict(model, checkpoint_path):
    pretrained_dict = torch.load(checkpoint_path)
    model_dict = model.state_dict()
    
    # Filter out non-matching keys
    pretrained_dict = {k: v for k, v in pretrained_dict.items() 
                      if k in model_dict and 
                      v.shape == model_dict[k].shape}
    
    # Update current model
    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)
    
    print(f"Loaded {len(pretrained_dict)}/{len(model_dict)} parameters")
    
    return model</code></pre>
                        <div class="fragment use-cases mt-md">
                            <h4>Common Use Cases</h4>
                            <ul>
                                <li>Loading backbone from larger model</li>
                                <li>Transferring between similar architectures</li>
                                <li>Mixing parameters from multiple models</li>
                                <li>Handling architecture changes during development</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Cross-Framework Considerations</h2>
                    <div class="cross-framework-section">
                        <h3>Format Conversions</h3>
                        <div class="conversion-examples">
                            <div class="conversion-card fragment">
                                <h4>PyTorch ‚Üí NumPy</h4>
                                <pre><code class="language-python"># Save for framework-agnostic use
tensor = model.weight.detach().cpu().numpy()
np.save('weights.npy', tensor)</code></pre>
                            </div>
                            <div class="conversion-card fragment">
                                <h4>ONNX Export</h4>
                                <pre><code class="language-python"># Export to ONNX for interoperability
torch.onnx.export(model, dummy_input,
                  "model.onnx",
                  export_params=True)</code></pre>
                            </div>
                            <div class="conversion-card fragment">
                                <h4>HuggingFace Format</h4>
                                <pre><code class="language-python"># Save in HuggingFace format
model.save_pretrained("./my-model")
tokenizer.save_pretrained("./my-model")</code></pre>
                            </div>
                        </div>
                        <div class="fragment warning-box mt-md">
                            <p>‚ö†Ô∏è Not all operations are supported across frameworks - test conversions thoroughly!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Production Deployment Considerations</h2>
                    <div class="deployment-section">
                        <div class="deployment-checklist">
                            <h3>Deployment Checklist</h3>
                            <div class="checklist-items">
                                <div class="checklist-item fragment">
                                    <span class="checkbox">‚òë</span>
                                    <div>
                                        <strong>Model Optimization</strong>
                                        <pre><code class="language-python"># Quantization for smaller size
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)</code></pre>
                                    </div>
                                </div>
                                <div class="checklist-item fragment">
                                    <span class="checkbox">‚òë</span>
                                    <div>
                                        <strong>TorchScript Conversion</strong>
                                        <pre><code class="language-python"># Compile for production
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")</code></pre>
                                    </div>
                                </div>
                                <div class="checklist-item fragment">
                                    <span class="checkbox">‚òë</span>
                                    <div>
                                        <strong>Model Versioning</strong>
                                        <pre><code class="language-python"># Include metadata
metadata = {
    'model_version': '2.1.0',
    'training_date': '2024-01-15',
    'accuracy': 0.942
}</code></pre>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding: Advanced I/O</h2>
                    <div data-mcq='{
                        "question": "Which technique is most appropriate for deploying a model to edge devices?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Save the full model with all training parameters",
                                "correct": false,
                                "explanation": "This would be too large for edge devices."
                            },
                            {
                                "text": "Use model quantization and TorchScript",
                                "correct": true,
                                "explanation": "Correct! Quantization reduces size and TorchScript provides optimized inference."
                            },
                            {
                                "text": "Save only the optimizer state",
                                "correct": false,
                                "explanation": "Optimizer state is not needed for inference."
                            },
                            {
                                "text": "Convert to Python pickle format",
                                "correct": false,
                                "explanation": "Pickle is not optimized for deployment and has security concerns."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- ============================= -->
            <!-- GPU USAGE SECTION -->
            <!-- ============================= -->
            
            <section>
                <section>
                    <h2 class="truncate-title">GPU Performance Evolution</h2>
                    <div class="gpu-timeline-container">
                        <p>GPU performance has increased by a factor of <strong>1000x every decade</strong> since 2000!</p>
                        <div id="gpu-timeline-viz" style="width: 100%; height: 400px; margin: 20px 0;"></div>
                        <div class="fragment emphasis-box" style="font-size: 0.5em;">
                            <p>This exponential growth has enabled the deep learning revolution</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computing Devices: CPU vs GPU</h2>
                    <div class="device-comparison">
                        <div class="device-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div class="cpu-info">
                                <h4>üñ•Ô∏è CPU</h4>
                                <ul>
                                    <li>All physical CPUs and memory</li>
                                    <li>Uses all CPU cores for computation</li>
                                    <li>Default device for tensors</li>
                                    <li>Good for sequential operations</li>
                                </ul>
                            </div>
                            <div class="gpu-info">
                                <h4>üéÆ GPU</h4>
                                <ul>
                                    <li>One card and its memory</li>
                                    <li>Massive parallel processing</li>
                                    <li>Requires explicit placement</li>
                                    <li>Ideal for matrix operations</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <p class="emphasis-box">‚ö†Ô∏è <span class="tooltip">CUDA<span class="tooltiptext">Compute Unified Device Architecture - NVIDIA's parallel computing platform and API model</span></span> and drivers must be installed for GPU usage</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Device Management Functions</h2>
                    <div class="framework-tabs">
                        <h4>PyTorch</h4>
                        <pre><code class="python" data-trim>
def cpu():
    """Get the CPU device."""
    return torch.device('cpu')

def gpu(i=0):
    """Get a GPU device."""
    return torch.device(f'cuda:{i}')

def num_gpus():
    """Get the number of available GPUs."""
    return torch.cuda.device_count()

def try_gpu(i=0):
    """Return gpu(i) if exists, otherwise return cpu()."""
    if num_gpus() >= i + 1:
        return gpu(i)
    return cpu()

# Usage
device = try_gpu()  # Use GPU if available
x = torch.tensor([1, 2, 3], device=device)
                        </code></pre>
                        <div class="fragment">
                            <p style="margin-top: 15px;">Similar patterns exist for MXNet, JAX, and TensorFlow</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does the gpu(1) function typically represent?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The first GPU in the system",
                                "correct": false,
                                "explanation": "GPU indexing starts at 0, so gpu(1) is the second GPU."
                            },
                            {
                                "text": "The second GPU in the system",
                                "correct": true,
                                "explanation": "Correct! GPU indexing starts at 0, so gpu(1) refers to the second GPU."
                            },
                            {
                                "text": "All GPUs in the system",
                                "correct": false,
                                "explanation": "A single gpu() call refers to one specific GPU, not all of them."
                            },
                            {
                                "text": "One CPU core",
                                "correct": false,
                                "explanation": "The gpu() function refers to GPUs, not CPUs."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Tensors and GPUs Section (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Tensors and GPUs</h2>
                    <div class="tensor-gpu-intro">
                        <p>Understanding tensor placement and device management</p>
                        <div class="key-concepts" style="margin-top: 30px;">
                            <div class="concept-card" style="background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 10px 0;">
                                <h4>Default Behavior</h4>
                                <p>Tensors are created on CPU by default (PyTorch, MXNet)</p>
                                <p>JAX/TensorFlow may use GPU by default if available</p>
                            </div>
                            <div class="concept-card fragment" style="background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 10px 0;">
                                <h4>Device Context</h4>
                                <p>Every tensor has an associated device <span class="tooltip">context<span class="tooltiptext">The computational device (CPU or GPU) where a tensor is stored and operations are performed</span></span></p>
                                <p>Operations require all tensors on the same device</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Creating Tensors on GPU</h2>
                    <pre><code class="python" data-trim data-line-numbers="1-3|5-7|9-11|13-15">
# PyTorch
X = torch.ones(2, 3, device=try_gpu())
# tensor([[1., 1., 1.], [1., 1., 1.]], device='cuda:0')

# MXNet
X = np.ones((2, 3), ctx=try_gpu())
# array([[1., 1., 1.], [1., 1., 1.]], ctx=gpu(0))

# JAX
X = jax.device_put(jnp.ones((2, 3)), try_gpu())
# Array([[1., 1., 1.], [1., 1., 1.]], dtype=float32)

# TensorFlow
with try_gpu():
    X = tf.ones((2, 3))
                    </code></pre>
                    <div class="fragment emphasis-box" style="margin-top: 20px;">
                        <p>üí° Always specify device at creation time when possible to avoid transfers</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Querying Tensor Device</h2>
                    <div class="device-query-examples">
                        <pre><code class="python" data-trim>
# PyTorch
x = torch.tensor([1, 2, 3])
print(x.device)  # device(type='cpu')

X = torch.ones(2, 3, device=gpu())
print(X.device)  # device(type='cuda', index=0)

# Check if tensor is on GPU
is_cuda = X.is_cuda  # True

# MXNet
x = np.array([1, 2, 3])
print(x.ctx)  # cpu(0)

# JAX
x = jnp.array([1, 2, 3])
print(x.device())  # gpu(id=0) or CpuDevice(id=0)

# TensorFlow
x = tf.constant([1, 2, 3])
print(x.device)  # '/device:GPU:0' or '/device:CPU:0'
                        </code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">GPU Memory Management</h2>
                    <div class="memory-management">
                        <div class="memory-tips">
                            <h4>Important Considerations</h4>
                            <ul>
                                <li>GPU memory is limited (typically 8-40GB)</li>
                                <li>Use <code>nvidia-smi</code> to monitor memory usage</li>
                                <li>Tensors on GPU only consume that GPU's memory</li>
                                <li>Out-of-memory errors require reducing batch size or model size</li>
                            </ul>
                        </div>
                        <div class="fragment" style="margin-top: 20px;">
                            <pre><code class="bash" data-trim>
# Monitor GPU memory usage
nvidia-smi

# Watch memory usage in real-time
watch -n 1 nvidia-smi
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why must all tensors be on the same device for operations?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To save memory",
                                "correct": false,
                                "explanation": "Device consistency is about computation location, not memory savings."
                            },
                            {
                                "text": "The framework needs to know where to store results and perform computation",
                                "correct": true,
                                "explanation": "Correct! Operations need all inputs on the same device to determine where to execute and store results."
                            },
                            {
                                "text": "GPUs cannot communicate with CPUs",
                                "correct": false,
                                "explanation": "GPUs and CPUs can communicate through data transfers."
                            },
                            {
                                "text": "It is a Python requirement",
                                "correct": false,
                                "explanation": "This is a deep learning framework requirement, not a Python language requirement."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Data Transfer Operations Section (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Data Transfer Between Devices</h2>
                    <div class="transfer-intro">
                        <p>Moving data between CPU and GPU is a critical operation</p>
                        <div id="device-transfer-viz" style="width: 100%; height: 350px; margin: 20px 0; border: 2px solid #e0e0e0; border-radius: 8px;"></div>
                        <div class="emphasis-box">
                            <p>‚ö†Ô∏è Data transfer is <strong>much slower</strong> than computation!</p>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Figure 6.7.1 from D2L", "url": "https://d2l.ai/chapter_builders-guide/use-gpu.html#copying"}]'>
                    <h2 class="truncate-title">Copying Tensors Between Devices</h2>
                    <div class="copying-example">
                        <img src="images/copyto.svg" alt="Data copying between devices" style="max-width: 60%; margin: 20px auto; display: block;">
                        <pre><code class="python" data-trim>
# PyTorch: Move X from GPU 0 to GPU 1
X = torch.ones(2, 3, device=gpu(0))
Y = torch.rand(2, 3, device=gpu(1))

# Copy X to second GPU
Z = X.cuda(1)  # Now on same device as Y
result = Z + Y  # Can now perform operation

# Check for redundant copies
if Z.device == gpu(1):
    Z2 = Z.cuda(1)  # Returns Z, no copy made
    print(Z2 is Z)  # True
                        </code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive: Device Transfer Visualization</h2>
                    <div id="transfer-demo" style="width: 100%; height: 450px;">
                        <div class="transfer-controls" style="display: flex; justify-content: center; gap: 15px; margin-bottom: 20px;">
                            <button id="create-cpu" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Create on CPU</button>
                            <button id="create-gpu0" style="background: #2DD2C0; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Create on GPU 0</button>
                            <button id="transfer-gpu" style="background: #FFA05F; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Transfer to GPU 1</button>
                            <button id="compute-op" style="background: #FC8484; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Compute X + Y</button>
                            <button id="reset-transfer" style="background: #666; color: white; border: none; padding: 8px 16px; border-radius: 5px;">Reset</button>
                        </div>
                        <div id="transfer-viz" style="height: 350px;"></div>
                        <div id="transfer-info" style="margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 5px; text-align: center;"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Performance Best Practices</h2>
                    <div class="best-practices" style="font-size: 0.5em;">
                        <div class="practice-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div class="do-card" style="background: #e8f5e9; padding: 20px; border-radius: 8px;">
                                <h4>‚úÖ Do</h4>
                                <ul style="text-align: left;">
                                    <li>Batch transfers (one big operation)</li>
                                    <li>Keep data on GPU throughout training</li>
                                    <li>Pre-allocate GPU memory</li>
                                    <li>Use <code>torch.no_grad()</code> for inference</li>
                                </ul>
                            </div>
                            <div class="dont-card" style="background: #ffebee; padding: 20px; border-radius: 8px;">
                                <h4>‚ùå Don't</h4>
                                <ul style="text-align: left;">
                                    <li>Transfer data in small chunks</li>
                                    <li>Print tensors frequently (causes transfer)</li>
                                    <li>Convert to NumPy unnecessarily</li>
                                    <li>Mix devices without checking</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px;">
                            <p>‚ö†Ô∏è <strong>Critical Performance Impact:</strong> When you print a GPU tensor or convert it to NumPy, PyTorch must:</p>
                            <ul style="text-align: left; margin-top: 10px;">
                                <li>Transfer data from GPU memory to CPU memory</li>
                                <li>Synchronize GPU operations (blocking)</li>
                                <li>Invoke Python's <span class="tooltip">GIL<span class="tooltiptext">Global Interpreter Lock - Python's mechanism that allows only one thread to execute Python bytecode at a time</span></span> during NumPy conversion</li>
                                <li>Create a copy in system RAM</li>
                            </ul>
                            <p style="margin-top: 10px;">This can cause significant slowdowns in training loops. Use <code>.detach().cpu().numpy()</code> explicitly to make transfers visible in your code!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens when you print a GPU tensor or convert it to NumPy?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Nothing special, it prints directly",
                                "correct": false,
                                "explanation": "GPU tensors must be transferred to CPU memory before printing or NumPy conversion."
                            },
                            {
                                "text": "The framework copies it to main memory first",
                                "correct": true,
                                "explanation": "Correct! The data is transferred to CPU memory, which adds overhead and can invoke the GIL."
                            },
                            {
                                "text": "It creates a view on the GPU",
                                "correct": false,
                                "explanation": "NumPy cannot directly access GPU memory; data must be copied."
                            },
                            {
                                "text": "It raises an error",
                                "correct": false,
                                "explanation": "The operation succeeds but involves an implicit data transfer."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Neural Networks on GPUs Section (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Neural Networks and GPUs</h2>
                    <div class="nn-gpu-intro">
                        <p>Deploying models and managing parameters on GPUs</p>
                        <div class="model-deployment" style="margin-top: 30px;">
                            <pre><code class="python" data-trim>
# PyTorch
net = nn.Sequential(nn.LazyLinear(1))
net = net.to(device=try_gpu())

# MXNet
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(ctx=try_gpu())

# TensorFlow
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    net = tf.keras.models.Sequential([
        tf.keras.layers.Dense(1)
    ])
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Automatic Device Inference</h2>
                    <div class="auto-device">
                        <p>When input is on GPU, the model computes on the same GPU:</p>
                        <pre><code class="python" data-trim>
# Create input on GPU
X = torch.rand(2, 10, device=try_gpu())

# Model on GPU processes input on same device
net = nn.Sequential(nn.Linear(10, 1)).to(try_gpu())
output = net(X)

print(output.device)  # cuda:0
print(output.is_cuda)  # True

# Verify parameters are on GPU
print(net[0].weight.device)  # cuda:0
                        </code></pre>
                        <div class="fragment emphasis-box" style="margin-top: 20px;">
                            <p>üí° The model automatically uses the device of its parameters</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens when you move a model to GPU using model.to(gpu())?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Only the model structure moves to GPU",
                                "correct": false,
                                "explanation": "The to() method moves both the model structure and all parameters."
                            },
                            {
                                "text": "All model parameters are moved to the specified GPU",
                                "correct": true,
                                "explanation": "Correct! The to() method recursively moves all parameters and buffers to the specified device."
                            },
                            {
                                "text": "A copy of the model is created on GPU",
                                "correct": false,
                                "explanation": "The model is moved in-place; no copy is created unless explicitly requested."
                            },
                            {
                                "text": "Only gradients are computed on GPU",
                                "correct": false,
                                "explanation": "Both forward and backward passes occur on the device where parameters reside."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Advanced GPU Concepts Section (Vertical) -->
            <section>
                <section>
                    <h2 class="truncate-title">Advanced GPU Concepts</h2>
                    <div class="advanced-intro">
                        <p>Multi-GPU environments and optimization strategies</p>
                        <div class="concept-overview" style="margin-top: 30px;">
                            <div class="concept-item" style="background: #f5f5f5; padding: 15px; border-radius: 8px; margin: 10px 0;">
                                <h4>üéÆ Multi-GPU Setup</h4>
                                <p>Using multiple GPUs for training</p>
                            </div>
                            <div class="concept-item fragment" style="background: #f5f5f5; padding: 15px; border-radius: 8px; margin: 10px 0;">
                                <h4>üíæ Memory Optimization</h4>
                                <p>Efficient GPU memory management</p>
                            </div>
                            <div class="concept-item fragment" style="background: #f5f5f5; padding: 15px; border-radius: 8px; margin: 10px 0;">
                                <h4>‚ö° Performance Tuning</h4>
                                <p>Maximizing GPU utilization</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Multi-GPU Environments</h2>
                    <div class="multi-gpu">
                        <pre><code class="python" data-trim>
# Get all available GPUs
def try_all_gpus():
    """Return all available GPUs, or [cpu()] if no GPU exists."""
    return [gpu(i) for i in range(num_gpus())]

# Example usage
devices = try_all_gpus()
print(f"Available devices: {devices}")
# [device('cuda:0'), device('cuda:1')]

# Distribute data across GPUs
if len(devices) > 1:
    # Split batch across GPUs
    batch_per_gpu = len(data) // len(devices)
    
    # Process on each GPU
    for i, device in enumerate(devices):
        start = i * batch_per_gpu
        end = start + batch_per_gpu
        data_chunk = data[start:end].to(device)
        # Process data_chunk on this GPU
                        </code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Common GPU Pitfalls and Solutions</h2>
                    <div class="pitfalls-solutions">
                        <table style="width: 100%; margin-top: 20px;">
                            <thead>
                                <tr>
                                    <th>Pitfall</th>
                                    <th>Solution</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Out of Memory (OOM)</td>
                                    <td>Reduce batch size, use gradient accumulation</td>
                                </tr>
                                <tr class="fragment">
                                    <td>Slow training despite GPU</td>
                                    <td>Check for CPU-GPU transfers in training loop</td>
                                </tr>
                                <tr class="fragment">
                                    <td>Device mismatch errors</td>
                                    <td>Ensure all tensors on same device before operations</td>
                                </tr>
                                <tr class="fragment">
                                    <td>Memory leaks</td>
                                    <td>Use <code>torch.cuda.empty_cache()</code>, delete unused tensors</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Performance Optimization Tips</h2>
                    <div class="optimization-tips" style="font-size: 0.5em;">
                        <div class="tip-cards" style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div class="tip-card" style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
                                <h4>üìä Monitoring</h4>
                                <pre><code class="bash">nvidia-smi -l 1  # Monitor every second</code></pre>
                            </div>
                            <div class="tip-card fragment" style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
                                <h4>üîÑ Async Operations</h4>
                                <pre><code class="python">torch.cuda.synchronize()  # Wait for completion</code></pre>
                            </div>
                            <div class="tip-card fragment" style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
                                <h4>üì¶ Batch Processing</h4>
                                <p>Larger batches = better GPU utilization</p>
                            </div>
                            <div class="tip-card fragment" style="background: #fff3e0; padding: 15px; border-radius: 8px;">
                                <h4>üéØ Mixed Precision</h4>
                                <pre><code class="python">with torch.cuda.amp.autocast():</code></pre>
                            </div>
                        </div>
                        <div class="fragment emphasis-box" style="margin-top: 20px;">
                            <p>üöÄ Key: Minimize CPU-GPU transfers and maximize GPU utilization!</p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which operation is typically the bottleneck in GPU computing?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Matrix multiplication on GPU",
                                "correct": false,
                                "explanation": "GPUs are highly optimized for matrix operations; this is rarely a bottleneck."
                            },
                            {
                                "text": "Data transfer between CPU and GPU",
                                "correct": true,
                                "explanation": "Correct! Data transfer is much slower than computation and is often the main bottleneck."
                            },
                            {
                                "text": "Forward pass computation",
                                "correct": false,
                                "explanation": "Forward pass is well-optimized on GPUs and rarely a bottleneck."
                            },
                            {
                                "text": "Gradient calculation",
                                "correct": false,
                                "explanation": "Automatic differentiation is efficient on GPUs."
                            }
                        ]
                    }'></div>
                </section>
            </section>

        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    
    <!-- Custom Scripts -->
    <script src="../shared/js/multiple-choice.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/title-handler.js"></script>
    <script src="js/builders-viz.js"></script>
    
    <script>
        // Initialize Reveal.js
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealHighlight, RevealNotes, RevealMath ]
        });
    </script>
</body>
</html>