\section{Conclusion}

\label{sec:conclusion}

In this paper, we propose a novel vision-language pretraining framework that incorporates knowledge information to model the semantic connections between vision and language entities. We introduce three types of graph-structured datasets into the training process, and adopt a multi-modal encoder to model the joint distribution of entities and their semantic relations. Extensive experiments on various downstream tasks including multi-modal, uni-modal, and graph-based tasks validate the transfer and generalization ability of our model. Our approach is now limited in injecting knowledge information into the CLIP models. However, our training objectives and new knowledge graph datasets are technically compatible with other large-scale pretraining frameworks. We will explore the possibility of further applications in the future.

% \textbf{Potential Negative Social Impact.} In our work, Knowledge-CLIP is pre-trained based on the initialization of CLIP, which greatly avoids the massive computational cost in the pretraining process. Nevertheless, it is still significantly larger than traditional models that are specifically designed for a single downstream task, which may result in increased carbon emissions. Future work may additionally explore the possibility of efficient pre-training.

\section{Acknowledgement}
This work is supported in part by the National Key R\&D Program of China under Grant 2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048, Guoqiang Institute of Tsinghua University and Beijing Academy of Artificial Intelligence. We also appreciate the generous donation of computing resources by High-Flyer AI.