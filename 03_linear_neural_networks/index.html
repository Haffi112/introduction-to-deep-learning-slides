<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Neural Networks - Introduction to Deep Learning</title>
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="css/linear-custom.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Linear Neural Networks for Regression</h1>
                <p>Chapter 3.1: Linear Regression</p>
                <p>Based on "Dive into Deep Learning" by Zhang et al.</p>
                <p class="mt-lg">
                    <small>Instructor: Hafsteinn Einarsson</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>
            
            <!-- Introduction Section -->
            <section>
                <h2 class="truncate-title">Why Start with Linear Models?</h2>
                <ul>
                    <li class="fragment">Focus on fundamentals of neural network training</li>
                    <li class="fragment">Understand parametrization and loss functions</li>
                    <li class="fragment">Master optimization techniques</li>
                    <li class="fragment">Build foundation for deep networks</li>
                </ul>
                <div class="fragment emphasis-box mt-lg">
                    <p>Linear models are the stepping stones to deep learning!</p>
                </div>
            </section>

            <section>
                <h2 class="truncate-title">Learning Objectives</h2>
                <ul>
                    <li class="fragment">Understand linear regression as a <span class="tooltip">neural network<span class="tooltiptext">A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) that process information</span></span></li>
                    <li class="fragment">Master the <span class="tooltip">squared loss<span class="tooltiptext">A loss function that penalizes predictions by the square of their difference from true values: L = (≈∑ - y)¬≤</span></span> function</li>
                    <li class="fragment">Learn <span class="tooltip">gradient descent<span class="tooltiptext">An optimization algorithm that iteratively adjusts parameters in the direction of steepest decrease of the loss function</span></span> optimization</li>
                    <li class="fragment">Connect to probabilistic interpretation via <span class="tooltip">MLE<span class="tooltiptext">Maximum Likelihood Estimation: finding parameter values that maximize the probability of observing the given data</span></span></li>
                    <li class="fragment">Implement vectorized operations for efficiency</li>
                </ul>
            </section>

            <!-- Basics Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.1", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#basics"}]'>
                    <h2 class="truncate-title">The Regression Problem</h2>
                    <p>Predicting numerical values from input features</p>
                    <div class="fragment mt-lg">
                        <p><strong>Example:</strong> House Price Prediction</p>
                        <ul>
                            <li>Features: area (sq ft), age (years)</li>
                            <li>Target: price ($)</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Goal: Learn a function that maps features to target values</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Key Terminology</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Data Components</h4>
                            <ul>
                                <li class="fragment"><strong>Training set:</strong> Dataset for learning</li>
                                <li class="fragment"><strong>Example:</strong> One data point</li>
                                <li class="fragment"><strong>Label (y):</strong> Target value</li>
                                <li class="fragment"><strong>Features (x):</strong> Input variables</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Notation</h4>
                            <ul>
                                <li class="fragment">n: number of examples</li>
                                <li class="fragment">d: number of features</li>
                                <li class="fragment">x<sup>(i)</sup>: i-th example</li>
                                <li class="fragment">x<sub>j</sub><sup>(i)</sup>: j-th feature of i-th example</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Linear Model</h2>
                    <p>Assumption: target is a weighted sum of features plus bias</p>
                    <div class="fragment mt-lg">
                        <p><strong>House price example:</strong></p>
                        $$\text{price} = w_{\text{area}} \cdot \text{area} + w_{\text{age}} \cdot \text{age} + b$$
                    </div>
                    <div class="fragment mt-lg">
                        <ul>
                            <li><strong>Weights (w):</strong> Influence of each feature</li>
                            <li><strong>Bias (b):</strong> Baseline value when features = 0</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Vector Notation</h2>
                    <p>Compact representation using linear algebra</p>
                    <div class="fragment mt-lg">
                        <p><strong>Single prediction:</strong></p>
                        $$\hat{y} = \mathbf{w}^T \mathbf{x} + b$$
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Batch predictions:</strong></p>
                        $$\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + b$$
                        <p class="small">where X ‚àà ‚Ñù<sup>n√ód</sup> is the design matrix</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Linear Regression Demo</h2>
                    <div id="linear-regression-demo" class="demo-container">
                        <svg id="regression-svg"></svg>
                        <div class="controls mt-md">
                            <button id="add-points">Add Random Points</button>
                            <button id="clear-points">Clear</button>
                            <button id="fit-line">Fit Line</button>
                            <label class="ml-md">
                                Show residuals: <input type="checkbox" id="show-residuals">
                            </label>
                        </div>
                        <div id="equation-display" class="mt-md"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the model ≈∑ = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b, what does the bias term b represent?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The error in our prediction",
                                "correct": false,
                                "explanation": "The bias is not the error; it is a learnable parameter that shifts the prediction."
                            },
                            {
                                "text": "The average of all features",
                                "correct": false,
                                "explanation": "The bias is independent of feature values."
                            },
                            {
                                "text": "The predicted value when all features are zero",
                                "correct": true,
                                "explanation": "Correct! The bias determines the baseline prediction when x = 0."
                            },
                            {
                                "text": "The learning rate",
                                "correct": false,
                                "explanation": "The learning rate is a hyperparameter for optimization, not part of the model."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Loss Function Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.1.2", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#loss-function"}]'>
                    <h2 class="truncate-title">Measuring Model Quality</h2>
                    <p>How well does our model fit the data?</p>
                    <div class="fragment mt-lg">
                        <p><strong>Loss Function:</strong> Quantifies prediction error</p>
                        <ul>
                            <li>Smaller loss = better fit</li>
                            <li>Perfect prediction ‚Üí loss = 0</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Squared Error Loss</h2>
                    <p>Most common loss for regression</p>
                    <div class="fragment mt-lg">
                        <p><strong>For single example:</strong></p>
                        $$l^{(i)}(\mathbf{w}, b) = \frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2$$
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>For entire dataset:</strong></p>
                        $$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b)$$
                    </div>
                    <div class="fragment">
                        <p class="small">The factor 1/2 simplifies derivatives</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Visualizing the Loss</h2>
                    <div id="loss-visualization" class="demo-container"></div>
                </section>

                <section>
                    <h2 class="truncate-title">Properties of Squared Loss</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Advantages</h4>
                            <ul class="fragment">
                                <li>Differentiable everywhere</li>
                                <li>Penalizes large errors heavily</li>
                                <li>Unique minimum (convex)</li>
                                <li>Simple gradient computation</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Disadvantages</h4>
                            <ul class="fragment">
                                <li>Sensitive to outliers</li>
                                <li>Assumes Gaussian noise</li>
                                <li>May overfit to anomalies</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does squared loss penalize large errors more than small errors?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because we multiply the error by 2",
                                "correct": false,
                                "explanation": "The factor of 1/2 is just for mathematical convenience and does not affect the relative penalties."
                            },
                            {
                                "text": "Because squaring makes large values grow faster than small values",
                                "correct": true,
                                "explanation": "Correct! For example, 2¬≤ = 4 but 10¬≤ = 100. The squared function grows quadratically."
                            },
                            {
                                "text": "Because we sum over all examples",
                                "correct": false,
                                "explanation": "Summing affects the total loss but not the relative penalty for individual errors."
                            },
                            {
                                "text": "Because the gradient is larger for large errors",
                                "correct": false,
                                "explanation": "While true, this is a consequence of squaring, not the cause of the penalty."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Optimization Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.1.3", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#analytic-solution"}]'>
                    <h2 class="truncate-title">Finding Optimal Parameters</h2>
                    <p>Two approaches to minimize the loss</p>
                    <div class="two-column mt-lg">
                        <div class="column fragment">
                            <h4>Analytic Solution</h4>
                            <ul>
                                <li>Closed-form formula</li>
                                <li>Exact solution</li>
                                <li>Requires matrix inversion</li>
                                <li>Limited to linear regression</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Gradient Descent</h4>
                            <ul>
                                <li>Iterative optimization</li>
                                <li>Approximate solution</li>
                                <li>Works for any differentiable loss</li>
                                <li>Scales to deep learning</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- Analytic Solution Derivation - Expanded Section -->
                <section>
                    <h2 class="truncate-title">Analytic Solution: Overview</h2>
                    <p>Finding the exact solution through calculus</p>
                    <div class="fragment mt-lg">
                        <p><strong>Goal:</strong> Minimize the squared error loss analytically</p>
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>Let's derive the famous normal equation step by step!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 1: The Objective</h2>
                    <p>We want to minimize the <strong>squared error loss</strong>:</p>
                    <div class="fragment mt-lg">
                        $$L(\mathbf{w}) = \|\color{#10099F}{\mathbf{y}} - \color{#2DD2C0}{X}\color{#FC8484}{\mathbf{w}}\|^2$$
                    </div>
                    <div class="fragment mt-lg">
                        <p>Where:</p>
                        <ul>
                            <li><span style="color: #10099F">$\mathbf{y}$</span>: $n \times 1$ vector (target values)</li>
                            <li><span style="color: #2DD2C0">$X$</span>: $n \times d$ design matrix</li>
                            <li><span style="color: #FC8484">$\mathbf{w}$</span>: $d \times 1$ vector of parameters</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 2: Expanding the Squared Norm</h2>
                    <p>The squared norm of a vector $\mathbf{a}$ is defined as:</p>
                    <div class="fragment mt-lg">
                        $$\|\mathbf{a}\|^2 = \mathbf{a}^\top \mathbf{a}$$
                    </div>
                    <div class="fragment mt-lg">
                        <p>Therefore:</p>
                        $$L(\mathbf{w}) = (\color{#10099F}{\mathbf{y}} - \color{#2DD2C0}{X}\color{#FC8484}{\mathbf{w}})^\top (\color{#10099F}{\mathbf{y}} - \color{#2DD2C0}{X}\color{#FC8484}{\mathbf{w}})$$
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>üîë Key insight: We convert the norm into an inner product!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we convert ||a||¬≤ to a·µÄa?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make it easier to take derivatives",
                                "correct": true,
                                "explanation": "Correct! Matrix multiplication form allows us to apply standard calculus rules for derivatives."
                            },
                            {
                                "text": "To reduce computational complexity",
                                "correct": false,
                                "explanation": "Both forms have the same computational complexity. The conversion is for mathematical convenience."
                            },
                            {
                                "text": "Because norms cannot be differentiated",
                                "correct": false,
                                "explanation": "Norms can be differentiated, but the inner product form is more convenient for applying matrix calculus rules."
                            },
                            {
                                "text": "To convert vectors to scalars",
                                "correct": false,
                                "explanation": "Both ||a||¬≤ and a·µÄa already produce scalar values. The conversion is for calculus convenience."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 3: Expanding the Quadratic Form</h2>
                    <p>Expand like a quadratic expression $(a-b)^2 = a^2 - 2ab + b^2$:</p>
                    <div class="fragment mt-lg">
                        $$L(\mathbf{w}) = \color{#10099F}{\mathbf{y}^\top \mathbf{y}} - \color{#2DD2C0}{2\mathbf{y}^\top X\mathbf{w}} + \color{#FC8484}{\mathbf{w}^\top X^\top X \mathbf{w}}$$
                    </div>
                    <div class="fragment mt-lg">
                        <ul>
                            <li><span style="color: #10099F">$\mathbf{y}^\top \mathbf{y}$</span>: Constant term (doesn't depend on $\mathbf{w}$)</li>
                            <li><span style="color: #2DD2C0">$-2\mathbf{y}^\top X\mathbf{w}$</span>: Linear term in $\mathbf{w}$</li>
                            <li><span style="color: #FC8484">$\mathbf{w}^\top X^\top X \mathbf{w}$</span>: Quadratic term in $\mathbf{w}$</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 4: Matrix Calculus Rules</h2>
                    <p>Key differentiation rules we'll use:</p>
                    <div class="fragment mt-lg">
                        <ol>
                            <li class="fragment">$\frac{\partial}{\partial \mathbf{w}} \; c = \mathbf{0}$ (constant rule)</li>
                            <li class="fragment mt-md">$\frac{\partial}{\partial \mathbf{w}} \; \mathbf{b}^\top \mathbf{w} = \mathbf{b}$ (linear rule)</li>
                            <li class="fragment mt-md">$\frac{\partial}{\partial \mathbf{w}} \; \mathbf{w}^\top A \mathbf{w} = (A + A^\top)\mathbf{w}$ (quadratic form rule)</li>
                            <li class="fragment mt-md">If $A$ is symmetric: $\frac{\partial}{\partial \mathbf{w}} \; \mathbf{w}^\top A \mathbf{w} = 2A\mathbf{w}$</li>
                        </ol>
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>Note: $X^\top X$ is always symmetric!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is X·µÄX always symmetric?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because (X·µÄX)·µÄ = X·µÄX",
                                "correct": true,
                                "explanation": "Correct! (X·µÄX)·µÄ = X·µÄ(X·µÄ)·µÄ = X·µÄX, proving it is symmetric."
                            },
                            {
                                "text": "Because X is a square matrix",
                                "correct": false,
                                "explanation": "X does not need to be square. It is typically n√ód where n is samples and d is features."
                            },
                            {
                                "text": "Because all matrix products are symmetric",
                                "correct": false,
                                "explanation": "Not all matrix products are symmetric. For example, AB ‚â† (AB)·µÄ in general."
                            },
                            {
                                "text": "It is not always symmetric",
                                "correct": false,
                                "explanation": "X·µÄX is always symmetric by the property (AB)·µÄ = B·µÄA·µÄ."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 5: Taking Derivatives</h2>
                    <p>Apply the rules term by term:</p>
                    <div class="fragment mt-lg">
                        <ul>
                            <li>$\frac{\partial}{\partial \mathbf{w}} \; \color{#10099F}{\mathbf{y}^\top \mathbf{y}} = \mathbf{0}$ (constant)</li>
                            <li class="mt-md">$\frac{\partial}{\partial \mathbf{w}} \; \color{#2DD2C0}{\mathbf{y}^\top X\mathbf{w}} = \color{#2DD2C0}{X^\top \mathbf{y}}$ (linear)</li>
                            <li class="mt-md">$\frac{\partial}{\partial \mathbf{w}} \; \color{#FC8484}{\mathbf{w}^\top X^\top X \mathbf{w}} = \color{#FC8484}{2X^\top X \mathbf{w}}$ (quadratic)</li>
                        </ul>
                    </div>
                    <div class="fragment mt-lg">
                        <p>Therefore:</p>
                        $$\nabla_\mathbf{w} L(\mathbf{w}) = \color{#2DD2C0}{-2X^\top \mathbf{y}} + \color{#FC8484}{2X^\top X \mathbf{w}}$$
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 6: Setting Gradient to Zero</h2>
                    <p>At the optimum, the gradient equals zero:</p>
                    <div class="fragment mt-lg">
                        $$\color{#2DD2C0}{-2X^\top \mathbf{y}} + \color{#FC8484}{2X^\top X \mathbf{w}} = \mathbf{0}$$
                    </div>
                    <div class="fragment mt-lg">
                        <p>Simplifying:</p>
                        $$\color{#FC8484}{X^\top X \mathbf{w}} = \color{#2DD2C0}{X^\top \mathbf{y}}$$
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>These are called the <span class="tooltip">normal equations<span class="tooltiptext">Called 'normal' because the error vector (y - Xw) is perpendicular (normal) to the column space of X at the solution.</span></span>!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step 7: Solving for w</h2>
                    <p>If $X^\top X$ is invertible:</p>
                    <div class="fragment mt-lg">
                        $$\color{#FC8484}{\mathbf{w}^*} = \color{#2DD2C0}{(X^\top X)^{-1} X^\top} \color{#10099F}{\mathbf{y}}$$
                    </div>
                    <div class="fragment mt-lg">
                        <p>This is the <strong>analytic solution</strong> to linear regression!</p>
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>üìù The matrix $(X^\top X)^{-1} X^\top$ is called the <span class="tooltip">Moore-Penrose pseudoinverse<span class="tooltiptext">A generalization of the inverse matrix that can handle non-square and singular matrices.</span></span></p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When does the analytic solution NOT exist?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When X·µÄX is not invertible (singular)",
                                "correct": true,
                                "explanation": "Correct! If X·µÄX is singular (determinant = 0), we cannot compute its inverse and the analytic solution does not exist."
                            },
                            {
                                "text": "When the data has noise",
                                "correct": false,
                                "explanation": "Noise does not prevent the analytic solution. It just means the solution will not perfectly fit all data points."
                            },
                            {
                                "text": "When there are too many features",
                                "correct": false,
                                "explanation": "Having many features does not prevent the solution, though it might make X·µÄX singular if features > samples."
                            },
                            {
                                "text": "When the loss function is non-convex",
                                "correct": false,
                                "explanation": "The squared loss for linear regression is always convex, regardless of the data."
                            }
                        ]
                    }'></div>
                </section>

                <section>
                    <h2 class="truncate-title">Key Insights</h2>
                    <p>Summary of the derivation approach:</p>
                    <div class="fragment mt-lg">
                        <ol>
                            <li><strong>Norm to inner product:</strong> $\|\mathbf{a}\|^2 = \mathbf{a}^\top \mathbf{a}$</li>
                            <li class="mt-md"><strong>Expand quadratic:</strong> Like $(a-b)^2$</li>
                            <li class="mt-md"><strong>Apply calculus rules:</strong> Term by term</li>
                            <li class="mt-md"><strong>Set gradient = 0:</strong> Find critical point</li>
                            <li class="mt-md"><strong>Solve linear system:</strong> Get closed form</li>
                        </ol>
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>üí° This same approach works for any quadratic objective!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Limitations of Analytic Solution</h2>
                    <div class="fragment">
                        <p><strong>When it fails:</strong></p>
                        <ul>
                            <li>$X^\top X$ not invertible (multicollinearity)</li>
                            <li>More features than samples ($d > n$)</li>
                            <li>Computational cost: $O(d^3)$ for inversion</li>
                            <li>Does not extend to nonlinear models</li>
                        </ul>
                    </div>
                    <div class="fragment mt-lg emphasis-box">
                        <p>‚ö†Ô∏è Rarely applicable in deep learning!</p>
                        <p class="small">That is why we need gradient descent!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.1.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#minibatch-stochastic-gradient-descent"}]'>
                    <h2 class="truncate-title">Gradient Descent</h2>
                    <p>Iteratively reduce error by following the gradient</p>
                    <div class="fragment mt-lg">
                        <p><strong>Update rule:</strong></p>
                        $$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \nabla_{(\mathbf{w},b)} L$$
                    </div>
                    <div class="fragment mt-lg">
                        <ul>
                            <li>Œ∑: <span class="tooltip">learning rate<span class="tooltiptext">A hyperparameter that controls the step size in gradient descent. Too small: slow convergence. Too large: may overshoot minimum.</span></span></li>
                            <li>‚àáL: gradient of loss</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Minibatch SGD</h2>
                    <p>Balance between efficiency and stability</p>
                    <div class="three-column mt-lg">
                        <div class="column fragment">
                            <h4>Batch GD</h4>
                            <ul>
                                <li>Use all data</li>
                                <li>Stable</li>
                                <li>Slow per iteration</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>SGD</h4>
                            <ul>
                                <li>One example</li>
                                <li>Noisy</li>
                                <li>Fast per iteration</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Minibatch SGD ‚úì</h4>
                            <ul>
                                <li>Small batch</li>
                                <li>Good balance</li>
                                <li>GPU efficient</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Typical batch sizes: 32, 64, 128, 256</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient Descent Animation</h2>
                    <div id="gradient-descent-demo" class="demo-container gradient-descent-layout">
                        <div class="viz-row">
                            <div id="loss-landscape" class="viz-panel"></div>
                            <div id="loss-history" class="viz-panel"></div>
                        </div>
                        <div class="controls-bar">
                            <label>LR: 
                                <input type="range" id="learning-rate" min="0.001" max="0.5" step="0.001" value="0.1">
                                <span id="lr-value">0.1</span>
                            </label>
                            <label>Batch: 
                                <select id="batch-size">
                                    <option value="1">1</option>
                                    <option value="32" selected>32</option>
                                    <option value="128">128</option>
                                    <option value="full">Full</option>
                                </select>
                            </label>
                            <button id="start-gd">Start</button>
                            <button id="reset-gd">Reset</button>
                        </div>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.1", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#gradient-descent"}]'>
                    <h2 class="truncate-title">Computing Gradients for a Minibatch</h2>
                    <p class="small">How do we calculate the gradients needed for updates?</p>
                    <div class="two-column mt-md">
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>Minibatch loss function:</strong></p>
                                <p class="small">$$L(\mathbf{w}, b) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l^{(i)}(\mathbf{w}, b)$$</p>
                            </div>
                            <div class="fragment mt-md">
                                <p class="small">where each sample's loss is:</p>
                                <p class="small">$$l^{(i)} = \frac{1}{2}(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})^2$$</p>
                            </div>
                        </div>
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>We need to compute:</strong></p>
                                <ul class="small">
                                    <li>$\frac{\partial L}{\partial \mathbf{w}}$ - gradient w.r.t. weights</li>
                                    <li class="mt-sm">$\frac{\partial L}{\partial b}$ - gradient w.r.t. bias</li>
                                </ul>
                                <div class="fragment mt-md">
                                    <p class="small emphasis-box">These gradients tell us how to adjust parameters to minimize loss</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient with Respect to Weights</h2>
                    <p class="small">Step-by-step derivation using the chain rule</p>
                    <div class="two-column mt-sm">
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>Step 1:</strong> Start with one sample's loss</p>
                                <p class="small">$$l^{(i)} = \frac{1}{2}\color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}^2$$</p>
                                <p class="small">Let's call the <span style="color: blue">blue part</span> the "error" $e^{(i)}$</p>
                            </div>
                            <div class="fragment mt-md">
                                <p class="small"><strong>Step 2:</strong> Apply chain rule</p>
                                <p class="small">$$\frac{\partial l^{(i)}}{\partial \mathbf{w}} = \color{blue}{e^{(i)}} \cdot \frac{\partial e^{(i)}}{\partial \mathbf{w}}$$</p>
                            </div>
                        </div>
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>Key insight:</strong></p>
                                <p class="small">$$\frac{\partial e^{(i)}}{\partial \mathbf{w}} = \frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^T\mathbf{x}^{(i)}) = \color{green}{\mathbf{x}^{(i)}}$$</p>
                                <p class="small">So: $\frac{\partial l^{(i)}}{\partial \mathbf{w}} = \color{blue}{e^{(i)}} \cdot \color{green}{\mathbf{x}^{(i)}}$</p>
                            </div>
                            <div class="fragment mt-md">
                                <p class="small"><strong>Step 3:</strong> Average over minibatch</p>
                                <p class="small">$$\frac{\partial L}{\partial \mathbf{w}} = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \color{green}{\mathbf{x}^{(i)}}\color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}$$</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient with Respect to Bias</h2>
                    <p class="small">Similar process, but simpler result</p>
                    <div class="two-column mt-sm">
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>Step 1:</strong> Same starting point</p>
                                <p class="small">$$l^{(i)} = \frac{1}{2}\color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}^2$$</p>
                            </div>
                            <div class="fragment mt-md">
                                <p class="small"><strong>Step 2:</strong> Apply chain rule</p>
                                <p class="small">$$\frac{\partial l^{(i)}}{\partial b} = \color{blue}{e^{(i)}} \cdot \frac{\partial e^{(i)}}{\partial b}$$</p>
                                <p class="small">The derivative of $b$ w.r.t. itself is <span style="color: orange">1</span></p>
                            </div>
                        </div>
                        <div class="column">
                            <div class="fragment">
                                <p class="small"><strong>Simplification:</strong></p>
                                <p class="small">$$\frac{\partial l^{(i)}}{\partial b} = \color{blue}{e^{(i)}} \cdot \color{orange}{1} = \color{blue}{e^{(i)}}$$</p>
                            </div>
                            <div class="fragment mt-md">
                                <p class="small"><strong>Step 3:</strong> Average over minibatch</p>
                                <p class="small">$$\frac{\partial L}{\partial b} = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}$$</p>
                                <p class="small emphasis-box mt-sm">Bias gradient = average error!</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Gradient Interpretation</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Weight Gradient</h4>
                            <div class="fragment">
                                $$\nabla_\mathbf{w} = \frac{1}{|\mathcal{B}|} \sum_{i} \mathbf{x}^{(i)} \cdot \text{error}^{(i)}$$
                                <ul class="mt-md small">
                                    <li>Direction depends on input $\mathbf{x}^{(i)}$</li>
                                    <li>Magnitude scales with error</li>
                                    <li>If error > 0: gradient points in direction of $\mathbf{x}$</li>
                                    <li>If error < 0: gradient points opposite to $\mathbf{x}$</li>
                                </ul>
                            </div>
                        </div>
                        <div class="column">
                            <h4>Bias Gradient</h4>
                            <div class="fragment">
                                $$\nabla_b = \frac{1}{|\mathcal{B}|} \sum_{i} \text{error}^{(i)}$$
                                <ul class="mt-md small">
                                    <li>Simply the average error</li>
                                    <li>No dependence on input</li>
                                    <li>If predictions too high: positive gradient</li>
                                    <li>If predictions too low: negative gradient</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p><strong>Key insight:</strong> Gradients tell us how to adjust parameters to reduce error!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Update Formulas</h2>
                    <p>For linear regression with squared loss:</p>
                    <div class="fragment mt-lg">
                        <p><strong>Weight update:</strong></p>
                        $$\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \color{green}{\mathbf{x}^{(i)}}\color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}$$
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Bias update:</strong></p>
                        $$b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \color{blue}{(\mathbf{w}^T\mathbf{x}^{(i)} + b - y^{(i)})}$$
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What happens if the learning rate is too large in gradient descent?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Convergence becomes slower",
                                "correct": false,
                                "explanation": "A large learning rate does not slow convergence; it can cause instability."
                            },
                            {
                                "text": "The algorithm may overshoot the minimum and diverge",
                                "correct": true,
                                "explanation": "Correct! Large steps can jump over the minimum, causing the loss to increase or oscillate."
                            },
                            {
                                "text": "The gradient becomes zero",
                                "correct": false,
                                "explanation": "The learning rate does not affect gradient computation, only how we use the gradient."
                            },
                            {
                                "text": "Memory usage increases",
                                "correct": false,
                                "explanation": "Learning rate does not affect memory consumption."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Probabilistic Interpretation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.3", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#the-normal-distribution-and-squared-loss"}]'>
                    <h2 class="truncate-title">Probabilistic View</h2>
                    <p>Why squared loss? A statistical perspective</p>
                    <div class="fragment mt-lg">
                        <p><strong>Assumption:</strong> Observations have Gaussian noise</p>
                        $$y = \mathbf{w}^T\mathbf{x} + b + \epsilon$$
                        $$\epsilon \sim \mathcal{N}(0, \sigma^2)$$
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Normal Distribution</h2>
                    <div id="normal-distribution-viz" class="demo-container"></div>
                    <div class="controls mt-md">
                        <label>Mean (Œº): 
                            <input type="range" id="normal-mean" min="-3" max="3" step="0.1" value="0">
                            <span id="mean-value">0</span>
                        </label>
                        <label class="ml-md">Std Dev (œÉ): 
                            <input type="range" id="normal-std" min="0.5" max="3" step="0.1" value="1">
                            <span id="std-value">1</span>
                        </label>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Maximum Likelihood Estimation</h2>
                    <p>Find parameters that maximize data probability</p>
                    <div class="fragment mt-lg">
                        <p><strong>Likelihood of observing y given x:</strong></p>
                        $$P(y|\mathbf{x}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y - \mathbf{w}^T\mathbf{x} - b)^2\right)$$
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Key insight:</strong></p>
                        <p>Maximizing likelihood ‚â° Minimizing squared error!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Maximizing Likelihood = Minimizing Error</h2>                    
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 20px; font-size: 0.65em;">
                        <div>
                            <div class="fragment">
                                <p style="margin-bottom: 0.3em;"><strong>Likelihood measures:</strong></p>
                                <p style="margin-bottom: 0.5em;">How probable is our data given the model?</p>
                                <ul style="margin-top: 0;">
                                    <li>Higher likelihood ‚Üí Model better explains observations</li>
                                    <li>Want to find w, b that maximize P(y|X)</li>
                                </ul>
                            </div>
                            
                            <div class="fragment" style="margin-top: 15px;">
                                <p style="margin-bottom: 0.5em;"><strong>Why use logarithms?</strong></p>
                                <ul style="margin-top: 0;">
                                    <li>Products become sums: log(‚àè) = Œ£log</li>
                                    <li>Numerical stability (avoid underflow)</li>
                                    <li>Monotonic: max(P) = max(log P)</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div>
                            <div class="fragment">
                                <p style="margin-bottom: 0.5em;"><strong>Why negative log?</strong></p>
                                <ul style="margin-top: 0;">
                                    <li>Optimization tools minimize (not maximize)</li>
                                    <li>max(log P) = min(-log P)</li>
                                    <li>Standard convention in ML</li>
                                </ul>
                            </div>
                            
                            <div class="fragment" style="margin-top: 15px;">
                                <p style="margin-bottom: 0.3em;"><strong>The key insight:</strong></p>
                                <p style="margin-bottom: 0.5em;">Under Gaussian noise assumption:</p>
                                <ul style="margin-top: 0;">
                                    <li>The exponent contains (y - ≈∑)¬≤</li>
                                    <li>Log brings it down as main term</li>
                                    <li>This IS the squared error!</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    
                    <div class="fragment emphasis-box" style="margin-top: 20px; font-size: 0.85em;">
                        <p>The error term <span style="color: #FC8484">(y - ≈∑)¬≤</span> emerges naturally from the Gaussian distribution!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step-by-Step Derivation</h2>
                    <div style="font-size: 0.45em;">                    
                    <div class="fragment mt-md">
                        <p><strong>Step 1: Single observation likelihood</strong></p>
                        $$P(y|\mathbf{x}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y - \mathbf{w}^T\mathbf{x} - b)^2\right)$$
                    </div>
                    
                    <div class="fragment mt-md">
                        <p><strong>Step 2: Take logarithm</strong></p>
                        $$\log P(y|\mathbf{x}) = \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(\exp\left(-\frac{1}{2\sigma^2}(y - \mathbf{w}^T\mathbf{x} - b)^2\right)\right)$$
                        $$= -\frac{1}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y - \mathbf{w}^T\mathbf{x} - b)^2$$
                    </div>
                    
                    <div class="fragment mt-md">
                        <p><strong>Step 3: For n independent observations</strong></p>
                        $$P(\mathbf{y}|\mathbf{X}) = \prod_{i=1}^n P(y^{(i)}|\mathbf{x}^{(i)})$$
                        $$\log P(\mathbf{y}|\mathbf{X}) = \sum_{i=1}^n \log P(y^{(i)}|\mathbf{x}^{(i)})$$
                    </div>
                    
                    <div class="fragment mt-md">
                        <p><strong>Step 4: Expand the sum</strong></p>
                        $$\log P(\mathbf{y}|\mathbf{X}) = -\frac{n}{2}\log(2\pi\sigma^2) - \sum_{i=1}^n\frac{1}{2\sigma^2}(y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)} - b)^2$$
                    </div>
                </div>
                </section>

                <section>
                    <h2 class="truncate-title">Step-by-Step Derivation (cont'd)</h2>
                    <p>Arriving at the squared loss</p>
                    <div style="font-size: 0.45em;"> 
                    <div class="fragment mt-md">
                        <p><strong>Step 5: Negate for minimization</strong></p>
                        $$-\log P(\mathbf{y}|\mathbf{X}) = \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n(y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)} - b)^2$$
                    </div>
                    
                    <div class="fragment mt-lg">
                        <p><strong>Step 6: Identify what matters for optimization</strong></p>
                        <ul>
                            <li><span style="color: #AAAAAA">$\frac{n}{2}\log(2\pi\sigma^2)$</span> is constant w.r.t. w and b</li>
                            <li><span style="color: #AAAAAA">$\frac{1}{2\sigma^2}$</span> is just a scaling factor</li>
                            <li>Only <span style="color: #10099F">$\sum_{i=1}^n(y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)} - b)^2$</span> depends on parameters!</li>
                        </ul>
                    </div>
                    
                    <div class="fragment mt-lg emphasis-box">
                        <p><strong>Therefore:</strong> Minimizing -log P(y|X) ‚â° Minimizing squared loss!</p>
                        $$\text{argmin}_{\mathbf{w},b} \left[-\log P(\mathbf{y}|\mathbf{X})\right] = \text{argmin}_{\mathbf{w},b} \sum_{i=1}^n(y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)} - b)^2$$
                    </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Connection to Squared Loss</h2>
                    <p>The fundamental equivalence</p>
                    
                    <div style="font-size: 0.45em;"> 
                    <div class="fragment mt-lg">
                        <p><strong>Final result:</strong></p>
                        $$-\log P(\mathbf{y}|\mathbf{X}) = \underbrace{\frac{1}{2\sigma^2}}_{\text{scaling}} \underbrace{\sum_{i=1}^n (y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)} - b)^2}_{\text{squared loss}} + \underbrace{\frac{n}{2}\log(2\pi\sigma^2)}_{\text{constant}}$$
                    </div>
                    
                    <div class="fragment mt-lg">
                        <p><strong>Key implications:</strong></p>
                        <ul>
                            <li>Squared loss is not arbitrary - it's principled!</li>
                            <li>Assumes errors are normally distributed</li>
                            <li>Justifies why MSE is so commonly used</li>
                        </ul>
                    </div>
                    
                    <div class="fragment mt-lg emphasis-box">
                        <p><strong>Remember:</strong> Minimizing squared loss = Maximum likelihood estimation under Gaussian noise!</p>
                    </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does assuming Gaussian noise lead to squared loss?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because Gaussian distribution has mean and variance",
                                "correct": false,
                                "explanation": "Having mean and variance does not directly lead to squared loss."
                            },
                            {
                                "text": "Because the negative log of the Gaussian PDF contains a squared term",
                                "correct": true,
                                "explanation": "Correct! The Gaussian PDF has exp(-(x-Œº)¬≤/2œÉ¬≤), and taking the negative log gives us the squared error term."
                            },
                            {
                                "text": "Because noise is always Gaussian in real data",
                                "correct": false,
                                "explanation": "Noise is not always Gaussian; this is an assumption we make."
                            },
                            {
                                "text": "Because squared loss is easier to compute",
                                "correct": false,
                                "explanation": "While squared loss is computationally convenient, this is not why it emerges from the Gaussian assumption."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Neural Network Perspective Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#linear-regression-as-a-neural-network"}]'>
                    <h2 class="truncate-title">Linear Regression as a Neural Network</h2>
                    <p>The simplest possible network architecture</p>
                    <div id="linear-network-viz" class="demo-container mt-lg"></div>
                </section>

                <section>
                    <h2 class="truncate-title">Network Components</h2>
                    <ul>
                        <li class="fragment"><strong>Input layer:</strong> Features x‚ÇÅ, x‚ÇÇ, ..., x‚Çê</li>
                        <li class="fragment"><strong>Weights:</strong> Connections w‚ÇÅ, w‚ÇÇ, ..., w‚Çê</li>
                        <li class="fragment"><strong>Bias:</strong> Additional parameter b</li>
                        <li class="fragment"><strong>Output:</strong> Single neuron producing ≈∑</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>No hidden layers = Linear model</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Biological Inspiration</h2>
                    <div class="fragment mt-lg">
                        <img src="/shared/images/neuron.svg" alt="Biological neuron diagram" style="max-width: 100%; height: auto;">
                    </div>
                    <div class="two-column">
                        <div class="column">
                            <h4>Biological Neuron</h4>
                            <ul class="fragment">
                                <li>Dendrites: inputs</li>
                                <li>Soma: processing</li>
                                <li>Axon: output</li>
                                <li>Synapses: connections</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Artificial Neuron</h4>
                            <ul class="fragment">
                                <li>Input features</li>
                                <li>Weighted sum</li>
                                <li>Output value</li>
                                <li>Weights</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <p class="small">Modern deep learning draws inspiration from many fields beyond neuroscience, including <span class="tooltip">mathematics<span class="tooltiptext">Linear algebra, calculus, and optimization theory provide the foundation for gradient descent, matrix operations, and loss functions</span></span>, <span class="tooltip">statistics<span class="tooltiptext">Statistical learning theory guides model selection, overfitting prevention, and understanding of generalization bounds</span></span>, <span class="tooltip">computer science<span class="tooltiptext">Algorithms, data structures, and computational complexity theory enable efficient implementation and scaling of neural networks</span></span>, <span class="tooltip">physics<span class="tooltiptext">Energy minimization principles, statistical mechanics, and dynamical systems theory inform optimization landscapes and training dynamics</span></span>, and <span class="tooltip">cognitive science<span class="tooltiptext">Understanding of perception, attention, and information processing inspires architectural choices and learning mechanisms</span></span>.</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes linear regression a neural network?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses gradient descent for optimization",
                                "correct": false,
                                "explanation": "Many non-neural methods also use gradient descent."
                            },
                            {
                                "text": "It has input neurons connected directly to an output neuron with weights",
                                "correct": true,
                                "explanation": "Correct! Linear regression can be viewed as a single-layer neural network with no hidden layers."
                            },
                            {
                                "text": "It processes data in batches",
                                "correct": false,
                                "explanation": "Batch processing is an optimization technique, not what defines a neural network."
                            },
                            {
                                "text": "It minimizes a loss function",
                                "correct": false,
                                "explanation": "Many machine learning methods minimize loss functions, not just neural networks."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Implementation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.1.2", "url": "https://d2l.ai/chapter_linear-regression/linear-regression.html#vectorization-for-speed"}]'>
                    <h2 class="truncate-title">Vectorization for Speed</h2>
                    <p>Why vectorized operations matter</p>
                    <pre class="fragment"><code class="python" data-line-numbers># Slow: explicit loop
c = torch.zeros(n)
for i in range(n):
    c[i] = a[i] + b[i]

# Fast: vectorized
c = a + b  # 100-1000x faster!</code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Always use vectorized operations in deep learning!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is vectorization important for deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses less memory",
                                "correct": false,
                                "explanation": "Vectorization typically uses similar amounts of memory but processes it more efficiently."
                            },
                            {
                                "text": "It leverages optimized linear algebra libraries and hardware parallelism",
                                "correct": true,
                                "explanation": "Correct! Vectorized operations can use BLAS libraries and GPU/CPU SIMD instructions for massive speedups."
                            },
                            {
                                "text": "It makes the code shorter",
                                "correct": false,
                                "explanation": "While vectorized code is often more concise, the main benefit is performance, not brevity."
                            },
                            {
                                "text": "It prevents errors in calculations",
                                "correct": false,
                                "explanation": "Both vectorized and loop-based code can be correct; vectorization is about performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Object-Oriented Design Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.2", "url": "https://d2l.ai/chapter_linear-regression/oo-design.html"}]'>
                    <h2 class="truncate-title">Object-Oriented Design for Deep Learning</h2>
                    <p>Building modular and reusable neural networks</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key Question:</strong> How do we organize code for complex deep learning projects?</p>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>OO design provides structure, reusability, and maintainability</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Object-Oriented Design?</h2>
                    <ul>
                        <li class="fragment">Code becomes complex as models grow</li>
                        <li class="fragment">Need to separate concerns (data, model, training)</li>
                        <li class="fragment">Facilitate code reuse across projects</li>
                        <li class="fragment">Enable easy experimentation and iteration</li>
                        <li class="fragment">Improve collaboration in teams</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Good architecture accelerates research and development!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Design Principles</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Separation of Concerns</h4>
                            <ul class="fragment">
                                <li>Data handling</li>
                                <li>Model architecture</li>
                                <li>Training logic</li>
                                <li>Evaluation metrics</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Modularity</h4>
                            <ul class="fragment">
                                <li>Reusable components</li>
                                <li>Plug-and-play design</li>
                                <li>Easy testing</li>
                                <li>Clear interfaces</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Class Hierarchy</h2>
                    <div id="class-hierarchy-diagram" class="demo-container"></div>
                    <p class="small mt-md">Inheritance structure for deep learning components</p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main benefit of using object-oriented design in deep learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes the code run faster",
                                "correct": false,
                                "explanation": "OO design is about code organization, not performance optimization."
                            },
                            {
                                "text": "It separates data, model, and training logic into reusable components",
                                "correct": true,
                                "explanation": "Correct! OO design creates modular, reusable components with clear responsibilities."
                            },
                            {
                                "text": "It reduces memory usage",
                                "correct": false,
                                "explanation": "Memory usage depends on the model and data, not the code organization pattern."
                            },
                            {
                                "text": "It automatically prevents bugs",
                                "correct": false,
                                "explanation": "While good design can reduce bugs, it does not automatically prevent them."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Core Components Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - OO Design Components", "url": "https://d2l.ai/chapter_linear-regression/oo-design.html"}]'>
                    <h2 class="truncate-title">Building Blocks of OO Deep Learning</h2>
                    <p>Three core classes for organizing deep learning code</p>
                    <div class="fragment mt-lg">
                        <ul>
                            <li><strong>Module:</strong> Neural network models</li>
                            <li><strong>DataModule:</strong> Data preparation and loading</li>
                            <li><strong>Trainer:</strong> Training orchestration</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Module Class</h2>
                    <p>Base class for all neural network models</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key responsibilities:</strong></p>
                        <ul>
                            <li><span class="tooltip">Parameters<span class="tooltiptext">Learnable weights and biases that define the model</span></span> storage and management</li>
                            <li>Forward pass computation</li>
                            <li>Loss calculation</li>
                            <li>Optimizer configuration</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The DataModule Class</h2>
                    <p>Encapsulates all data-related operations</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key responsibilities:</strong></p>
                        <ul>
                            <li>Data generation or loading</li>
                            <li>Train/validation/test splits</li>
                            <li>Batching and shuffling</li>
                            <li>Data transformations</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Data Pipeline Flow</h2>
                    <div id="data-pipeline-viz" class="demo-container"></div>
                    <p class="small mt-md">From raw data to training batches</p>
                </section>

                <section>
                    <h2 class="truncate-title">The Trainer Class</h2>
                    <p>Manages the entire training process</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key responsibilities:</strong></p>
                        <ul>
                            <li>Training loop execution</li>
                            <li>Gradient computation and updates</li>
                            <li>Progress tracking</li>
                            <li>Model checkpointing</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Which class is responsible for managing the training loop?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Module",
                                "correct": false,
                                "explanation": "Module defines the model architecture and forward pass, not the training loop."
                            },
                            {
                                "text": "DataModule",
                                "correct": false,
                                "explanation": "DataModule handles data preparation and loading, not training."
                            },
                            {
                                "text": "Trainer",
                                "correct": true,
                                "explanation": "Correct! The Trainer class orchestrates the training loop, including forward passes, loss computation, and parameter updates."
                            },
                            {
                                "text": "HyperParameters",
                                "correct": false,
                                "explanation": "HyperParameters is a base class for storing configuration, not for training."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Implementation Details Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Implementation Utilities", "url": "https://d2l.ai/chapter_linear-regression/oo-design.html"}]'>
                    <h2 class="truncate-title">Implementing the OO Pattern</h2>
                    <p>Utilities and techniques for clean implementation</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key features:</strong></p>
                        <ul>
                            <li>Dynamic method registration</li>
                            <li>Automatic hyperparameter tracking</li>
                            <li>Interactive progress visualization</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The add_to_class Decorator</h2>
                    <p>Dynamically add methods to existing classes</p>
                    <pre class="fragment"><code class="python" data-line-numbers>def add_to_class(Class):
    """Register functions as methods in created class."""
    def wrapper(obj):
        setattr(Class, obj.__name__, obj)
    return wrapper

# Usage example
@add_to_class(Module)
def training_step(self, batch):
    l = self.loss(self(*batch[:-1]), batch[-1])
    self.plot('loss', l, train=True)
    return l</code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p>Enables modular code organization!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">HyperParameters Base Class</h2>
                    <p>Automatic configuration management</p>
                    <pre class="fragment"><code class="python" data-line-numbers>@d2l.add_to_class(d2l.HyperParameters)
def save_hyperparameters(self, ignore=[]):
    """Save function arguments into class attributes."""
    frame = inspect.currentframe().f_back
    _, _, _, local_vars = inspect.getargvalues(frame)
    self.hparams = {k:v for k, v in local_vars.items()
                    if k not in set(ignore+['self']) and not k.startswith('_')}
    for k, v in self.hparams.items():
        setattr(self, k, v)</code></pre>
                    <div class="fragment">
                        <p class="small">Automatically tracks all initialization parameters!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">ProgressBoard Visualization</h2>
                    <p>Real-time training metrics</p>
                    <div class="fragment">
                        <ul>
                            <li>Live loss curves</li>
                            <li>Training vs validation metrics</li>
                            <li>Automatic smoothing</li>
                            <li>Interactive updates</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Visual feedback accelerates debugging!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Module Class - Base for Models</h2>
                    <p>The base class for all neural network models</p>
                    <pre class="fragment"><code class="python" data-line-numbers>class Module(nn.Module, d2l.HyperParameters):  #@save
    """The base class of models."""
    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):
        super().__init__()
        self.save_hyperparameters()
        self.board = ProgressBoard()

    def loss(self, y_hat, y):
        raise NotImplementedError

    def forward(self, X):
        assert hasattr(self, 'net'), 'Neural network is defined'
        return self.net(X)

    def plot(self, key, value, train):
        """Plot a point in animation."""
        assert hasattr(self, 'trainer'), 'Trainer is not inited'
        self.board.xlabel = 'epoch'
        if train:
            x = self.trainer.train_batch_idx / \
                self.trainer.num_train_batches
            n = self.trainer.num_train_batches / \
                self.plot_train_per_epoch
        else:
            x = self.trainer.epoch + 1
            n = self.trainer.num_val_batches / \
                self.plot_valid_per_epoch
        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),
                        ('train_' if train else 'val_') + key,
                        every_n=int(n))

    def training_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('loss', l, train=True)
        return l

    def validation_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('loss', l, train=False)

    def configure_optimizers(self):
        raise NotImplementedError</code></pre>
                    <div class="fragment mt-md">
                        <p class="small">Subclass of <code>nn.Module</code> with automatic plotting and hyperparameter management</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">DataModule Class - Data Handling</h2>
                    <p>Base class for data preparation and loading</p>
                    <pre class="fragment"><code class="python" data-line-numbers>class DataModule(d2l.HyperParameters):  #@save
    """The base class of data."""
    def __init__(self, root='../data', num_workers=4):
        self.save_hyperparameters()

    def get_dataloader(self, train):
        raise NotImplementedError

    def train_dataloader(self):
        return self.get_dataloader(train=True)

    def val_dataloader(self):
        return self.get_dataloader(train=False)</code></pre>
                    <div class="fragment mt-md">
                        <ul class="small">
                            <li>Handles data preparation and preprocessing</li>
                            <li>Returns training and validation data loaders</li>
                            <li>Data loaders yield batches for training/validation</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Trainer Class - Training Logic</h2>
                    <p>Orchestrates the training process</p>
                    <pre class="fragment"><code class="python" data-line-numbers>class Trainer(d2l.HyperParameters):  #@save
    """The base class for training models with data."""
    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
        self.save_hyperparameters()
        assert num_gpus == 0, 'No GPU support yet'

    def prepare_data(self, data):
        self.train_dataloader = data.train_dataloader()
        self.val_dataloader = data.val_dataloader()
        self.num_train_batches = len(self.train_dataloader)
        self.num_val_batches = (len(self.val_dataloader)
                                if self.val_dataloader is not None else 0)

    def prepare_model(self, model):
        model.trainer = self
        model.board.xlim = [0, self.max_epochs]
        self.model = model

    def fit(self, model, data):
        self.prepare_data(data)
        self.prepare_model(model)
        self.optim = model.configure_optimizers()
        self.epoch = 0
        self.train_batch_idx = 0
        self.val_batch_idx = 0
        for self.epoch in range(self.max_epochs):
            self.fit_epoch()

    def fit_epoch(self):
        raise NotImplementedError</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What does the save_hyperparameters() method do?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Saves the model weights to disk",
                                "correct": false,
                                "explanation": "save_hyperparameters saves configuration parameters, not model weights."
                            },
                            {
                                "text": "Automatically captures and stores all initialization arguments",
                                "correct": true,
                                "explanation": "Correct! It uses Python introspection to automatically save all constructor arguments as hyperparameters."
                            },
                            {
                                "text": "Optimizes hyperparameters for better performance",
                                "correct": false,
                                "explanation": "This method stores hyperparameters but does not optimize them."
                            },
                            {
                                "text": "Loads hyperparameters from a configuration file",
                                "correct": false,
                                "explanation": "save_hyperparameters captures arguments from the current function call, not from files."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Synthetic Data Introduction Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html"}]'>
                    <h2 class="truncate-title">Synthetic Regression Data</h2>
                    <p>Creating controlled datasets for learning and validation</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key Question:</strong> Why create artificial data when real data exists?</p>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Synthetic data helps us understand, validate, and debug our algorithms!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Use Synthetic Data?</h2>
                    <ul>
                        <li class="fragment"><strong>Known ground truth:</strong> We know the exact parameters</li>
                        <li class="fragment"><strong>Controlled complexity:</strong> Start simple, add complexity gradually</li>
                        <li class="fragment"><strong>Algorithm validation:</strong> Check if model recovers true parameters</li>
                        <li class="fragment"><strong>Debugging:</strong> Isolate issues without data uncertainties</li>
                        <li class="fragment"><strong>Educational value:</strong> Perfect for understanding concepts</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>If we can't solve a problem we created, how can we solve real problems?</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Benefits for Learning</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Real Data Challenges</h4>
                            <ul class="fragment">
                                <li>Unknown true relationship</li>
                                <li>Missing values</li>
                                <li>Outliers and noise</li>
                                <li>Complex patterns</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Synthetic Data Advantages</h4>
                            <ul class="fragment">
                                <li>Perfect ground truth</li>
                                <li>Complete control</li>
                                <li>Adjustable noise</li>
                                <li>Gradual complexity</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of using synthetic data for learning algorithms?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It is easier to generate than collecting real data",
                                "correct": false,
                                "explanation": "While convenient, the main advantage is not about ease of generation."
                            },
                            {
                                "text": "We know the true parameters and can verify if our model recovers them",
                                "correct": true,
                                "explanation": "Correct! With synthetic data, we know the ground truth and can validate that our implementation works correctly."
                            },
                            {
                                "text": "It always has less noise than real data",
                                "correct": false,
                                "explanation": "We actually add controlled noise to synthetic data to make it realistic."
                            },
                            {
                                "text": "It requires less memory to store",
                                "correct": false,
                                "explanation": "Memory requirements depend on dataset size, not whether it is synthetic or real."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Data Generation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3.1", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html#generating-the-dataset"}]'>
                    <h2 class="truncate-title">Generating the Dataset</h2>
                    <p>Creating data with a known linear relationship</p>
                    <div class="fragment mt-lg">
                        <p><strong>The generative model:</strong></p>
                        $$\mathbf{y} = \mathbf{X}\mathbf{w} + b + \boldsymbol{\epsilon}$$
                    </div>
                    <div class="fragment">
                        <ul>
                            <li>X: Features from standard normal distribution</li>
                            <li>w, b: Known true parameters</li>
                            <li>Œµ: Gaussian noise</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Generation Process</h2>
                    <div class="fragment">
                        <p><strong>Step 1:</strong> Generate features</p>
                        $$\mathbf{X} \sim \mathcal{N}(0, 1)^{n \times d}$$
                    </div>
                    <div class="fragment mt-md">
                        <p><strong>Step 2:</strong> Apply linear transformation</p>
                        $$\mathbf{y}_{\text{clean}} = \mathbf{X}\mathbf{w} + b$$
                    </div>
                    <div class="fragment mt-md">
                        <p><strong>Step 3:</strong> Add noise</p>
                        $$\mathbf{y} = \mathbf{y}_{\text{clean}} + \boldsymbol{\epsilon}, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)$$
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Example Parameters</h2>
                    <p>A concrete example with 2D features</p>
                    <div class="fragment mt-lg">
                        <ul>
                            <li><strong>True weights:</strong> $\mathbf{w} = [2, -3.4]^T$</li>
                            <li><strong>True bias:</strong> $b = 4.2$</li>
                            <li><strong>Noise level:</strong> $\sigma = 0.01$</li>
                            <li><strong>Dataset size:</strong> 1000 training + 1000 validation</li>
                        </ul>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>These become our ground truth for validation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Interactive Data Generation</h2>
                    <div id="data-generation-3d"></div>
                    <div style="position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%); display: flex; align-items: center; gap: 20px; background: rgba(255, 255, 255, 0.95); padding: 10px 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <label style="display: flex; align-items: center; gap: 8px; margin: 0;">
                            Noise (œÉ): 
                            <input type="range" id="noise-level-3d" min="0" max="0.5" step="0.01" value="0.01" style="width: 110px;">
                            <span id="noise-value-3d" style="font-family: monospace; min-width: 35px; color: #10099F; font-weight: bold;">0.01</span>
                        </label>
                        <label style="display: flex; align-items: center; gap: 8px; margin: 0;">
                            Samples: 
                            <input type="range" id="num-samples-3d" min="10" max="200" step="10" value="50" style="width: 110px;">
                            <span id="samples-value-3d" style="font-family: monospace; min-width: 30px; color: #10099F; font-weight: bold;">50</span>
                        </label>
                        <button id="regenerate-data-3d" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer; font-weight: bold;">Regenerate</button>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In the equation y = Xw + b + Œµ, what does Œµ represent?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The error in our model predictions",
                                "correct": false,
                                "explanation": "Œµ is not prediction error; it is noise added during data generation."
                            },
                            {
                                "text": "Random noise added to simulate real-world measurement errors",
                                "correct": true,
                                "explanation": "Correct! Œµ represents Gaussian noise that makes the synthetic data more realistic by simulating measurement errors."
                            },
                            {
                                "text": "The learning rate for optimization",
                                "correct": false,
                                "explanation": "The learning rate is a hyperparameter for training, not part of data generation."
                            },
                            {
                                "text": "The residual after fitting",
                                "correct": false,
                                "explanation": "Residuals are computed after model fitting; Œµ is added during data generation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Data Module Implementation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3.1", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html#generating-the-dataset"}]'>
                    <h2 class="truncate-title">SyntheticRegressionData Class</h2>
                    <p>Object-oriented approach to data generation</p>
                    <pre class="fragment"><code class="python" data-line-numbers>class SyntheticRegressionData(d2l.DataModule):
    """Synthetic data for linear regression."""
    def __init__(self, w, b, noise=0.01, num_train=1000, 
                 num_val=1000, batch_size=32):
        super().__init__()
        self.save_hyperparameters()
        n = num_train + num_val
        self.X = torch.randn(n, len(w))
        noise = torch.randn(n, 1) * noise
        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Class Design Benefits</h2>
                    <ul>
                        <li class="fragment"><strong>Encapsulation:</strong> Data and methods in one place</li>
                        <li class="fragment"><strong>Reusability:</strong> Easy to create multiple datasets</li>
                        <li class="fragment"><strong>Configurability:</strong> Adjustable parameters</li>
                        <li class="fragment"><strong>Integration:</strong> Works with training framework</li>
                        <li class="fragment"><strong>Extensibility:</strong> Can add new features easily</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Good design accelerates experimentation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding the Components</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Initialization</h4>
                            <ul class="fragment">
                                <li>Store true parameters</li>
                                <li>Set dataset sizes</li>
                                <li>Configure batch size</li>
                                <li>Generate all data upfront</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Data Generation</h4>
                            <ul class="fragment">
                                <li>Sample from normal distribution</li>
                                <li>Apply linear transformation</li>
                                <li>Add controlled noise</li>
                                <li>Store features and labels</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Using the Data Module</h2>
                    <pre><code class="python" data-line-numbers># Create synthetic dataset
data = SyntheticRegressionData(
    w=torch.tensor([2, -3.4]), 
    b=4.2,
    noise=0.01,
    num_train=1000,
    num_val=1000,
    batch_size=32
)

# Inspect the data
print('Features:', data.X[0])
print('Label:', data.y[0])
print('Dataset size:', len(data.X))</code></pre>
                    <div class="fragment mt-md">
                        <p class="small">Clean interface hides complexity!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does SyntheticRegressionData inherit from DataModule?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To automatically generate random data",
                                "correct": false,
                                "explanation": "Inheritance provides structure, not automatic data generation."
                            },
                            {
                                "text": "To integrate with the training framework and inherit data handling methods",
                                "correct": true,
                                "explanation": "Correct! Inheriting from DataModule provides a standard interface for data loading and integration with the training pipeline."
                            },
                            {
                                "text": "To make the code run faster",
                                "correct": false,
                                "explanation": "Inheritance is about code organization, not performance."
                            },
                            {
                                "text": "To save memory",
                                "correct": false,
                                "explanation": "Inheritance does not affect memory usage for data storage."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Data Iteration Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3.2", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html#reading-the-dataset"}]'>
                    <h2 class="truncate-title">Data Iteration and Minibatches</h2>
                    <p>Efficiently feeding data to the model</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key concepts:</strong></p>
                        <ul>
                            <li>Minibatch processing</li>
                            <li>Random shuffling for training</li>
                            <li>Sequential access for validation</li>
                            <li>Memory-efficient iteration</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Manual Data Loader Implementation</h2>
                    <pre><code class="python" data-line-numbers>@d2l.add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    if train:
        indices = list(range(0, self.num_train))
        # Shuffle for training
        random.shuffle(indices)
    else:
        indices = list(range(self.num_train, 
                           self.num_train + self.num_val))
    
    for i in range(0, len(indices), self.batch_size):
        batch_indices = torch.tensor(
            indices[i: i + self.batch_size])
        yield self.X[batch_indices], self.y[batch_indices]</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Why Minibatches?</h2>
                    <div class="three-column">
                        <div class="column fragment">
                            <h4>Single Sample</h4>
                            <ul>
                                <li>Very noisy gradients</li>
                                <li>Poor hardware utilization</li>
                                <li>Fast iteration</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Minibatch ‚úì</h4>
                            <ul>
                                <li>Balanced noise</li>
                                <li>GPU efficient</li>
                                <li>Good convergence</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Full Batch</h4>
                            <ul>
                                <li>Stable gradients</li>
                                <li>Memory intensive</li>
                                <li>Slow iteration</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Minibatches balance efficiency and stability!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Shuffling Importance</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>With Shuffling</h4>
                            <ul class="fragment">
                                <li>Breaks data order patterns</li>
                                <li>Better gradient estimates</li>
                                <li>Faster convergence</li>
                                <li>Prevents overfitting to order</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Without Shuffling</h4>
                            <ul class="fragment">
                                <li>May learn spurious patterns</li>
                                <li>Biased gradient estimates</li>
                                <li>Slower convergence</li>
                                <li>Reproducible for debugging</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we shuffle data during training but not during validation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Validation data is smaller and does not need shuffling",
                                "correct": false,
                                "explanation": "The size of validation data is not the reason for not shuffling."
                            },
                            {
                                "text": "Shuffling training data prevents learning order-dependent patterns, while consistent validation order aids debugging",
                                "correct": true,
                                "explanation": "Correct! Shuffling during training improves generalization, while consistent validation order helps with reproducibility and debugging."
                            },
                            {
                                "text": "Shuffling takes too much time during validation",
                                "correct": false,
                                "explanation": "Shuffling is a fast operation and time is not the concern."
                            },
                            {
                                "text": "Validation does not use minibatches",
                                "correct": false,
                                "explanation": "Validation also uses minibatches for efficiency."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Framework Data Loaders Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3.3", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html#concise-implementation-of-the-data-loader"}]'>
                    <h2 class="truncate-title">Framework Data Loaders</h2>
                    <p>Leveraging built-in PyTorch functionality</p>
                    <div class="fragment mt-lg">
                        <p><strong>Why use framework loaders?</strong></p>
                        <ul>
                            <li>Optimized for performance</li>
                            <li>Multi-threading support</li>
                            <li>Memory pinning for GPU</li>
                            <li>Advanced sampling strategies</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">PyTorch DataLoader Implementation</h2>
                    <pre><code class="python" data-line-numbers>@d2l.add_to_class(d2l.DataModule)
def get_tensorloader(self, tensors, train, indices=slice(0, None)):
    tensors = tuple(a[indices] for a in tensors)
    dataset = torch.utils.data.TensorDataset(*tensors)
    return torch.utils.data.DataLoader(
        dataset, 
        self.batch_size,
        shuffle=train  # Automatic shuffling!
    )

@d2l.add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    i = slice(0, self.num_train) if train else \
        slice(self.num_train, None)
    return self.get_tensorloader((self.X, self.y), train, i)</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Advantages</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Manual Implementation</h4>
                            <ul class="fragment">
                                <li>Educational value</li>
                                <li>Full control</li>
                                <li>Simple debugging</li>
                                <li>Single-threaded</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Framework Loader</h4>
                            <ul class="fragment">
                                <li>Production ready</li>
                                <li>Multi-threading</li>
                                <li>GPU optimization</li>
                                <li>Rich functionality</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Use framework loaders for real projects!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Advanced Features</h2>
                    <pre><code class="python" data-line-numbers># Additional DataLoader features
dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,        # Parallel data loading
    pin_memory=True,      # Faster GPU transfer
    drop_last=True,       # Drop incomplete last batch
    prefetch_factor=2     # Prefetch batches
)

# Query properties
print(f'Number of batches: {len(dataloader)}')
print(f'Batch size: {dataloader.batch_size}')

# Iterate with progress
for batch_idx, (X, y) in enumerate(dataloader):
    # Process batch
    pass</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of using PyTorch DataLoader over a manual implementation?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It uses less code",
                                "correct": false,
                                "explanation": "While concise, the main advantage is not brevity."
                            },
                            {
                                "text": "It provides optimized multi-threaded data loading and GPU transfer",
                                "correct": true,
                                "explanation": "Correct! Framework data loaders offer production-ready optimizations like parallel loading, memory pinning, and efficient GPU transfers."
                            },
                            {
                                "text": "It automatically cleans the data",
                                "correct": false,
                                "explanation": "Data loaders handle iteration, not data cleaning."
                            },
                            {
                                "text": "It generates synthetic data automatically",
                                "correct": false,
                                "explanation": "Data loaders iterate over existing data, they do not generate it."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Practical Considerations Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.3.4", "url": "https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html#summary"}]'>
                    <h2 class="truncate-title">Practical Considerations</h2>
                    <p>Real-world data loading challenges and solutions</p>
                    <div class="fragment mt-lg">
                        <ul>
                            <li>Large datasets that don't fit in memory</li>
                            <li>Streaming data from networks</li>
                            <li>On-the-fly data augmentation</li>
                            <li>Distributed training requirements</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Memory Management Strategies</h2>
                    <div class="three-column">
                        <div class="column fragment">
                            <h4>In-Memory</h4>
                            <ul>
                                <li>Fast access</li>
                                <li>Limited size</li>
                                <li>Good for small data</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Memory-Mapped</h4>
                            <ul>
                                <li>Virtual memory</li>
                                <li>OS manages paging</li>
                                <li>Medium datasets</li>
                            </ul>
                        </div>
                        <div class="column fragment">
                            <h4>Streaming</h4>
                            <ul>
                                <li>Load on demand</li>
                                <li>Unlimited size</li>
                                <li>I/O overhead</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Data Augmentation Pipeline</h2>
                    <pre><code class="python" data-line-numbers>class AugmentedDataLoader:
    def __init__(self, base_data, augmentations):
        self.base_data = base_data
        self.augmentations = augmentations
    
    def __iter__(self):
        for X, y in self.base_data:
            # Apply augmentations on-the-fly
            X_aug = self.augmentations(X)
            yield X_aug, y

# Example: Add noise during training
def add_training_noise(X, noise_level=0.01):
    return X + torch.randn_like(X) * noise_level</code></pre>
                    <div class="fragment">
                        <p class="small">Generate variations without storing them!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Best Practices</h2>
                    <ul>
                        <li class="fragment"><strong>Start simple:</strong> Use synthetic data for initial testing</li>
                        <li class="fragment"><strong>Profile loading:</strong> Identify bottlenecks early</li>
                        <li class="fragment"><strong>Use caching:</strong> Avoid redundant computations</li>
                        <li class="fragment"><strong>Prefetch data:</strong> Overlap I/O with computation</li>
                        <li class="fragment"><strong>Monitor memory:</strong> Watch for leaks and inefficiencies</li>
                        <li class="fragment"><strong>Test determinism:</strong> Ensure reproducible results</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Debugging with Synthetic Data</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Common Issues</h4>
                            <ul class="fragment">
                                <li>Model not converging</li>
                                <li>Exploding gradients</li>
                                <li>Poor generalization</li>
                                <li>Slow training</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Synthetic Data Tests</h4>
                            <ul class="fragment">
                                <li>Can recover true params?</li>
                                <li>Loss decreases to noise floor?</li>
                                <li>Batch size effects?</li>
                                <li>Learning rate sensitivity?</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Always validate on synthetic data first!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When should you use on-the-fly data generation instead of pre-generating all data?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When the data is small enough to fit in memory",
                                "correct": false,
                                "explanation": "If data fits in memory, pre-generation is usually more efficient."
                            },
                            {
                                "text": "When you need different random augmentations each epoch or have limited memory",
                                "correct": true,
                                "explanation": "Correct! On-the-fly generation is ideal for data augmentation and when storage is limited."
                            },
                            {
                                "text": "When you want faster training",
                                "correct": false,
                                "explanation": "On-the-fly generation typically adds computational overhead, making training slower."
                            },
                            {
                                "text": "When using validation data",
                                "correct": false,
                                "explanation": "Validation data should be consistent across epochs for fair comparison."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Final Quiz -->
            <section>
                <h2 class="truncate-title">Test Your Understanding</h2>
                <div data-mcq='{
                    "question": "Why do we start with linear models before deep networks?",
                    "type": "single",
                    "options": [
                        {
                            "text": "Linear models are more accurate than deep networks",
                            "correct": false,
                            "explanation": "Deep networks are generally more powerful but also more complex to understand and train."
                        },
                        {
                            "text": "They require less data to train",
                            "correct": false,
                            "explanation": "While true, this is not the main pedagogical reason for starting with linear models."
                        },
                        {
                            "text": "To understand fundamental concepts without architectural complexity",
                            "correct": true,
                            "explanation": "Correct! Linear models let us focus on loss functions, optimization, and training basics without the complexity of deep architectures."
                        },
                        {
                            "text": "They are faster to compute",
                            "correct": false,
                            "explanation": "While linear models are indeed faster, this is not the primary educational motivation."
                        }
                    ]
                }'></div>
            </section>

            <!-- Implementing from Scratch Introduction Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html"}]'>
                    <h2 class="truncate-title">Linear Regression Implementation from Scratch</h2>
                    <p>Building the foundation of deep learning step by step</p>
                    <div class="fragment mt-lg">
                        <p><strong>Why implement from scratch?</strong></p>
                        <ul>
                            <li>Understand every component deeply</li>
                            <li>Demystify the "magic" of frameworks</li>
                            <li>Build intuition for debugging</li>
                            <li>Learn transferable principles</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Complete Pipeline</h2>
                    <div id="training-pipeline-overview" class="demo-container"></div>
                    <p class="small mt-md">Every deep learning model follows this fundamental pattern</p>
                </section>

                <section>
                    <h2 class="truncate-title">Benefits of Understanding Fundamentals</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Using Frameworks Only</h4>
                            <ul class="fragment">
                                <li>Quick to start</li>
                                <li>Black box operations</li>
                                <li>Hard to debug</li>
                                <li>Limited customization</li>
                                <li>Framework dependent</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Understanding Internals</h4>
                            <ul class="fragment">
                                <li>Deeper understanding</li>
                                <li>Know what's happening</li>
                                <li>Effective debugging</li>
                                <li>Custom implementations</li>
                                <li>Framework agnostic</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework-Agnostic Principles</h2>
                    <p>Core concepts that apply everywhere</p>
                    <ul class="fragment">
                        <li><strong>Parameter initialization:</strong> Starting point matters</li>
                        <li><strong>Forward propagation:</strong> Computing predictions</li>
                        <li><strong>Loss calculation:</strong> Measuring error</li>
                        <li><strong>Backpropagation:</strong> Computing gradients</li>
                        <li><strong>Parameter updates:</strong> Learning from mistakes</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>These principles power all neural networks!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">What We'll Build</h2>
                    <p>A complete linear regression system</p>
                    <div class="fragment mt-lg">
                        <pre><code class="python" data-line-numbers>class LinearRegressionScratch:
    def __init__(self, num_inputs, lr, sigma=0.01):
        # Initialize parameters
        
    def forward(self, X):
        # Compute predictions
        
    def loss(self, y_hat, y):
        # Calculate error
        
    def sgd(self, params, lr):
        # Update parameters
        
    def train_epoch(self, train_iter):
        # Training loop</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary benefit of implementing models from scratch?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It runs faster than framework implementations",
                                "correct": false,
                                "explanation": "Framework implementations are usually highly optimized and run faster."
                            },
                            {
                                "text": "It requires less code than using frameworks",
                                "correct": false,
                                "explanation": "From-scratch implementations typically require more code than framework abstractions."
                            },
                            {
                                "text": "It provides deep understanding of how models work internally",
                                "correct": true,
                                "explanation": "Correct! Implementing from scratch reveals the inner workings, helping you understand, debug, and customize models effectively."
                            },
                            {
                                "text": "It is required for production deployment",
                                "correct": false,
                                "explanation": "Production systems typically use optimized framework implementations."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Model Definition Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4.1", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#defining-the-model"}]'>
                    <h2 class="truncate-title">Defining the Model</h2>
                    <p>Parameter initialization and forward pass</p>
                    <div class="fragment mt-lg">
                        <p><strong>Two key components:</strong></p>
                        <ul>
                            <li>Parameters: weights (w) and bias (b)</li>
                            <li>Forward function: computing predictions</li>
                        </ul>
                    </div>
                    <div class="fragment mt-lg">
                        $$\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + b$$
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter Initialization</h2>
                    <pre><code class="python" data-line-numbers>def init_params(num_inputs, sigma=0.01):
    """Initialize model parameters."""
    w = torch.normal(0, sigma, (num_inputs, 1), 
                     requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]

# Example: 2 input features
w, b = init_params(2)
print(f'w: {w}')  # Random values ~N(0, 0.01¬≤)
print(f'b: {b}')  # [0.]</code></pre>
                    <div class="fragment mt-md">
                        <p class="small">Small random weights prevent <span class="tooltip">symmetry breaking<span class="tooltiptext">If all weights start the same, neurons learn identical features</span></span></p>
                    </div>
                </section>

                <!--<section>
                    <h2 class="truncate-title">Why Random Initialization?</h2>
                    <div id="initialization-comparison" class="demo-container"></div>
                    <div class="controls mt-md">
                        <button id="init-zeros">Initialize with Zeros</button>
                        <button id="init-random">Initialize Randomly</button>
                        <button id="init-large">Initialize with Large Values</button>
                    </div>
                    <p class="small mt-md">Different initializations lead to different learning dynamics</p>
                </section>

                <section>
                    <h2 class="truncate-title">The Forward Pass</h2>
                    <pre><code class="python" data-line-numbers>def forward(X, w, b):
    """Linear model forward pass."""
    return torch.matmul(X, w) + b

# Example with batch of 10 samples, 2 features
X = torch.randn(10, 2)
y_hat = forward(X, w, b)
print(f'Input shape: {X.shape}')      # (10, 2)
print(f'Weight shape: {w.shape}')     # (2, 1)
print(f'Output shape: {y_hat.shape}') # (10, 1)</code></pre>
                    <div class="fragment mt-md">
                        <p><strong>Matrix dimensions:</strong></p>
                        <ul>
                            <li>X: (batch_size, num_features)</li>
                            <li>w: (num_features, 1)</li>
                            <li>y_hat: (batch_size, 1)</li>
                        </ul>
                    </div>
                </section>-->

                <section>
                    <h2 class="truncate-title">Vectorization Benefits</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Loop Implementation</h4>
                            <pre><code class="python">def forward_slow(X, w, b):
    y_hat = []
    for i in range(len(X)):
        pred = b
        for j in range(len(w)):
            pred += X[i][j] * w[j]
        y_hat.append(pred)
    return y_hat</code></pre>
                            <p class="small fragment">Slow, especially for large batches</p>
                        </div>
                        <div class="column">
                            <h4>Vectorized</h4>
                            <pre><code class="python">def forward(X, w, b):
    return torch.matmul(X, w) + b</code></pre>
                            <p class="small fragment">Fast, leverages optimized libraries</p>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Vectorization can be 10-100x faster!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we initialize weights with small random values instead of zeros?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Random values make the model train faster",
                                "correct": false,
                                "explanation": "Speed is not the primary reason for random initialization."
                            },
                            {
                                "text": "To break symmetry and ensure different neurons learn different features",
                                "correct": true,
                                "explanation": "Correct! Random initialization breaks symmetry, allowing each neuron to learn unique patterns during training."
                            },
                            {
                                "text": "Zero initialization causes numerical errors",
                                "correct": false,
                                "explanation": "Zero initialization is numerically stable but leads to symmetry problems."
                            },
                            {
                                "text": "It uses less memory",
                                "correct": false,
                                "explanation": "Memory usage is the same regardless of initialization values."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Loss Function Implementation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4.2", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#defining-the-loss-function"}]'>
                    <h2 class="truncate-title">Defining the Loss Function</h2>
                    <p>Measuring how wrong our predictions are</p>
                    <div class="fragment mt-lg">
                        <p><strong>Squared Loss (L2 Loss):</strong></p>
                        $$\ell^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$
                    </div>
                    <div class="fragment">
                        <p><strong>Why divide by 2?</strong> Makes the gradient cleaner!</p>
                        <p class="small">Derivative of $x^2/2$ is simply $x$</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Single Sample vs Minibatch Loss</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Single Sample</h4>
                            <div class="fragment">
                                $$\ell(\mathbf{w}, b) = \frac{1}{2} (\hat{y} - y)^2$$
                                <pre><code class="python">def loss_single(y_hat, y):
    return ((y_hat - y) ** 2) / 2</code></pre>
                            </div>
                        </div>
                        <div class="column">
                            <h4>Minibatch (Average)</h4>
                            <div class="fragment">
                                $$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^{n} \ell^{(i)}(\mathbf{w}, b)$$
                                <pre><code class="python">def loss(y_hat, y):
    l = (y_hat - y) ** 2 / 2
    return l.mean()</code></pre>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Loss Function Properties</h2>
                    <div class="controls" style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px;">
                        <label style="display: flex; align-items: center; gap: 8px;">
                            True value (y): 
                            <input type="range" id="true-value" min="-2" max="2" step="0.1" value="0" style="width: 150px;">
                            <span id="true-value-display" style="font-family: monospace; min-width: 35px;">0</span>
                        </label>
                        <button id="show-gradient" style="background: #10099F; color: white; border: none; padding: 8px 16px; border-radius: 5px; cursor: pointer; font-weight: bold;">Show Gradient</button>
                    </div>
                    <div id="loss-properties-viz" class="demo-container"></div>
                    <p class="small mt-md" style="text-align: center;">Squared loss is convex with a unique minimum</p>
                </section>

                <section>
                    <h2 class="truncate-title">Implementation Details</h2>
                    <pre><code class="python" data-line-numbers>def squared_loss(y_hat, y):
    """Squared loss for regression."""
    # Reshape y to match y_hat dimensions
    y = y.reshape(y_hat.shape)
    
    # Compute squared differences
    squared_diff = (y_hat - y) ** 2
    
    # Divide by 2 for cleaner gradients
    loss_per_sample = squared_diff / 2
    
    # Return mean loss across batch
    return loss_per_sample.mean()

# Example
y_hat = torch.tensor([[1.5], [2.0], [3.5]])
y = torch.tensor([[1.0], [2.0], [3.0]])
loss_val = squared_loss(y_hat, y)
print(f'Loss: {loss_val:.4f}')  # 0.0625</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Why Squared Loss?</h2>
                    <ul>
                        <li class="fragment"><strong>Mathematical convenience:</strong> Easy to differentiate</li>
                        <li class="fragment"><strong>Convex optimization:</strong> Guaranteed global minimum</li>
                        <li class="fragment"><strong>Penalizes large errors more:</strong> Quadratic growth</li>
                        <li class="fragment"><strong>Maximum likelihood interpretation:</strong> Assumes Gaussian noise</li>
                        <li class="fragment"><strong>Smooth gradients:</strong> Stable optimization</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>The foundation of many regression problems!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we divide the squared loss by 2?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the loss values smaller",
                                "correct": false,
                                "explanation": "The division by 2 is not about scaling the loss magnitude."
                            },
                            {
                                "text": "To simplify the gradient computation by canceling with the power rule derivative",
                                "correct": true,
                                "explanation": "Correct! When we differentiate (y_hat - y)¬≤/2, the 2 from the power rule cancels with the division by 2, giving us simply (y_hat - y)."
                            },
                            {
                                "text": "It is a requirement for convergence",
                                "correct": false,
                                "explanation": "The optimization would converge with or without the division by 2."
                            },
                            {
                                "text": "To normalize the loss between 0 and 1",
                                "correct": false,
                                "explanation": "Dividing by 2 does not normalize the loss to [0,1] range."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Optimization Algorithm Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4.3", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#defining-the-optimization-algorithm"}]'>
                    <h2 class="truncate-title">Defining the Optimization Algorithm</h2>
                    <p>Stochastic Gradient Descent (SGD) from scratch</p>
                    <div class="fragment mt-lg">
                        <p><strong>The update rule:</strong></p>
                        $$\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\mathbf{w}} \ell^{(i)}(\mathbf{w}, b)$$
                    </div>
                    <div class="fragment">
                        <ul>
                            <li>Œ∑: learning rate (step size)</li>
                            <li>|ùìë|: batch size</li>
                            <li>‚àá: gradient</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">SGD Implementation</h2>
                    <pre><code class="python" data-line-numbers>def sgd(params, lr, batch_size):
    """Stochastic gradient descent."""
    with torch.no_grad():
        for param in params:
            # Update in the negative gradient direction
            param -= lr * param.grad / batch_size
            # Clear gradients for next iteration
            param.grad.zero_()

# Usage example
lr = 0.03
batch_size = 10

# After computing gradients via backprop
sgd([w, b], lr, batch_size)</code></pre>
                    <div class="fragment mt-md">
                        <p class="small">Note: We divide by batch_size because PyTorch sums gradients by default</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Understanding the Gradient</h2>
                    <div class="fragment">
                        <p><strong>For squared loss:</strong></p>
                        $$\frac{\partial \ell}{\partial \mathbf{w}} = (\hat{y} - y) \cdot \mathbf{x}$$
                        $$\frac{\partial \ell}{\partial b} = \hat{y} - y$$
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Intuition:</strong></p>
                        <ul>
                            <li>Error (≈∑ - y) tells us direction and magnitude</li>
                            <li>Multiply by input x for weight gradient</li>
                            <li>Bias gradient is just the error</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Learning Rate Effects</h2>
                    <div id="learning-rate-effects" class="demo-container"></div>
                    <div class="controls mt-md">
                        <label>Learning Rate: 
                            <select id="lr-select">
                                <option value="0.001">0.001 (Too Small)</option>
                                <option value="0.03" selected>0.03 (Just Right)</option>
                                <option value="0.5">0.5 (Too Large)</option>
                            </select>
                        </label>
                        <button id="restart-sgd">Restart Training</button>
                    </div>
                    <p class="small mt-md">Learning rate controls convergence speed and stability</p>
                </section>

                <section>
                    <h2 class="truncate-title">Minibatch Gradient Accumulation</h2>
                    <pre><code class="python" data-line-numbers>def train_epoch(X, y, w, b, lr, batch_size):
    """One epoch of training."""
    num_batches = len(X) // batch_size
    
    for i in range(num_batches):
        # Get minibatch
        start = i * batch_size
        end = start + batch_size
        X_batch = X[start:end]
        y_batch = y[start:end]
        
        # Forward pass
        y_hat = forward(X_batch, w, b)
        
        # Compute loss
        l = squared_loss(y_hat, y_batch)
        
        # Backward pass (compute gradients)
        l.backward()
        
        # Update parameters
        sgd([w, b], lr, batch_size)</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">In-Place Parameter Updates</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Why In-Place?</h4>
                            <ul class="fragment">
                                <li>Memory efficient</li>
                                <li>Preserves gradient tracking</li>
                                <li>Required for autograd</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Implementation</h4>
                            <pre class="fragment"><code class="python"># Correct: in-place
param -= lr * grad

# Wrong: creates new tensor
param = param - lr * grad</code></pre>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Use -= for in-place updates!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In SGD, why do we divide the gradient by the batch size?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make the gradients smaller",
                                "correct": false,
                                "explanation": "While it does make gradients smaller, this is not the primary reason."
                            },
                            {
                                "text": "Because PyTorch sums gradients over the batch, and we want the average",
                                "correct": true,
                                "explanation": "Correct! PyTorch accumulates (sums) gradients across the batch. Dividing by batch size gives us the average gradient, which provides more stable updates."
                            },
                            {
                                "text": "It is required for convergence",
                                "correct": false,
                                "explanation": "SGD can converge without this division, though you would need to adjust the learning rate."
                            },
                            {
                                "text": "To prevent gradient explosion",
                                "correct": false,
                                "explanation": "While it helps with stability, gradient explosion is typically handled by gradient clipping."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Training Loop Implementation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#training"}]'>
                    <h2 class="truncate-title">Training Loop Implementation</h2>
                    <p>Bringing all components together</p>
                    <div class="fragment mt-lg">
                        <p><strong>The training loop:</strong></p>
                        <ol>
                            <li>Iterate through epochs</li>
                            <li>Shuffle and batch data</li>
                            <li>Forward pass ‚Üí Loss ‚Üí Backward ‚Üí Update</li>
                            <li>Track training metrics</li>
                        </ol>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Complete Training Loop</h2>
                    <pre><code class="python" data-line-numbers>def train(data_iter, w, b, lr, num_epochs, batch_size):
    """Complete training loop."""
    for epoch in range(num_epochs):
        # Track epoch loss
        total_loss, num_batches = 0.0, 0
        
        for X_batch, y_batch in data_iter:
            # Forward pass
            y_hat = forward(X_batch, w, b)
            
            # Compute loss
            l = squared_loss(y_hat, y_batch)
            
            # Backward pass
            l.backward()
            
            # Update parameters
            sgd([w, b], lr, batch_size)
            
            # Track metrics
            total_loss += l.item()
            num_batches += 1
        
        # Print epoch statistics
        avg_loss = total_loss / num_batches
        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Training Flow Visualization</h2>
                    <div id="training-flow-viz" class="demo-container"></div>
                    <div class="controls mt-md">
                        <button id="start-training-flow">Start Training</button>
                        <button id="pause-training-flow">Pause</button>
                        <button id="reset-training-flow">Reset</button>
                    </div>
                    <p class="small mt-md">Watch how data flows through the training pipeline</p>
                </section>

                <section>
                    <h2 class="truncate-title">Monitoring Training Progress</h2>
                    <pre><code class="python" data-line-numbers># Training with progress tracking
num_epochs = 3
lr = 0.03
batch_size = 10

# Initialize tracking
loss_history = []

for epoch in range(num_epochs):
    epoch_loss = []
    for X, y in data_iter:
        y_hat = net(X, w, b)
        l = loss(y_hat, y)
        l.backward()
        sgd([w, b], lr, batch_size)
        epoch_loss.append(l.item())
    
    avg_loss = sum(epoch_loss) / len(epoch_loss)
    loss_history.append(avg_loss)
    
    print(f'Epoch {epoch + 1}/{num_epochs}')
    print(f'  Loss: {avg_loss:.6f}')
    print(f'  w: {w.data.numpy().flatten()}')
    print(f'  b: {b.item():.4f}')</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Convergence Tracking</h2>
                    <div id="convergence-tracking-viz" class="demo-container"></div>
                    <div class="mt-md">
                        <p class="small">True parameters: w = [2, -3.4], b = 4.2</p>
                        <div id="parameter-comparison"></div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Common Training Issues</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Problems</h4>
                            <ul class="fragment">
                                <li>Loss not decreasing</li>
                                <li>Loss exploding</li>
                                <li>Slow convergence</li>
                                <li>Oscillating loss</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Solutions</h4>
                            <ul class="fragment">
                                <li>Check learning rate</li>
                                <li>Gradient clipping</li>
                                <li>Adjust batch size</li>
                                <li>Better initialization</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Start with small learning rates and increase gradually!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the correct order of operations in a training loop?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Update ‚Üí Forward ‚Üí Loss ‚Üí Backward",
                                "correct": false,
                                "explanation": "Parameters should be updated after computing gradients, not before."
                            },
                            {
                                "text": "Forward ‚Üí Loss ‚Üí Backward ‚Üí Update",
                                "correct": true,
                                "explanation": "Correct! First compute predictions (forward), calculate loss, compute gradients (backward), then update parameters."
                            },
                            {
                                "text": "Loss ‚Üí Forward ‚Üí Update ‚Üí Backward",
                                "correct": false,
                                "explanation": "You need predictions from the forward pass before computing loss."
                            },
                            {
                                "text": "Backward ‚Üí Forward ‚Üí Loss ‚Üí Update",
                                "correct": false,
                                "explanation": "Gradients are computed via backward pass after the loss is calculated."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Putting It All Together Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html"}]'>
                    <h2 class="truncate-title">Putting It All Together</h2>
                    <p>Complete implementation with real results</p>
                    <div class="fragment mt-lg">
                        <p><strong>Training configuration:</strong></p>
                        <ul>
                            <li>True weights: w = [2, -3.4]</li>
                            <li>True bias: b = 4.2</li>
                            <li>Learning rate: 0.03</li>
                            <li>Epochs: 3</li>
                            <li>Batch size: 10</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Complete LinearRegressionScratch Class</h2>
                    <pre><code class="python" data-line-numbers>class LinearRegressionScratch:
    def __init__(self, num_inputs, lr, sigma=0.01):
        self.w = torch.normal(0, sigma, (num_inputs, 1),
                             requires_grad=True)
        self.b = torch.zeros(1, requires_grad=True)
        self.lr = lr
    
    def forward(self, X):
        """Linear model prediction."""
        return torch.matmul(X, self.w) + self.b
    
    def loss(self, y_hat, y):
        """Squared loss function."""
        return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2
    
    def sgd(self, batch_size):
        """SGD optimizer."""
        with torch.no_grad():
            for param in [self.w, self.b]:
                param -= self.lr * param.grad / batch_size
                param.grad.zero_()</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Training the Model</h2>
                    <pre><code class="python" data-line-numbers># Generate synthetic data
true_w = torch.tensor([2, -3.4])
true_b = 4.2
X, y = synthetic_data(true_w, true_b, 1000)

# Initialize model
model = LinearRegressionScratch(num_inputs=2, lr=0.03)

# Create data iterator
batch_size = 10
data_iter = load_array((X, y), batch_size)

# Train for 3 epochs
num_epochs = 3
for epoch in range(num_epochs):
    for X_batch, y_batch in data_iter:
        l = model.loss(model.forward(X_batch), y_batch)
        l.backward()
        model.sgd(batch_size)
    
    # Evaluate
    with torch.no_grad():
        train_l = model.loss(model.forward(X), y)
        print(f'Epoch {epoch + 1}, loss {float(train_l):.6f}')</code></pre>
                </section>

                <section>
                    <h2 class="truncate-title">Results: Parameter Recovery</h2>
                    <div class="fragment">
                        <pre><code class="python">print(f'Error in estimating w: {true_w - model.w.reshape(true_w.shape)}')
print(f'Error in estimating b: {true_b - model.b}')</code></pre>
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Typical output after 3 epochs:</strong></p>
                        <pre><code class="text">Error in estimating w: tensor([ 0.0003, -0.0002])
Error in estimating b: tensor([0.0008])</code></pre>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Successfully recovered the true parameters! üéâ</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Dynamics</h2>
                    <div id="complete-training-viz" class="demo-container"></div>
                    <div class="controls mt-md">
                        <button id="run-complete-training">Run Training</button>
                        <label class="ml-md">Speed: 
                            <select id="training-speed">
                                <option value="slow">Slow</option>
                                <option value="normal" selected>Normal</option>
                                <option value="fast">Fast</option>
                            </select>
                        </label>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "After training on synthetic data with known parameters, what indicates successful learning?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The loss reaches exactly zero",
                                "correct": false,
                                "explanation": "Due to noise in the data, the loss will not reach exactly zero even with perfect parameters."
                            },
                            {
                                "text": "The learned parameters closely match the true parameters used to generate the data",
                                "correct": true,
                                "explanation": "Correct! When the model recovers parameters close to the true values (within noise tolerance), it shows the implementation works correctly."
                            },
                            {
                                "text": "The training runs for exactly 3 epochs",
                                "correct": false,
                                "explanation": "The number of epochs is a hyperparameter choice, not an indicator of success."
                            },
                            {
                                "text": "The gradients become zero",
                                "correct": false,
                                "explanation": "Gradients approaching zero might indicate convergence but not necessarily to the correct parameters."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Exercises and Extensions Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.4.6", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#exercises"}]'>
                    <h2 class="truncate-title">Exercises and Extensions</h2>
                    <p>Deepening understanding through experimentation</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key experiments to try:</strong></p>
                        <ul>
                            <li>What if we don't initialize parameters?</li>
                            <li>How does learning rate affect convergence?</li>
                            <li>What happens with different loss functions?</li>
                            <li>Can we add gradient clipping?</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Experiment 1: No Initialization</h2>
                    <pre><code class="python" data-line-numbers># What happens without proper initialization?
class UninitializedModel:
    def __init__(self, num_inputs):
        # PyTorch would throw an error!
        # self.w = ???  # No initialization
        # self.b = ???  # No initialization
        pass

# Problems:
# 1. No gradients can be computed
# 2. Forward pass would fail
# 3. Model has no learnable parameters

# Lesson: Always initialize parameters!</code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Parameters must exist and have requires_grad=True!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Experiment 2: Learning Rate Effects</h2>
                    <div id="lr-experiment-viz" class="demo-container"></div>
                    <div class="controls mt-md">
                        <label>Learning Rate: 
                            <input type="range" id="lr-exp-slider" min="-3" max="0" step="0.1" value="-1.5">
                            <span id="lr-exp-value">0.03</span>
                        </label>
                        <button id="run-lr-experiment">Run Experiment</button>
                    </div>
                    <p class="small mt-md">Explore how learning rate affects convergence</p>
                </section>

                <section>
                    <h2 class="truncate-title">Experiment 3: Alternative Loss Functions</h2>
                    <div class="fragment">
                        <h4>L1 Loss (MAE)</h4>
                        <pre><code class="python">def l1_loss(y_hat, y):
    return torch.abs(y_hat - y).mean()</code></pre>
                        <ul class="fragment small">
                            <li>Less sensitive to outliers</li>
                            <li>Non-smooth at zero</li>
                        </ul>
                    </div>
                    <div class="fragment mt-lg">
                        <h4>Huber Loss</h4>
                        <pre><code class="python">def huber_loss(y_hat, y, delta=1.0):
    diff = torch.abs(y_hat - y)
    return torch.where(diff < delta,
                     0.5 * diff ** 2,
                     delta * diff - 0.5 * delta ** 2).mean()</code></pre>
                        <ul class="fragment small">
                            <li>Best of both worlds</li>
                            <li>Smooth and robust</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Experiment 4: Gradient Clipping</h2>
                    <pre><code class="python" data-line-numbers>def sgd_with_clipping(params, lr, batch_size, max_norm=1.0):
    """SGD with gradient clipping."""
    with torch.no_grad():
        # Compute total gradient norm
        total_norm = 0
        for param in params:
            if param.grad is not None:
                total_norm += param.grad.data.norm(2).item() ** 2
        total_norm = total_norm ** 0.5
        
        # Clip gradients if needed
        clip_coef = max_norm / (total_norm + 1e-6)
        if clip_coef < 1:
            for param in params:
                param.grad.data.mul_(clip_coef)
        
        # Standard SGD update
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()</code></pre>
                    <div class="fragment">
                        <p class="small">Prevents exploding gradients!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What would happen if we used a learning rate of 10.0 instead of 0.03?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Training would converge much faster to the optimal solution",
                                "correct": false,
                                "explanation": "A very large learning rate typically causes instability, not faster convergence."
                            },
                            {
                                "text": "The loss would likely oscillate or diverge instead of decreasing smoothly",
                                "correct": true,
                                "explanation": "Correct! A learning rate of 10.0 is too large, causing the optimizer to overshoot the minimum and potentially diverge."
                            },
                            {
                                "text": "It would have no effect on training",
                                "correct": false,
                                "explanation": "Learning rate directly controls the step size and significantly affects training dynamics."
                            },
                            {
                                "text": "The model would achieve better final accuracy",
                                "correct": false,
                                "explanation": "Too large a learning rate typically results in worse or unstable performance."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Implementation from Scratch Summary -->
            <section>
                <h2 class="truncate-title">From Scratch Implementation: Key Takeaways</h2>
                <ul>
                    <li class="fragment">Understanding fundamentals enables better debugging and customization</li>
                    <li class="fragment">Parameter initialization matters - small random values break symmetry</li>
                    <li class="fragment">Squared loss provides smooth gradients for stable optimization</li>
                    <li class="fragment">SGD updates parameters in the negative gradient direction</li>
                    <li class="fragment">The training loop combines forward pass, loss, backward pass, and update</li>
                    <li class="fragment">Hyperparameters like learning rate critically affect convergence</li>
                    <li class="fragment">Synthetic data validates that our implementation works correctly</li>
                </ul>
            </section>

            <!-- SECTION: Introduction to Modern Frameworks -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html"}]'>
                    <h2 class="truncate-title">Concise Implementation with Modern Frameworks</h2>
                    <p>Moving from scratch implementation to high-level APIs</p>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Modern deep learning frameworks make implementation faster and more reliable</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html"}]'>
                    <h2 class="truncate-title">The Evolution of Deep Learning Frameworks</h2>
                    <div class="row">
                        <div class="col-6">
                            <h4>First Generation</h4>
                            <ul>
                                <li class="fragment">Theano (2010)</li>
                                <li class="fragment">DistBelief (2012)</li>
                                <li class="fragment">Caffe (2014)</li>
                                <li class="fragment">Manual gradient computation</li>
                            </ul>
                        </div>
                        <div class="col-6">
                            <h4>Modern Frameworks</h4>
                            <ul>
                                <li class="fragment">PyTorch</li>
                                <li class="fragment">TensorFlow/Keras</li>
                                <li class="fragment">JAX</li>
                                <li class="fragment">Automatic differentiation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why Use High-Level APIs?</h2>
                    <ul>
                        <li class="fragment">
                            <strong>Less Code:</strong> Framework handles repetitive tasks
                            <pre class="fragment"><code class="python">
# From Scratch: ~50 lines
def linear_regression_scratch(X, w, b):
    # Manual implementation...
    
# With Framework: 3 lines
model = nn.Linear(2, 1)
optimizer = optim.SGD(model.parameters(), lr=0.03)
loss_fn = nn.MSELoss()
                            </code></pre>
                        </li>
                        <li class="fragment"><strong>Optimized Performance:</strong> Heavily optimized implementations</li>
                        <li class="fragment"><strong>Tested Reliability:</strong> Battle-tested by thousands of users</li>
                        <li class="fragment"><strong>Hardware Acceleration:</strong> Automatic GPU/TPU support</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Components</h2>
                    <div class="row">
                        <div class="col-6">
                            <h4>What Frameworks Provide</h4>
                            <ul>
                                <li class="fragment">Predefined layers (<span class="tooltip">Dense/Linear<span class="tooltiptext">Fully connected layer where each input connects to each output</span></span>)</li>
                                <li class="fragment">Loss functions (MSE, CrossEntropy)</li>
                                <li class="fragment">Optimizers (SGD, Adam, RMSprop)</li>
                                <li class="fragment">Data loaders and preprocessing</li>
                                <li class="fragment">Model serialization</li>
                            </ul>
                        </div>
                        <div class="col-6">
                            <h4>What You Focus On</h4>
                            <ul>
                                <li class="fragment">Model architecture</li>
                                <li class="fragment">Hyperparameter tuning</li>
                                <li class="fragment">Data preparation</li>
                                <li class="fragment">Experimentation</li>
                                <li class="fragment">Novel algorithms</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of using modern deep learning frameworks over implementing from scratch?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They make the code run faster on CPU only",
                                "correct": false,
                                "explanation": "Frameworks provide many benefits beyond CPU performance, including GPU support."
                            },
                            {
                                "text": "They provide tested, optimized implementations of common components",
                                "correct": true,
                                "explanation": "Correct! Frameworks offer reliable, optimized implementations of layers, optimizers, and loss functions."
                            },
                            {
                                "text": "They eliminate the need to understand how algorithms work",
                                "correct": false,
                                "explanation": "Understanding fundamentals is still crucial; frameworks just reduce implementation burden."
                            },
                            {
                                "text": "They only work with small datasets",
                                "correct": false,
                                "explanation": "Modern frameworks are designed to handle datasets of all sizes efficiently."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- SECTION: Defining Models with High-Level APIs -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5.1", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html#defining-the-model"}]'>
                    <h2 class="truncate-title">Defining Models with High-Level APIs</h2>
                    <p>Using framework layers instead of manual implementation</p>
                    <pre class="fragment"><code class="python">
import torch
from torch import nn

class LinearRegression(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Linear(2, 1)  # 2 inputs, 1 output
                    </code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>One line replaces our entire manual linear layer implementation!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Fully Connected Layer</h2>
                    <div class="row">
                        <div class="col-6">
                            <pre><code class="python"># Manual implementation
def linear(X, w, b):
    return torch.matmul(X, w) + b

# Initialize parameters
w = torch.normal(0, 0.01, 
                 size=(2, 1))
b = torch.zeros(1)
                            </code></pre>
                        </div>
                        <div class="col-6">
                            <pre><code class="python"># Framework implementation
# All in one line!
layer = nn.Linear(2, 1)

# Or with lazy initialization
layer = nn.LazyLinear(1)
# Input size inferred on first use
                            </code></pre>
                        </div>
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>LazyLinear:</strong> Automatically infers input dimensions - convenient for complex architectures!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "PyTorch Documentation - nn.Linear", "url": "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"}]'>
                    <h2 class="truncate-title">Complete Model Definition</h2>
                    <pre><code class="python">
class LinearRegression(nn.Module):
    def __init__(self, lr=0.03):
        super().__init__()
        # Define the layer
        self.net = nn.LazyLinear(1)
        
        # Initialize parameters (optional - frameworks have good defaults)
        self.net.weight.data.normal_(0, 0.01)
        self.net.bias.data.fill_(0)
        
    def forward(self, X):
        # Just call the layer!
        return self.net(X)
                    </code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>The <code>forward</code> method defines how data flows through the model</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Framework Comparison</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Framework</th>
                                <th>Linear Layer</th>
                                <th>Key Feature</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td>PyTorch</td>
                                <td><code>nn.Linear</code> / <code>nn.LazyLinear</code></td>
                                <td>Dynamic graphs</td>
                            </tr>
                            <tr class="fragment">
                                <td>TensorFlow/Keras</td>
                                <td><code>keras.layers.Dense</code></td>
                                <td>Production ready</td>
                            </tr>
                            <tr class="fragment">
                                <td>JAX/Flax</td>
                                <td><code>nn.Dense</code></td>
                                <td>Functional style</td>
                            </tr>
                            <tr class="fragment">
                                <td>MXNet/Gluon</td>
                                <td><code>nn.Dense</code></td>
                                <td>Hybrid programming</td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="fragment mt-lg">All provide similar high-level abstractions!</p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the advantage of LazyLinear over regular Linear layers?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It runs faster during training",
                                "correct": false,
                                "explanation": "LazyLinear has similar performance; the advantage is in convenience."
                            },
                            {
                                "text": "It automatically infers input dimensions from data",
                                "correct": true,
                                "explanation": "Correct! LazyLinear determines input size automatically on first forward pass."
                            },
                            {
                                "text": "It uses less memory",
                                "correct": false,
                                "explanation": "Memory usage is similar; the benefit is in not specifying input dimensions."
                            },
                            {
                                "text": "It provides better gradients",
                                "correct": false,
                                "explanation": "Both layer types compute identical gradients."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- SECTION: Built-in Loss Functions -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5.2", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html#defining-the-loss-function"}]'>
                    <h2 class="truncate-title">Built-in Loss Functions</h2>
                    <div class="row">
                        <div class="col-6">
                            <pre><code class="python"> # From Scratch
def squared_loss(y_hat, y):
    return ((y_hat - y) ** 2) / 2

def mse_loss(y_hat, y):
    return squared_loss(y_hat, y).mean()
                            </code></pre>
                        </div>
                        <div class="col-6">
                            <pre><code class="python"> # Using Framework
# PyTorch
loss_fn = nn.MSELoss()

# That's it!
loss = loss_fn(y_hat, y)
                            </code></pre>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Frameworks provide optimized, tested implementations</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Common Loss Functions</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Loss Function</th>
                                <th>Use Case</th>
                                <th>PyTorch</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td>Mean Squared Error</td>
                                <td>Regression</td>
                                <td><code>nn.MSELoss()</code></td>
                            </tr>
                            <tr class="fragment">
                                <td>Cross-Entropy</td>
                                <td>Classification</td>
                                <td><code>nn.CrossEntropyLoss()</code></td>
                            </tr>
                            <tr class="fragment">
                                <td>L1 Loss</td>
                                <td>Robust regression</td>
                                <td><code>nn.L1Loss()</code></td>
                            </tr>
                            <tr class="fragment">
                                <td>Huber Loss</td>
                                <td>Outlier-robust regression</td>
                                <td><code>nn.HuberLoss()</code></td>
                            </tr>
                            <tr class="fragment">
                                <td>Binary Cross-Entropy</td>
                                <td>Binary classification</td>
                                <td><code>nn.BCELoss()</code></td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <section data-sources='[{"text": "Robust Statistics - Huber Loss", "url": "https://en.wikipedia.org/wiki/Huber_loss"}]'>
                    <h2 class="truncate-title">Advanced Loss: Huber Loss</h2>
                    <p>Combines MSE and L1 for robustness to outliers</p>
                    <div class="math-block">
                        $$l(y,y') = \begin{cases}
                        |y-y'| - \frac{\sigma}{2} & \text{if } |y-y'| > \sigma \\
                        \frac{1}{2\sigma}(y-y')^2 & \text{otherwise}
                        \end{cases}$$
                    </div>
                    <pre class="fragment"><code class="python">
# Robust to outliers - best of both worlds!
loss_fn = nn.HuberLoss(delta=1.0)

# Acts like MSE for small errors, L1 for large errors
loss = loss_fn(predictions, targets)
                    </code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Frameworks make it trivial to experiment with different loss functions</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "When would you choose Huber loss over MSE?",
                        "type": "single",
                        "options": [
                            {
                                "text": "When you want faster training",
                                "correct": false,
                                "explanation": "Huber loss is slightly more complex to compute than MSE."
                            },
                            {
                                "text": "When your data contains outliers",
                                "correct": true,
                                "explanation": "Correct! Huber loss is robust to outliers by using L1 loss for large errors."
                            },
                            {
                                "text": "When doing classification",
                                "correct": false,
                                "explanation": "Both Huber and MSE are regression losses; use CrossEntropy for classification."
                            },
                            {
                                "text": "When you have very small datasets",
                                "correct": false,
                                "explanation": "Dataset size doesnt determine the choice between Huber and MSE."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- SECTION: Optimization with Framework Optimizers -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5.3", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html#defining-the-optimization-algorithm"}]'>
                    <h2 class="truncate-title">Framework Optimizers</h2>
                    <div class="row">
                        <div class="col-6">
                            <h4>Manual SGD</h4>
                            <pre><code class="python">
def sgd(params, lr, batch_size):
    with torch.no_grad():
                        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
                            </code></pre>
                        </div>
                        <div class="col-6">
                            <h4>Framework SGD</h4>
                            <pre><code class="python">
# All optimization logic encapsulated
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.03
)

# Single line to update
optimizer.step()
optimizer.zero_grad()
                            </code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Rich Optimizer Ecosystem</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Optimizer</th>
                                <th>Key Feature</th>
                                <th>When to Use</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td><code>SGD</code></td>
                                <td>Simple, reliable</td>
                                <td>Good baseline, simple problems</td>
                            </tr>
                            <tr class="fragment">
                                <td><code>Adam</code></td>
                                <td>Adaptive learning rates</td>
                                <td>Default choice, most problems</td>
                            </tr>
                            <tr class="fragment">
                                <td><code>AdamW</code></td>
                                <td>Adam + weight decay</td>
                                <td>Better generalization</td>
                            </tr>
                            <tr class="fragment">
                                <td><code>RMSprop</code></td>
                                <td>Adaptive, no momentum</td>
                                <td>RNNs, noisy gradients</td>
                            </tr>
                            <tr class="fragment">
                                <td><code>LBFGS</code></td>
                                <td>Second-order</td>
                                <td>Small datasets, high precision</td>
                            </tr>
                        </tbody>
                    </table>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Modern optimizers handle many challenges automatically!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Configuring Optimizers</h2>
                    <pre><code class="python">
# Basic configuration
optimizer = torch.optim.SGD(model.parameters(), lr=0.03)

# With momentum and weight decay
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.03,
    momentum=0.9,
    weight_decay=0.0001
)

# Modern adaptive optimizer
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    weight_decay=0.01
)
                    </code></pre>
                    <p class="fragment mt-lg">Frameworks handle the complex math - you focus on hyperparameters!</p>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of Adam over basic SGD?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Adam always converges faster",
                                "correct": false,
                                "explanation": "Adam often converges faster but not always; SGD can be better for some problems."
                            },
                            {
                                "text": "Adam uses less memory",
                                "correct": false,
                                "explanation": "Adam actually uses more memory to store moving averages of gradients."
                            },
                            {
                                "text": "Adam adapts learning rates per parameter automatically",
                                "correct": true,
                                "explanation": "Correct! Adam maintains adaptive learning rates for each parameter based on gradient history."
                            },
                            {
                                "text": "Adam does not require a learning rate",
                                "correct": false,
                                "explanation": "Adam still requires a learning rate, though it is less sensitive to the exact value."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- SECTION: Training Loop and Results -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.5.4", "url": "https://d2l.ai/chapter_linear-regression/linear-regression-concise.html#training"}]'>
                    <h2 class="truncate-title">High-Level Training API</h2>
                    <pre><code class="python">
# Complete training in few lines!
model = LinearRegression(lr=0.03)
data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)
trainer = Trainer(max_epochs=3)

# One line to train!
trainer.fit(model, data)
                    </code></pre>
                    <div class="fragment">
                        <img src="images/training-loss-graph.jpg" alt="Training Loss Graph" style="max-width: 60%; margin: 20px auto; display: block;">
                    </div>
                    <div class="fragment emphasis-box">
                        <p>The framework handles the entire training loop!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">What Happens Inside .fit()</h2>
                    <pre><code class="python">
# What trainer.fit() does internally:
for epoch in range(max_epochs):
    for X, y in data.train_dataloader():
        # Forward pass
        y_hat = model(X)
        
        # Compute loss
        loss = loss_fn(y_hat, y)
        
        # Backward pass
        loss.backward()
        
        # Update parameters
        optimizer.step()
        optimizer.zero_grad()
    
    # Validation, logging, checkpointing...
                    </code></pre>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Abstracts away the boilerplate while remaining customizable</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Comparing Implementations: Code Length</h2>
                    <div class="row">
                        <div class="col-6">
                            <h4>From Scratch</h4>
                            <ul>
                                <li>Data generation: ~20 lines</li>
                                <li>Model definition: ~10 lines</li>
                                <li>Loss function: ~5 lines</li>
                                <li>Optimizer: ~10 lines</li>
                                <li>Training loop: ~25 lines</li>
                                <li class="fragment"><strong>Total: ~70 lines</strong></li>
                            </ul>
                        </div>
                        <div class="col-6">
                            <h4>With Framework</h4>
                            <ul>
                                <li>Import statements: ~3 lines</li>
                                <li>Model class: ~8 lines</li>
                                <li>Loss: 1 line</li>
                                <li>Optimizer: 1 line</li>
                                <li>Training: ~4 lines</li>
                                <li class="fragment"><strong>Total: ~17 lines</strong></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Verifying Our Results</h2>
                    <pre><code class="python">
# Extract learned parameters
w, b = model.get_w_b()

# Compare with true parameters
print(f'True w: {true_w}, Learned w: {w}')
print(f'True b: {true_b}, Learned b: {b}')

# Output:
# True w: [2, -3.4], Learned w: [1.9996, -3.3989]
# True b: 4.2, Learned b: 4.1987
                    </code></pre>
                    <div class="fragment">
                        <p>Error in w: ~0.01, Error in b: ~0.001</p>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Framework implementation achieves same accuracy as from-scratch!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the primary benefit of using high-level training APIs like .fit()?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They make the model train faster",
                                "correct": false,
                                "explanation": "Training speed is similar; the benefit is in development speed and reliability."
                            },
                            {
                                "text": "They eliminate the need to understand the training process",
                                "correct": false,
                                "explanation": "Understanding the training process is still important for debugging and optimization."
                            },
                            {
                                "text": "They reduce boilerplate code and handle common training tasks",
                                "correct": true,
                                "explanation": "Correct! High-level APIs handle logging, validation, checkpointing, and other common tasks automatically."
                            },
                            {
                                "text": "They automatically choose the best hyperparameters",
                                "correct": false,
                                "explanation": "You still need to specify hyperparameters; the API handles the training mechanics."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Summary Section -->
            <section>
                <h2 class="truncate-title">Concise Implementation: Key Takeaways</h2>
                <ul>
                    <li class="fragment">Modern frameworks provide powerful abstractions for common tasks</li>
                    <li class="fragment">Less code means fewer bugs and faster experimentation</li>
                    <li class="fragment">Framework implementations are optimized and thoroughly tested</li>
                    <li class="fragment">Understanding fundamentals is still crucial for:
                        <ul>
                            <li>Debugging when things go wrong</li>
                            <li>Implementing novel architectures</li>
                            <li>Optimizing performance</li>
                        </ul>
                    </li>
                    <li class="fragment">Start with frameworks, implement from scratch when needed</li>
                </ul>
            </section>

            <!-- Generalization Section -->
            <section>
                <h1 class="truncate-title">Generalization in Machine Learning</h1>
                <p>From Memorization to Pattern Discovery</p>
                <p class="fragment">The fundamental challenge: How do we know our model has learned patterns vs memorized data?</p>
            </section>

            <!-- Vertical Group 1: Introduction to Generalization -->
            <section>
                <section>
                    <h2 class="truncate-title">The Student Analogy</h2>
                    <div class="row">
                        <div class="col-half">
                            <h3>Extraordinary Ellie</h3>
                            <ul>
                                <li class="fragment">Perfect memorization skills</li>
                                <li class="fragment">100% on previously seen questions</li>
                                <li class="fragment">Freezes on new questions</li>
                            </ul>
                        </div>
                        <div class="col-half">
                            <h3>Inductive Irene</h3>
                            <ul>
                                <li class="fragment">Poor memorization</li>
                                <li class="fragment">90% accuracy on patterns</li>
                                <li class="fragment">Maintains 90% on new questions</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Goal: Be like Irene - discover patterns that generalize!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 3.6", "url": "https://d2l.ai/chapter_linear-regression/generalization.html"}]'>
                    <h2 class="truncate-title">Why Generalization Matters</h2>
                    <ul>
                        <li class="fragment">We want to predict <strong>tomorrow's</strong> stock prices, not yesterday's</li>
                        <li class="fragment">Diagnose <strong>unseen</strong> patients, not memorize past cases</li>
                        <li class="fragment">Recognize <strong>new</strong> images, not recall training data</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>The leap from particular observations to general statements is fundamental to all science</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Scale Challenge</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Domain</th>
                                <th>Typical Dataset Size</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td>Rare Diseases</td>
                                <td>~100s of examples</td>
                            </tr>
                            <tr class="fragment">
                                <td>Medical Problems</td>
                                <td>~1000s of examples</td>
                            </tr>
                            <tr class="fragment">
                                <td>ImageNet</td>
                                <td>Millions of images</td>
                            </tr>
                            <tr class="fragment">
                                <td>Flickr YFC100M</td>
                                <td>100+ million images</td>
                            </tr>
                        </tbody>
                    </table>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Even 100M images is infinitesimal compared to all possible megapixel images!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main goal when training machine learning models?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To achieve 100% accuracy on training data",
                                "correct": false,
                                "explanation": "Perfect training accuracy often indicates memorization, not pattern learning."
                            },
                            {
                                "text": "To discover patterns that generalize to new data",
                                "correct": true,
                                "explanation": "Correct! We want models that learn general patterns applicable to unseen data."
                            },
                            {
                                "text": "To minimize the number of parameters",
                                "correct": false,
                                "explanation": "While simpler models can help, this is not the main goal."
                            },
                            {
                                "text": "To use the largest possible dataset",
                                "correct": false,
                                "explanation": "More data helps, but the goal is generalization, not just data quantity."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Vertical Group 2: Training vs Generalization Error -->
            <section>
                <section>
                    <h2 class="truncate-title">IID Assumption</h2>
                    <div class="emphasis-box">
                        <p><strong>Independent and Identically Distributed (IID)</strong></p>
                        <p>Training and test data are drawn from the same distribution</p>
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">Training: sampled from distribution P(X,Y)</li>
                        <li class="fragment">Testing: also sampled from P(X,Y)</li>
                        <li class="fragment">Without IID, we cannot expect generalization</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Why should patterns from distribution P help with distribution Q?</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Error</h2>
                    <p>A <strong>statistic</strong> calculated on the training dataset</p>
                    <div class="math-box">
                        $$R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)}))$$
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">Empirical risk on observed data</li>
                        <li class="fragment">We can calculate this exactly</li>
                        <li class="fragment">Tends to be optimistic (biased estimate)</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Generalization Error</h2>
                    <p>An <strong>expectation</strong> over the underlying distribution</p>
                    <div class="math-box">
                        $$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))]$$
                        $$= \int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) \;d\mathbf{x} dy$$
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">True risk on infinite stream of data</li>
                        <li class="fragment">Cannot calculate exactly (unknown p)</li>
                        <li class="fragment">Estimate using held-out test set</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">The Central Question</h2>
                    <div class="emphasis-box">
                        <p class="small">When should we expect training error ‚âà generalization error?</p>
                    </div>
                    <div class="row mt-lg">
                        <div class="col-half">
                            <h3>Factors that Help</h3>
                            <ul>
                                <li class="fragment">Simple models</li>
                                <li class="fragment">Abundant data</li>
                                <li class="fragment">Regularization</li>
                            </ul>
                        </div>
                        <div class="col-half">
                            <h3>Factors that Hurt</h3>
                            <ul>
                                <li class="fragment">Complex models</li>
                                <li class="fragment">Limited data</li>
                                <li class="fragment">Overfitting</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why can we never calculate the true generalization error exactly?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Because we do not have enough computational power",
                                "correct": false,
                                "explanation": "Computational power is not the issue - the problem is more fundamental."
                            },
                            {
                                "text": "Because we do not know the true distribution p(x,y)",
                                "correct": true,
                                "explanation": "Correct! The generalization error requires knowing the exact probability distribution, which we never have in practice."
                            },
                            {
                                "text": "Because the test set is too small",
                                "correct": false,
                                "explanation": "Even with a large test set, we only get an estimate, not the exact value."
                            },
                            {
                                "text": "Because models are too complex",
                                "correct": false,
                                "explanation": "Model complexity affects the gap between training and generalization error, not our ability to calculate it."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Vertical Group 3: Model Complexity -->
            <section>
                <section data-sources='[{"text": "Vapnik et al. 1994", "url": "https://d2l.ai/chapter_references/zreferences.html#id339"}]'>
                    <h2 class="truncate-title">Model Complexity</h2>
                    <p>Not just about parameter count!</p>
                    <ul>
                        <li class="fragment">Number of parameters</li>
                        <li class="fragment">Range of parameter values</li>
                        <li class="fragment">Function class expressiveness</li>
                        <li class="fragment">Kernel methods: infinite parameters, controlled complexity</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>A model that can fit any labels has not necessarily learned anything!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Popper Connection</h2>
                    <blockquote class="fragment">
                        "A theory that can explain any and all observations is not a scientific theory at all!"
                        <footer>- Karl Popper</footer>
                    </blockquote>
                    <ul class="mt-lg">
                        <li class="fragment">Good models rule out possibilities</li>
                        <li class="fragment">If a model can fit random labels ‚Üí suspicious</li>
                        <li class="fragment">Constraints force pattern discovery</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Complexity vs Error</h2>
                    <img src="images/capacity-vs-error.jpg" alt="Model complexity vs error" style="max-width: 80%;">
                    <ul class="mt-sm">
                        <li class="fragment">Training error decreases with complexity</li>
                        <li class="fragment">Generalization error has a sweet spot</li>
                        <li class="fragment">Too simple ‚Üí underfitting</li>
                        <li class="fragment">Too complex ‚Üí overfitting</li>
                    </ul>
                </section>

                <section>
                    <h2 class="truncate-title">Polynomial Curve Fitting Example</h2>
                    <div class="math-box">
                        $$\hat{y}= \sum_{i=0}^d x^i w_i$$
                    </div>
                    <table class="mt-lg">
                        <thead>
                            <tr>
                                <th>Polynomial Degree</th>
                                <th>Behavior</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td>Low (d=1,2)</td>
                                <td>May underfit complex patterns</td>
                            </tr>
                            <tr class="fragment">
                                <td>Moderate (d=3,4)</td>
                                <td>Often good balance</td>
                            </tr>
                            <tr class="fragment">
                                <td>High (d=n-1)</td>
                                <td>Can fit training perfectly, poor generalization</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "According to the philosophy of Karl Popper, what makes a good scientific model?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It can explain every possible observation",
                                "correct": false,
                                "explanation": "This makes it unfalsifiable and thus not scientific according to Popper."
                            },
                            {
                                "text": "It rules out certain possibilities but still fits observed data",
                                "correct": true,
                                "explanation": "Correct! A good model is falsifiable - it makes specific predictions that could be wrong."
                            },
                            {
                                "text": "It has the most parameters",
                                "correct": false,
                                "explanation": "More parameters often lead to overfitting, not better science."
                            },
                            {
                                "text": "It achieves zero training error",
                                "correct": false,
                                "explanation": "Zero training error often indicates overfitting, not good generalization."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Vertical Group 4: Underfitting and Overfitting -->
            <section>
                <section>
                    <h2 class="truncate-title">Underfitting</h2>
                    <div class="emphasis-box">
                        <p>Model is too simple to capture patterns</p>
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">High training error</li>
                        <li class="fragment">High validation error</li>
                        <li class="fragment">Small gap between them</li>
                        <li class="fragment">Model lacks expressiveness</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Solution: Increase model complexity</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Overfitting</h2>
                    <div class="emphasis-box">
                        <p>Model memorizes rather than generalizes</p>
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">Low training error</li>
                        <li class="fragment">High validation error</li>
                        <li class="fragment">Large gap between them</li>
                        <li class="fragment">Model is too flexible</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Solution: Regularization or more data</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Dataset Size Impact</h2>
                    <p>More data almost always helps!</p>
                    <ul>
                        <li class="fragment">Fixed model + more data ‚Üí lower generalization error</li>
                        <li class="fragment">Small datasets ‚Üí high overfitting risk</li>
                        <li class="fragment">Deep learning needs thousands of examples</li>
                        <li class="fragment">Internet scale enables modern success</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Model complexity should scale with data availability</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Deep Learning Paradox</h2>
                    <div class="emphasis-box">
                        <p>Deep networks can fit arbitrary labels yet generalize well!</p>
                    </div>
                    <ul class="mt-lg">
                        <li class="fragment">Classical theory suggests they should overfit</li>
                        <li class="fragment">In practice, they often generalize beautifully</li>
                        <li class="fragment">Low training error ‚â† low generalization error</li>
                        <li class="fragment">But also doesn't guarantee high error!</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>We rely heavily on validation sets for deep learning</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What characterizes underfitting?",
                        "type": "multiple",
                        "options": [
                            {
                                "text": "High training error",
                                "correct": true,
                                "explanation": "Correct! Underfitting means the model cannot even fit the training data well."
                            },
                            {
                                "text": "Small gap between training and validation error",
                                "correct": true,
                                "explanation": "Correct! Both errors are similarly high, indicating consistent poor performance."
                            },
                            {
                                "text": "Model is too complex",
                                "correct": false,
                                "explanation": "Underfitting occurs when the model is too simple, not too complex."
                            },
                            {
                                "text": "Model needs more expressiveness",
                                "correct": true,
                                "explanation": "Correct! The model lacks capacity to capture the underlying patterns."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Vertical Group 5: Model Selection -->
            <section>
                <section>
                    <h2 class="truncate-title">Model Selection Challenge</h2>
                    <p>How do we choose the best model?</p>
                    <ul>
                        <li class="fragment">Cannot use training data (biased)</li>
                        <li class="fragment">Should not use test data (data leakage)</li>
                        <li class="fragment">Need a third option...</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Solution: Validation Set!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Three-Way Data Split</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Dataset</th>
                                <th>Purpose</th>
                                <th>When to Use</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="fragment">
                                <td>Training</td>
                                <td>Fit model parameters</td>
                                <td>During training</td>
                            </tr>
                            <tr class="fragment">
                                <td>Validation</td>
                                <td>Select hyperparameters</td>
                                <td>Model selection</td>
                            </tr>
                            <tr class="fragment">
                                <td>Test</td>
                                <td>Final evaluation</td>
                                <td>Only once at end</td>
                            </tr>
                        </tbody>
                    </table>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Never touch test data until final evaluation!</p>
                    </div>
                </section>

                <section data-sources='[{"text": "Ong et al. 2005", "url": "https://d2l.ai/chapter_references/zreferences.html#id341"}]'>
                    <h2 class="truncate-title">The Reality of Test Sets</h2>
                    <ul>
                        <li class="fragment">Ideally: Use test set only once</li>
                        <li class="fragment">Reality: Test sets get reused</li>
                        <li class="fragment">Risk: Overfitting to test set!</li>
                        <li class="fragment">Benchmarks used for decades</li>
                    </ul>
                    <div class="fragment emphasis-box mt-lg">
                        <p>In this course: "test" often means validation</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">K-Fold Cross-Validation</h2>
                    <p>When data is scarce, use all of it wisely!</p>
                    <ol>
                        <li class="fragment">Split data into K equal folds</li>
                        <li class="fragment">Train on K-1 folds, validate on 1</li>
                        <li class="fragment">Repeat K times with different validation fold</li>
                        <li class="fragment">Average results across all K experiments</li>
                    </ol>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Pros: Uses all data | Cons: K times more expensive</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is K-fold cross-validation particularly useful?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It makes training faster",
                                "correct": false,
                                "explanation": "K-fold is actually K times slower since we train K models."
                            },
                            {
                                "text": "It prevents overfitting completely",
                                "correct": false,
                                "explanation": "No technique completely prevents overfitting, though K-fold helps estimate it better."
                            },
                            {
                                "text": "It allows using all data when data is scarce",
                                "correct": true,
                                "explanation": "Correct! K-fold lets us use all data for both training and validation (just not simultaneously)."
                            },
                            {
                                "text": "It eliminates the need for a test set",
                                "correct": false,
                                "explanation": "We still need a separate test set for final evaluation."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Summary Section -->
            <section>
                <h2 class="truncate-title">Key Takeaways on Generalization</h2>
                <ol>
                    <li class="fragment">Use validation sets (or K-fold CV) for model selection</li>
                    <li class="fragment">Complex models need more data</li>
                    <li class="fragment">Complexity = parameters + value ranges</li>
                    <li class="fragment">More data ‚Üí better generalization</li>
                    <li class="fragment">Everything assumes IID - relaxing this is hard!</li>
                </ol>
                <div class="fragment emphasis-box mt-lg">
                    <p>The art of ML: Finding the sweet spot between underfitting and overfitting</p>
                </div>
            </section>

            <!-- Weight Decay Introduction Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.7", "url": "https://d2l.ai/chapter_linear-regression/weight-decay.html"}]'>
                    <h2 class="truncate-title">Weight Decay: Our First Regularization Technique</h2>
                    <p>Restricting model complexity to combat overfitting</p>
                    <div class="fragment mt-lg">
                        <p><strong>The Challenge:</strong></p>
                        <ul>
                            <li>More data isn't always available</li>
                            <li>Feature reduction can be too blunt</li>
                            <li>Need fine-grained complexity control</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">When More Data Isn't an Option</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Ideal Solution</h4>
                            <ul class="fragment">
                                <li>Collect more training data</li>
                                <li>Better quality annotations</li>
                                <li>More diverse examples</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Reality Constraints</h4>
                            <ul class="fragment">
                                <li>Data collection is expensive</li>
                                <li>Time limitations</li>
                                <li>Privacy concerns</li>
                                <li>Domain-specific scarcity</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>We need techniques that work with existing data!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Monomial Explosion Problem</h2>
                    <p>Why feature reduction isn't always enough</p>
                    <div class="fragment">
                        <p>With <span class="tooltip">monomials<span class="tooltiptext">Products of powers of variables, e.g., x‚ÇÅ¬≤x‚ÇÇ or x‚ÇÉx‚ÇÖ¬≤</span></span> of degree $d$ and $k$ variables:</p>
                        <p class="math-display">$$\text{Number of terms} = \binom{k-1+d}{k-1}$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <table>
                            <thead>
                                <tr>
                                    <th>Variables (k)</th>
                                    <th>Degree 2</th>
                                    <th>Degree 3</th>
                                    <th>Degree 4</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>10</td>
                                    <td>55</td>
                                    <td>220</td>
                                    <td>715</td>
                                </tr>
                                <tr>
                                    <td>20</td>
                                    <td>210</td>
                                    <td>1,540</td>
                                    <td>8,855</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Weight Decay: The Core Intuition</h2>
                    <div class="fragment">
                        <p><strong>Key Insight:</strong> Among all functions, $f = 0$ is the simplest</p>
                    </div>
                    <div class="fragment mt-lg">
                        <p>We measure complexity by distance from zero:</p>
                        <p class="math-display">$$\text{Complexity}(f) \propto ||\mathbf{w}||$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <ul>
                            <li>Smaller weights ‚Üí Simpler function</li>
                            <li>Larger weights ‚Üí More complex function</li>
                            <li>Control complexity by penalizing large weights</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why is weight decay also called a fine-grained tool for adjusting model complexity?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It completely removes features from the model",
                                "correct": false,
                                "explanation": "Weight decay doesn not remove features; it shrinks their weights."
                            },
                            {
                                "text": "It allows continuous adjustment of parameter values rather than discrete feature selection",
                                "correct": true,
                                "explanation": "Correct! Weight decay provides a continuous way to adjust complexity by controlling the magnitude of weights, unlike feature selection which is binary (keep or remove)."
                            },
                            {
                                "text": "It only works with small datasets",
                                "correct": false,
                                "explanation": "Weight decay is useful for datasets of all sizes, especially when overfitting is a concern."
                            },
                            {
                                "text": "It makes training faster",
                                "correct": false,
                                "explanation": "Weight decay is about regularization, not training speed."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Mathematical Foundation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.7.1", "url": "https://d2l.ai/chapter_linear-regression/weight-decay.html#norms-and-weight-decay"}]'>
                    <h2 class="truncate-title">Mathematical Foundation of Weight Decay</h2>
                    <p>From norms to regularized objectives</p>
                    <div class="fragment mt-lg">
                        <p><strong>Building blocks:</strong></p>
                        <ul>
                            <li>$\ell_2$ norm for measuring size</li>
                            <li>Penalty terms in optimization</li>
                            <li>Trade-off hyperparameters</li>
                            <li>Modified gradient updates</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The Regularized Loss Function</h2>
                    <div class="fragment">
                        <p><strong>Original Loss:</strong></p>
                        <p class="math-display">$$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$$</p>
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>Regularized Loss:</strong></p>
                        <p class="math-display">$$L(\mathbf{w}, b) + \frac{\lambda}{2} ||\mathbf{w}||^2$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <ul>
                            <li>$\lambda$ = regularization constant (hyperparameter)</li>
                            <li>$\lambda = 0$: No regularization</li>
                            <li>$\lambda > 0$: Increasing penalty on large weights</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Why $\ell_2$ Norm?</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Computational Benefits</h4>
                            <ul class="fragment">
                                <li>No square root needed</li>
                                <li>Simple derivative: $\nabla_w \frac{1}{2}||\mathbf{w}||^2 = \mathbf{w}$</li>
                                <li>Sum of derivatives = derivative of sum</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Statistical Properties</h4>
                            <ul class="fragment">
                                <li>Penalizes large weights more</li>
                                <li>Distributes weights evenly</li>
                                <li>Robust to measurement errors</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>The factor of $\frac{1}{2}$ cancels nicely with derivatives!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Modified Gradient Descent Update</h2>
                    <div class="fragment">
                        <p><strong>Standard SGD update:</strong></p>
                        <p class="math-display">$$\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)$$</p>
                    </div>
                    <div class="fragment mt-lg">
                        <p><strong>With weight decay:</strong></p>
                        <p class="math-display">$$\mathbf{w} \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)$$</p>
                    </div>
                    <div class="fragment mt-md">
                        <p>The $(1 - \eta\lambda)$ factor <strong>decays</strong> weights at each step!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">$\ell_1$ vs $\ell_2$ Regularization</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>$\ell_2$ (Ridge Regression)</h4>
                            <ul class="fragment">
                                <li>Penalty: $\frac{\lambda}{2}||\mathbf{w}||^2$</li>
                                <li>Shrinks all weights</li>
                                <li>Keeps all features</li>
                                <li>Smooth optimization</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>$\ell_1$ (Lasso Regression)</h4>
                            <ul class="fragment">
                                <li>Penalty: $\lambda||\mathbf{w}||_1$</li>
                                <li>Sparse solutions</li>
                                <li>Feature selection</li>
                                <li>Non-smooth at zero</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Choose based on your goal: all features (Ridge) or feature selection (Lasso)</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the effect of the (1 - Œ∑Œª) term in the weight decay update rule?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It increases the learning rate",
                                "correct": false,
                                "explanation": "The term does not increase learning rate; it affects weight magnitude."
                            },
                            {
                                "text": "It adds noise to the gradients",
                                "correct": false,
                                "explanation": "This term does not add noise; it systematically reduces weight values."
                            },
                            {
                                "text": "It shrinks the weights towards zero at each update step",
                                "correct": true,
                                "explanation": "Correct! Since (1 - Œ∑Œª) < 1 when Œ∑, Œª > 0, multiplying weights by this factor reduces their magnitude at each step, hence weight decay."
                            },
                            {
                                "text": "It normalizes the gradient",
                                "correct": false,
                                "explanation": "This term affects the weights directly, not the gradient normalization."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Implementation from Scratch Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.7.2-3.7.3", "url": "https://d2l.ai/chapter_linear-regression/weight-decay.html#implementation-from-scratch"}]'>
                    <h2 class="truncate-title">Implementing Weight Decay from Scratch</h2>
                    <p>Building intuition through code</p>
                    <div class="fragment mt-lg">
                        <p><strong>Implementation steps:</strong></p>
                        <ol>
                            <li>Create high-dimensional synthetic data</li>
                            <li>Define $\ell_2$ penalty function</li>
                            <li>Modify loss to include penalty</li>
                            <li>Compare with/without regularization</li>
                        </ol>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">High-Dimensional Synthetic Data</h2>
                    <pre><code class="python" data-line-numbers>class Data(d2l.DataModule):
    def __init__(self, num_train, num_val, num_inputs, batch_size):
        self.save_hyperparameters()
        n = num_train + num_val
        self.X = torch.randn(n, num_inputs)
        noise = torch.randn(n, 1) * 0.01
        
        # True model: y = 0.05 + 0.01 * sum(x_i) + noise
        w = torch.ones((num_inputs, 1)) * 0.01
        b = 0.05
        self.y = torch.matmul(self.X, w) + b + noise</code></pre>
                    <div class="fragment mt-md">
                        <p><strong>Overfitting setup:</strong> 200 features, only 20 training samples!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">The $\ell_2$ Penalty Function</h2>
                    <pre><code class="python" data-line-numbers>def l2_penalty(w):
    """Compute L2 norm squared divided by 2"""
    return (w ** 2).sum() / 2</code></pre>
                    <div class="fragment mt-lg">
                        <p><strong>Modified Model Class:</strong></p>
                        <pre><code class="python" data-line-numbers>class WeightDecayScratch(d2l.LinearRegressionScratch):
    def __init__(self, num_inputs, lambd, lr, sigma=0.01):
        super().__init__(num_inputs, lr, sigma)
        self.save_hyperparameters()

    def loss(self, y_hat, y):
        return (super().loss(y_hat, y) + 
                self.lambd * l2_penalty(self.w))</code></pre>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training Without Regularization (Œª=0)</h2>
                    <img src="../shared/images/weight-decay-no-reg.jpg" alt="Training without regularization" style="max-width: 60%; height: auto;">
                    <div class="mt-md">
                        <p><strong>Observations:</strong></p>
                        <ul>
                            <li>Training loss drops significantly</li>
                            <li>Validation loss remains high</li>
                            <li>L2 norm of w: ~0.01</li>
                            <li class="fragment"><strong>Classic overfitting!</strong></li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Training With Weight Decay (Œª=3)</h2>
                    <img src="../shared/images/weight-decay-with-reg.jpg" alt="Training with weight decay" style="max-width: 60%; height: auto;">
                    <div class="mt-md">
                        <p><strong>Observations:</strong></p>
                        <ul>
                            <li>Training loss higher than before</li>
                            <li>Validation loss much lower!</li>
                            <li>L2 norm of w: ~0.0015 (10x smaller)</li>
                            <li class="fragment"><strong>Regularization works!</strong></li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why does training loss increase but validation loss decrease with weight decay?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The model is broken and not learning properly",
                                "correct": false,
                                "explanation": "The model is learning; it is just optimizing a different objective that includes the penalty."
                            },
                            {
                                "text": "Weight decay prevents the model from perfectly fitting training data, improving generalization",
                                "correct": true,
                                "explanation": "Correct! Weight decay adds a penalty that prevents overfitting to training data, leading to higher training loss but better generalization (lower validation loss)."
                            },
                            {
                                "text": "The validation set is easier than the training set",
                                "correct": false,
                                "explanation": "Both sets come from the same distribution; the difference is due to regularization."
                            },
                            {
                                "text": "Weight decay only affects validation performance",
                                "correct": false,
                                "explanation": "Weight decay affects training by adding a penalty term to the loss function."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Framework Implementation Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.7.4", "url": "https://d2l.ai/chapter_linear-regression/weight-decay.html#concise-implementation"}]'>
                    <h2 class="truncate-title">Framework Implementation of Weight Decay</h2>
                    <p>Leveraging built-in optimizer support</p>
                    <div class="fragment mt-lg">
                        <p><strong>Advantages of framework implementation:</strong></p>
                        <ul>
                            <li>Computational efficiency</li>
                            <li>No additional overhead</li>
                            <li>Parameter-specific policies</li>
                            <li>Integration with all optimizers</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">PyTorch Implementation</h2>
                    <pre><code class="python" data-line-numbers>class WeightDecay(d2l.LinearRegression):
    def __init__(self, wd, lr):
        super().__init__(lr)
        self.save_hyperparameters()
        self.wd = wd

    def configure_optimizers(self):
        # Apply weight decay only to weights, not bias
        return torch.optim.SGD([
            {'params': self.net.weight, 'weight_decay': self.wd},
            {'params': self.net.bias}  # No weight decay for bias
        ], lr=self.lr)</code></pre>
                    <div class="fragment emphasis-box mt-md">
                        <p>Different parameters can have different regularization!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Parameter-Specific Policies</h2>
                    <div class="fragment">
                        <p><strong>Common practices:</strong></p>
                        <table>
                            <thead>
                                <tr>
                                    <th>Parameter Type</th>
                                    <th>Weight Decay?</th>
                                    <th>Reason</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Weights</td>
                                    <td>Yes ‚úì</td>
                                    <td>Primary complexity control</td>
                                </tr>
                                <tr>
                                    <td>Biases</td>
                                    <td>Often No</td>
                                    <td>Less impact on complexity</td>
                                </tr>
                                <tr>
                                    <td>BatchNorm</td>
                                    <td>No</td>
                                    <td>Already normalized</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Computational Benefits</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Manual Implementation</h4>
                            <ul class="fragment">
                                <li>Compute penalty separately</li>
                                <li>Add to loss</li>
                                <li>Backprop through penalty</li>
                                <li>Extra computation</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Framework Integration</h4>
                            <ul class="fragment">
                                <li>Direct weight modification</li>
                                <li>No extra backprop</li>
                                <li>Optimized implementations</li>
                                <li>Zero overhead</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Optimizer touches each parameter anyway - weight decay is "free"!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why might we not apply weight decay to bias parameters?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Bias parameters are too small to matter",
                                "correct": false,
                                "explanation": "Bias values can be significant; size is not the reason."
                            },
                            {
                                "text": "Framework optimizers cannot handle bias weight decay",
                                "correct": false,
                                "explanation": "Frameworks can apply weight decay to any parameter; it is a design choice."
                            },
                            {
                                "text": "Biases represent offsets and do not contribute much to model complexity",
                                "correct": true,
                                "explanation": "Correct! Biases are simple offset terms that shift the decision boundary but do not increase model complexity like weights do."
                            },
                            {
                                "text": "Weight decay on biases causes numerical instability",
                                "correct": false,
                                "explanation": "There is no inherent numerical instability; it is about the role biases play."
                            }
                        ]
                    }'></div>
                </section>
            </section>

            <!-- Practical Insights Section with Vertical Slides -->
            <section>
                <section data-sources='[{"text": "Dive into Deep Learning - Section 3.7", "url": "https://d2l.ai/chapter_linear-regression/weight-decay.html#summary"}]'>
                    <h2 class="truncate-title">Practical Insights and Best Practices</h2>
                    <p>Using weight decay effectively</p>
                    <div class="fragment mt-lg">
                        <p><strong>Key considerations:</strong></p>
                        <ul>
                            <li>Choosing Œª values</li>
                            <li>Monitoring regularization effects</li>
                            <li>Combining with other techniques</li>
                            <li>Deep network considerations</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Choosing the Regularization Strength</h2>
                    <div class="fragment">
                        <p><strong>Œª Selection Strategy:</strong></p>
                        <ol>
                            <li>Start with common values: [0.0001, 0.001, 0.01, 0.1]</li>
                            <li>Use validation set for selection</li>
                            <li>Grid or random search</li>
                            <li>Monitor both losses</li>
                        </ol>
                    </div>
                    <div class="fragment mt-lg">
                        <table>
                            <thead>
                                <tr>
                                    <th>Œª Value</th>
                                    <th>Effect</th>
                                    <th>Use Case</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>0.0001</td>
                                    <td>Minimal</td>
                                    <td>Large datasets</td>
                                </tr>
                                <tr>
                                    <td>0.01</td>
                                    <td>Moderate</td>
                                    <td>Standard choice</td>
                                </tr>
                                <tr>
                                    <td>0.1+</td>
                                    <td>Strong</td>
                                    <td>Small data/high dimensions</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">When to Use L1 vs L2</h2>
                    <div class="two-column">
                        <div class="column">
                            <h4>Use L2 When:</h4>
                            <ul class="fragment">
                                <li>All features are relevant</li>
                                <li>Want smooth optimization</li>
                                <li>Dealing with collinear features</li>
                                <li>Default choice for DNNs</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h4>Use L1 When:</h4>
                            <ul class="fragment">
                                <li>Feature selection needed</li>
                                <li>Interpretability important</li>
                                <li>Sparse data</li>
                                <li>Memory constraints</li>
                            </ul>
                        </div>
                    </div>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Can also use Elastic Net: L1 + L2 combined!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Key Takeaways on Weight Decay</h2>
                    <ol>
                        <li class="fragment">Weight decay = L2 regularization for SGD</li>
                        <li class="fragment">Prevents overfitting by penalizing large weights</li>
                        <li class="fragment">Trade-off controlled by Œª hyperparameter</li>
                        <li class="fragment">Framework integration provides efficiency</li>
                        <li class="fragment">Different parameters can have different policies</li>
                        <li class="fragment">Essential tool in the deep learning toolkit</li>
                    </ol>
                    <div class="fragment emphasis-box mt-lg">
                        <p>Start with weight decay as your first defense against overfitting!</p>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "You have a model overfitting on a small dataset with many features. What weight decay strategy should you use?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Use a very small Œª (0.0001) to avoid underfitting",
                                "correct": false,
                                "explanation": "With small data and many features, you need stronger regularization to combat overfitting."
                            },
                            {
                                "text": "Use a larger Œª (0.1 or higher) for stronger regularization",
                                "correct": true,
                                "explanation": "Correct! Small datasets with many features are prone to overfitting, requiring stronger regularization through larger Œª values."
                            },
                            {
                                "text": "Do not use weight decay at all",
                                "correct": false,
                                "explanation": "This scenario is exactly when weight decay is most beneficial."
                            },
                            {
                                "text": "Use negative Œª to encourage larger weights",
                                "correct": false,
                                "explanation": "Negative Œª would make overfitting worse by encouraging complexity."
                            }
                        ]
                    }'></div>
                </section>
            </section>

        </div>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
    
    <!-- Three.js for 3D visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/build/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>
    
    <!-- Shared utilities -->
    <script src="../shared/js/d3-utils.js"></script>
    <script src="../shared/js/animation-lib.js"></script>
    <script src="../shared/js/neural-viz.js"></script>
    <script src="../shared/js/multiple-choice.js"></script>
    <script src="../shared/js/title-handler.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    
    <!-- Custom scripts for this presentation -->
    <script src="js/linear-regression-viz.js"></script>
    <script src="js/gradient-descent-animation.js"></script>
    <script src="js/loss-landscape.js"></script>
    <script src="js/oo-design-viz.js"></script>
    <script src="js/synthetic-data-viz.js"></script>
    <script src="js/synthetic-data-3d.js"></script>
    <script src="js/scratch-implementation-viz.js"></script>
    
    <script>
        // Global animation tracking system
        window.activeAnimations = {
            animationFrames: [],
            intervals: [],
            clearAll: function() {
                this.animationFrames.forEach(id => cancelAnimationFrame(id));
                this.intervals.forEach(id => clearInterval(id));
                this.animationFrames = [];
                this.intervals = [];
            }
        };

        // Track current slide to prevent re-initialization
        let currentSlideId = null;

        // Initialize Reveal.js
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes ]
        });

        // Initialize visualizations when slides are ready
        Reveal.on('ready', () => {
            // Initialize visualizations on the current slide
            initializeCurrentSlideVisualizations();
        });

        // Re-initialize visualizations when changing slides
        Reveal.on('slidechanged', event => {
            initializeCurrentSlideVisualizations();
        });

        function initializeCurrentSlideVisualizations() {
            const currentSlide = Reveal.getCurrentSlide();
            const indices = Reveal.getIndices();
            const slideId = indices.h + '-' + indices.v;
            
            // Don't re-initialize if we're still on the same slide
            if (slideId === currentSlideId) return;
            
            currentSlideId = slideId;
            
            // Clear all running animations first
            if (window.activeAnimations) {
                window.activeAnimations.clearAll();
            }
            
            // Linear regression demo
            if (currentSlide.querySelector('#linear-regression-demo')) {
                if (typeof initLinearRegressionDemo !== 'undefined') {
                    initLinearRegressionDemo();
                }
            }
            
            // Gradient descent animation
            if (currentSlide.querySelector('#gradient-descent-demo')) {
                if (typeof initGradientDescentAnimation !== 'undefined') {
                    initGradientDescentAnimation();
                }
            }
            
            // Loss visualization
            if (currentSlide.querySelector('#loss-visualization')) {
                if (typeof initLossVisualization !== 'undefined') {
                    initLossVisualization();
                }
                if (typeof initLossLandscape !== 'undefined') {
                    initLossLandscape();
                }
            }
            
            // Normal distribution
            if (currentSlide.querySelector('#normal-distribution-viz')) {
                if (typeof initNormalDistribution !== 'undefined') {
                    initNormalDistribution();
                }
            }
            
            // Linear network visualization
            if (currentSlide.querySelector('#linear-network-viz')) {
                if (typeof initLinearNetworkViz !== 'undefined') {
                    initLinearNetworkViz();
                }
            }
            
            // OO Design visualizations
            if (currentSlide.querySelector('#class-hierarchy-diagram')) {
                if (typeof initClassHierarchyDiagram !== 'undefined') {
                    initClassHierarchyDiagram();
                }
            }
            
            if (currentSlide.querySelector('#module-composition')) {
                if (typeof initModuleComposition !== 'undefined') {
                    initModuleComposition();
                }
            }
            
            if (currentSlide.querySelector('#data-pipeline-viz')) {
                if (typeof initDataPipeline !== 'undefined') {
                    initDataPipeline();
                }
            }
            
            if (currentSlide.querySelector('#code-structure-demo')) {
                if (typeof initCodeStructureDemo !== 'undefined') {
                    initCodeStructureDemo();
                }
            }
            
            if (currentSlide.querySelector('#training-loop-animation')) {
                if (typeof initTrainingLoopAnimation !== 'undefined') {
                    initTrainingLoopAnimation();
                }
            }
            
            if (currentSlide.querySelector('#benefits-comparison')) {
                if (typeof initBenefitsComparison !== 'undefined') {
                    initBenefitsComparison();
                }
            }
            
            // Synthetic data visualizations
            if (currentSlide.querySelector('#validation-cycle-viz')) {
                if (typeof initValidationCycle !== 'undefined') {
                    initValidationCycle();
                }
            }
            
            if (currentSlide.querySelector('#data-generation-demo')) {
                if (typeof initDataGenerationDemo !== 'undefined') {
                    initDataGenerationDemo();
                }
            }
            
            if (currentSlide.querySelector('#data-generation-3d')) {
                if (typeof initDataGeneration3D !== 'undefined') {
                    initDataGeneration3D();
                }
            }
            
            if (currentSlide.querySelector('#iteration-demo')) {
                if (typeof initIterationDemo !== 'undefined') {
                    initIterationDemo();
                }
            }
            
            if (currentSlide.querySelector('#pipeline-composition-viz')) {
                if (typeof initPipelineComposition !== 'undefined') {
                    initPipelineComposition();
                }
            }
            
            // From Scratch Implementation visualizations
            if (currentSlide.querySelector('#training-pipeline-overview')) {
                if (typeof initTrainingPipelineOverview !== 'undefined') {
                    initTrainingPipelineOverview();
                }
            }
            
            if (currentSlide.querySelector('#initialization-comparison')) {
                if (typeof initInitializationComparison !== 'undefined') {
                    initInitializationComparison();
                }
            }
            
            if (currentSlide.querySelector('#loss-properties-viz')) {
                if (typeof initLossPropertiesViz !== 'undefined') {
                    initLossPropertiesViz();
                }
            }
            
            if (currentSlide.querySelector('#learning-rate-effects')) {
                if (typeof initLearningRateEffects !== 'undefined') {
                    initLearningRateEffects();
                }
            }
            
            if (currentSlide.querySelector('#training-flow-viz')) {
                if (typeof initTrainingFlowViz !== 'undefined') {
                    initTrainingFlowViz();
                }
            }
            
            if (currentSlide.querySelector('#convergence-tracking-viz')) {
                if (typeof initConvergenceTrackingViz !== 'undefined') {
                    initConvergenceTrackingViz();
                }
            }
            
            if (currentSlide.querySelector('#complete-training-viz')) {
                if (typeof initCompleteTrainingViz !== 'undefined') {
                    initCompleteTrainingViz();
                }
            }
            
            if (currentSlide.querySelector('#lr-experiment-viz')) {
                if (typeof initLRExperimentViz !== 'undefined') {
                    initLRExperimentViz();
                }
            }
        }
    </script>
</body>
</html>