<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natural Language Processing: Pretraining</title>

    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="../shared/css/reveal-theme.css">
    <link rel="stylesheet" href="../shared/css/common.css">
    <link rel="stylesheet" href="../shared/css/quiz.css">

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="../shared/reveal.js/plugin/highlight/monokai.css">

    <style>
        /* Lecture-specific styles */
        .network-container {
            margin: 20px auto;
            position: relative;
        }

        .demo-controls {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            margin-bottom: 15px;
            padding: 10px;
            background: #f9f9f9;
            border-radius: 5px;
            z-index: 100;
            position: relative;
        }

        .demo-controls label {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 0.9em;
        }

        .demo-controls button {
            background: #10099F;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 0.9em;
        }

        .demo-controls button:hover {
            background: #0d0880;
        }

        .node {
            fill: #10099F;
            stroke: #fff;
            stroke-width: 2px;
        }

        .node.context {
            fill: #2DD2C0;
        }

        .node.output {
            fill: #FC8484;
        }

        .link {
            stroke: #999;
            stroke-opacity: 0.6;
            stroke-width: 1.5px;
        }

        .node-label {
            font-size: 12px;
            font-weight: bold;
            fill: #262626;
        }

        .particle {
            fill: #FFA05F;
            r: 4;
        }

        .tree-node {
            fill: #2DD2C0;
            stroke: #10099F;
            stroke-width: 2px;
        }

        .tree-node.leaf {
            fill: #10099F;
        }

        .tree-node.highlighted {
            fill: #FAC55B;
            stroke: #FF6B6B;
            stroke-width: 3px;
        }

        .tree-link {
            stroke: #999;
            stroke-width: 2px;
        }

        .tree-link.highlighted {
            stroke: #FF6B6B;
            stroke-width: 4px;
        }

        .embedding-box {
            fill: #2DD2C0;
            stroke: #10099F;
            stroke-width: 2px;
            opacity: 0.8;
        }

        .bpe-container {
            font-family: 'Source Code Pro', monospace;
            background: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }

        .bpe-step {
            margin: 5px 0;
            padding: 5px;
            background: white;
            border-left: 3px solid #10099F;
        }

        .token {
            display: inline-block;
            margin: 2px;
            padding: 3px 8px;
            background: #2DD2C0;
            color: white;
            border-radius: 3px;
            font-size: 0.9em;
        }

        .merged-token {
            background: #10099F;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.85em;
        }

        .comparison-table th {
            background: #10099F;
            color: white;
            padding: 10px;
            text-align: center;
        }

        .comparison-table td {
            border: 1px solid #EEEEEE;
            padding: 10px;
            text-align: center;
        }

        .equation-highlight {
            background: #FFF9E6;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #FAC55B;
            margin: 15px 0;
        }

        .code-block {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Source Code Pro', monospace;
            font-size: 0.8em;
            margin: 10px 0;
            text-align: left;
        }

        .vector-space {
            position: relative;
            width: 600px;
            height: 400px;
            margin: 0 auto;
        }

        .word-point {
            cursor: pointer;
            transition: all 0.3s;
        }

        .word-point:hover {
            r: 8;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section class="title-slide">
                <img src="../shared/images/uoi_logo_blue.png" alt="University of Iceland Logo" class="ui-logo">
                <h1 class="truncate-title">Natural Language Processing: Pretraining</h1>
                <p>From Word2Vec to BERT</p>
                <p class="mt-lg">
                    <small>Introduction to Deep Learning</small><br>
                    <small>University of Iceland</small>
                </p>
            </section>

            <!-- Word Embedding Basics -->
            <section>

                <!-- Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - Chapter 15: NLP Pretraining", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/index.html"}]'>
                    <h2 class="truncate-title">Why Study Text Representations?</h2>
                    <div style="display: flex; gap: 20px; align-items: center;">
                        <div style="flex: 1;">
                            <img src="images/nlp-map-pretrain.svg" alt="NLP Pretraining Map" style="width: 100%; max-width: 500px;">
                        </div>
                        <div style="flex: 1; text-align: left; font-size: 0.9em;">
                            <p><strong>Natural language</strong> is fundamental to human communication</p>
                            <p style="margin-top: 15px;">Vast amounts of text data exist:</p>
                            <ul style="font-size: 0.9em;">
                                <li>Social media, emails, reviews</li>
                                <li>News articles, research papers</li>
                                <li>Books and documents</li>
                            </ul>
                            <p style="margin-top: 15px;"><strong>Goal:</strong> Enable computers to understand and process text</p>
                        </div>
                    </div>
                </section>

                <!-- One-Hot Vectors -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.1: Word Embedding", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html"}]'>
                    <h2 class="truncate-title">The Problem with One-Hot Vectors</h2>
                    <div style="font-size: 0.7em;">
                        <p>Traditional approach: represent each word as a one-hot vector</p>
                        <div style="margin: 20px 0;">
                            <p style="font-family: monospace; font-size: 0.9em;">
                                "cat" = [1, 0, 0, 0, 0, ..., 0]<br>
                                "dog" = [0, 1, 0, 0, 0, ..., 0]<br>
                                "bank" = [0, 0, 1, 0, 0, ..., 0]
                            </p>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Problem:</strong> Cannot express similarity between words!</p>
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>Cosine similarity</strong> between any two one-hot vectors:</p>
                            <div class="equation-highlight">
                                $$\frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|} = 0$$
                            </div>
                            <p style="font-size: 0.9em; color: #666;">All words appear equally distant from each other</p>
                        </div>
                    </div>
                </section>

                <!-- Word Embedding Concept -->
                <section>
                    <h2 class="truncate-title">Word Embedding: Dense Vector Representations</h2>
                    <p><span class="tooltip">Word embedding<span class="tooltiptext">The technique of mapping words to continuous vector spaces where semantic relationships are preserved</span></span> maps words to fixed-length vectors</p>
                    <div style="display: flex; gap: 30px; margin-top: 20px; align-items: center;">
                        <div style="flex: 1;">
                            <h4>One-Hot (sparse)</h4>
                            <p style="font-size: 0.8em;">Dimension: 10,000+</p>
                            <div style="background: #f5f5f5; padding: 10px; border-radius: 5px;">
                                <code>[0,0,0,1,0,0,...,0]</code>
                            </div>
                        </div>
                        <div style="font-size: 2em; color: #10099F;">→</div>
                        <div style="flex: 1;">
                            <h4>Embedding (dense)</h4>
                            <p style="font-size: 0.8em;">Dimension: 100-300</p>
                            <div style="background: #f5f5f5; padding: 10px; border-radius: 5px;">
                                <code>[0.2,−0.5,0.8,...]</code>
                            </div>
                        </div>
                    </div>
                    <div class="emphasis-box" style="margin-top: 25px;">
                        <p><strong>Key Insight:</strong> Similar words have similar vectors!</p>
                        <p style="font-size: 0.9em; margin-top: 10px;">
                            <code>similarity("cat", "dog") > similarity("cat", "democracy")</code>
                        </p>
                    </div>
                </section>

                <!-- Self-Supervised Learning -->
                <section>
                    <h2 class="truncate-title">Self-Supervised Learning</h2>
                    <p><span class="tooltip">word2vec<span class="tooltiptext">A family of models (skip-gram and CBOW) that learn word embeddings from large text corpora using self-supervised learning</span></span> learns word vectors without labeled data</p>
                    <div style="margin-top: 20px;">
                        <div style="background: #E8F4F8; padding: 15px; border-radius: 5px; border-left: 4px solid #2DD2C0;">
                            <p><strong>Self-Supervised:</strong> Generate labels from the data itself</p>
                        </div>
                        <div style="margin-top: 20px; text-align: left;">
                            <p><strong>Two models in word2vec:</strong></p>
                            <ol style="margin-top: 10px;">
                                <li><strong>Skip-gram:</strong> Predict context words given a center word</li>
                                <li><strong>CBOW:</strong> Predict center word given context words</li>
                            </ol>
                        </div>
                    </div>
                    <div style="margin-top: 20px; font-size: 0.9em;">
                        <p><strong>Example text:</strong> "the man loves his son"</p>
                        <p style="margin-top: 10px;">
                            Center word: <span style="color: #10099F; font-weight: bold;">"loves"</span><br>
                            Context (window=2): "the", "man", "his", "son"
                        </p>
                    </div>
                </section>

                <!-- MCQ: Word Embeddings -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of word embeddings (like word2vec) over one-hot encodings?",
                        "type": "single",
                        "options": [
                            {
                                "text": "Word embeddings require less memory than one-hot vectors",
                                "correct": false,
                                "explanation": "While word embeddings are often more compact, the main advantage is not about memory but about capturing semantic relationships."
                            },
                            {
                                "text": "Word embeddings capture semantic similarity between words through vector similarity",
                                "correct": true,
                                "explanation": "Correct! Word embeddings map similar words to nearby points in vector space, allowing us to measure semantic similarity using cosine similarity or distance. One-hot vectors treat all words as equally distant."
                            },
                            {
                                "text": "Word embeddings are easier to compute than one-hot vectors",
                                "correct": false,
                                "explanation": "Actually, word embeddings require training on large corpora and are more computationally expensive to create than one-hot vectors, which are trivial to compute."
                            },
                            {
                                "text": "Word embeddings work better with small datasets",
                                "correct": false,
                                "explanation": "Word embeddings actually require large datasets to train effectively. With small datasets, simple representations like one-hot vectors might be sufficient."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- word2vec Models -->
            <section>

                <!-- Intuition: word2vec -->
                <section>
                    <h2 class="truncate-title">word2vec: The Core Intuition</h2>
                    <div style="text-align: left; font-size: 0.55em;">
                        <div class="emphasis-box" style="margin-bottom: 20px;">
                            <p style="font-size: 1.1em;"><strong>Key Idea:</strong> "You shall know a word by the company it keeps" — J.R. Firth (1957)</p>
                        </div>

                        <div style="display: flex; gap: 20px; align-items: flex-start;">
                            <div style="flex: 1;">
                                <p><strong>The Problem:</strong></p>
                                <ul style="font-size: 0.9em;">
                                    <li>One-hot vectors don't capture meaning</li>
                                    <li>All words appear equally distant</li>
                                    <li>No notion of similarity</li>
                                </ul>
                            </div>
                            <div style="flex: 1;">
                                <p><strong>The Solution:</strong></p>
                                <ul style="font-size: 0.9em;">
                                    <li><span style="color: #10099F; font-weight: bold;">Learn</span> dense vectors from context</li>
                                    <li>Similar words → similar contexts</li>
                                    <li>Similar contexts → similar vectors</li>
                                </ul>
                            </div>
                        </div>

                        <div style="margin-top: 25px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p><strong>Example:</strong></p>
                            <p style="margin-top: 10px; font-family: monospace; font-size: 0.9em;">
                                "The <span style="color: #10099F; font-weight: bold;">cat</span> sat on the mat"<br>
                                "The <span style="color: #10099F; font-weight: bold;">dog</span> sat on the mat"
                            </p>
                            <p style="margin-top: 10px; font-size: 0.95em;">
                                → "cat" and "dog" appear in similar contexts<br>
                                → Their learned vectors will be similar!
                            </p>
                        </div>

                        <div style="margin-top: 20px; font-size: 0.95em;">
                            <p><strong>How word2vec works:</strong> Train a neural network to predict context words from a center word (or vice versa). The learned word vectors are a <em>byproduct</em> of this training!</p>
                        </div>
                    </div>
                </section>

                <!-- Skip-Gram Model -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.1.3: Skip-Gram Model", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-skip-gram-model"}]'>
                    <h2 class="truncate-title">Skip-Gram Model</h2>
                    <div style="display: flex; gap: 20px; align-items: flex-start;">
                        <div style="flex: 1;">
                            <img src="images/skip-gram.svg" alt="Skip-Gram Model" style="width: 100%; max-width: 400px;">
                        </div>
                        <div style="flex: 1; text-align: left; font-size: 0.7em;">
                            <p><strong>Idea:</strong> Use center word to predict context</p>
                            <div style="margin-top: 15px;">
                                <p><strong>Conditional probability:</strong></p>
                                <div class="equation-highlight">
                                    $$P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)}$$
                                </div>
                            </div>
                            <div style="margin-top: 15px;">
                                <ul>
                                    <li>$\mathbf{v}_c$: center word vector (when word appears as center)</li>
                                    <li>$\mathbf{u}_o$: context word vector (when word appears in context)</li>
                                    <li>Each word has <strong><span class="tooltip">two vectors<span class="tooltiptext">Why two vectors? Words play different roles: as the target word being predicted (center) vs. as context helping to predict another word. Having separate vectors for each role gives the model more flexibility and leads to better embeddings. After training, typically only one vector (or their average) is used.</span></span></strong>!</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Skip-Gram Training -->
                <section>
                    <h2 class="truncate-title">Skip-Gram: Training Objective</h2>
                    <div style="font-size: 0.7em;">
                        <p>Given text sequence of length $T$ with context window $m$:</p>
                        <div class="equation-highlight">
                            <p><strong>Likelihood function:</strong></p>
                            $$\prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P(w^{(t+j)} \mid w^{(t)})$$
                        </div>
                        <div class="equation-highlight" style="margin-top: 15px;">
                            <p><strong>Loss function (negative log-likelihood):</strong></p>
                            $$-\sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P(w^{(t+j)} \mid w^{(t)})$$
                        </div>
                        <div style="margin-top: 20px; font-size: 0.9em;">
                            <p><strong>Training:</strong> Maximize likelihood = Minimize loss using <span class="tooltip">SGD<span class="tooltiptext">Stochastic Gradient Descent: An optimization algorithm that updates parameters using gradients computed on small random samples</span></span></p>
                        </div>
                    </div>
                </section>

                <!-- Skip-Gram Gradient -->
                <section>
                    <h2 class="truncate-title">Skip-Gram: Gradient Calculation</h2>
                    <div style="font-size: 0.7em;">
                        <p>Gradient with respect to center word vector $\mathbf{v}_c$:</p>
                        <div class="equation-highlight">
                            $$\begin{aligned}
                            \frac{\partial \log P(w_o \mid w_c)}{\partial \mathbf{v}_c} &= \mathbf{u}_o - \sum_{j \in \mathcal{V}} P(w_j \mid w_c) \mathbf{u}_j
                            \end{aligned}$$
                        </div>
                        <div style="margin-top: 20px;">
                            <div style="background: #FFF3CD; padding: 15px; border-radius: 5px; border-left: 4px solid #FAC55B;">
                                <p><strong>⚠️ Computational Challenge:</strong></p>
                                <p style="font-size: 0.9em; margin-top: 8px;">
                                    Summation over <strong>entire vocabulary</strong> ($|\mathcal{V}|$) at each step!
                                </p>
                                <p style="font-size: 0.9em; margin-top: 8px;">
                                    For large vocabularies (100K-1M words), this is extremely expensive!
                                </p>
                            </div>
                        </div>
                        <p style="margin-top: 15px; font-size: 0.9em;">
                            <strong>Solution:</strong> Approximate training methods →
                        </p>
                    </div>
                </section>

                <!-- CBOW Model -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.1.4: CBOW Model", "url": "https://d2l.ai/chapter_natural-language_processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model"}]'>
                    <h2 class="truncate-title">CBOW: Continuous Bag of Words</h2>
                    <div style="display: flex; gap: 20px; align-items: flex-start; font-size: 0.7em;">
                        <div style="flex: 1; text-align: left; font-size: 0.9em;">
                            <div style="display: flex; gap: 20px; align-items: center;">
                                <div style="flex: 1;">
                                    <img src="images/cbow.svg" alt="CBOW Model" style="width: 100%; max-width: 150px;">
                                </div>
                                <div style="flex: 1;">
                                    <p><strong>Idea:</strong> Use context to predict center word</p>
                                </div>
                            </div>
                            <div style="display: flex; gap: 20px;">
                                <div style="flex: 1;">
                                    <p><strong>Average context vectors:</strong></p>
                                    <div class="equation-highlight">
                                        $$\bar{\mathbf{v}}_o = \frac{1}{2m}\sum_{i=1}^{2m} \mathbf{v}_{o_i}$$
                                    </div>
                                </div>
                                <div style="flex: 1;">
                                    <p><strong>Probability:</strong></p>
                                    <div class="equation-highlight">
                                        $$P(w_c \mid \mathcal{W}_o) = \frac{\exp(\mathbf{u}_c^\top \bar{\mathbf{v}}_o)}{\sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \bar{\mathbf{v}}_o)}$$
                                    </div>
                                </div>
                            </div>
                            <div style="margin-top: 5px; background: #f5f5f5; padding: 12px; border-radius: 5px;">
                                <p style="font-size: 0.9em;"><strong>Variables explained:</strong></p>
                                <ul style="font-size: 0.85em; margin-top: 3px;">
                                    <li>$\mathcal{W}_o$ = set of all context words around the center</li>
                                    <li>$\bar{\mathbf{v}}_o$ = average vector of all context words</li>
                                    <li>$\mathbf{u}_c$ = center word vector (the word we're predicting)</li>
                                    <li>Numerator: how well the averaged context matches the center word</li>
                                    <li>Denominator: normalization over all possible center words</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Skip-Gram vs CBOW Comparison -->
                <section>
                    <h2 class="truncate-title">Skip-Gram vs CBOW: Comparison</h2>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Skip-Gram</th>
                                <th>CBOW</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Input</strong></td>
                                <td>Center word</td>
                                <td>Context words</td>
                            </tr>
                            <tr>
                                <td><strong>Output</strong></td>
                                <td>Context words</td>
                                <td>Center word</td>
                            </tr>
                            <tr>
                                <td><strong>Architecture</strong></td>
                                <td>One-to-many</td>
                                <td>Many-to-one</td>
                            </tr>
                            <tr>
                                <td><strong>Training speed</strong></td>
                                <td>Slower</td>
                                <td>Faster</td>
                            </tr>
                            <tr>
                                <td><strong>Rare words</strong></td>
                                <td>Better</td>
                                <td>Worse</td>
                            </tr>
                            <tr>
                                <td><strong>Small datasets</strong></td>
                                <td>Better</td>
                                <td>Worse</td>
                            </tr>
                        </tbody>
                    </table>
                    <div class="emphasis-box" style="margin-top: 20px;">
                        <p><strong>Output vectors:</strong> Skip-gram uses <strong>center</strong> vectors; CBOW uses <strong>context</strong> vectors</p>
                    </div>
                </section>

                <!-- MCQ: Skip-Gram vs CBOW -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Given the sentence \"The cat sat on the mat\", with \"sat\" as the center word and window size 2, what does skip-gram predict?",
                        "type": "single",
                        "options": [
                            {
                                "text": "The word \"sat\" given context words \"The\", \"cat\", \"on\", \"the\"",
                                "correct": false,
                                "explanation": "This describes the CBOW model, which predicts the center word from context."
                            },
                            {
                                "text": "The context words \"The\", \"cat\", \"on\", \"the\" given center word \"sat\"",
                                "correct": true,
                                "explanation": "Correct! Skip-gram uses the center word to predict surrounding context words."
                            },
                            {
                                "text": "Only the immediate neighbors \"cat\" and \"on\" of the center word",
                                "correct": false,
                                "explanation": "With window size 2, skip-gram predicts all words within 2 positions, not just immediate neighbors."
                            },
                            {
                                "text": "The next word in the sequence after \"sat\"",
                                "correct": false,
                                "explanation": "Skip-gram is not a language model; it predicts all context words within the window, not just the next word."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Approximate Training -->
            <section>

                <!-- Intuition: Approximate Training -->
                <section>
                    <h2 class="truncate-title">Why Approximate Training?</h2>
                    <div style="text-align: left; font-size: 0.5em;">
                        <div style="background: #FFE6E6; padding: 15px; border-radius: 5px; margin-bottom: 5px;">
                            <p><strong>The Computational Problem:</strong> Recall the Skip-Gram softmax:</p>
                            <div class="equation-highlight" style="margin-top: 10px;">
                                $$P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)}$$
                            </div>
                            <p style="margin-top: 10px; color: #FC8484; font-weight: bold;">
                                ⚠️ Problem: The denominator sums over ALL words in vocabulary!
                            </p>
                        </div>

                        <div style="display: flex; gap: 20px; margin-top: 5px;">
                            <div style="flex: 1; background: #f5f5f5; padding: 15px; border-radius: 5px;">
                                <p style="margin: 5px 0;"><strong style="color: #FC8484;">Expensive:</strong></p>
                                <ul style="font-size: 0.9em; margin: 5px 0;">
                                    <li>Vocabulary size: 10,000-100,000+ words</li>
                                    <li>Every gradient update requires summing over ALL words</li>
                                    <li>Training becomes prohibitively slow</li>
                                </ul>
                            </div>
                            <div style="flex: 1; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <p style="margin: 5px 0;"><strong style="color: #10099F;">Solution:</strong></p>
                                <ul style="font-size: 0.9em; margin: 5px 0;">
                                    <li><strong>Don't</strong> compute exact probability</li>
                                    <li>Use <strong>approximation</strong> techniques</li>
                                    <li>Train almost as well, but much faster!</li>
                                </ul>
                            </div>
                        </div>

                        <div class="emphasis-box" style="margin-top: 5px;">
                            <p><strong>Two Main Approaches:</strong></p>
                            <ol style="font-size: 0.95em; margin-top: 10px; margin-left: 20px;">
                                <li><strong style="color: #10099F;">Negative Sampling:</strong> Instead of normalizing over all words, sample a few "negative" examples</li>
                                <li><strong style="color: #2DD2C0;">Hierarchical Softmax:</strong> Organize vocabulary as a tree to reduce computation from $O(|\mathcal{V}|)$ to $O(\log |\mathcal{V}|)$</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <!-- Negative Sampling Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.2: Approximate Training", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html"}]'>
                    <h2 class="truncate-title">Approximate Training: Negative Sampling</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>Problem:</strong> Computing softmax over entire vocabulary is expensive!</p>
                        <div style="margin-top: 20px;">
                            <p><strong>Solution:</strong> Sample negative examples instead</p>
                        </div>
                        <div style="margin-top: 20px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p><strong>Negative Sampling:</strong></p>
                            <ul style="margin-top: 10px; font-size: 0.9em;">
                                <li>For each positive pair $(w_c, w_o)$, sample $K$ <span class="tooltip">noise words<span class="tooltiptext">Words randomly sampled from the vocabulary that don't appear in the actual context window</span></span></li>
                                <li>Model event $D=1$: word comes from context</li>
                                <li>Model event $D=0$: word doesn't come from context</li>
                            </ul>
                        </div>
                    </div>
                    <div style="margin-top: 20px; font-size: 0.7em;">
                        <p><strong>Probability using sigmoid function:</strong></p>
                        <div class="equation-highlight">
                            $$P(D=1 \mid w_c, w_o) = \sigma(\mathbf{u}_o^\top \mathbf{v}_c) = \frac{1}{1+\exp(-\mathbf{u}_o^\top \mathbf{v}_c)}$$
                        </div>
                    </div>
                </section>

                <!-- Negative Sampling Loss -->
                <section>
                    <h2 class="truncate-title">Negative Sampling: Loss Function</h2>
                    <div style="font-size: 0.5em;">
                        <p>Approximate conditional probability with positive and negative samples:</p>
                        <div class="equation-highlight">
                            $$P(w^{(t+j)} \mid w^{(t)}) = P(D=1 \mid w^{(t)}, w^{(t+j)}) \prod_{k=1, w_k \sim P(w)}^K P(D=0 \mid w^{(t)}, w_k)$$
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>Logarithmic loss:</strong></p>
                            <div class="equation-highlight">
                                $$\begin{aligned}
                                -\log P(w^{(t+j)} \mid w^{(t)}) &= -\log \sigma(\mathbf{u}_{i_{t+j}}^\top \mathbf{v}_{i_t}) \\
                                &\quad - \sum_{k=1, w_k \sim P(w)}^K \log \sigma(-\mathbf{u}_{h_k}^\top \mathbf{v}_{i_t})
                                \end{aligned}$$
                            </div>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Key Advantage:</strong> Computational cost is $O(K)$ instead of $O(|\mathcal{V}|)$</p>
                            <p style="font-size: 0.9em; margin-top: 8px;">Typical choice: $K = 5$ to $20$</p>
                        </div>
                    </div>
                </section>

                <!-- Hierarchical Softmax -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.2.2: Hierarchical Softmax", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#hierarchical-softmax"}]'>
                    <h2 class="truncate-title">Hierarchical Softmax</h2>
                    <div style="display: flex; gap: 20px; align-items: center;">
                        <div style="flex: 1;">
                            <img src="images/hi-softmax.svg" alt="Hierarchical Softmax" style="width: 100%; max-width: 400px;">
                        </div>
                        <div style="flex: 1; text-align: left; font-size: 0.9em;">
                            <p><strong>Idea:</strong> Organize vocabulary as binary tree</p>
                            <ul style="margin-top: 15px;">
                                <li>Each <strong>leaf</strong> = one word</li>
                                <li>Each <strong>path</strong> = sequence of binary decisions</li>
                                <li>Path length $L(w) = O(\log |\mathcal{V}|)$</li>
                            </ul>
                            <div style="margin-top: 15px;">
                                <p><strong>Probability as path product:</strong></p>
                                <div class="equation-highlight">
                                    $$P(w_o \mid w_c) = \prod_{j=1}^{L(w_o)-1} \sigma([n(w_o,j) = 1] \cdot \mathbf{u}_{n(w_o,j)}^\top \mathbf{v}_c)$$
                                </div>
                                <p style="font-size: 0.85em; margin-top: 8px;">
                                    where $[n(w_o,j) = 1]$ is +1 if we go left at node $j$, -1 if we go right
                                </p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Interactive Hierarchical Softmax -->
                <section>
                    <h2 class="truncate-title">Hierarchical Softmax: Interactive Tree</h2>
                    <div class="demo-controls">
                        <label>
                            Target Word:
                            <select id="tree-word" style="padding: 5px; border-radius: 3px; border: 1px solid #ccc;">
                                <option value="w3">w₃</option>
                                <option value="w1">w₁</option>
                                <option value="w2">w₂</option>
                                <option value="w4">w₄</option>
                            </select>
                        </label>
                        <button id="tree-highlight">Show Path</button>
                        <button id="tree-reset">Reset</button>
                    </div>
                    <div id="tree-viz" class="network-container" style="width: 700px; height: 450px; margin: 0 auto;"></div>
                    <div id="path-calculation" style="margin-top: 15px; padding: 15px; background: #f9f9f9; border-radius: 5px; font-size: 0.9em; min-height: 80px;">
                        <strong>Click "Show Path" to see the probability calculation</strong>
                    </div>
                </section>

                <!-- MCQ: Approximate Training -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "Why do we need approximate training methods like negative sampling for word2vec?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To improve the quality of word embeddings",
                                "correct": false,
                                "explanation": "Approximate methods are primarily for computational efficiency, not necessarily better quality."
                            },
                            {
                                "text": "To reduce computational cost from O(|V|) to O(K) or O(log|V|)",
                                "correct": true,
                                "explanation": "Correct! The full softmax requires summing over the entire vocabulary at each step, which is prohibitively expensive for large vocabularies."
                            },
                            {
                                "text": "To handle out-of-vocabulary words",
                                "correct": false,
                                "explanation": "Approximate training does not specifically address out-of-vocabulary words; subword methods like fastText handle that."
                            },
                            {
                                "text": "To make the model work with smaller datasets",
                                "correct": false,
                                "explanation": "Approximate training is about computational efficiency, not dataset size requirements."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- GloVe -->
            <section>

                <!-- Intuition: GloVe -->
                <section>
                    <h2 class="truncate-title">GloVe: The Core Intuition</h2>
                    <div style="text-align: left; font-size: 0.75em;">
                        <div class="emphasis-box" style="margin-bottom: 20px;">
                            <p style="font-size: 1.05em;"><strong>Central Question:</strong> Can we learn word vectors directly from global co-occurrence statistics?</p>
                        </div>

                        <div style="display: flex; gap: 20px; align-items: flex-start; margin-top: 20px;">
                            <div style="flex: 1; background: #FFF3CD; padding: 15px; border-radius: 5px;">
                                <p><strong style="color: #FAC55B;">word2vec approach:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>Uses <strong>local</strong> context windows</li>
                                    <li>Trains on individual word pairs</li>
                                    <li>Processes text <em>sequentially</em></li>
                                    <li>May see same pair many times</li>
                                </ul>
                            </div>
                            <div style="flex: 1; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <p><strong style="color: #10099F;">GloVe approach:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>Uses <strong>global</strong> statistics</li>
                                    <li>Pre-computes all co-occurrences</li>
                                    <li>Trains on co-occurrence <em>counts</em></li>
                                    <li>More efficient use of data!</li>
                                </ul>
                            </div>
                        </div>

                        <div style="margin-top: 25px; background: #f9f9f9; padding: 15px; border-radius: 5px;">
                            <p><strong>Why this matters:</strong> Instead of processing the same word pairs repeatedly, GloVe counts them once and learns from the aggregate statistics!</p>
                        </div>
                    </div>
                </section>

                <!-- GloVe Mathematical Formulation -->
                <section>
                    <h2 class="truncate-title">GloVe: Mathematical Formulation</h2>
                    <div style="text-align: left; font-size: 0.55em;">
                        <div style="margin-top: 25px; background: #f9f9f9; padding: 15px; border-radius: 5px;">
                            <p><strong>The Key Insight:</strong></p>
                            <p style="margin-top: 10px; font-size: 0.95em;">
                                Instead of predicting context words, <strong>directly model the logarithm of co-occurrence counts</strong>:
                            </p>
                            <div class="equation-highlight" style="margin-top: 10px;">
                                $$\mathbf{v}_i^\top \mathbf{u}_j \approx \log x_{ij}$$
                            </div>
                            <p style="margin-top: 10px; font-size: 0.9em;">
                                where $x_{ij}$ = number of times words $i$ and $j$ appear together
                            </p>
                        </div>

                        <div style="margin-top: 20px;">
                            <p><strong>Why this works:</strong> Words that co-occur frequently should have similar vectors. By fitting to log counts, we capture these relationships directly!</p>
                        </div>

                        <div style="margin-top: 15px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p style="margin-bottom: 8px;"><strong>Advantages:</strong></p>
                            <ul style="font-size: 0.9em; margin-top: 5px;">
                                <li>Uses all available statistical information</li>
                                <li>More stable training (no stochastic sampling)</li>
                                <li>Can parallelize easily</li>
                                <li>Often faster convergence</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- GloVe Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.5: GloVe", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html"}]'>
                    <h2 class="truncate-title">GloVe: Global Vectors for Word Representation</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>Key Insight:</strong> Word co-occurrences contain rich semantic information</p>
                        <div style="margin-top: 20px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p><strong>Examples:</strong></p>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>"solid" co-occurs more with "ice" than "steam"</li>
                                <li>"gas" co-occurs more with "steam" than "ice"</li>
                                <li>These patterns reveal semantic relationships!</li>
                            </ul>
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>GloVe vs word2vec:</strong></p>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li><strong>word2vec:</strong> Local context windows, online learning</li>
                                <li><strong>GloVe:</strong> Global co-occurrence statistics, batch learning</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- GloVe Co-occurrence Table -->
                <section>
                    <h2 class="truncate-title">GloVe: Co-occurrence Probability Ratios</h2>
                    <div style="font-size: 0.7em;">
                        <p>Ratios of co-occurrence probabilities reveal relationships:</p>
                        <table class="comparison-table" style="margin-top: 20px;">
                            <thead>
                                <tr>
                                    <th>$w_k$</th>
                                    <th>solid</th>
                                    <th>gas</th>
                                    <th>water</th>
                                    <th>fashion</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>$P(w_k \mid \text{ice})$</td>
                                    <td>0.00019</td>
                                    <td>0.000066</td>
                                    <td>0.003</td>
                                    <td>0.000017</td>
                                </tr>
                                <tr>
                                    <td>$P(w_k \mid \text{steam})$</td>
                                    <td>0.000022</td>
                                    <td>0.00078</td>
                                    <td>0.0022</td>
                                    <td>0.000018</td>
                                </tr>
                                <tr style="background: #FFF9E6;">
                                    <td><strong>$P(w_k \mid \text{ice}) / P(w_k \mid \text{steam})$</strong></td>
                                    <td><strong>8.9</strong></td>
                                    <td><strong>0.085</strong></td>
                                    <td><strong>1.36</strong></td>
                                    <td><strong>0.96</strong></td>
                                </tr>
                            </tbody>
                        </table>
                        <div style="margin-top: 20px; text-align: left; font-size: 0.9em;">
                            <ul>
                                <li>Large ratio (8.9): "solid" related to "ice", not "steam"</li>
                                <li>Small ratio (0.085): "gas" related to "steam", not "ice"</li>
                                <li>Ratio ≈ 1: "water" related to both, "fashion" to neither</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- GloVe Loss Function -->
                <!-- GloVe Loss Function -->
                <section>
                    <h2 class="truncate-title">GloVe: Loss Function</h2>
                    <div style="font-size: 0.55em;">
                        <p>Fit the model to logarithm of co-occurrence counts:</p>
                        <div class="equation-highlight">
                            $$\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j \approx \log x_{ij}$$
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>Full loss function with weighting:</strong></p>
                            <div class="equation-highlight">
                                $$\sum_{i \in \mathcal{V}} \sum_{j \in \mathcal{V}} h(x_{ij}) \left(\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j - \log x_{ij}\right)^2$$
                            </div>
                        </div>
                        <div style="margin-top: 20px; text-align: left;">
                            <p><strong>Components:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li>$x_{ij}$: co-occurrence count of words $i$ and $j$</li>
                                <li>$b_i, c_j$: bias terms for each word</li>
                                <li>$h(x)$: <span class="tooltip">weighting function<span class="tooltiptext">The weighting function h(x) balances the influence of word pairs: it reduces the weight of very rare pairs (which are noisy) and very frequent pairs (which dominate the loss). Typically h(x) = (x/x_max)^α if x < x_max, else 1. This ensures the model focuses on moderately frequent, informative co-occurrences.</span></span> (balances rare/frequent pairs)</li>
                            </ul>
                        </div>
                        <div class="emphasis-box" style="margin-top: 15px;">
                            <p><strong>Key property:</strong> $\mathbf{v}_i$ and $\mathbf{u}_i$ are <strong>symmetric</strong> in GloVe!</p>
                            <p style="font-size: 0.9em; margin-top: 8px;">Final embedding: $\mathbf{v}_i + \mathbf{u}_i$</p>
                        </div>
                    </div>
                </section>

                <!-- GloVe Weighting Function -->
                <section>
                    <h2 class="truncate-title">GloVe: Why Weighting Matters</h2>
                    <div style="font-size: 0.6em;">
                        <div style="background: #f9f9f9; padding: 15px; border-radius: 5px; text-align: left;">
                            <p style="font-size: 1.1em;"><strong>The weighting function $h(x)$ is crucial:</strong></p>
                            <ul style="font-size: 0.95em; margin-top: 12px;">
                                <li><strong>Rare pairs</strong> ($x_{ij}$ very small): Noisy, unreliable → downweight</li>
                                <li><strong>Very frequent pairs</strong> (common words like "the"): Dominate loss → cap their influence</li>
                                <li><strong>Moderately frequent pairs</strong>: Most informative → higher weight</li>
                            </ul>
                        </div>
                        
                        <div style="margin-top: 20px; display: flex; gap: 20px; align-items: center;">
                            <div style="flex: 1;">
                                <p><strong>Common choice:</strong></p>
                                <div class="equation-highlight">
                                    $$h(x) = \begin{cases}
                                    \left(\frac{x}{x_{\text{max}}}\right)^{0.75} & \text{if } x < x_{\text{max}} \\
                                    1 & \text{otherwise}
                                    \end{cases}$$
                                </div>
                                <p style="margin-top: 10px;">with $x_{\text{max}} = 100$</p>
                            </div>
                            <div style="flex: 1;">
                                <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                    <p><strong>Effect:</strong></p>
                                    <ul style="font-size: 0.9em; margin-top: 8px;">
                                        <li>$x = 1$: $h(x) = 0.01$ (very low weight)</li>
                                        <li>$x = 10$: $h(x) = 0.18$ (moderate weight)</li>
                                        <li>$x = 100$: $h(x) = 1.0$ (full weight)</li>
                                        <li>$x = 1000$: $h(x) = 1.0$ (capped)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="emphasis-box" style="margin-top: 20px; background: #FFF9E6; border-left: 4px solid #FAC55B;">
                            <p><strong>Result:</strong> GloVe focuses on the "Goldilocks zone" of co-occurrences—not too rare, not too common, but just right for learning meaningful relationships!</p>
                        </div>
                    </div>
                </section>

                <!-- MCQ: GloVe -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main difference between word2vec and GloVe?",
                        "type": "single",
                        "options": [
                            {
                                "text": "word2vec produces better embeddings than GloVe",
                                "correct": false,
                                "explanation": "Both methods can produce high-quality embeddings; neither is universally better."
                            },
                            {
                                "text": "GloVe uses global co-occurrence statistics while word2vec uses local context windows",
                                "correct": true,
                                "explanation": "Correct! GloVe pre-computes global co-occurrence counts and trains on these statistics, while word2vec samples local context windows during training."
                            },
                            {
                                "text": "word2vec can handle multiple languages but GloVe cannot",
                                "correct": false,
                                "explanation": "Both methods can be applied to any language with sufficient text data."
                            },
                            {
                                "text": "GloVe uses a neural network while word2vec does not",
                                "correct": false,
                                "explanation": "Both use vector representations and optimization, though they differ in their training objectives."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Subword Embeddings -->
            <section>

                <!-- Intuition: Subword Embeddings -->
                <section>
                    <h2 class="truncate-title">Subword Embeddings: The Problem</h2>
                    <div style="text-align: left; font-size: 0.75em;">
                        <div style="background: #FFE6E6; padding: 15px; border-radius: 5px; margin-bottom: 20px;">
                            <p><strong>The Limitation of Word-Level Models:</strong></p>
                            <p style="margin-top: 10px; font-size: 0.95em;">
                                word2vec and GloVe treat each word as an <strong>atomic unit</strong>:
                            </p>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>"teach", "teacher", "teaching" → completely different vectors</li>
                                <li>Rare words get poor embeddings (not enough training data)</li>
                                <li>New words (out-of-vocabulary) → no embedding at all!</li>
                            </ul>
                        </div>

                        <div style="display: flex; gap: 20px; margin-top: 20px;">
                            <div style="flex: 1; background: #f5f5f5; padding: 15px; border-radius: 5px;">
                                <p><strong>Word-level approach:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>One vector per word</li>
                                    <li>Fixed vocabulary</li>
                                    <li>Each word learned independently</li>
                                    <li>❌ Can't handle rare/new words</li>
                                </ul>
                            </div>
                            <div style="flex: 1; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <p><strong style="color: #10099F;">Subword approach:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>Vectors for word <em>pieces</em></li>
                                    <li>Open vocabulary</li>
                                    <li>Share information via subwords</li>
                                    <li>✓ Handles any word!</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Subword Embeddings: The Solution</h2>
                    <div style="text-align: left; font-size: 0.75em;">
                        <div class="emphasis-box" style="margin-bottom: 20px;">
                            <p style="font-size: 1.05em;"><strong>Key Insight:</strong> Words have internal structure that carries meaning!</p>
                            <p style="font-size: 0.95em; margin-top: 8px;">
                                "un<strong>help</strong>ful" = "un" + "help" + "ful"<br>
                                Share information between related words!
                            </p>
                        </div>

                        <div style="margin-top: 20px; background: #f9f9f9; padding: 15px; border-radius: 5px;">
                            <p><strong>Example:</strong> Representing "unhelpfulness"</p>
                            <p style="margin-top: 10px; font-family: monospace; font-size: 0.9em;">
                                "unhelpfulness" → ["un", "help", "ful", "ness"]
                            </p>
                            <p style="margin-top: 10px; font-size: 0.95em;">
                                Word vector = sum/average of subword vectors<br>
                                → Even if we've never seen "unhelpfulness", we can represent it!
                            </p>
                        </div>

                        <div class="emphasis-box" style="margin-top: 20px; background: #FFF9E6; border-left: 4px solid #FAC55B;">
                            <p><strong>Result:</strong> Subword embeddings can handle any word by breaking it into meaningful pieces!</p>
                        </div>
                    </div>
                </section>

                <!-- Subword Embedding Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.6: Subword Embedding", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html"}]'>
                    <h2 class="truncate-title">The Motivation for Subword Embeddings</h2>
                    <div style="text-align: left;">
                        <p><strong>Problem:</strong> word2vec and GloVe ignore word internal structure</p>
                        <div style="margin-top: 20px; background: #FFF3CD; padding: 15px; border-radius: 5px;">
                            <p><strong>Examples of <span class="tooltip">morphological<span class="tooltiptext">Morphology: The study of word structure and formation, including roots, prefixes, suffixes, and inflections</span></span> relationships:</strong></p>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>"help", "helps", "helped", "helping" (different vectors!)</li>
                                <li>"dog" : "dogs" :: "cat" : "cats" (no shared structure)</li>
                                <li>"boy" : "boyfriend" :: "girl" : "girlfriend"</li>
                            </ul>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Solution:</strong> Represent words as <strong>sum of subword vectors</strong></p>
                        </div>
                    </div>
                </section>

                <!-- fastText Model -->
                <section>
                    <h2 class="truncate-title">fastText: Character n-gram Embeddings</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>Idea:</strong> Represent word by its character <span class="tooltip">n-grams<span class="tooltiptext">n-gram: A contiguous sequence of n items (here, characters) from a text</span></span></p>
                        <div style="margin-top: 20px; background: #E8F4F8; padding: 15px; border-radius: 5px; font-family: monospace; font-size: 0.9em;">
                            <p><strong>Example:</strong> "where" with n=3</p>
                            <p style="margin-top: 10px;">
                                &lt;where&gt; → {<span style="color: #10099F;">&lt;wh</span>, <span style="color: #10099F;">whe</span>, <span style="color: #10099F;">her</span>, <span style="color: #10099F;">ere</span>, <span style="color: #10099F;">re&gt;</span>, &lt;where&gt;}
                            </p>
                            <p style="margin-top: 10px; font-family: sans-serif; font-size: 0.95em;">
                                Special characters &lt; and &gt; distinguish prefixes/suffixes
                            </p>
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>Word vector as sum of subword vectors:</strong></p>
                            <div class="equation-highlight">
                                $$\mathbf{v}_w = \sum_{g \in \mathcal{G}_w} \mathbf{z}_g$$
                            </div>
                            <p style="font-size: 0.9em; margin-top: 10px;">
                                where $\mathcal{G}_w$ = all n-grams of word $w$ (typically 3 ≤ n ≤ 6)
                            </p>
                        </div>
                    </div>
                </section>

                <!-- fastText Benefits -->
                <section>
                    <h2 class="truncate-title">fastText: Benefits</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">✓ Rare Words</h4>
                                <p style="font-size: 0.85em;">Even rare words share subwords with common words</p>
                                <p style="font-size: 0.8em; margin-top: 8px; font-family: monospace;">
                                    "basketball" shares "ball" with "football"
                                </p>
                            </div>
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">✓ Out-of-Vocabulary</h4>
                                <p style="font-size: 0.85em;">Can represent unseen words using learned subwords</p>
                                <p style="font-size: 0.8em; margin-top: 8px; font-family: monospace;">
                                    "antiestablishment" = anti + establish + ment
                                </p>
                            </div>
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">✓ Morphology</h4>
                                <p style="font-size: 0.85em;">Captures word formation patterns</p>
                                <p style="font-size: 0.8em; margin-top: 8px; font-family: monospace;">
                                    "running" shares "run" with "runner"
                                </p>
                            </div>
                            <div style="background: #FFF3CD; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #FAC55B; margin-bottom: 10px;">✗ Larger Vocabulary</h4>
                                <p style="font-size: 0.85em;">More parameters to store</p>
                                <p style="font-size: 0.8em; margin-top: 8px;">
                                    Need vectors for all n-grams
                                </p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- BPE Introduction -->
                <section>
                    <h2 class="truncate-title">Byte Pair Encoding (BPE)</h2>
                    <div style="text-align: left;">
                        <p><strong>Problem with fastText:</strong> Fixed n-gram lengths, can't control vocabulary size</p>
                        <div style="margin-top: 20px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p><strong>BPE Solution:</strong> Variable-length subwords in fixed-size vocabulary</p>
                            <ul style="font-size: 0.9em; margin-top: 10px;">
                                <li>Start with characters as initial symbols</li>
                                <li><strong>Iteratively merge</strong> most frequent consecutive symbol pairs</li>
                                <li>Stop when desired vocabulary size reached</li>
                            </ul>
                        </div>
                        <div style="margin-top: 20px; font-size: 0.9em;">
                            <p><strong>Used in modern models:</strong> GPT-2, RoBERTa, BART, etc.</p>
                        </div>
                    </div>
                </section>

                <!-- BPE Algorithm -->
                <section>
                    <h2 class="truncate-title">BPE Algorithm: Step by Step</h2>
                    <div class="code-block" style="font-size: 0.75em;">
<pre><code class="language-python"># Initial vocabulary: characters + special symbols
symbols = ['a','b','c',...,'z','_','[UNK]']

# Words with frequencies
token_freqs = {
    'f a s t _': 4,
    'f a s t e r _': 3,
    't a l l _': 5,
    't a l l e r _': 4
}

# Iteratively merge most frequent pairs
for iteration in range(num_merges):
    # 1. Count all consecutive symbol pairs
    pairs = count_pairs(token_freqs)

    # 2. Find most frequent pair
    max_pair = get_max_freq_pair(pairs)

    # 3. Merge this pair in all words
    merge_symbols(max_pair, token_freqs, symbols)
</code></pre>
                    </div>
                </section>

                <!-- Interactive BPE -->
                <section>
                    <h2 class="truncate-title">BPE in Action</h2>
                    <div class="demo-controls">
                        <button id="bpe-step">Next Step</button>
                        <button id="bpe-reset-algo">Reset</button>
                        <label style="margin-left: 15px;">
                            <input type="checkbox" id="bpe-auto">
                            Auto-play
                        </label>
                    </div>
                    <div id="bpe-viz" style="width: 700px; margin: 20px auto; padding: 20px; background: #f9f9f9; border-radius: 5px; font-size: 0.7em;">
                        <div style="margin-bottom: 15px;">
                            <strong>Step <span id="bpe-step-num">0</span>/10:</strong>
                            <span id="bpe-merge-info">Initial state</span>
                        </div>
                        <div class="bpe-container">
                            <div id="bpe-tokens" style="min-height: 120px;">
                                <div class="bpe-step">
                                    <span class="token">f</span> <span class="token">a</span> <span class="token">s</span> <span class="token">t</span> <span class="token">_</span> (freq: 4)
                                </div>
                                <div class="bpe-step">
                                    <span class="token">f</span> <span class="token">a</span> <span class="token">s</span> <span class="token">t</span> <span class="token">e</span> <span class="token">r</span> <span class="token">_</span> (freq: 3)
                                </div>
                                <div class="bpe-step">
                                    <span class="token">t</span> <span class="token">a</span> <span class="token">l</span> <span class="token">l</span> <span class="token">_</span> (freq: 5)
                                </div>
                                <div class="bpe-step">
                                    <span class="token">t</span> <span class="token">a</span> <span class="token">l</span> <span class="token">l</span> <span class="token">e</span> <span class="token">r</span> <span class="token">_</span> (freq: 4)
                                </div>
                            </div>
                        </div>
                        <div style="margin-top: 15px; padding: 10px; background: white; border-radius: 5px;">
                            <strong>New symbols learned:</strong>
                            <div id="bpe-vocab" style="margin-top: 8px; font-family: monospace; font-size: 0.85em;">
                                (none yet)
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ: Subword Embedding -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the main advantage of subword embeddings (fastText, BPE) over word-level embeddings (word2vec, GloVe)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They train faster on large corpora",
                                "correct": false,
                                "explanation": "Subword methods are typically slower due to the larger vocabulary of subword units."
                            },
                            {
                                "text": "They can handle rare words and out-of-vocabulary words better",
                                "correct": true,
                                "explanation": "Correct! By decomposing words into subwords, these methods can represent unseen words using learned subword embeddings and better handle rare words by sharing subwords with common words."
                            },
                            {
                                "text": "They produce embeddings with lower dimensionality",
                                "correct": false,
                                "explanation": "Dimensionality is a hyperparameter choice and not inherently different between word-level and subword methods."
                            },
                            {
                                "text": "They do not require as much training data",
                                "correct": false,
                                "explanation": "All embedding methods benefit from large training corpora; subword methods do not reduce data requirements."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Word Embedding Applications -->
            <section>

                <!-- Word Similarity and Analogy -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.7: Word Similarity and Analogy", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html"}]'>
                    <h2 class="truncate-title">Applying Word Embeddings</h2>
                    <div style="text-align: left;">
                        <p>Once trained, word embeddings can be used for:</p>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">1. Word Similarity</h4>
                                <p style="font-size: 0.85em;">Find semantically similar words using cosine similarity</p>
                                <div style="margin-top: 10px; font-family: monospace; font-size: 0.8em;">
                                    <code>
                                    similar("chip"):<br>
                                    → chips (0.856)<br>
                                    → intel (0.749)<br>
                                    → electronics (0.749)
                                    </code>
                                </div>
                            </div>
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">2. Word Analogy</h4>
                                <p style="font-size: 0.85em;">Complete analogies using vector arithmetic</p>
                                <div style="margin-top: 10px; font-family: monospace; font-size: 0.8em;">
                                    <code>
                                    man : woman :: king : ?<br>
                                    <br>
                                    vec(king) - vec(man)<br>
                                    + vec(woman) ≈ vec(queen)
                                    </code>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Word Analogy Examples -->
                <section>
                    <h2 class="truncate-title">Word Analogy: Vector Arithmetic</h2>
                    <p>Word embeddings capture various relationships through vector arithmetic:</p>
                    <div style="margin-top: 20px; font-size: 0.7em;">
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Relationship Type</th>
                                    <th>Analogy</th>
                                    <th>Vector Arithmetic</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Gender</strong></td>
                                    <td>man : woman :: son : daughter</td>
                                    <td style="font-family: monospace; font-size: 0.8em;">vec(son) - vec(man) + vec(woman)</td>
                                </tr>
                                <tr>
                                    <td><strong>Geography</strong></td>
                                    <td>Beijing : China :: Tokyo : Japan</td>
                                    <td style="font-family: monospace; font-size: 0.8em;">vec(Tokyo) - vec(Beijing) + vec(China)</td>
                                </tr>
                                <tr>
                                    <td><strong>Grammar</strong></td>
                                    <td>bad : worst :: big : biggest</td>
                                    <td style="font-family: monospace; font-size: 0.8em;">vec(big) - vec(bad) + vec(worst)</td>
                                </tr>
                                <tr>
                                    <td><strong>Verb Tense</strong></td>
                                    <td>do : did :: go : went</td>
                                    <td style="font-family: monospace; font-size: 0.8em;">vec(go) - vec(do) + vec(did)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="emphasis-box" style="margin-top: 20px;">
                        <p><strong>Remarkable property:</strong> Embeddings capture both semantic AND syntactic relationships!</p>
                    </div>
                </section>

                <!-- Interactive Word Space -->
                <section>
                    <h2 class="truncate-title">Word Vector Space Visualization</h2>
                    <div class="demo-controls">
                        <label>
                            Search word:
                            <input type="text" id="word-search" placeholder="e.g., king" style="padding: 5px; border-radius: 3px; border: 1px solid #ccc; width: 150px;">
                        </label>
                        <button id="word-find">Find Similar</button>
                        <button id="word-analogy-demo">Show Analogy</button>
                    </div>
                    <div id="word-space-viz" class="vector-space"></div>
                </section>

                <!-- MCQ: Word Similarity -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "To find the word that completes the analogy \"king : man :: queen : ?\", which vector arithmetic operation should we use?",
                        "type": "single",
                        "options": [
                            {
                                "text": "vec(queen) - vec(king) + vec(man)",
                                "correct": false,
                                "explanation": "This computes a different relationship. We need to apply the (king - man) transformation to queen."
                            },
                            {
                                "text": "vec(queen) - vec(man) + vec(king)",
                                "correct": false,
                                "explanation": "This applies the transformation in the wrong direction."
                            },
                            {
                                "text": "vec(king) - vec(man) + vec(queen)",
                                "correct": false,
                                "explanation": "We are looking for the unknown word, not computing from queen."
                            },
                            {
                                "text": "vec(queen) + vec(man) - vec(king)",
                                "correct": true,
                                "explanation": "Correct! This computes the gender transformation from king to queen, then applies it inversely. Actually for this specific analogy, we want: vec(queen) - vec(king) + vec(man) ≈ vec(woman). The question setup might be better phrased as finding what \"man\" is to \"king\" as \"woman\" is to \"queen\"."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- Context-Sensitive Models -->
            <section>

                <!-- Intuition: Context-Sensitive Models -->
                <section>
                    <h2 class="truncate-title">Context-Sensitive Models: The Problem</h2>
                    <div style="text-align: left; font-size: 0.55em;">
                        <div style="background: #FFE6E6; padding: 15px; border-radius: 5px; margin-bottom: 20px;">
                            <p><strong>The Fundamental Limitation of Static Embeddings:</strong></p>
                            <p style="margin-top: 10px; font-size: 0.95em;">
                                In word2vec, GloVe, and fastText, each word has a <strong>single fixed vector</strong>:
                            </p>
                            <div style="margin-top: 15px; background: white; padding: 12px; border-radius: 5px; border-left: 4px solid #FC8484;">
                                <p style="font-family: monospace; font-size: 0.9em;">
                                    "I deposited money in the <strong style="color: #FC8484;">bank</strong>"<br>
                                    "I sat by the river <strong style="color: #FC8484;">bank</strong>"
                                </p>
                                <p style="margin-top: 10px; font-size: 0.9em;">
                                    → Same vector for "bank" in both sentences!<br>
                                    → But the meanings are completely different!
                                </p>
                            </div>
                        </div>

                        <div style="display: flex; gap: 20px; margin-top: 20px;">
                            <div style="flex: 1; background: #f5f5f5; padding: 15px; border-radius: 5px;">
                                <p><strong>Static Embeddings:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>word2vec, GloVe, fastText</li>
                                    <li>One vector per word</li>
                                    <li>Context-<strong style="color: #FC8484;">independent</strong></li>
                                    <li>Lookup table of embeddings</li>
                                </ul>
                            </div>
                            <div style="flex: 1; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <p><strong style="color: #10099F;">Context-Sensitive Embeddings:</strong></p>
                                <ul style="font-size: 0.9em; margin-top: 10px;">
                                    <li>ELMo, GPT, BERT</li>
                                    <li>Different vector per <em>usage</em></li>
                                    <li>Context-<strong style="color: #2DD2C0;">dependent</strong></li>
                                    <li>Computed from surrounding words</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">Context-Sensitive Models: The Solution</h2>
                    <div style="text-align: left; font-size: 0.55em;">
                        <div class="emphasis-box" style="margin-bottom: 15px;">
                            <p style="font-size: 1.05em; margin: 0;"><strong>Key Insight:</strong> Word meaning depends on context!</p>
                            <p style="font-size: 0.95em; margin-top: 5px; margin-bottom: 0;">
                                Solution: Generate <strong>different</strong> embeddings for the same word in different contexts
                            </p>
                        </div>

                        <div style="margin-top: 15px; background: #f9f9f9; padding: 15px; border-radius: 5px;">
                            <p style="margin: 0;"><strong>How it works:</strong></p>
                            <ol style="font-size: 0.95em; margin-top: 8px; margin-bottom: 0; margin-left: 20px;">
                                <li>Process entire sentence with deep neural network (often a Transformer)</li>
                                <li>Each word's representation is computed based on <em>all other words</em> in context</li>
                                <li>Same word → different contexts → different embeddings</li>
                            </ol>
                        </div>

                        <div style="margin-top: 15px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                            <p style="margin: 0;"><strong>Example:</strong></p>
                            <p style="margin-top: 8px; margin-bottom: 0; font-family: monospace; font-size: 0.9em;">
                                "I deposited money in the <strong style="color: #10099F;">bank</strong>"<br>
                                → bank_embedding = f("bank", ["I", "deposited", "money", "in", "the"])<br><br>
                                "I sat by the river <strong style="color: #10099F;">bank</strong>"<br>
                                → bank_embedding = f("bank", ["I", "sat", "by", "the", "river"])
                            </p>
                            <p style="margin-top: 8px; margin-bottom: 0; font-size: 0.95em;">
                                → Different contexts → Different embeddings!
                            </p>
                        </div>

                        <div style="margin-top: 15px;">
                            <p style="margin: 0;"><strong>The Evolution:</strong> Static → Context-Sensitive → Foundation Models</p>
                        </div>
                    </div>
                </section>

                <!-- Context-Sensitive Introduction -->
                <section data-sources='[{"text": "Dive into Deep Learning - 15.8: BERT", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html"}]'>
                    <h2 class="truncate-title">Limitations of Context-Independent Embeddings</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>Problem:</strong> Same word, different meanings</p>
                        <div style="margin-top: 20px; background: #FFF3CD; padding: 15px; border-radius: 5px;">
                            <p style="font-family: monospace; font-size: 0.9em;">
                                1. "I went to the <span style="color: #10099F; font-weight: bold;">bank</span> to deposit money"<br>
                                2. "I sat by the river <span style="color: #10099F; font-weight: bold;">bank</span>"
                            </p>
                            <p style="margin-top: 10px;">
                                word2vec/GloVe assign the <strong>same vector</strong> to "bank" in both sentences!
                            </p>
                        </div>
                        <div style="margin-top: 20px;">
                            <p><strong>Context-independent representation:</strong> $f(x)$</p>
                            <p style="margin-top: 10px;"><strong>Context-sensitive representation:</strong> $f(x, c(x))$</p>
                            <p style="font-size: 0.9em; margin-top: 10px; color: #666;">
                                where $c(x)$ is the context surrounding $x$
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Evolution of Context-Sensitive Models -->
                <section>
                    <h2 class="truncate-title">Evolution: Toward Context-Sensitive Embeddings</h2>
                    <div style="margin-top: 20px; font-size: 0.7em;">
                        <div style="position: relative; padding-left: 30px;">
                            <!-- Timeline -->
                            <div style="position: absolute; left: 10px; top: 0; bottom: 0; width: 3px; background: #10099F;"></div>

                            <!-- 2017: ELMo precursors -->
                            <div style="position: relative; margin-bottom: 25px;">
                                <div style="position: absolute; left: -24px; width: 16px; height: 16px; border-radius: 50%; background: #2DD2C0; border: 3px solid white;"></div>
                                <div style="background: #f5f5f5; padding: 12px; border-radius: 5px;">
                                    <strong style="color: #10099F;">2017: TagLM, CoVe</strong>
                                    <p style="font-size: 0.85em; margin-top: 5px;">Early context-sensitive models</p>
                                </div>
                            </div>

                            <!-- 2018: ELMo -->
                            <div style="position: relative; margin-bottom: 25px;">
                                <div style="position: absolute; left: -24px; width: 16px; height: 16px; border-radius: 50%; background: #2DD2C0; border: 3px solid white;"></div>
                                <div style="background: #E8F4F8; padding: 12px; border-radius: 5px;">
                                    <strong style="color: #10099F;">2018: ELMo</strong>
                                    <p style="font-size: 0.85em; margin-top: 5px;"><strong>Bidirectional LSTM</strong>, context-sensitive, but <strong>task-specific</strong> architectures</p>
                                </div>
                            </div>

                            <!-- 2018: GPT -->
                            <div style="position: relative; margin-bottom: 25px;">
                                <div style="position: absolute; left: -24px; width: 16px; height: 16px; border-radius: 50%; background: #2DD2C0; border: 3px solid white;"></div>
                                <div style="background: #E8F4F8; padding: 12px; border-radius: 5px;">
                                    <strong style="color: #10099F;">2018: GPT</strong>
                                    <p style="font-size: 0.85em; margin-top: 5px;"><strong>Transformer decoder</strong>, task-agnostic, but <strong>left-to-right</strong> only</p>
                                </div>
                            </div>

                            <!-- 2018: BERT -->
                            <div style="position: relative;">
                                <div style="position: absolute; left: -24px; width: 16px; height: 16px; border-radius: 50%; background: #10099F; border: 3px solid white;"></div>
                                <div style="background: #D5E8F7; padding: 12px; border-radius: 5px; border: 2px solid #10099F;">
                                    <strong style="color: #10099F;">2018: BERT</strong>
                                    <p style="font-size: 0.85em; margin-top: 5px;"><strong>Bidirectional Transformer</strong>, task-agnostic, revolutionized NLP!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- ELMo GPT BERT Comparison -->
                <section>
                    <h2 class="truncate-title">Comparing ELMo, GPT, and BERT</h2>
                    <div style="margin-top: 20px;">
                        <img src="images/elmo-gpt-bert.svg" alt="ELMo GPT BERT Comparison" style="width: 100%; max-width: 400px;">
                    </div>
                    <table class="comparison-table" style="margin-top: 20px; font-size: 0.8em;">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Architecture</th>
                                <th>Context</th>
                                <th>Task-Agnostic?</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>ELMo</strong></td>
                                <td>Bidirectional LSTM</td>
                                <td>✓ Bidirectional</td>
                                <td>✗ Task-specific</td>
                            </tr>
                            <tr>
                                <td><strong>GPT</strong></td>
                                <td>Transformer Decoder</td>
                                <td>✗ Left-to-right</td>
                                <td>✓ Task-agnostic</td>
                            </tr>
                            <tr style="background: #E8F4F8;">
                                <td><strong>BERT</strong></td>
                                <td>Transformer Encoder</td>
                                <td>✓ Bidirectional</td>
                                <td>✓ Task-agnostic</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <!-- MCQ: Context-Sensitive -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What is the key advantage of context-sensitive embeddings (like ELMo, GPT, BERT) over context-independent embeddings (like word2vec, GloVe)?",
                        "type": "single",
                        "options": [
                            {
                                "text": "They require less training data",
                                "correct": false,
                                "explanation": "Context-sensitive models typically require more training data, not less."
                            },
                            {
                                "text": "They are faster to train and use",
                                "correct": false,
                                "explanation": "Context-sensitive models are generally more computationally expensive."
                            },
                            {
                                "text": "They can assign different representations to the same word based on context",
                                "correct": true,
                                "explanation": "Correct! Context-sensitive models can handle polysemy by generating different representations for the same word in different contexts (e.g., \"bank\" as financial institution vs. river bank)."
                            },
                            {
                                "text": "They work better for word analogy tasks",
                                "correct": false,
                                "explanation": "Word analogy tasks are typically performed with context-independent embeddings, which capture static word relationships."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- BERT: Bidirectional Transformers -->
            <section>

                <!-- Intuition: BERT -->
                <section>
                    <h2 class="truncate-title">BERT: The Core Intuition</h2>
                    <div style="text-align: left; font-size: 0.65em;">
                        <div class="emphasis-box" style="margin-bottom: 15px;">
                            <p style="font-size: 1.05em;"><strong>The Breakthrough:</strong> True bidirectional context-sensitive representations</p>
                        </div>

                        <div style="background: #FFF3CD; padding: 12px; border-radius: 5px; margin-bottom: 15px;">
                            <p><strong>Previous Context-Sensitive Models:</strong></p>
                            <ul style="font-size: 0.9em; margin-top: 8px;">
                                <li><strong>ELMo:</strong> Combines left-to-right and right-to-left LSTMs (shallow bidirectional)</li>
                                <li><strong>GPT:</strong> Only left-to-right context (unidirectional)</li>
                                <li><strong>Problem:</strong> Not fully utilizing context from both directions simultaneously</li>
                            </ul>
                        </div>

                        <div style="background: #E8F4F8; padding: 12px; border-radius: 5px;">
                            <p><strong style="color: #10099F;">BERT's Innovation:</strong></p>
                            <p style="margin-top: 8px; font-size: 0.95em;">
                                Use the <strong>Transformer encoder</strong> to process the <em>entire</em> sentence in both directions at once!
                            </p>
                            <p style="margin-top: 8px; font-family: monospace; font-size: 0.85em; background: white; padding: 8px; border-radius: 3px;">
                                "The cat sat on the [MASK]"
                            </p>
                            <p style="margin-top: 6px; font-size: 0.9em;">
                                → To predict [MASK], use context from <strong style="color: #10099F;">both left AND right</strong>
                            </p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title">BERT: Bidirectional vs Unidirectional</h2>
                    <div style="text-align: left; font-size: 0.75em;">
                        <div style="display: flex; gap: 20px; margin-bottom: 15px;">
                            <div style="flex: 1; background: #f5f5f5; padding: 12px; border-radius: 5px;">
                                <p><strong>Unidirectional (GPT):</strong></p>
                                <p style="margin-top: 8px; font-family: monospace; font-size: 0.85em;">
                                    The → cat → sat → on → the → ?
                                </p>
                                <p style="margin-top: 6px; font-size: 0.9em;">Only sees words to the left</p>
                            </div>
                            <div style="flex: 1; background: #E8F4F8; padding: 12px; border-radius: 5px;">
                                <p><strong style="color: #10099F;">Bidirectional (BERT):</strong></p>
                                <p style="margin-top: 8px; font-family: monospace; font-size: 0.85em;">
                                    The ← cat ← sat ← ? → on → the
                                </p>
                                <p style="margin-top: 6px; font-size: 0.9em;">Sees words from both sides!</p>
                            </div>
                        </div>

                        <div style="background: #f9f9f9; padding: 12px; border-radius: 5px;">
                            <p><strong>How BERT achieves this:</strong></p>
                            <ol style="font-size: 0.95em; margin-top: 8px; margin-left: 20px;">
                                <li><strong>Masked Language Modeling (MLM):</strong> Randomly mask words and predict them using full context</li>
                                <li><strong>Transformer Architecture:</strong> Self-attention allows each word to attend to all other words</li>
                                <li><strong>Pretraining:</strong> Train on huge text corpora, then fine-tune on specific tasks</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <!-- BERT Introduction -->
                <section>
                    <h2 class="truncate-title">BERT: Bidirectional Encoder Representations from Transformers</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>BERT combines the best of both worlds:</strong></p>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">From ELMo</h4>
                                <p style="font-size: 0.9em;">✓ <strong>Bidirectional</strong> context encoding</p>
                            </div>
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">From GPT</h4>
                                <p style="font-size: 0.9em;">✓ <strong>Task-agnostic</strong> architecture</p>
                            </div>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Architecture:</strong> <span class="tooltip">Transformer Encoder<span class="tooltiptext">The encoder part of the Transformer architecture, using self-attention to process input sequences bidirectionally</span></span></p>
                            <p style="font-size: 0.9em; margin-top: 10px;">
                                Self-attention allows each token to attend to all other tokens in the sequence
                            </p>
                        </div>
                        <div style="margin-top: 20px; font-size: 0.9em;">
                            <p><strong>Impact:</strong> Improved state-of-the-art on 11 NLP tasks</p>
                            <ul style="margin-top: 8px;">
                                <li>Single text classification (sentiment analysis)</li>
                                <li>Text pair classification (natural language inference)</li>
                                <li>Question answering</li>
                                <li>Text tagging (named entity recognition)</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- BERT Input Representation -->
                <section>
                    <h2 class="truncate-title">BERT: Input Representation</h2>
                    <div style="margin-top: 20px;">
                        <img src="images/bert-input.svg" alt="BERT Input Representation" style="width: 100%; max-width: 400px;">
                    </div>
                    <div style="margin-top: 20px; text-align: left; font-size: 0.7em;">
                        <p><strong>Three types of embeddings are summed:</strong></p>
                        <ol style="margin-top: 10px; font-size: 0.9em;">
                            <li><strong>Token embeddings:</strong> Learned representations for each token</li>
                            <li><strong>Segment embeddings:</strong> $\mathbf{e}_A$ for first sentence, $\mathbf{e}_B$ for second</li>
                            <li><strong>Positional embeddings:</strong> Learned (not sinusoidal like original Transformer)</li>
                        </ol>
                        <div style="margin-top: 15px; background: #E8F4F8; padding: 12px; border-radius: 5px;">
                            <p style="font-size: 0.9em;">
                                <strong>Special tokens:</strong><br>
                                &lt;cls&gt;: Classification token (start of sequence)<br>
                                &lt;sep&gt;: Separator between sentences
                            </p>
                        </div>
                    </div>
                </section>

                <!-- BERT Pretraining Tasks -->
                <section>
                    <h2 class="truncate-title">BERT Pretraining: Two Tasks</h2>
                    <div style="text-align: left;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">1. Masked Language Modeling (MLM)</h4>
                                <p style="font-size: 0.85em; margin-bottom: 10px;">Predict masked tokens using bidirectional context</p>
                                <div style="background: white; padding: 10px; border-radius: 3px; font-family: monospace; font-size: 0.8em;">
                                    this movie is [MASK]<br>
                                    → predict "great"
                                </div>
                            </div>
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">2. Next Sentence Prediction (NSP)</h4>
                                <p style="font-size: 0.85em; margin-bottom: 10px;">Predict if sentence B follows sentence A</p>
                                <div style="background: white; padding: 10px; border-radius: 3px; font-family: monospace; font-size: 0.8em;">
                                    [CLS] A [SEP] B [SEP]<br>
                                    → IsNext or NotNext
                                </div>
                            </div>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Self-supervised:</strong> Both tasks derive labels from raw text automatically!</p>
                        </div>
                    </div>
                </section>

                <!-- Masked Language Modeling -->
                <section>
                    <h2 class="truncate-title" style="margin-bottom: 15px;">Masked Language Modeling (MLM)</h2>
                    <div style="text-align: left; font-size: 0.6em;">
                        <p style="margin-bottom: 10px;"><strong>Procedure:</strong></p>
                        <ol style="margin-top: 10px; margin-bottom: 15px; font-size: 0.9em;">
                            <li>Randomly select <strong>15%</strong> of tokens for prediction</li>
                            <li>For each selected token:</li>
                        </ol>
                        <div style="margin-left: 30px; background: #f5f5f5; padding: 12px; border-radius: 5px;">
                            <div style="margin-bottom: 8px;">
                                <strong style="color: #10099F;">80%:</strong> Replace with [MASK]<br>
                                <code style="margin-left: 20px;">this movie is [MASK]</code>
                            </div>
                            <div style="margin-bottom: 8px;">
                                <strong style="color: #2DD2C0;">10%:</strong> Replace with random token<br>
                                <code style="margin-left: 20px;">this movie is apple</code>
                            </div>
                            <div>
                                <strong style="color: #FAC55B;">10%:</strong> Keep unchanged<br>
                                <code style="margin-left: 20px;">this movie is great</code>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2 class="truncate-title" style="margin-bottom: 15px;">Why the 80/10/10 Split?</h2>
                    <div style="text-align: left; font-size: 0.6em;">
                        <div style="background: #E8F4F8; padding: 12px; border-radius: 5px;">
                            <div style="background: white; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                                <p style="font-size: 0.9em; margin-bottom: 6px;"><strong style="color: #FC8484;">The Problem:</strong></p>
                                <p style="font-size: 0.85em; margin-bottom: 0;">
                                    During <strong>pretraining</strong>: Model sees [MASK] tokens<br>
                                    During <strong>fine-tuning</strong>: Model sees real words, never [MASK]!<br>
                                    → This mismatch hurts performance
                                </p>
                            </div>
                            <div style="background: white; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                                <p style="font-size: 0.9em; margin-bottom: 6px;"><strong style="color: #10099F;">The Solution:</strong></p>
                                <ul style="font-size: 0.85em; margin-left: 20px; margin-bottom: 0;">
                                    <li><strong>80% [MASK]:</strong> Main training signal - model learns to predict masked words</li>
                                    <li><strong>10% random:</strong> Teaches model to handle unexpected/wrong tokens robustly</li>
                                    <li><strong>10% unchanged:</strong> Bridges pretraining and fine-tuning - model sees real tokens and learns their representations</li>
                                </ul>
                            </div>
                            <p style="font-size: 0.85em; margin-bottom: 0; font-style: italic;">
                                This mixing strategy makes BERT more robust and reduces the pretraining-finetuning gap!
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Next Sentence Prediction -->
                <section>
                    <h2 class="truncate-title">Next Sentence Prediction (NSP)</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <p><strong>Task:</strong> Predict if sentence B follows sentence A in the original text</p>
                        <div style="margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px; margin-bottom: 15px;">
                                <p style="font-size: 0.9em;"><strong style="color: #2DD2C0;">✓ IsNext (50%):</strong></p>
                                <div style="margin-top: 8px; font-family: monospace; font-size: 0.85em; background: white; padding: 10px; border-radius: 3px;">
                                    [CLS] The man went to the store. [SEP]<br>
                                    He bought a gallon of milk. [SEP]
                                </div>
                            </div>
                            <div style="background: #FFE6E6; padding: 15px; border-radius: 5px;">
                                <p style="font-size: 0.9em;"><strong style="color: #FC8484;">✗ NotNext (50%):</strong></p>
                                <div style="margin-top: 8px; font-family: monospace; font-size: 0.85em; background: white; padding: 10px; border-radius: 3px;">
                                    [CLS] The man went to the store. [SEP]<br>
                                    Penguins are flightless birds. [SEP]
                                </div>
                            </div>
                        </div>
                        <div style="margin-top: 20px;">
                            <p style="font-size: 0.9em;">
                                <strong>Prediction uses:</strong> [CLS] token representation<br>
                                <span style="color: #666;">Self-attention allows [CLS] to encode information about both sentences</span>
                            </p>
                        </div>
                    </div>
                </section>

                <!-- BERT Training Scale -->
                <section>
                    <h2 class="truncate-title">BERT: Training at Scale</h2>
                    <div style="text-align: left; font-size: 0.5em;">
                        <div style="background: #E8F4F8; padding: 20px; border-radius: 5px;">
                            <p><strong>Training Data:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li><strong>BookCorpus:</strong> 800 million words</li>
                                <li><strong>English Wikipedia:</strong> 2.5 billion words</li>
                                <li><strong>Total:</strong> 3.3 billion words</li>
                            </ul>
                        </div>
                        <div style="margin-top: 20px; background: #FFF9E6; padding: 20px; border-radius: 5px;">
                            <p><strong>Model Variants:</strong></p>
                            <table style="font-size: 0.85em; width: 100%;">
                                <tr>
                                    <td><strong>BERT-Base:</strong></td>
                                    <td>12 layers, 768 hidden, 12 heads, 110M parameters</td>
                                </tr>
                                <tr>
                                    <td><strong>BERT-Large:</strong></td>
                                    <td>24 layers, 1024 hidden, 16 heads, 340M parameters</td>
                                </tr>
                            </table>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Final loss:</strong> Linear combination of MLM and NSP losses</p>
                            <div class="equation-highlight" style="margin-top: 10px;">
                                $$\mathcal{L} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP}$$
                            </div>
                        </div>
                    </div>
                </section>

                <!-- MCQ: BERT Pretraining -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "In BERTs masked language modeling, why do we replace selected tokens with [MASK] only 80% of the time?",
                        "type": "single",
                        "options": [
                            {
                                "text": "To make training faster",
                                "correct": false,
                                "explanation": "The mixing strategy does not affect training speed."
                            },
                            {
                                "text": "To prevent overfitting to the training data",
                                "correct": false,
                                "explanation": "While this adds some regularization, it is not the primary reason."
                            },
                            {
                                "text": "To reduce the mismatch between pretraining and fine-tuning",
                                "correct": true,
                                "explanation": "Correct! During fine-tuning, the model never sees [MASK] tokens. By sometimes keeping the original token or using a random token, we reduce this mismatch and make the model less biased toward masked positions."
                            },
                            {
                                "text": "To handle out-of-vocabulary words",
                                "correct": false,
                                "explanation": "The 80/10/10 strategy is not related to OOV handling."
                            }
                        ]
                    }'></div>
                </section>

            </section>

            <!-- nanochat: Putting It All Together -->
            <section>

                <!-- nanochat Introduction -->
                <section data-sources='[{"text": "nanochat GitHub Repository", "url": "https://github.com/karpathy/nanochat"}]'>
                    <h2 class="truncate-title">nanochat: The Best ChatGPT That $100 Can Buy</h2>
                    <div style="text-align: left; font-size: 0.7em;">
                        <div style="display: flex; gap: 20px; align-items: center; margin-top: 20px;">
                            <div style="flex: 1;">
                                <img src="https://github.com/karpathy/nanochat/raw/master/dev/nanochat.png" alt="nanochat logo" style="width: 100%; max-width: 300px; border-radius: 10px;">
                            </div>
                            <div style="flex: 1;">
                                <p><strong>By Andrej Karpathy</strong></p>
                                <p style="margin-top: 15px; font-size: 0.9em;">A full-stack LLM implementation in a single, clean, minimal, hackable codebase</p>
                                <div style="margin-top: 20px; background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                    <p style="font-size: 0.9em;"><strong>Complete pipeline includes:</strong></p>
                                    <ul style="font-size: 0.85em; margin-top: 8px;">
                                        <li>Tokenization (BPE)</li>
                                        <li>Pretraining</li>
                                        <li>Finetuning</li>
                                        <li>Evaluation</li>
                                        <li>Inference & Web UI</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Training time:</strong> ~4 hours on single 8×H100 node (~$100)</p>
                        </div>
                    </div>
                </section>

                <!-- Training Scale -->
                <section>
                    <h2 class="truncate-title">Training Tiers and Scaling</h2>
                    <div style="font-size: 0.7em;">
                        <p>nanochat demonstrates how models scale with compute:</p>
                        <table class="comparison-table" style="margin-top: 20px;">
                            <thead>
                                <tr>
                                    <th>Tier</th>
                                    <th>Cost</th>
                                    <th>Time</th>
                                    <th>Compute (FLOPs)</th>
                                    <th>Performance</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr style="background: #E8F4F8;">
                                    <td><strong>Speedrun</strong></td>
                                    <td>~$100</td>
                                    <td>4 hours</td>
                                    <td>4×10¹⁹</td>
                                    <td>Basic (kindergartener)</td>
                                </tr>
                                <tr>
                                    <td><strong>Medium (d26)</strong></td>
                                    <td>~$300</td>
                                    <td>12 hours</td>
                                    <td>~1.2×10²⁰</td>
                                    <td>GPT-2 level</td>
                                </tr>
                                <tr>
                                    <td><strong>Large</strong></td>
                                    <td>~$1000</td>
                                    <td>41.6 hours</td>
                                    <td>~4×10²⁰</td>
                                    <td>Stronger performance</td>
                                </tr>
                            </tbody>
                        </table>
                        <div style="margin-top: 25px;">
                            <div class="emphasis-box">
                                <p><strong>Scaling Law:</strong> Model performance improves with compute</p>
                                <p style="font-size: 0.9em; margin-top: 8px;">
                                    <span class="tooltip">FLOPs<span class="tooltiptext">Floating Point Operations: A measure of computational work. Modern LLMs require 10²³-10²⁵ FLOPs for training</span></span>
                                    = 6 × Parameters × Tokens
                                </p>
                            </div>
                        </div>
                        <div style="margin-top: 20px; font-size: 0.85em; color: #666; text-align: left;">
                            <p><strong>Hardware:</strong> Single 8×H100 node at ~$24/hour</p>
                            <p><strong>Note:</strong> Can also run on 8×A100 (slower) or even single GPU (8× longer)</p>
                        </div>
                    </div>
                </section>

                <!-- From Theory to Practice -->
                <section>
                    <h2 class="truncate-title">From Theory to Practice</h2>
                    <div style="font-size: 0.5em;">
                        <p>How lecture concepts map to nanochat implementation:</p>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 25px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 12px;">✓ Subword Embeddings</h4>
                                <p style="font-size: 0.85em;"><strong>Lecture:</strong> BPE algorithm</p>
                                <p style="font-size: 0.85em; margin-top: 5px;"><strong>nanochat:</strong> Custom Rust BPE tokenizer</p>
                                <div style="margin-top: 10px; padding: 8px; background: white; border-radius: 3px; font-family: monospace; font-size: 0.75em;">
                                    rustbpe/src/lib.rs
                                </div>
                            </div>

                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 12px;">✓ Self-Supervised Learning</h4>
                                <p style="font-size: 0.85em;"><strong>Lecture:</strong> Next token prediction</p>
                                <p style="font-size: 0.85em; margin-top: 5px;"><strong>nanochat:</strong> Autoregressive language modeling</p>
                                <div style="margin-top: 10px; padding: 8px; background: white; border-radius: 3px; font-family: monospace; font-size: 0.75em;">
                                    scripts/base_train.py
                                </div>
                            </div>

                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 12px;">✓ Transformer Architecture</h4>
                                <p style="font-size: 0.85em;"><strong>Lecture:</strong> Self-attention mechanism</p>
                                <p style="font-size: 0.85em; margin-top: 5px;"><strong>nanochat:</strong> Transformer decoder</p>
                                <div style="margin-top: 10px; padding: 8px; background: white; border-radius: 3px; font-family: monospace; font-size: 0.75em;">
                                    nanochat/model.py
                                </div>
                            </div>

                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 12px;">✓ Large-Scale Training</h4>
                                <p style="font-size: 0.85em;"><strong>Lecture:</strong> Massive text corpora</p>
                                <p style="font-size: 0.85em; margin-top: 5px;"><strong>nanochat:</strong> FineWeb dataset</p>
                                <div style="margin-top: 10px; padding: 8px; background: white; border-radius: 3px; font-family: monospace; font-size: 0.75em;">
                                    nanochat/dataset.py
                                </div>
                            </div>
                        </div>
                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Codebase:</strong> ~8,300 lines across 44 files - minimal and hackable!</p>
                        </div>
                    </div>
                </section>

                <!-- Why nanochat Matters -->
                <section>
                    <h2 class="truncate-title">Why nanochat Matters for Learning</h2>
                    <div style="text-align: left; margin-top: 25px; font-size: 0.5em;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                            <div style="background: #E8F4F8; padding: 20px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 15px;">🎓 Educational Value</h4>
                                <ul style="font-size: 0.9em;">
                                    <li>Clean, readable codebase</li>
                                    <li>Single cohesive implementation</li>
                                    <li>No complex frameworks or abstractions</li>
                                    <li>Easy to fork and experiment</li>
                                </ul>
                            </div>

                            <div style="background: #E8F4F8; padding: 20px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 15px;">💰 Accessibility</h4>
                                <ul style="font-size: 0.9em;">
                                    <li>Affordable compute budget</li>
                                    <li>Complete pipeline in hours</li>
                                    <li>End-to-end visibility</li>
                                    <li>Real ChatGPT-like interface</li>
                                </ul>
                            </div>
                        </div>

                        <div style="margin-top: 25px; background: #FFF9E6; padding: 20px; border-radius: 5px; border-left: 4px solid #FAC55B;">
                            <p style="font-size: 0.95em;"><strong>Philosophy:</strong> Not an exhaustively configurable framework</p>
                            <p style="font-size: 0.85em; margin-top: 10px;">
                                nanochat prioritizes <strong>cognitive simplicity</strong> over configurability.
                                It's a strong baseline designed to be understood and modified.
                            </p>
                        </div>

                        <div class="emphasis-box" style="margin-top: 20px;">
                            <p><strong>Part of LLM101n:</strong> Capstone project for Eureka Labs course</p>
                            <p style="font-size: 0.9em; margin-top: 8px;">
                                Brings together all concepts: tokenization, pretraining, fine-tuning, evaluation
                            </p>
                        </div>

                        <div style="margin-top: 20px; text-align: center;">
                            <p style="font-size: 0.9em; color: #666;">
                                <strong>Repository:</strong> <code style="background: #f5f5f5; padding: 3px 8px; border-radius: 3px;">github.com/karpathy/nanochat</code>
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Interactive Demo Info -->
                <section>
                    <h2 class="truncate-title">Running nanochat</h2>
                    <div style="text-align: left; font-size: 0.5em;">
                        <p style="margin-bottom: 20px;">Quick start on an 8×H100 node:</p>
                        <div style="background: #262626; color: #f5f5f5; padding: 20px; border-radius: 5px; font-family: 'Source Code Pro', monospace; font-size: 0.8em; margin-bottom: 20px;">
                            <code>
                                # Clone and run the speedrun script<br>
                                git clone https://github.com/karpathy/nanochat.git<br>
                                cd nanochat<br>
                                bash speedrun.sh<br>
                                <br>
                                # Wait ~4 hours...<br>
                                <br>
                                # Then serve your LLM<br>
                                python -m scripts.chat_web
                            </code>
                        </div>

                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-top: 20px;">
                            <div style="background: #E8F4F8; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #10099F; margin-bottom: 10px;">What speedrun.sh does:</h4>
                                <ol style="font-size: 0.85em;">
                                    <li>Downloads training data</li>
                                    <li>Trains base model</li>
                                    <li>Performs midtraining</li>
                                    <li>Supervised finetuning</li>
                                    <li>Runs evaluations</li>
                                    <li>Generates report card</li>
                                </ol>
                            </div>

                            <div style="background: #FFF3CD; padding: 15px; border-radius: 5px;">
                                <h4 style="color: #FAC55B; margin-bottom: 10px;">Customization:</h4>
                                <ul style="font-size: 0.85em;">
                                    <li>Adjust <code>--depth</code> for model size</li>
                                    <li>Tune <code>--device_batch_size</code> for VRAM</li>
                                    <li>Download more data shards</li>
                                    <li>Modify hyperparameters</li>
                                </ul>
                            </div>
                        </div>

                        <div style="margin-top: 20px; padding: 15px; background: #f9f9f9; border-radius: 5px; border: 2px solid #10099F;">
                            <p style="font-size: 0.9em;">
                                <strong>💡 Tip:</strong> Works on single GPU too (omit <code>torchrun</code>) - just takes 8× longer
                            </p>
                        </div>
                    </div>
                </section>

                <!-- MCQ: nanochat -->
                <section>
                    <h2 class="truncate-title">Test Your Understanding</h2>
                    <div data-mcq='{
                        "question": "What makes nanochat particularly valuable as a learning resource compared to production LLM frameworks?",
                        "type": "single",
                        "options": [
                            {
                                "text": "It achieves better performance than GPT-4",
                                "correct": false,
                                "explanation": "nanochat is a small model designed for learning, not production-level performance."
                            },
                            {
                                "text": "It provides a minimal, hackable codebase covering the complete LLM pipeline",
                                "correct": true,
                                "explanation": "Correct! nanochat prioritizes cognitive simplicity and educational value by providing a clean, end-to-end implementation in ~8K lines of code that students can understand and modify."
                            },
                            {
                                "text": "It requires no computational resources to train",
                                "correct": false,
                                "explanation": "nanochat still requires significant compute (8×H100 node for ~4 hours), though this is accessible compared to production models."
                            },
                            {
                                "text": "It uses a completely different architecture than transformers",
                                "correct": false,
                                "explanation": "nanochat uses standard transformer architecture, implementing the same concepts covered in this lecture."
                            }
                        ]
                    }'></div>
                </section>

            </section>

        </div>
    </div>

    <!-- Reveal.js and plugins -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.js"></script>

    <!-- D3.js for visualizations -->
    <script src="https://d3js.org/d3.v7.min.js"></script>

    <!-- Shared utilities -->
    <script src="../shared/js/title-handler.js"></script>
    <script src="../shared/js/tooltip-modal.js"></script>
    <script src="../shared/js/source-modal-v2.js"></script>
    <script src="../shared/js/multiple-choice.js"></script>

    <!-- Lecture-specific JavaScript -->
    <script src="js/visualizations.js"></script>

    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            math: {
                mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
                config: 'TeX-AMS_SVG-full'
            },
            plugins: [ RevealMath, RevealHighlight, RevealNotes ]
        });
    </script>
</body>
</html>
